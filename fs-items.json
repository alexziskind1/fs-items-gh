[
  {
    "id": 1003999469,
    "title": "Update legacy Python image for CI tests in Linux",
    "dateCreated": "2021-09-22T08:25:27Z",
    "dateModified": "2021-09-22T08:25:27Z",
    "description": "Instead of legacy, use next-generation convenience images, built from the ground up with CI, efficiency, and determinism in mind. Here are some of the highlights:\r\n\r\n- Faster spin-up time - In Docker terminology, these next-gen images will generally have fewer and smaller layers. Using these new images will lead to faster image downloads when a build starts, and a higher likelihood that the image is already cached on the host.\r\n\r\n- Improved reliability and stability - The existing legacy convenience images are rebuilt practically every day with potential changes from upstream that we cannot always test fast enough. This leads to frequent breaking changes, which is not the best environment for stable, deterministic builds. Next-gen images will only be rebuilt for security and critical-bugs, leading to more stable and deterministic images.\r\n\r\nMore info: https://circleci.com/docs/2.0/circleci-images",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 1003904803,
    "title": "Run tests in parallel",
    "dateCreated": "2021-09-22T07:00:44Z",
    "dateModified": "2021-09-22T07:00:44Z",
    "description": "Run CI tests in parallel to speed up the test suite.",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 1002704096,
    "title": "Fix missing conda deps",
    "dateCreated": "2021-09-21T15:23:01Z",
    "dateModified": "2021-09-21T15:23:01Z",
    "description": "`aiohttp` was added as a dependency in #2662 but was missing for the conda build, which causes the 1.12.0 and 1.12.1 to fail.\r\n\r\nFix #2932.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 1001267888,
    "title": "Dummy labels no longer on by default in `to_tf_dataset`",
    "dateCreated": "2021-09-20T18:26:59Z",
    "dateModified": "2021-09-20T18:26:59Z",
    "description": "After more experimentation, I think I have a way to do things that doesn't depend on adding `dummy_labels` - they were quite a hacky solution anyway!",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 1001085353,
    "title": "Fix fn kwargs in filter",
    "dateCreated": "2021-09-20T15:10:26Z",
    "dateModified": "2021-09-20T15:10:26Z",
    "description": "#2836 broke the `fn_kwargs` parameter of `filter`, as mentioned in https://github.com/huggingface/datasets/issues/2927\r\n\r\nI fixed that and added a test to make sure it doesn't happen again (for either map or filter)\r\n\r\nFix #2927",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 1001026680,
    "title": "Introduce web and wiki config in triviaqa dataset",
    "dateCreated": "2021-09-20T14:17:23Z",
    "dateModified": "2021-09-20T14:17:23Z",
    "description": "The TriviaQA paper suggests that the two subsets (Wikipedia and Web)\r\nshould be treated differently. There are also different leaderboards\r\nfor the two sets on CodaLab. For that reason, introduce additional\r\nbuilder configs in the trivia_qa dataset.",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 1000844077,
    "title": "Fix minor URL format in scitldr dataset",
    "dateCreated": "2021-09-20T11:11:32Z",
    "dateModified": "2021-09-20T11:11:32Z",
    "description": "While investigating issue #2918, I found this minor format issues in the URLs (if runned in a Windows machine).",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 1000798338,
    "title": "Don't use old, incompatible cache for the new `filter`",
    "dateCreated": "2021-09-20T10:18:59Z",
    "dateModified": "2021-09-20T10:18:59Z",
    "description": "#2836 changed `Dataset.filter` and the resulting data that are stored in the cache are different and incompatible with the ones of the previous `filter` implementation.\r\n\r\nHowever the caching mechanism wasn't able to differentiate between the old and the new implementation of filter (only the method name was taken into account). \r\n\r\nThis is an issue because anyone that update `datasets` and re-runs some code that uses `filter` would see an error, because the cache would try to load an incompatible `filter` result.\r\n\r\nTo fix this I added the notion of versioning for dataset transform in the caching mechanism, and bumped the version of the `filter` implementation to 2.0.0\r\n\r\nThis way the new `filter` outputs are now considered different from the old ones from the caching point of view.\r\n\r\nThis should fix #2943\r\n\r\ncc @anton-l",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 1000754824,
    "title": "Update meteor score from nltk update",
    "dateCreated": "2021-09-20T09:28:46Z",
    "dateModified": "2021-09-20T09:28:46Z",
    "description": "It looks like there were issues in NLTK on the way the METEOR score was computed.\r\nA fix was added in NLTK at https://github.com/nltk/nltk/pull/2763, and therefore the scoring function no longer returns the same values.\r\n\r\nI updated the score of the example in the docs",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 1000624883,
    "title": "Protect master branch",
    "dateCreated": "2021-09-20T06:47:01Z",
    "dateModified": "2021-09-20T06:47:01Z",
    "description": "After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\r\n- 00cc036fea7c7745cfe722360036ed306796a3f2\r\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\r\n- ...\r\n\r\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\r\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\r\n  - Currently, simple merge commits are already disabled\r\n  - I propose to disable rebase merging as well\r\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\r\n  - ~~This protection would reject direct pushes to master branch~~\r\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\r\n- [x] Protect the master branch only from direct pushing of **merge commits**\r\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\r\n  - No need to disable/re-enable this protection on each release \r\n\r\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 1000544370,
    "title": "Add  `remove_columns` to `IterableDataset ` ",
    "dateCreated": "2021-09-20T04:01:00Z",
    "dateModified": "2021-09-20T04:01:00Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is.\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"c4\", 'realnewslike', streaming =True, split='train')\r\ndataset = dataset.remove_columns('url')\r\n```\r\n```\r\nAttributeError: 'IterableDataset' object has no attribute 'remove_columns'\r\n```\r\n\r\n**Describe the solution you'd like**\r\n\r\nIt would be nice to have `.remove_columns()` to match the `Datasets` api. \r\n\r\n\r\n**Describe alternatives you've considered**\r\n\r\nThis can be done with a single call to `.map()`, \r\n\r\nI can try to help add this. \ud83e\udd17",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 1000355115,
    "title": "Backwards compatibility broken for cached datasets that use `.filter()`",
    "dateCreated": "2021-09-19T16:16:37Z",
    "dateModified": "2021-09-19T16:16:37Z",
    "description": "## Describe the bug\r\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \r\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\r\n\r\nRelated feature: https://github.com/huggingface/datasets/pull/2836\r\n\r\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \r\n\r\n## Workaround\r\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\r\n\r\n## Steps to reproduce the bug\r\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\r\n\r\n2. `pip install datasets==1.11.0` and run the following snippet:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nids = [\"1272-141231-0000\"]\r\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\nds = ds.filter(lambda x: x[\"id\"] in ids)\r\n```\r\n3. `pip install datasets==1.12.1` and re-run the code again\r\n\r\n## Expected results\r\nSame result as with the previous `datasets` version.\r\n\r\n## Actual results\r\n```bash\r\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\r\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\r\nTraceback (most recent call last):\r\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\r\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\r\n    indices = self.map(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\r\n    return self._map_single(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\r\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\r\n    return cls(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\r\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\r\n    return Features(recursive_reorder(self, other))\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\r\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\r\nValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 1000309765,
    "title": "Add SEDE dataset",
    "dateCreated": "2021-09-19T13:11:24Z",
    "dateModified": "2021-09-19T13:11:24Z",
    "description": "This PR adds the SEDE dataset for the task of realistic Text-to-SQL, following the instructions of how to add a database and a dataset card.\r\n\r\nPlease see our paper for more details: https://arxiv.org/abs/2106.05006",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 1000000711,
    "title": "OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError",
    "dateCreated": "2021-09-18T10:39:13Z",
    "dateModified": "2021-09-18T10:39:13Z",
    "description": "## Describe the bug\r\n\r\nCannot download OSCAR `unshuffled_original_ko` due to `NonMatchingSplitsSizesError`.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n>>> dataset = datasets.load_dataset('oscar', 'unshuffled_original_ko')\r\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=25292102197, num_examples=7345075, dataset_name='oscar'), 'recorded': SplitInfo(name='train', num_bytes=25284578514, num_examples=7344907, dataset_name='oscar')}]\r\n```\r\n\r\n## Expected results\r\n\r\nLoading is successful.\r\n\r\n## Actual results\r\n\r\nLoading throws above error.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.4.0-81-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 999680796,
    "title": "add swedish_medical_ner dataset",
    "dateCreated": "2021-09-17T20:03:05Z",
    "dateModified": "2021-09-17T20:03:05Z",
    "description": "Adding the Swedish Medical NER dataset, listed in \"Biomedical Datasets - BigScience Workshop 2021\"",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 999639630,
    "title": "MENYO-20k repo has moved, updating URL",
    "dateCreated": "2021-09-17T19:01:54Z",
    "dateModified": "2021-09-17T19:01:54Z",
    "description": "Dataset repo moved to https://github.com/uds-lsv/menyo-20k_MT, now editing URL to match.\r\n\r\nhttps://github.com/uds-lsv/menyo-20k_MT/blob/master/data/train.tsv is the file we're looking for",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 999552263,
    "title": "Take namespace into account in caching",
    "dateCreated": "2021-09-17T16:57:33Z",
    "dateModified": "2021-09-17T16:57:33Z",
    "description": "Loading a dataset \"username/dataset_name\" hosted by a user on the hub used to cache the dataset only taking into account the dataset name, and ignorign the username. Because of this, if a user later loads \"dataset_name\" without specifying the username, it would reload the dataset from the cache instead of failing.\r\n\r\nI changed the dataset cache and module cache mechanism to include the username in the name of the cache directory that is used:\r\n<s>\r\n`~/.cache/huggingface/datasets/username/dataset_name`  for the data\r\n`~/.cache/huggingface/modules/datasets_modules/datasets/username/dataset_name` for the python files\r\n</s>\r\nEDIT: actually using three underscores:\r\n`~/.cache/huggingface/datasets/username___dataset_name` for the data\r\n`~/.cache/huggingface/modules/datasets_modules/datasets/username___dataset_name` for the python files\r\n\r\nThis PR should fix the issue https://github.com/huggingface/datasets/issues/2842\r\n\r\ncc @stas00 ",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 999548277,
    "title": "load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied",
    "dateCreated": "2021-09-17T16:52:10Z",
    "dateModified": "2021-09-17T16:52:10Z",
    "description": "## Describe the bug\r\nStandard process to download and load the wiki_bio dataset causes PermissionError in Windows 10 and 11.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset('wiki_bio')\r\n```\r\n\r\n## Expected results\r\nIt is expected that the dataset downloads without any errors.\r\n\r\n## Actual results\r\nPermissionError see trace below:\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\builder.py\", line 644, in download_and_prepare\r\n    self._save_info()\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\contextlib.py\", line 120, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\builder.py\", line 598, in incomplete_dir\r\n    os.rename(tmp_dir, dirname)\r\nPermissionError: [WinError 5] Access is denied: 'C:\\\\Users\\\\username\\\\.cache\\\\huggingface\\\\datasets\\\\wiki_bio\\\\default\\\\1.1.0\\\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9.incomplete' -> 'C:\\\\Users\\\\username\\\\.cache\\\\huggingface\\\\datasets\\\\wiki_bio\\\\default\\\\1.1.0\\\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9'\r\n```\r\nBy commenting out the os.rename() [L604](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L604) and the shutil.rmtree() [L607](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L607) lines, in my virtual environment, I was able to get the load process to complete, rename the directory manually and then rerun the `load_dataset('wiki_bio')` to get what I needed.\r\n\r\nIt seems that os.rename() in the `incomplete_dir` content manager is the culprit. Here's another project [Conan](https://github.com/conan-io/conan/issues/6560) with similar issue with os.rename() if it helps debug this issue.\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Windows-10-10.0.22449-SP0\r\n- Python version: 3.8.12\r\n- PyArrow version: 5.0.0\r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 999521647,
    "title": "Check that array is not Float as nan != nan",
    "dateCreated": "2021-09-17T16:16:41Z",
    "dateModified": "2021-09-17T16:16:41Z",
    "description": "The Exception wants to check for issues with StructArrays/ListArrays but catches FloatArrays with value nan as nan != nan.\r\nPass on FloatArrays as we should not raise an Exception for them.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 999518469,
    "title": "Add Jigsaw unintended Bias",
    "dateCreated": "2021-09-17T16:12:31Z",
    "dateModified": "2021-09-17T16:12:31Z",
    "description": "Hi,\r\n\r\nHere's a first attempt at this dataset. Would be great if it could be merged relatively quickly as it is needed for Bigscience-related stuff. \r\n\r\nThis requires manual download, and I had some trouble generating dummy_data in this setting, so welcoming feedback there.",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 999477413,
    "title": "to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows",
    "dateCreated": "2021-09-17T15:26:53Z",
    "dateModified": "2021-09-17T15:26:53Z",
    "description": "To reproduce:\r\n```python\r\nimport datasets as ds\r\nimport weakref\r\nimport gc\r\n\r\nd = ds.load_dataset(\"mnist\", split=\"train\")\r\nref = weakref.ref(d._data.table)\r\ntfd = d.to_tf_dataset(\"image\", batch_size=1, shuffle=False, label_cols=\"label\")\r\ndel tfd, d\r\ngc.collect()\r\nassert ref() is None, \"Error: there is at least one reference left\"\r\n```\r\n\r\nThis causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.\r\n\r\nMoreover the CI test of the `to_tf_dataset` method isn't able to clean up the temporary arrow files because of this.\r\n\r\ncc @Rocketknight1 ",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 999392566,
    "title": "Replace script_version with revision",
    "dateCreated": "2021-09-17T14:04:39Z",
    "dateModified": "2021-09-17T14:04:39Z",
    "description": "As discussed in https://github.com/huggingface/datasets/pull/2718#discussion_r707013278, the parameter name `script_version` is no longer applicable to datasets without loading script (i.e., datasets only with raw data files).\r\n\r\nThis PR replaces the parameter name `script_version` with `revision`.\r\n\r\nThis way, we are also aligned with:\r\n- Transformers: `AutoTokenizer.from_pretrained(..., revision=...)`\r\n- Hub: `HfApi.dataset_info(..., revision=...)`, `HfApi.upload_file(..., revision=...)`",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 999317750,
    "title": "Conda build fails",
    "dateCreated": "2021-09-17T12:49:22Z",
    "dateModified": "2021-09-17T12:49:22Z",
    "description": "## Describe the bug\r\nCurrent `datasets` version in conda is 1.9 instead of 1.12.\r\n\r\nThe build of the conda package fails.\r\n",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 998326359,
    "title": "Fix bug in to_tf_dataset",
    "dateCreated": "2021-09-16T15:08:03Z",
    "dateModified": "2021-09-16T15:08:03Z",
    "description": "Replace `set_format()` to `with_format()` so that we don't alter the original dataset in `to_tf_dataset()`",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 998154311,
    "title": "Mutable columns argument breaks set_format",
    "dateCreated": "2021-09-16T12:27:22Z",
    "dateModified": "2021-09-16T12:27:22Z",
    "description": "## Describe the bug\r\nIf you pass a mutable list to the `columns` argument of `set_format` and then change the list afterwards, the returned columns also change.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"glue\", \"cola\")\r\n\r\ncolumn_list = [\"idx\", \"label\"]\r\ndataset.set_format(\"python\", columns=column_list)\r\ncolumn_list[1] = \"foo\"  # Change the list after we call `set_format`\r\ndataset['train'][:4].keys()\r\n```\r\n\r\n## Expected results\r\n```python\r\ndict_keys(['idx', 'label'])\r\n```\r\n\r\n## Actual results\r\n```python\r\ndict_keys(['idx'])\r\n```",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 997960024,
    "title": "Add regression test for null Sequence",
    "dateCreated": "2021-09-16T08:58:33Z",
    "dateModified": "2021-09-16T08:58:33Z",
    "description": "Relates to #2892 and #2900.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 997941506,
    "title": "Update BibTeX entry",
    "dateCreated": "2021-09-16T08:39:20Z",
    "dateModified": "2021-09-16T08:39:20Z",
    "description": "Update BibTeX entry.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 997654680,
    "title": "Datasets 1.12 dataset.filter TypeError: get_indices_from_mask_function() got an unexpected keyword argument",
    "dateCreated": "2021-09-16T01:14:02Z",
    "dateModified": "2021-09-16T01:14:02Z",
    "description": "## Describe the bug\r\nUpgrading to 1.12 caused `dataset.filter` call to fail with \r\n\r\n> get_indices_from_mask_function() got an unexpected keyword argument valid_rel_labels\r\n\r\n\r\n## Steps to reproduce the bug\r\n```pythondef \r\n\r\nfilter_good_rows(\r\n    ex: Dict,\r\n    valid_rel_labels: Set[str],\r\n    valid_ner_labels: Set[str],\r\n    tokenizer: PreTrainedTokenizerFast,\r\n) -> bool:\r\n    \"\"\"Get the good rows\"\"\"\r\n    encoding = get_encoding_for_text(text=ex[\"text\"], tokenizer=tokenizer)\r\n    ex[\"encoding\"] = encoding\r\n    for relation in ex[\"relations\"]:\r\n        if not is_valid_relation(relation, valid_rel_labels):\r\n            return False\r\n    for span in ex[\"spans\"]:\r\n        if not is_valid_span(span, valid_ner_labels, encoding):\r\n            return False\r\n    return True\r\n    \r\ndef get_dataset():    \r\n    loader_path = str(Path(__file__).parent / \"prodigy_dataset_builder.py\")\r\n    ds = load_dataset(\r\n        loader_path,\r\n        name=\"prodigy-dataset\",\r\n        data_files=sorted(file_paths),\r\n        cache_dir=cache_dir,\r\n    )[\"train\"]\r\n\r\n    valid_ner_labels = set(vocab.ner_category)\r\n    valid_relations = set(vocab.relation_types.keys())\r\n    ds = ds.filter(\r\n        filter_good_rows,\r\n        fn_kwargs=dict(\r\n            valid_rel_labels=valid_relations,\r\n            valid_ner_labels=valid_ner_labels,\r\n            tokenizer=vocab.tokenizer,\r\n        ),\r\n        keep_in_memory=True,\r\n        num_proc=num_proc,\r\n    )\r\n\r\n```\r\n\r\n`ds` is a `DatasetDict` produced by a jsonl dataset.\r\nThis runs fine on 1.11 but fails on 1.12\r\n\r\n**Stack Trace**\r\n\r\n\r\n\r\n## Expected results\r\n\r\nI expect 1.12 datasets filter to filter the dataset without raising as it does on 1.11\r\n\r\n## Actual results\r\n```\r\ntf_ner_rel_lib/dataset.py:695: in load_prodigy_arrow_datasets_from_jsonl\r\n    ds = ds.filter(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper\r\n    out = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2169: in filter\r\n    indices = self.map(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1686: in map\r\n    return self._map_single(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper\r\n    out = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2048: in _map_single\r\n    batch = apply_function_on_filtered_inputs(\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\ninputs = {'_input_hash': [2108817714, 1477695082, -1021597032, 2130671338, -1260483858, -1203431639, ...], '_task_hash': [18070...ons', 'relations', 'relations', ...], 'answer': ['accept', 'accept', 'accept', 'accept', 'accept', 'accept', ...], ...}\r\nindices = [0, 1, 2, 3, 4, 5, ...], check_same_num_examples = False, offset = 0\r\n\r\n    def apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples=False, offset=0):\r\n        \"\"\"Utility to apply the function on a selection of columns.\"\"\"\r\n        nonlocal update_data\r\n        fn_args = [inputs] if input_columns is None else [inputs[col] for col in input_columns]\r\n        if offset == 0:\r\n            effective_indices = indices\r\n        else:\r\n            effective_indices = [i + offset for i in indices] if isinstance(indices, list) else indices + offset\r\n        processed_inputs = (\r\n>           function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n        )\r\nE       TypeError: get_indices_from_mask_function() got an unexpected keyword argument 'valid_rel_labels'\r\n\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1939: TypeError\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: Mac\r\n- Python version: 3.8.9\r\n- PyArrow version: pyarrow==5.0.0\r\n\r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 997463277,
    "title": "Error when downloading datasets to non-traditional cache directories",
    "dateCreated": "2021-09-15T19:59:46Z",
    "dateModified": "2021-09-15T19:59:46Z",
    "description": "## Describe the bug\r\nWhen the cache directory is linked (soft link) to a directory on a NetApp device, the download fails. \r\n\r\n## Steps to reproduce the bug\r\n```bash\r\nln -s /path/to/netapp/.cache ~/.cache\r\n```\r\n\r\n```python\r\nload_dataset(\"imdb\")\r\n```\r\n\r\n## Expected results\r\nSuccessfully loading IMDB dataset\r\n\r\n## Actual results\r\n```\r\ndatasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=33432835, \r\nnum_examples=25000, dataset_name='imdb'), 'recorded': SplitInfo(name='train', num_bytes=0, num_examples=0,\r\n dataset_name='imdb')}, {'expected': SplitInfo(name='test', num_bytes=32650697, num_examples=25000, dataset_name='imdb'),\r\n 'recorded': SplitInfo(name='test', num_bytes=659932, num_examples=503, dataset_name='imdb')}, {'expected':\r\n SplitInfo(name='unsupervised', num_bytes=67106814, num_examples=50000, dataset_name='imdb'), 'recorded':\r\n SplitInfo(name='unsupervised', num_bytes=0, num_examples=0, dataset_name='imdb')}]\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.1.2\r\n- Platform: Ubuntu \r\n- Python version: 3.8\r\n\r\n## Extra notes\r\nStranger yet, trying to debug the phenomenon, I found the range of results to vary a lot without clear direction:\r\n - With `cache_dir=\"/path/to/netapp/.cache\"` the same thing happens.\r\n - However, when linking `~/netapp/` to `/path/to/netapp` *and* setting `cache_dir=\"~/netapp/.cache/huggingface/datasets\"` - it does work\r\n - On the other hand, when linking `~/.cache` to `~/netapp/.cache` without using `cache_dir`, it does work anymore.\r\n\r\nWhile I could test it only for a NetApp device, it might have to do with any other mounted FS.\r\n\r\nThanks :)\r\n\r\n",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 997407034,
    "title": "Add tutorial for no-code dataset upload",
    "dateCreated": "2021-09-15T18:54:42Z",
    "dateModified": "2021-09-15T18:54:42Z",
    "description": "This PR is for a tutorial for uploading a dataset to the Hub. It relies on the Hub UI elements to upload a dataset, introduces the online tagging tool for creating tags, and the Dataset card template to get a head start on filling it out. The addition of this tutorial should make it easier for beginners to upload a dataset without accessing the terminal or knowing Git.",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 997378113,
    "title": "\"File name too long\" error for file locks",
    "dateCreated": "2021-09-15T18:16:50Z",
    "dateModified": "2021-09-15T18:16:50Z",
    "description": "## Describe the bug\r\n\r\nGetting the following error when calling `load_dataset(\"gar1t/test\")`:\r\n\r\n```\r\nOSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'\r\n```\r\n\r\n## Steps to reproduce the bug\r\n\r\nWhere the user cache dir (e.g. `~/.cache`) is on a file system that limits filenames to 255 chars (e.g. ext4):\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"gar1t/test\")\r\n```\r\n\r\n## Expected results\r\n\r\nExpect the function to return without an error.\r\n\r\n## Actual results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/builder.py\", line 644, in download_and_prepare\r\n    self._save_info()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/builder.py\", line 765, in _save_info\r\n    with FileLock(lock_path):\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 323, in __enter__\r\n    self.acquire()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 272, in acquire\r\n    self._acquire()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 403, in _acquire\r\n    fd = os.open(self._lock_file, open_mode)\r\nOSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-27-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- PyArrow version: 5.0.0\r\n",
    "status": "open",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 997351590,
    "title": "Loading an autonlp dataset raises in normal mode but not in streaming mode",
    "dateCreated": "2021-09-15T17:44:38Z",
    "dateModified": "2021-09-15T17:44:38Z",
    "description": "## Describe the bug\r\n\r\nThe same dataset (from autonlp) raises an error in normal mode, but does not raise in streaming mode\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(\"severo/autonlp-data-sentiment_detection-3c8bcd36\", split=\"train\", streaming=False)\r\n## raises an error\r\n\r\nload_dataset(\"severo/autonlp-data-sentiment_detection-3c8bcd36\", split=\"train\", streaming=True)\r\n## does not raise an error\r\n```\r\n\r\n## Expected results\r\n\r\nBoth calls should raise the same error\r\n\r\n## Actual results\r\n\r\nCall with streaming=False:\r\n\r\n```\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 5825.42it/s]\r\nUsing custom data configuration autonlp-data-sentiment_detection-3c8bcd36-fe30267462d1d42b\r\nDownloading and preparing dataset json/autonlp-data-sentiment_detection-3c8bcd36 to /home/slesage/.cache/huggingface/datasets/json/autonlp-data-sentiment_detection-3c8bcd36-fe30267462d1d42b/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 15923.71it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 3346.88it/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 636, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 726, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 1187, in _prepare_split\r\n    writer.write_table(table)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 418, in write_table\r\n    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 418, in <listcomp>\r\n    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\r\n  File \"pyarrow/table.pxi\", line 1249, in pyarrow.lib.Table.__getitem__\r\n  File \"pyarrow/table.pxi\", line 1825, in pyarrow.lib.Table.column\r\n  File \"pyarrow/table.pxi\", line 1800, in pyarrow.lib.Table._ensure_integer_index\r\nKeyError: 'Field \"splits\" does not exist in table schema'\r\n```\r\n\r\nCall with `streaming=False`:\r\n\r\n```\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6000.43it/s]\r\nUsing custom data configuration autonlp-data-sentiment_detection-3c8bcd36-fe30267462d1d42b\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 46916.15it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 148734.18it/s]\r\n```\r\n\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.1.dev0\r\n- Platform: Linux-5.11.0-1017-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 997332662,
    "title": "Fix conversion of multidim arrays in list to arrow",
    "dateCreated": "2021-09-15T17:21:36Z",
    "dateModified": "2021-09-15T17:21:36Z",
    "description": "Arrow only supports 1-dim arrays. Previously we were converting all the numpy arrays to python list before instantiating arrow arrays to workaround this limitation.\r\nHowever in #2361 we started to keep numpy arrays in order to keep their dtypes.\r\nIt works when we pass any multi-dim numpy array (the conversion to arrow has been added on our side), but not for lists of multi-dim numpy arrays.\r\n\r\nIn this PR I added two strategies:\r\n- one that takes a list of multi-dim numpy arrays on returns an arrow array in an optimized way (more common case)\r\n- one that takes a list of possibly very nested data (lists, dicts, tuples) containing multi-dim arrays. This one is less optimized since it converts all the multi-dim numpy arrays into lists of 1-d arrays for compatibility with arrow. This strategy is simpler that just trying to create the arrow array from a possibly very nested data structure, but in the future we can improve it if needed.\r\n\r\nFix https://github.com/huggingface/datasets/issues/2921",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 997325424,
    "title": "Using a list of multi-dim numpy arrays raises an error \"can only convert 1-dimensional array values\"",
    "dateCreated": "2021-09-15T17:12:11Z",
    "dateModified": "2021-09-15T17:12:11Z",
    "description": "This error has been introduced in https://github.com/huggingface/datasets/pull/2361\r\n\r\nTo reproduce:\r\n```python\r\nimport numpy as np\r\nfrom datasets import Dataset\r\n\r\nd = Dataset.from_dict({\"a\": [np.zeros((2, 2))]})\r\n```\r\nraises\r\n```python\r\nTraceback (most recent call last):\r\n  File \"playground/ttest.py\", line 5, in <module>\r\n    d = Dataset.from_dict({\"a\": [np.zeros((2, 2))]}).with_format(\"torch\")\r\n  File \"/Users/quentinlhoest/Desktop/hf/nlp/src/datasets/arrow_dataset.py\", line 458, in from_dict\r\n    pa_table = InMemoryTable.from_pydict(mapping=mapping)\r\n  File \"/Users/quentinlhoest/Desktop/hf/nlp/src/datasets/table.py\", line 365, in from_pydict\r\n    return cls(pa.Table.from_pydict(*args, **kwargs))\r\n  File \"pyarrow/table.pxi\", line 1639, in pyarrow.lib.Table.from_pydict\r\n  File \"pyarrow/array.pxi\", line 332, in pyarrow.lib.asarray\r\n  File \"pyarrow/array.pxi\", line 223, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/Users/quentinlhoest/Desktop/hf/nlp/src/datasets/arrow_writer.py\", line 107, in __arrow_array__\r\n    out = pa.array(self.data, type=type)\r\n  File \"pyarrow/array.pxi\", line 306, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 39, in pyarrow.lib._sequence_to_array\r\n  File \"pyarrow/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Can only convert 1-dimensional array values",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 997323014,
    "title": "Fix unwanted tqdm bar when accessing examples",
    "dateCreated": "2021-09-15T17:09:11Z",
    "dateModified": "2021-09-15T17:09:11Z",
    "description": "A change in #2814 added bad progress bars in `map_nested`. Now they're disabled by default\r\n\r\nFix #2919 ",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 997127487,
    "title": "Unwanted progress bars when accessing examples",
    "dateCreated": "2021-09-15T14:05:10Z",
    "dateModified": "2021-09-15T14:05:10Z",
    "description": "When accessing examples from a dataset formatted for pytorch, some progress bars appear when accessing examples:\r\n```python\r\nIn [1]: import datasets as ds                                        \r\n\r\nIn [2]: d = ds.Dataset.from_dict({\"a\": [0, 1, 2]}).with_format(\"torch\")                                                           \r\n\r\nIn [3]: d[0]                                                         \r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 3172.70it/s]\r\nOut[3]: {'a': tensor(0)}\r\n```\r\n\r\nThis is because the pytorch formatter calls `map_nested` that uses progress bars\r\n\r\ncc @sgugger ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 997063347,
    "title": "`Can not decode content-encoding: gzip` when loading `scitldr` dataset with streaming",
    "dateCreated": "2021-09-15T13:06:07Z",
    "dateModified": "2021-09-15T13:06:07Z",
    "description": "## Describe the bug\r\n\r\nTrying to load the `\"FullText\"` config of the `\"scitldr\"` dataset with `streaming=True` raises an error from `aiohttp`:\r\n```python\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\ncc @lhoestq \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\niter_dset = iter(\r\n    load_dataset(\"scitldr\", name=\"FullText\", split=\"test\", streaming=True)\r\n)\r\n\r\nnext(iter_dset)\r\n```\r\n\r\n## Expected results\r\nReturns the first sample of the dataset\r\n\r\n## Actual results\r\nCalling `__next__` crashes with the following Traceback:\r\n\r\n```python\r\n----> 1 next(dset_iter)\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n    339\r\n    340     def __iter__(self):\r\n--> 341         for key, example in self._iter():\r\n    342             if self.features:\r\n    343                 # we encode the example for ClassLabel feature types for example\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in _iter(self)\r\n    336         else:\r\n    337             ex_iterable = self._ex_iterable\r\n--> 338         yield from ex_iterable\r\n    339\r\n    340     def __iter__(self):\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n     76\r\n     77     def __iter__(self):\r\n---> 78         for key, example in self.generate_examples_fn(**self.kwargs):\r\n     79             yield key, example\r\n     80\r\n\r\n~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\scitldr\\72d6e2195786c57e1d343066fb2cc4f93ea39c5e381e53e6ae7c44bbfd1f05ef\\scitldr.py in _generate_examples(self, filepath, split)\r\n    162\r\n    163         with open(filepath, encoding=\"utf-8\") as f:\r\n--> 164             for id_, row in enumerate(f):\r\n    165                 data = json.loads(row)\r\n    166                 if self.config.name == \"AIC\":\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\implementations\\http.py in read(self, length)\r\n    496         else:\r\n    497             length = min(self.size - self.loc, length)\r\n--> 498         return super().read(length)\r\n    499\r\n    500     async def async_fetch_all(self):\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\spec.py in read(self, length)\r\n   1481             # don't even bother calling fetch\r\n   1482             return b\"\"\r\n-> 1483         out = self.cache._fetch(self.loc, self.loc + length)\r\n   1484         self.loc += len(out)\r\n   1485         return out\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\caching.py in _fetch(self, start, end)\r\n    378         elif start < self.start:\r\n    379             if self.end - end > self.blocksize:\r\n--> 380                 self.cache = self.fetcher(start, bend)\r\n    381                 self.start = start\r\n    382             else:\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in wrapper(*args, **kwargs)\r\n     86     def wrapper(*args, **kwargs):\r\n     87         self = obj or args[0]\r\n---> 88         return sync(self.loop, func, *args, **kwargs)\r\n     89\r\n     90     return wrapper\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in sync(loop, func, timeout, *args, **kwargs)\r\n     67         raise FSTimeoutError\r\n     68     if isinstance(result[0], BaseException):\r\n---> 69         raise result[0]\r\n     70     return result[0]\r\n     71\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in _runner(event, coro, result, timeout)\r\n     23         coro = asyncio.wait_for(coro, timeout=timeout)\r\n     24     try:\r\n---> 25         result[0] = await coro\r\n     26     except Exception as ex:\r\n     27         result[0] = ex\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\implementations\\http.py in async_fetch_range(self, start, end)\r\n    538             if r.status == 206:\r\n    539                 # partial content, as expected\r\n--> 540                 out = await r.read()\r\n    541             elif \"Content-Length\" in r.headers:\r\n    542                 cl = int(r.headers[\"Content-Length\"])\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\aiohttp\\client_reqrep.py in read(self)\r\n   1030         if self._body is None:\r\n   1031             try:\r\n-> 1032                 self._body = await self.content.read()\r\n   1033                 for trace in self._traces:\r\n   1034                     await trace.send_response_chunk_received(\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\aiohttp\\streams.py in read(self, n)\r\n    342     async def read(self, n: int = -1) -> bytes:\r\n    343         if self._exception is not None:\r\n--> 344             raise self._exception\r\n    345\r\n    346         # migration problem; with DataQueue you have to catch\r\n\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.8.5\r\n- PyArrow version: 2.0.0\r\n- aiohttp version: 3.7.4.post0\r\n",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 997041658,
    "title": "windows download abnormal",
    "dateCreated": "2021-09-15T12:45:35Z",
    "dateModified": "2021-09-15T12:45:35Z",
    "description": "## Describe the bug\r\nThe script clearly exists (accessible from the browser), but the script download fails on windows. Then I tried it again and it can be downloaded normally on linux. why??\r\n## Steps to reproduce the bug\r\n```python3.7 + windows\r\n![image](https://user-images.githubusercontent.com/52347799/133436174-4303f847-55d5-434f-a749-08da3bb9b654.png)\r\n\r\n\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\nIt can be downloaded normally.\r\n\r\n## Actual results\r\nit cann't\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:1.11.0\r\n- Platform:windows\r\n- Python version:3.7\r\n- PyArrow version:\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 997003661,
    "title": "Add OpenAI's pass@k code evaluation metric",
    "dateCreated": "2021-09-15T12:05:43Z",
    "dateModified": "2021-09-15T12:05:43Z",
    "description": "This PR introduces the `code_eval` metric which implements [OpenAI's code evaluation harness](https://github.com/openai/human-eval) introduced in the [Codex paper](https://arxiv.org/abs/2107.03374). It is heavily based on the original implementation and just adapts the interface to follow the `predictions`/`references` convention.\r\n\r\nThe addition of this metric should enable the evaluation against the code evaluation datasets added in #2897 and #2893.\r\n\r\nA few open questions:\r\n\r\n- The implementation makes heavy use of multiprocessing which this PR does not touch. Is this conflicting with multiprocessing natively integrated in `datasets`?\r\n- This metric executes generated Python code and as such it poses dangers of executing malicious code. OpenAI addresses this issue by 1) commenting the `exec` call in the code so the user has to actively uncomment it and read the warning and 2) suggests using a sandbox environment (gVisor container). Should we add a similar safeguard? E.g. a prompt that needs to be answered when initialising the metric? Or at least a warning message?\r\n- Naming: the implementation sticks to the `predictions`/`references` naming, however, the references are not reference solutions but unittest to test the solution. While reference solutions are also available they are not used. Should the naming be adapted?",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 996870071,
    "title": "Fix fsspec AbstractFileSystem access",
    "dateCreated": "2021-09-15T09:39:20Z",
    "dateModified": "2021-09-15T09:39:20Z",
    "description": "This addresses the issue from #2914 by changing the way fsspec's AbstractFileSystem is accessed.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 996770168,
    "title": "Having a dependency defining fsspec entrypoint raises an AttributeError when importing datasets",
    "dateCreated": "2021-09-15T07:54:06Z",
    "dateModified": "2021-09-15T07:54:06Z",
    "description": "## Describe the bug\r\nIn one of my project, I defined a custom fsspec filesystem with an entrypoint.\r\nMy guess is that by doing so, a variable named `spec` is created in the module `fsspec` (created by entering a for loop as there are entrypoints defined, see the loop in question [here](https://github.com/intake/filesystem_spec/blob/0589358d8a029ed6b60d031018f52be2eb721291/fsspec/__init__.py#L55)).\r\nSo that `fsspec.spec`, that was previously referring to the `spec` submodule, is now referring to that `spec` variable.\r\nThis make the import of datasets failing as it is using that `fsspec.spec`.\r\n\r\n## Steps to reproduce the bug\r\nI could reproduce the bug with a dummy poetry project.\r\n\r\nHere is the pyproject.toml:\r\n```toml\r\n[tool.poetry]\r\nname = \"debug-datasets\"\r\nversion = \"0.1.0\"\r\ndescription = \"\"\r\nauthors = [\"Pierre Godard\"]\r\n\r\n[tool.poetry.dependencies]\r\npython = \"^3.8\"\r\ndatasets = \"^1.11.0\"\r\n\r\n[tool.poetry.dev-dependencies]\r\n\r\n[build-system]\r\nrequires = [\"poetry-core>=1.0.0\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n\r\n[tool.poetry.plugins.\"fsspec.specs\"]\r\n\"file2\" = \"fsspec.implementations.local.LocalFileSystem\"\r\n```\r\n\r\nThe only other file being a `debug_datasets/__init__.py` empty file.\r\n\r\nThe overall structure of the project is as follows:\r\n```\r\n.\r\n\u251c\u2500\u2500 pyproject.toml\r\n\u2514\u2500\u2500 debug_datasets\r\n    \u2514\u2500\u2500 __init__.py\r\n```\r\n\r\nThen, within the project folder run:\r\n\r\n```\r\npoetry install\r\npoetry run python\r\n```\r\n\r\nAnd in the python interpreter, try to import `datasets`:\r\n\r\n```\r\nimport datasets\r\n```\r\n\r\n## Expected results\r\nThe import should run successfully.\r\n\r\n## Actual results\r\n\r\nHere is the trace of the error I get:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/__init__.py\", line 33, in <module>\r\n    from .arrow_dataset import Dataset, concatenate_datasets\r\n  File \"/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 48, in <module>\r\n    from .filesystems import extract_path_from_uri, is_remote_filesystem\r\n  File \"/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/filesystems/__init__.py\", line 30, in <module>\r\n    def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:\r\nAttributeError: 'EntryPoint' object has no attribute 'AbstractFileSystem'\r\n```\r\n\r\n## Suggested fix\r\n\r\n`datasets/filesystems/__init__.py`, line 30, replace:\r\n```\r\n    def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:\r\n```\r\nby:\r\n```\r\n    def is_remote_filesystem(fs: fsspec.AbstractFileSystem) -> bool:\r\n```\r\n\r\nI will come up with a PR soon if this effectively solves the issue.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: WSL2 (Ubuntu 20.04.1 LTS)\r\n- Python version: 3.8.5\r\n- PyArrow version: 5.0.0\r\n- `fsspec` version: 2021.8.1\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 996436368,
    "title": "timit_asr dataset only includes one text phrase",
    "dateCreated": "2021-09-14T21:06:07Z",
    "dateModified": "2021-09-14T21:06:07Z",
    "description": "## Describe the bug\r\nThe dataset 'timit_asr' only includes one text phrase. It only includes the transcription \"Would such an act of refusal be useful?\" multiple times rather than different phrases.\r\n\r\n## Steps to reproduce the bug\r\nNote: I am following the tutorial https://huggingface.co/blog/fine-tune-wav2vec2-english\r\n\r\n1. Install the dataset and other packages\r\n```python\r\n!pip install datasets>=1.5.0\r\n!pip install transformers==4.4.0\r\n!pip install soundfile\r\n!pip install jiwer\r\n```\r\n2. Load the dataset\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\n\r\ntimit = load_dataset(\"timit_asr\")\r\n```\r\n3. Remove columns that we don't want\r\n```python\r\ntimit = timit.remove_columns([\"phonetic_detail\", \"word_detail\", \"dialect_region\", \"id\", \"sentence_type\", \"speaker_id\"])\r\n```\r\n4. Write a short function to display some random samples of the dataset.\r\n```python\r\nfrom datasets import ClassLabel\r\nimport random\r\nimport pandas as pd\r\nfrom IPython.display import display, HTML\r\n\r\ndef show_random_elements(dataset, num_examples=10):\r\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\r\n    picks = []\r\n    for _ in range(num_examples):\r\n        pick = random.randint(0, len(dataset)-1)\r\n        while pick in picks:\r\n            pick = random.randint(0, len(dataset)-1)\r\n        picks.append(pick)\r\n    \r\n    df = pd.DataFrame(dataset[picks])\r\n    display(HTML(df.to_html()))\r\n\r\nshow_random_elements(timit[\"train\"].remove_columns([\"file\"]))\r\n```\r\n\r\n## Expected results\r\n10 random different transcription phrases.\r\n\r\n## Actual results\r\n10 of the same transcription phrase \"Would such an act of refusal be useful?\"\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.4.1\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyArrow version: not listed\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 996256005,
    "title": "Update link to Blog in docs footer",
    "dateCreated": "2021-09-14T17:23:14Z",
    "dateModified": "2021-09-14T17:23:14Z",
    "description": "Update link.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 996202598,
    "title": "Fix exception chaining",
    "dateCreated": "2021-09-14T16:19:29Z",
    "dateModified": "2021-09-14T16:19:29Z",
    "description": "Fix exception chaining to avoid tracebacks with message: `During handling of the above exception, another exception occurred:`",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 996149632,
    "title": "feat: \ud83c\udfb8 pass additional arguments to get private configs + info",
    "dateCreated": "2021-09-14T15:24:19Z",
    "dateModified": "2021-09-14T15:24:19Z",
    "description": "`use_auth_token` can now be passed to the functions to get the configs\r\nor infos of private datasets on the hub",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 996002180,
    "title": "fix anli splits",
    "dateCreated": "2021-09-14T13:10:35Z",
    "dateModified": "2021-09-14T13:10:35Z",
    "description": "I can't run the tests for dummy data, facing this error \r\n\r\n`ImportError while loading conftest '/home/zaid/tmp/fix_anli_splits/datasets/tests/conftest.py'.\r\ntests/conftest.py:10: in <module>\r\n    from datasets import config\r\nE   ImportError: cannot import name 'config' from 'datasets' (unknown location)`",
    "status": "open",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 995970612,
    "title": "Update Zenodo metadata with creator names and affiliation",
    "dateCreated": "2021-09-14T12:39:37Z",
    "dateModified": "2021-09-14T12:39:37Z",
    "description": "This PR helps in prefilling author data when automatically generating the DOI after each release.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 995968152,
    "title": "add story_cloze dataset",
    "dateCreated": "2021-09-14T12:36:53Z",
    "dateModified": "2021-09-14T12:36:53Z",
    "description": "@lhoestq I have spent some time but I still I can't succeed in correctly testing the dummy_data.",
    "status": "open",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 995962905,
    "title": "feat: \ud83c\udfb8 add a function to get a dataset config's split names",
    "dateCreated": "2021-09-14T12:31:22Z",
    "dateModified": "2021-09-14T12:31:22Z",
    "description": "Also: pass additional arguments (use_auth_token) to get private configs + info of private datasets on the hub\r\n\r\nQuestions:\r\n\r\n- <strike>I'm not sure how the versions work: I changed 1.12.1.dev0 to 1.12.1.dev1, was it correct?</strike> no -> reverted\r\n- Should I add a section in https://github.com/huggingface/datasets/blob/master/docs/source/load_hub.rst? (there is no section for get_dataset_infos)",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 995843964,
    "title": "Update BibTeX entry",
    "dateCreated": "2021-09-14T10:16:17Z",
    "dateModified": "2021-09-14T10:16:17Z",
    "description": "Update BibTeX entry.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 995814222,
    "title": "FORCE_REDOWNLOAD does not work",
    "dateCreated": "2021-09-14T09:45:26Z",
    "dateModified": "2021-09-14T09:45:26Z",
    "description": "## Describe the bug\r\nWith GenerateMode.FORCE_REDOWNLOAD, the documentation says \r\n    +------------------------------------+-----------+---------+\r\n    |                                    | Downloads | Dataset |\r\n    +====================================+===========+=========+\r\n    | `REUSE_DATASET_IF_EXISTS` (default)| Reuse     | Reuse   |\r\n    +------------------------------------+-----------+---------+\r\n    | `REUSE_CACHE_IF_EXISTS`            | Reuse     | Fresh   |\r\n    +------------------------------------+-----------+---------+\r\n    | `FORCE_REDOWNLOAD`                 | Fresh     | Fresh   |\r\n    +------------------------------------+-----------+---------+\r\n\r\nHowever, the old dataset is loaded even when FORCE_REDOWNLOAD is chosen.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n\r\nimport pandas as pd\r\nfrom datasets import load_dataset, GenerateMode\r\npd.DataFrame(range(5), columns=['numbers']).to_csv('/tmp/test.tsv.gz', index=False)\r\nee = load_dataset('csv', data_files=['/tmp/test.tsv.gz'], delimiter='\\t', split='train', download_mode=GenerateMode.FORCE_REDOWNLOAD)\r\nprint(ee)\r\npd.DataFrame(range(10), columns=['numerals']).to_csv('/tmp/test.tsv.gz', index=False)\r\nee = load_dataset('csv', data_files=['/tmp/test.tsv.gz'], delimiter='\\t', split='train', download_mode=GenerateMode.FORCE_REDOWNLOAD)\r\nprint(ee)\r\n\r\n```\r\n\r\n## Expected results\r\nDataset({\r\n    features: ['numbers'],\r\n    num_rows: 5\r\n})\r\nDataset({\r\n    features: ['numerals'],\r\n    num_rows: 10\r\n})\r\n\r\n## Actual results\r\nDataset({\r\n    features: ['numbers'],\r\n    num_rows: 5\r\n})\r\nDataset({\r\n    features: ['numbers'],\r\n    num_rows: 5\r\n})\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-4.14.181-108.257.amzn1.x86_64-x86_64-with-glibc2.10\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n",
    "status": "open",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 995715191,
    "title": "Fix xpathopen to accept positional arguments",
    "dateCreated": "2021-09-14T08:02:50Z",
    "dateModified": "2021-09-14T08:02:50Z",
    "description": "Fix `xpathopen()` so that it also accepts positional arguments.\r\n\r\nFix #2901.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 995254216,
    "title": "Add WIT Dataset",
    "dateCreated": "2021-09-13T19:38:49Z",
    "dateModified": "2021-09-13T19:38:49Z",
    "description": "## Adding a Dataset\r\n- **Name:** *WIT*\r\n- **Description:** *Wikipedia-based Image Text Dataset*\r\n- **Paper:** *[WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning\r\n](https://arxiv.org/abs/2103.01913)*\r\n- **Data:** *https://github.com/google-research-datasets/wit*\r\n- **Motivation:**  (excerpt from their Github README.md)\r\n\r\n> - The largest multimodal dataset (publicly available at the time of this writing) by the number of image-text examples.\r\n> - A massively multilingual dataset (first of its kind) with coverage for over 100+ languages.\r\n> - A collection of diverse set of concepts and real world entities.\r\n> - Brings forth challenging real-world test sets.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 995232844,
    "title": "Incompatibility with pytest",
    "dateCreated": "2021-09-13T19:12:17Z",
    "dateModified": "2021-09-13T19:12:17Z",
    "description": "## Describe the bug\r\n\r\npytest complains about xpathopen / path.open(\"w\")\r\n\r\n## Steps to reproduce the bug\r\n\r\nCreate a test file, `test.py`:\r\n\r\n```python\r\nimport datasets as ds\r\ndef load_dataset():\r\n    ds.load_dataset(\"counter\", split=\"train\", streaming=True)\r\n```\r\n\r\nAnd launch it with pytest:\r\n\r\n```bash\r\npython -m pytest test.py\r\n```\r\n\r\n## Expected results\r\n\r\nIt should give something like:\r\n\r\n```\r\ncollected 1 item\r\n\r\ntest.py .                                                                                                                                                                                                                                             [100%]\r\n\r\n======= 1 passed in 3.15s =======\r\n```\r\n\r\n## Actual results\r\n\r\n```\r\n============================================================================================================================= test session starts ==============================================================================================================================\r\nplatform linux -- Python 3.8.11, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\r\nrootdir: /home/slesage/hf/datasets-preview-backend, configfile: pyproject.toml\r\nplugins: anyio-3.3.1\r\ncollected 1 item\r\n\r\ntests/queries/test_rows.py .                                                                                                                                                                                                                                             [100%]Traceback (most recent call last):\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pytest/__main__.py\", line 5, in <module>\r\n    raise SystemExit(pytest.console_main())\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/config/__init__.py\", line 185, in console_main\r\n    code = main()\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/config/__init__.py\", line 162, in main\r\n    ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_hooks.py\", line 265, in __call__\r\n    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_manager.py\", line 80, in _hookexec\r\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 60, in _multicall\r\n    return outcome.get_result()\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_result.py\", line 60, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 39, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/main.py\", line 316, in pytest_cmdline_main\r\n    return wrap_session(config, _main)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/main.py\", line 304, in wrap_session\r\n    config.hook.pytest_sessionfinish(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_hooks.py\", line 265, in __call__\r\n    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_manager.py\", line 80, in _hookexec\r\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 55, in _multicall\r\n    gen.send(outcome)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/terminal.py\", line 803, in pytest_sessionfinish\r\n    outcome.get_result()\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_result.py\", line 60, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 39, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/cacheprovider.py\", line 428, in pytest_sessionfinish\r\n    config.cache.set(\"cache/nodeids\", sorted(self.cached_nodeids))\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/cacheprovider.py\", line 188, in set\r\n    f = path.open(\"w\")\r\nTypeError: xpathopen() takes 1 positional argument but 2 were given\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.0\r\n- Platform: Linux-5.11.0-1017-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 994922580,
    "title": "Fix null sequence encoding",
    "dateCreated": "2021-09-13T13:55:08Z",
    "dateModified": "2021-09-13T13:55:08Z",
    "description": "The Sequence feature encoding was failing when a `None` sequence was used in a dataset.\r\n\r\nFix https://github.com/huggingface/datasets/issues/2892",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 994082432,
    "title": "Dataset",
    "dateCreated": "2021-09-12T07:38:53Z",
    "dateModified": "2021-09-12T07:38:53Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 994032814,
    "title": "Hug emoji",
    "dateCreated": "2021-09-12T03:27:51Z",
    "dateModified": "2021-09-12T03:27:51Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 993798386,
    "title": "Add OpenAI's HumanEval dataset",
    "dateCreated": "2021-09-11T09:37:47Z",
    "dateModified": "2021-09-11T09:37:47Z",
    "description": "This PR adds OpenAI's [HumanEval](https://github.com/openai/human-eval) dataset. The dataset consists of 164 handcrafted programming problems with solutions and unittests to verify solution. This dataset is useful to evaluate code generation models.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 993613113,
    "title": "add multi-proc in `to_csv`",
    "dateCreated": "2021-09-10T21:35:09Z",
    "dateModified": "2021-09-10T21:35:09Z",
    "description": "This PR extends the multi-proc method used in #2747 for`to_json` to `to_csv` as well. \r\n\r\nResults on my machine post benchmarking on `ascent_kb` dataset (giving ~45% improvement when compared to num_proc = 1):\r\n```\r\nTime taken on 1 num_proc, 10000 batch_size  674.2055702209473\r\nTime taken on 4 num_proc, 10000 batch_size  425.6553490161896\r\n\r\nTime taken on 1 num_proc, 50000 batch_size  623.5897650718689\r\nTime taken on 4 num_proc, 50000 batch_size  380.0402421951294\r\n\r\nTime taken on 4 num_proc, 100000 batch_size  361.7168130874634\r\n```\r\nThis is a WIP as writing tests is pending for this PR. \r\n\r\nI'm also exploring [this](https://arrow.apache.org/docs/python/csv.html#incremental-writing) approach for which I'm using `pyarrow-5.0.0`.\r\n",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 993462274,
    "title": "Use pyarrow.Table.replace_schema_metadata instead of pyarrow.Table.cast",
    "dateCreated": "2021-09-10T17:56:57Z",
    "dateModified": "2021-09-10T17:56:57Z",
    "description": "This PR partially addresses #2252.\r\n\r\n``update_metadata_with_features`` uses ``Table.cast`` which slows down ``load_from_disk`` (and possibly other methods that use it) for very large datasets. Since ``update_metadata_with_features`` is only updating the schema metadata, it makes more sense to use ``pyarrow.Table.replace_schema_metadata`` which is much faster. This PR adds a ``replace_schema_metadata`` method to all table classes, and modifies  ``update_metadata_with_features`` to use it instead of ``cast``.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 993375654,
    "title": "Fix COUNTER dataset",
    "dateCreated": "2021-09-10T16:07:29Z",
    "dateModified": "2021-09-10T16:07:29Z",
    "description": "Fix filename generating `FileNotFoundError`.\r\n\r\nRelated to #2866.\r\nCC: @severo.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 993342781,
    "title": "add mbpp dataset",
    "dateCreated": "2021-09-10T15:27:30Z",
    "dateModified": "2021-09-10T15:27:30Z",
    "description": "This PR adds the mbpp dataset introduced by Google [here](https://github.com/google-research/google-research/tree/master/mbpp) as mentioned in #2816.\r\n\r\nThe dataset contain two versions: a full and a sanitized one. They have a slightly different schema and it is current state the loading preserves the original schema. An open question is whether to harmonize the two schemas when loading the dataset or to preserve the original one. Since not all fields are overlapping the schema will not be exactly the same.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 993274572,
    "title": "Error when encoding a dataset with None objects with a Sequence feature",
    "dateCreated": "2021-09-10T14:11:43Z",
    "dateModified": "2021-09-10T14:11:43Z",
    "description": "There is an error when encoding a dataset with None objects with a Sequence feature\r\n\r\nTo reproduce:\r\n```python\r\nfrom datasets import Dataset, Features, Value, Sequence\r\ndata = {\"a\": [[0], None]}\r\nfeatures = Features({\"a\": Sequence(Value(\"int32\"))})\r\ndataset = Dataset.from_dict(data, features=features)\r\n```\r\nraises\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-24-40add67f8751> in <module>\r\n      2 data = {\"a\": [[0], None]}\r\n      3 features = Features({\"a\": Sequence(Value(\"int32\"))})\r\n----> 4 dataset = Dataset.from_dict(data, features=features)\r\n[...]\r\n~/datasets/features.py in encode_nested_example(schema, obj)\r\n    888         if isinstance(obj, str):  # don't interpret a string as a list\r\n    889             raise ValueError(\"Got a string but expected a list instead: '{}'\".format(obj))\r\n--> 890         return [encode_nested_example(schema.feature, o) for o in obj]\r\n    891     # Object with special encoding:\r\n    892     # ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\r\n\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\n\r\nInstead, if should run without error, as if the `features` were not passed",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 993161984,
    "title": "[WIP] Allow dynamic first dimension for ArrayXD",
    "dateCreated": "2021-09-10T11:52:52Z",
    "dateModified": "2021-09-10T11:52:52Z",
    "description": "Add support for dynamic first dimension for ArrayXD features. See issue [#887](https://github.com/huggingface/datasets/issues/887).\r\nFollowing changes allow for `to_pylist` method of `ArrayExtensionArray` to return a list of numpy arrays where fist dimension can vary.\r\n\r\n@lhoestq Could you suggest how you want to extend test suit. For now I added only very limited testing.",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 993074102,
    "title": "0x290B112ED1280537B24Ee6C268a004994a16e6CE",
    "dateCreated": "2021-09-10T09:51:17Z",
    "dateModified": "2021-09-10T09:51:17Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 992968382,
    "title": "Coc",
    "dateCreated": "2021-09-10T07:32:07Z",
    "dateModified": "2021-09-10T07:32:07Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 992676535,
    "title": "v1.11.1 release date",
    "dateCreated": "2021-09-09T21:53:15Z",
    "dateModified": "2021-09-09T21:53:15Z",
    "description": "Hello, i need to use latest features in one of my packages but there have been no new datasets release since 2 months ago.\r\n\r\nWhen do you plan to publush v1.11.1 release?",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 992576305,
    "title": "#2837 Use cache folder for lockfile",
    "dateCreated": "2021-09-09T19:55:56Z",
    "dateModified": "2021-09-09T19:55:56Z",
    "description": "Fixes #2837 \r\n\r\nUse a cache folder directory to store the FileLock.\r\n\r\nThe issue was that the lock file was in a readonly folder.\r\n",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 992534632,
    "title": "Hj",
    "dateCreated": "2021-09-09T18:58:52Z",
    "dateModified": "2021-09-09T18:58:52Z",
    "description": null,
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 992160544,
    "title": "Adding an Elastic Search index to a Dataset",
    "dateCreated": "2021-09-09T12:21:39Z",
    "dateModified": "2021-09-09T12:21:39Z",
    "description": "## Describe the bug\r\nWhen trying to index documents from the squad dataset, the connection to ElasticSearch seems to break:\r\n\r\nReusing dataset squad (/Users/andreasmotz/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\r\n 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589     | 9501/10570 [00:01<00:00, 6335.61docs/s]\r\n\r\nNo error is thrown, but the indexing breaks ~90%.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom datasets import load_dataset\r\nfrom elasticsearch import Elasticsearch\r\nes = Elasticsearch()\r\nsquad = load_dataset('squad', split='validation')\r\nindex_name = \"corpus\"\r\nes_config = {\r\n    \"settings\": {\r\n        \"number_of_shards\": 1,\r\n        \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\r\n    },\r\n    \"mappings\": {\r\n        \"properties\": {\r\n            \"idx\" : {\"type\" : \"keyword\"},\r\n            \"title\" : {\"type\" : \"keyword\"},\r\n            \"text\": {\r\n                \"type\": \"text\",\r\n                \"analyzer\": \"standard\",\r\n                \"similarity\": \"BM25\"\r\n            },\r\n        }\r\n    },\r\n}\r\nclass IndexBuilder:\r\n    \"\"\"\r\n    Elastic search indexing of a corpus\r\n    \"\"\"\r\n    def __init__(\r\n        self,\r\n        *args,\r\n        #corpus : None,\r\n        dataset : squad,\r\n        index_name = str,\r\n        query = str,\r\n        config = dict,\r\n        **kwargs,\r\n    ):\r\n        #instantiate HuggingFace dataset\r\n        self.dataset = dataset\r\n        #instantiate ElasticSearch config\r\n        self.config = config\r\n        self.es = Elasticsearch()\r\n        self.index_name = index_name\r\n        self.query = query\r\n    def elastic_index(self):\r\n        print(self.es.info)\r\n        self.es.indices.delete(index=self.index_name, ignore=[400, 404])\r\n        search_index = self.dataset.add_elasticsearch_index(column='context', host='localhost', port='9200', es_index_name=self.index_name, es_index_config=self.config)\r\n        return search_index\r\n    def exact_match_method(self, index):\r\n        scores, retrieved_examples = index.get_nearest_examples('context', query=self.query, k=1)\r\n        return scores, retrieved_examples\r\nif __name__ == \"__main__\":\r\n    print(type(squad))\r\n    Index = IndexBuilder(dataset=squad, index_name='corpus_index', query='Where was Chopin born?', config=es_config)\r\n    search_index = Index.elastic_index()\r\n    scores, examples = Index.exact_match_method(search_index)\r\n    print(scores, examples)\r\n    for name in squad.column_names:\r\n        print(type(squad[name]))\r\n```\r\n\r\n## Environment info\r\nWe run the code in Poetry. This might be the issue, since the script runs successfully in our local environment.\r\n\r\nPoetry:\r\n- Python version: 3.8\r\n- PyArrow: 4.0.1\r\n- Elasticsearch: 7.13.4\r\n- datasets: 1.10.2\r\n\r\nLocal:\r\n- Python version: 3.8\r\n- PyArrow: 3.0.0\r\n- Elasticsearch: 7.7.1\r\n- datasets: 1.7.0\r\n",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 992135698,
    "title": "Add IC, SI, ER tasks to SUPERB",
    "dateCreated": "2021-09-09T11:56:03Z",
    "dateModified": "2021-09-09T11:56:03Z",
    "description": "This PR adds 3 additional classification tasks to SUPERB\r\n\r\n#### Intent Classification\r\nDataset URL seems to be down at the moment :( See the note below.\r\nS3PRL source: https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/fluent_commands/dataset.py\r\nInstructions: https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#ic-intent-classification---fluent-speech-commands\r\n\r\n#### Speaker Identification\r\nManual download script:\r\n```\r\nmkdir VoxCeleb1\r\ncd VoxCeleb1\r\n            \r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partaa\r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partab\r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partac\r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partad\r\ncat vox1_dev* > vox1_dev_wav.zip\r\nunzip vox1_dev_wav.zip\r\n            \r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip\r\nunzip vox1_test_wav.zip\r\n            \r\n# download the official SUPERB train-dev-test split\r\nwget https://raw.githubusercontent.com/s3prl/s3prl/master/s3prl/downstream/voxceleb1/veri_test_class.txt\r\n```\r\nS3PRL source: https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/voxceleb1/dataset.py\r\nInstructions: https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#sid-speaker-identification\r\n\r\n#### Intent Classification\r\nManual download requires going through a slow application process, see the note below.\r\nS3PRL source: https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/emotion/IEMOCAP_preprocess.py\r\nInstructions: https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#er-emotion-recognition\r\n\r\n#### :warning:  Note\r\nThese datasets either require manual downloads or have broken/unstable links. You can get all necessary archives in this repo: https://huggingface.co/datasets/anton-l/superb_source_data_dumps/tree/main",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 991969875,
    "title": "Fix data URLs and metadata in DocRED dataset",
    "dateCreated": "2021-09-09T08:55:34Z",
    "dateModified": "2021-09-09T08:55:34Z",
    "description": "The host of `docred` dataset has updated the `dev` data file. This PR:\r\n- Updates the dev URL\r\n- Updates dataset metadata\r\n\r\nThis PR also fixes the URL of the `train_distant` split, which was wrong.\r\n\r\nFix #2882.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 991800141,
    "title": "`load_dataset('docred')` results in a `NonMatchingChecksumError` ",
    "dateCreated": "2021-09-09T05:55:02Z",
    "dateModified": "2021-09-09T05:55:02Z",
    "description": "## Describe the bug\r\nI get consistent `NonMatchingChecksumError: Checksums didn't match for dataset source files` errors when trying to execute `datasets.load_dataset('docred')`.\r\n\r\n## Steps to reproduce the bug\r\nIt is quasi only this code:\r\n```python\r\nimport datasets\r\ndata = datasets.load_dataset('docred')\r\n```\r\n\r\n## Expected results\r\nThe DocRED dataset should be loaded without any problems.\r\n\r\n## Actual results\r\n```\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-4-b1b83f25a16c> in <module>\r\n----> 1 d = datasets.load_dataset('docred')\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\r\n    845 \r\n    846     # Download and prepare data\r\n--> 847     builder_instance.download_and_prepare(\r\n    848         download_config=download_config,\r\n    849         download_mode=download_mode,\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    613                             logger.warning(\"HF google storage unreachable. Downloading and preparing it from source\")\r\n    614                     if not downloaded_from_gcs:\r\n--> 615                         self._download_and_prepare(\r\n    616                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    617                         )\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    673         # Checksums verification\r\n    674         if verify_infos:\r\n--> 675             verify_checksums(\r\n    676                 self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n    677             )\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     38     if len(bad_urls) > 0:\r\n     39         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 40         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     41     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     42 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/uc?export=download&id=1fDmfUUo5G7gfaoqWWvK81u08m71TK2g7']\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: Linux-5.11.0-7633-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.5\r\n- PyArrow version: 5.0.0\r\n\r\nThis error also happened on my Windows-partition, after freshly installing python 3.9 and `datasets`.\r\n\r\n## Remarks\r\n\r\n- I have already called `rm -rf /home/<user>/.cache/huggingface`, i.e., I have tried clearing the cache.\r\n- The problem does not exist for other datasets, i.e., it seems to be DocRED-specific.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 991639142,
    "title": "Add BIOSSES dataset",
    "dateCreated": "2021-09-09T00:35:36Z",
    "dateModified": "2021-09-09T00:35:36Z",
    "description": "Adding the biomedical semantic sentence similarity dataset, BIOSSES, listed in \"Biomedical Datasets - BigScience Workshop 2021\"",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 990877940,
    "title": "Extend support for streaming datasets that use pathlib.Path stem/suffix",
    "dateCreated": "2021-09-08T08:42:43Z",
    "dateModified": "2021-09-08T08:42:43Z",
    "description": "This PR extends the support in streaming mode for datasets that use `pathlib`, by patching the properties `pathlib.Path.stem` and `pathlib.Path.suffix`.\r\n\r\nRelated to #2876, #2874, #2866.\r\n\r\nCC: @severo",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 990257404,
    "title": "In v1.4.1, all TIMIT train transcripts are \"Would such an act of refusal be useful?\"",
    "dateCreated": "2021-09-07T18:53:45Z",
    "dateModified": "2021-09-07T18:53:45Z",
    "description": "## Describe the bug\r\nUsing version 1.4.1 of `datasets`, TIMIT transcripts are all the same.\r\n\r\n## Steps to reproduce the bug\r\nI was following this tutorial\r\n- https://huggingface.co/blog/fine-tune-wav2vec2-english\r\n\r\nBut here's a distilled repro:\r\n```python\r\n!pip install datasets==1.4.1\r\nfrom datasets import load_dataset\r\ntimit = load_dataset(\"timit_asr\", cache_dir=\"./temp\")\r\nunique_transcripts = set(timit[\"train\"][\"text\"])\r\nprint(unique_transcripts)\r\nassert len(unique_transcripts) > 1\r\n```\r\n## Expected results\r\nExpected the correct TIMIT data. Or an error saying that this version of `datasets` can't produce it.\r\n\r\n## Actual results\r\nEvery train transcript was \"Would such an act of refusal be useful?\" Every test transcript was \"The bungalow was pleasantly situated near the shore.\"\r\n\r\n## Environment info\r\n- `datasets` version: 1.4.1\r\n- Platform: Darwin-18.7.0-x86_64-i386-64bit\r\n- Python version: 3.7.9\r\n- PyTorch version (GPU?): 1.9.0 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: tried both\r\n- Using distributed or parallel set-up in script?: no\r\n- \r\n\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 990093316,
    "title": "NotADirectoryError: [WinError 267] During load_from_disk",
    "dateCreated": "2021-09-07T15:15:05Z",
    "dateModified": "2021-09-07T15:15:05Z",
    "description": "## Describe the bug\r\nTrying to load saved dataset or dataset directory from Amazon S3 on a Windows machine fails.\r\nPerforming the same operation succeeds on non-windows environment (AWS Sagemaker).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Followed https://huggingface.co/docs/datasets/filesystems.html#loading-a-processed-dataset-from-s3\r\n\r\nfrom datasets import load_from_disk\r\nfrom datasets.filesystems import S3FileSystem\r\n\r\n\r\ns3_file = \"output of save_to_disk\"\r\n\r\ns3_filesystem = S3FileSystem()\r\n\r\nload_from_disk(s3_file, fs=s3_filesystem)\r\n```\r\n\r\n## Expected results\r\nload_from_disk succeeds without error\r\n\r\n## Actual results\r\nSeems like it succeeds in pulling the file into a windows temp directory, as it exists in my system, but fails to process it.\r\n```\r\nException ignored in: <finalize object at 0x26409231ce0; dead>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\weakref.py\", line 566, in __call__\r\n    return info.func(*info.args, **(info.kwargs or {}))\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 817, in _cleanup\r\n    cls._rmtree(name)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 813, in _rmtree\r\n    _shutil.rmtree(name, onerror=onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 740, in rmtree\r\n    return _rmtree_unsafe(path, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 613, in _rmtree_unsafe\r\n    _rmtree_unsafe(fullname, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 613, in _rmtree_unsafe\r\n    _rmtree_unsafe(fullname, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 613, in _rmtree_unsafe\r\n    _rmtree_unsafe(fullname, onerror)\r\n  [Previous line repeated 2 more times]\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 618, in _rmtree_unsafe\r\n    onerror(os.unlink, fullname, sys.exc_info())\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 805, in onerror\r\n    cls._rmtree(path)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 813, in _rmtree\r\n    _shutil.rmtree(name, onerror=onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 740, in rmtree\r\n    return _rmtree_unsafe(path, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 599, in _rmtree_unsafe\r\n    onerror(os.scandir, path, sys.exc_info())\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 596, in _rmtree_unsafe\r\n    with os.scandir(path) as scandir_it:\r\nNotADirectoryError: [WinError 267] The directory name is invalid: 'C:\\\\Users\\\\grassycup\\\\AppData\\\\Local\\\\Temp\\\\tmp45f_qbma\\\\tests3bucket\\\\output\\\\test_output\\\\train\\\\dataset.arrow'\r\nException ignored in: <finalize object at 0x264091c7880; dead>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\weakref.py\", line 566, in __call__\r\n    return info.func(*info.args, **(info.kwargs or {}))\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 817, in _cleanup\r\n    cls._rmtree(name)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 813, in _rmtree\r\n    _shutil.rmtree(name, onerror=onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 740, in rmtree\r\n    return _rmtree_unsafe(path, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 613, in _rmtree_unsafe\r\n    _rmtree_unsafe(fullname, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 613, in _rmtree_unsafe\r\n    _rmtree_unsafe(fullname, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 613, in _rmtree_unsafe\r\n    _rmtree_unsafe(fullname, onerror)\r\n  [Previous line repeated 2 more times]\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 618, in _rmtree_unsafe\r\n    onerror(os.unlink, fullname, sys.exc_info())\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 805, in onerror\r\n    cls._rmtree(path)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 813, in _rmtree\r\n    _shutil.rmtree(name, onerror=onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 740, in rmtree\r\n    return _rmtree_unsafe(path, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 599, in _rmtree_unsafe\r\n    onerror(os.scandir, path, sys.exc_info())\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 596, in _rmtree_unsafe\r\n    with os.scandir(path) as scandir_it:\r\nNotADirectoryError: [WinError 267] The directory name is invalid:\r\n'C:\\\\Users\\\\grassycup\\\\AppData\\\\Local\\\\Temp\\\\tmp45f_qbma\\\\tests3bucket\\\\output\\\\test_output\\\\train\\\\dataset.arrow'\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: Windows-10-10.0.19042-SP0\r\n- Python version: 3.8.11\r\n- PyArrow version: 3.0.0\r\n",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 990027249,
    "title": "Don't keep the dummy data folder or dataset_infos.json when resolving data files",
    "dateCreated": "2021-09-07T14:09:04Z",
    "dateModified": "2021-09-07T14:09:04Z",
    "description": "When there's no dataset script, all the data files of a folder or a repository on the Hub are loaded as data files.\r\n\r\nThere are already a few exceptions:\r\n- files starting with \".\" are ignored\r\n- the dataset card \"README.md\" is ignored\r\n- any file named \"config.json\" is ignored (currently it isn't used anywhere, but it could be used in the future to define splits or configs for example, but not 100% sure)\r\n\r\nHowever any data files in a folder named \"dummy\" should be ignored as well as they should only be used to test the dataset.\r\nSame for \"dataset_infos.json\" which should only be used to get the `dataset.info`",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 990001079,
    "title": "Extend support for streaming datasets that use pathlib.Path.glob",
    "dateCreated": "2021-09-07T13:43:45Z",
    "dateModified": "2021-09-07T13:43:45Z",
    "description": "This PR extends the support in streaming mode for datasets that use `pathlib`, by patching the method `pathlib.Path.glob`.\r\n\r\nRelated to #2874, #2866.\r\n\r\nCC: @severo",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 989919398,
    "title": "Add Congolese Swahili speech datasets",
    "dateCreated": "2021-09-07T12:13:50Z",
    "dateModified": "2021-09-07T12:13:50Z",
    "description": "## Adding a Dataset\r\n- **Name:** Congolese Swahili speech corpora\r\n- **Data:** https://gamayun.translatorswb.org/data/\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n\r\nAlso related: https://mobile.twitter.com/OktemAlp/status/1435196393631764482",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 989685328,
    "title": "Support streaming datasets that use pathlib",
    "dateCreated": "2021-09-07T07:35:49Z",
    "dateModified": "2021-09-07T07:35:49Z",
    "description": "This PR extends the support in streaming mode for datasets that use `pathlib.Path`.\r\n\r\nRelated to: #2866.\r\nCC: @severo ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 989587695,
    "title": "adding swedish_medical_ner",
    "dateCreated": "2021-09-07T04:44:53Z",
    "dateModified": "2021-09-07T04:44:53Z",
    "description": "Adding the Swedish Medical NER dataset, listed in \"Biomedical Datasets - BigScience Workshop 2021\"\r\n\r\nCode refactored ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 989453069,
    "title": "adding swedish_medical_ner",
    "dateCreated": "2021-09-06T22:00:52Z",
    "dateModified": "2021-09-06T22:00:52Z",
    "description": "Adding the Swedish Medical NER dataset, listed in \"Biomedical Datasets - BigScience Workshop 2021\"",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 989436088,
    "title": "datasets.config.PYARROW_VERSION has no attribute 'major'",
    "dateCreated": "2021-09-06T21:06:57Z",
    "dateModified": "2021-09-06T21:06:57Z",
    "description": "In the test_dataset_common.py script, line 288-289\r\n\r\n```\r\nif datasets.config.PYARROW_VERSION.major < 3:\r\n   packaged_datasets = [pd for pd in packaged_datasets if pd[\"dataset_name\"] != \"parquet\"]\r\n```\r\n\r\nwhich throws the error below. `datasets.config.PYARROW_VERSION` itself return the string '4.0.1'. I have tested this on both datasets.__version_=='1.11.0' and '1.9.0'. I am using Mac OS.\r\n\r\n```\r\nimport datasets\r\ndatasets.config.PYARROW_VERSION.major\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/var/folders/1f/0wqmlgp90qjd5mpj53fnjq440000gn/T/ipykernel_73361/2547517336.py in <module>\r\n      1 import datasets\r\n----> 2 datasets.config.PYARROW_VERSION.major\r\n\r\nAttributeError: 'str' object has no attribute 'major'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: Darwin-20.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.11\r\n- PyArrow version: 4.0.1\r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 988276859,
    "title": "Fix three typos in two files for documentation",
    "dateCreated": "2021-09-04T11:49:43Z",
    "dateModified": "2021-09-04T11:49:43Z",
    "description": "Changed \"bacth_size\" to \"batch_size\" (2x)\r\nChanged \"intsructions\" to \"instructions\"",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 987676420,
    "title": "TypeError: 'NoneType' object is not callable",
    "dateCreated": "2021-09-03T11:27:39Z",
    "dateModified": "2021-09-03T11:27:39Z",
    "description": "## Describe the bug\r\n\r\nTypeError: 'NoneType' object is not callable\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\ndataset = datasets.load_dataset(\"glue\", 'cola')\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform:\r\n- Python version: 3.7\r\n- PyArrow version:\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 987139146,
    "title": "Add Common Objects in 3D (CO3D)",
    "dateCreated": "2021-09-02T20:36:12Z",
    "dateModified": "2021-09-02T20:36:12Z",
    "description": "## Adding a Dataset\r\n- **Name:** *Common Objects in 3D (CO3D)*\r\n- **Description:** *See blog post [here](https://ai.facebook.com/blog/common-objects-in-3d-dataset-for-3d-reconstruction)*\r\n- **Paper:** *[link to paper](https://arxiv.org/abs/2109.00512)*\r\n- **Data:** *[link to data](https://ai.facebook.com/datasets/co3d-downloads/)*\r\n- **Motivation:** *excerpt from above blog post:*\r\n\r\n> As the first data set of its kind, CO3D will aptly enable reconstruction of real-life 3D objects. Indeed, CO3D already provides training data to enable our NeRFormer to tackle the new-view synthesis (NVS) task. Here, photorealistic NVS is a major step on the path to fully immersive AR/VR effects, where objects can be virtually transported across different environments, which will allow connecting users by sharing or recollecting their experiences.\r\n> \r\n> Besides practical applications in AR/VR, we hope that the data set will become a standard testbed for the recent proliferation of methods (including NeRFormer, Implicit Differentiable Renderer, NeRF, and others) that reconstruct 3D scenes by means of an implicit shape model.\r\n> \r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 986971224,
    "title": "Add CaSiNo dataset",
    "dateCreated": "2021-09-02T17:06:23Z",
    "dateModified": "2021-09-02T17:06:23Z",
    "description": "Hi. I request you to add our dataset to the repository. \r\n\r\nThis data was recently published at NAACL 2021: https://aclanthology.org/2021.naacl-main.254.pdf",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 986706676,
    "title": "\"counter\" dataset raises an error in normal mode, but not in streaming mode",
    "dateCreated": "2021-09-02T13:10:53Z",
    "dateModified": "2021-09-02T13:10:53Z",
    "description": "## Describe the bug\r\n\r\n`counter` dataset raises an error on `load_dataset()`, but simply returns an empty iterator in streaming mode.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n>>> import datasets as ds\r\n>>> a = ds.load_dataset('counter', split=\"train\", streaming=False)\r\nUsing custom data configuration default\r\nDownloading and preparing dataset counter/default (download: 1.29 MiB, generated: 2.48 MiB, post-processed: Unknown size, total: 3.77 MiB) to /home/slesage/.cache/huggingface/datasets/counter/default/1.0.0/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9...\r\nTraceback (most recent call last):\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 726, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 1124, in _prepare_split\r\n    for key, record in utils.tqdm(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/tqdm/std.py\", line 1185, in __iter__\r\n    for obj in iterable:\r\n  File \"/home/slesage/.cache/huggingface/modules/datasets_modules/datasets/counter/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9/counter.py\", line 161, in _generate_examples\r\n    with derived_file.open(encoding=\"utf-8\") as f:\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py\", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py\", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 636, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 728, in _download_and_prepare\r\n    raise OSError(\r\nOSError: Cannot find data file.\r\nOriginal error:\r\n[Errno 2] No such file or directory: '/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml'\r\n```\r\n\r\n```python\r\n>>> import datasets as ds\r\n>>> b = ds.load_dataset('counter', split=\"train\", streaming=True)\r\nUsing custom data configuration default\r\n>>> list(b)\r\n[]\r\n```\r\n\r\n## Expected results\r\n\r\nAn exception should be raised in streaming mode\r\n\r\n## Actual results\r\n\r\nNo exception is raised in streaming mode: there is no way to tell if something has broken or if the dataset is simply empty.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.11.1.dev0\r\n- Platform: Linux-5.11.0-1016-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 986460698,
    "title": "Add MultiEURLEX dataset",
    "dateCreated": "2021-09-02T09:42:24Z",
    "dateModified": "2021-09-02T09:42:24Z",
    "description": "**Add new MultiEURLEX Dataset**\r\n\r\nMultiEURLEX comprises 65k EU laws in 23 official EU languages (some low-ish resource). Each EU law has been annotated with EUROVOC concepts (labels) by the Publication Office of EU. As with the English EURLEX, the goal is to predict the relevant EUROVOC concepts (labels); this is multi-label classification task (given the text, predict multiple labels).",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 986159438,
    "title": "Fix data URL in ToTTo dataset",
    "dateCreated": "2021-09-02T05:25:08Z",
    "dateModified": "2021-09-02T05:25:08Z",
    "description": "Data source host changed their data URL: google-research-datasets/ToTTo@cebeb43.\r\n\r\nFix #2860.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 986156755,
    "title": "Update dataset URL",
    "dateCreated": "2021-09-02T05:22:18Z",
    "dateModified": "2021-09-02T05:22:18Z",
    "description": null,
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 985763001,
    "title": "Only retain relevant statistics in certain metrics",
    "dateCreated": "2021-09-01T22:18:10Z",
    "dateModified": "2021-09-01T22:18:10Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nAs I understand, in the `add_batch()` function, the raw predictions and references are kept (in memory?) until `compute()` is called.\r\nhttps://github.com/huggingface/datasets/blob/e248247518140d5b0527ce2843a1a327e2902059/src/datasets/metric.py#L423-L442\r\n\r\nThis takes O(n) memory. However, for many (most?) metrics, this is not necessary. E.g., for accuracy, only the # correct and # total need to be recorded.\r\n\r\n**Describe the solution you'd like**\r\nProbably an inheritance hierarchy where `\"predictions\"` and `\"references\"` are not always the two keys for the final metric computation. Each metric should create and maintain its own relevant statistics, again for example, `\"n_correct\"` and `\"n_total\"` for accuracy.\r\n\r\nI believe the metrics in AllenNLP (https://github.com/allenai/allennlp/tree/39c40fe38cd2fd36b3465b0b3c031f54ec824160/allennlp/training/metrics) can be used as a good reference.\r\n\r\n**Describe alternatives you've considered**\r\nAt least `Metric.compute()` shouldn't hard-code `\"predictions\"` and `\"references\"` so that custom subclasses may override this behavior.\r\nhttps://github.com/huggingface/datasets/blob/e248247518140d5b0527ce2843a1a327e2902059/src/datasets/metric.py#L399-L400",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 985081871,
    "title": "fix: \ud83d\udc1b be more specific when catching exceptions",
    "dateCreated": "2021-09-01T12:18:12Z",
    "dateModified": "2021-09-01T12:18:12Z",
    "description": "The same specific exception is catched in other parts of the same\r\nfunction.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 985013339,
    "title": "Cannot download TOTTO dataset",
    "dateCreated": "2021-09-01T11:04:10Z",
    "dateModified": "2021-09-01T11:04:10Z",
    "description": "Error: Couldn't find file at https://storage.googleapis.com/totto/totto_data.zip\r\n\r\n`datasets version: 1.11.0`\r\n# How to reproduce:\r\n\r\n```py\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('totto')\r\n```\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 984324500,
    "title": "Loading allenai/c4 in streaming mode does too many HEAD requests",
    "dateCreated": "2021-08-31T21:11:04Z",
    "dateModified": "2021-08-31T21:11:04Z",
    "description": "This does 60,000+ HEAD requests to get all the ETags of all the data files:\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"allenai/c4\", streaming=True)\r\n```\r\nIt makes loading the dataset completely impractical.\r\n\r\nThe ETags are used to compute the config id (it must depend on the data files being used).\r\nInstead of using the ETags, we could simply use the commit hash of the dataset repository on the hub, as well and the glob pattern used to resolve the files (here it's `*` by default, to load all the files of the repository)",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 984145568,
    "title": "Fix s3fs version in CI",
    "dateCreated": "2021-08-31T18:05:43Z",
    "dateModified": "2021-08-31T18:05:43Z",
    "description": "The latest s3fs version has new constrains on aiobotocore, and therefore on boto3 and botocore\r\n\r\nThis PR changes the constrains to avoid the new conflicts\r\n\r\nIn particular it pins the version of s3fs.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 984093938,
    "title": "Update: Openwebtext - update size",
    "dateCreated": "2021-08-31T17:11:03Z",
    "dateModified": "2021-08-31T17:11:03Z",
    "description": "Update the size of the Openwebtext dataset\r\n\r\nI also regenerated the dataset_infos.json but the data file checksum didn't change, and the number of examples either (8013769 examples)\r\n\r\nrelated to #2839 ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 983876734,
    "title": "fix: \ud83d\udc1b remove URL's query string only if it's ?dl=1",
    "dateCreated": "2021-08-31T13:40:07Z",
    "dateModified": "2021-08-31T13:40:07Z",
    "description": "A lot of URL use the query strings, for example\r\nhttp://opus.nlpl.eu/download.php?f=Bianet/v1/moses/en-ku.txt.zip, we\r\nmust not remove it when trying to detect the protocol. We thus remove it\r\nonly in the case of the query string being ?dl=1 which occurs on dropbox\r\nand dl.orangedox.com. Also: add unit tests.\r\n\r\nSee https://github.com/huggingface/datasets/pull/2843 for the original\r\ndiscussion.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 983858229,
    "title": "Fix windows CI CondaError",
    "dateCreated": "2021-08-31T13:22:02Z",
    "dateModified": "2021-08-31T13:22:02Z",
    "description": "From this thread: https://github.com/conda/conda/issues/6057\r\n\r\nWe can fix the conda error\r\n```\r\nCondaError: Cannot link a source that does not exist.\r\nC:\\Users\\...\\Anaconda3\\Scripts\\conda.exe\r\n```\r\n\r\nby doing\r\n```bash\r\nconda update conda\r\n```\r\n\r\nbefore doing any install in the windows CI",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 1003999470,
    "title": "Update legacy Python image for CI tests in Linux",
    "dateCreated": "2021-09-22T08:25:27Z",
    "dateModified": "2021-09-22T08:25:27Z",
    "description": "Instead of legacy, use next-generation convenience images, built from the ground up with CI, efficiency, and determinism in mind. Here are some of the highlights:\r\n\r\n- Faster spin-up time - In Docker terminology, these next-gen images will generally have fewer and smaller layers. Using these new images will lead to faster image downloads when a build starts, and a higher likelihood that the image is already cached on the host.\r\n\r\n- Improved reliability and stability - The existing legacy convenience images are rebuilt practically every day with potential changes from upstream that we cannot always test fast enough. This leads to frequent breaking changes, which is not the best environment for stable, deterministic builds. Next-gen images will only be rebuilt for security and critical-bugs, leading to more stable and deterministic images.\r\n\r\nMore info: https://circleci.com/docs/2.0/circleci-images",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 1003999471,
    "title": "Run tests in parallel",
    "dateCreated": "2021-09-22T07:00:44Z",
    "dateModified": "2021-09-22T07:00:44Z",
    "description": "Run CI tests in parallel to speed up the test suite.",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 1003999472,
    "title": "Fix missing conda deps",
    "dateCreated": "2021-09-21T15:23:01Z",
    "dateModified": "2021-09-21T15:23:01Z",
    "description": "`aiohttp` was added as a dependency in #2662 but was missing for the conda build, which causes the 1.12.0 and 1.12.1 to fail.\r\n\r\nFix #2932.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 1003999473,
    "title": "Dummy labels no longer on by default in `to_tf_dataset`",
    "dateCreated": "2021-09-20T18:26:59Z",
    "dateModified": "2021-09-20T18:26:59Z",
    "description": "After more experimentation, I think I have a way to do things that doesn't depend on adding `dummy_labels` - they were quite a hacky solution anyway!",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 1003999474,
    "title": "Fix fn kwargs in filter",
    "dateCreated": "2021-09-20T15:10:26Z",
    "dateModified": "2021-09-20T15:10:26Z",
    "description": "#2836 broke the `fn_kwargs` parameter of `filter`, as mentioned in https://github.com/huggingface/datasets/issues/2927\r\n\r\nI fixed that and added a test to make sure it doesn't happen again (for either map or filter)\r\n\r\nFix #2927",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 1003999475,
    "title": "Introduce web and wiki config in triviaqa dataset",
    "dateCreated": "2021-09-20T14:17:23Z",
    "dateModified": "2021-09-20T14:17:23Z",
    "description": "The TriviaQA paper suggests that the two subsets (Wikipedia and Web)\r\nshould be treated differently. There are also different leaderboards\r\nfor the two sets on CodaLab. For that reason, introduce additional\r\nbuilder configs in the trivia_qa dataset.",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 1003999476,
    "title": "Fix minor URL format in scitldr dataset",
    "dateCreated": "2021-09-20T11:11:32Z",
    "dateModified": "2021-09-20T11:11:32Z",
    "description": "While investigating issue #2918, I found this minor format issues in the URLs (if runned in a Windows machine).",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 1003999477,
    "title": "Don't use old, incompatible cache for the new `filter`",
    "dateCreated": "2021-09-20T10:18:59Z",
    "dateModified": "2021-09-20T10:18:59Z",
    "description": "#2836 changed `Dataset.filter` and the resulting data that are stored in the cache are different and incompatible with the ones of the previous `filter` implementation.\r\n\r\nHowever the caching mechanism wasn't able to differentiate between the old and the new implementation of filter (only the method name was taken into account). \r\n\r\nThis is an issue because anyone that update `datasets` and re-runs some code that uses `filter` would see an error, because the cache would try to load an incompatible `filter` result.\r\n\r\nTo fix this I added the notion of versioning for dataset transform in the caching mechanism, and bumped the version of the `filter` implementation to 2.0.0\r\n\r\nThis way the new `filter` outputs are now considered different from the old ones from the caching point of view.\r\n\r\nThis should fix #2943\r\n\r\ncc @anton-l",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 1003999478,
    "title": "Update meteor score from nltk update",
    "dateCreated": "2021-09-20T09:28:46Z",
    "dateModified": "2021-09-20T09:28:46Z",
    "description": "It looks like there were issues in NLTK on the way the METEOR score was computed.\r\nA fix was added in NLTK at https://github.com/nltk/nltk/pull/2763, and therefore the scoring function no longer returns the same values.\r\n\r\nI updated the score of the example in the docs",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 1003999479,
    "title": "Protect master branch",
    "dateCreated": "2021-09-20T06:47:01Z",
    "dateModified": "2021-09-20T06:47:01Z",
    "description": "After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\r\n- 00cc036fea7c7745cfe722360036ed306796a3f2\r\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\r\n- ...\r\n\r\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\r\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\r\n  - Currently, simple merge commits are already disabled\r\n  - I propose to disable rebase merging as well\r\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\r\n  - ~~This protection would reject direct pushes to master branch~~\r\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\r\n- [x] Protect the master branch only from direct pushing of **merge commits**\r\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\r\n  - No need to disable/re-enable this protection on each release \r\n\r\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 1003999480,
    "title": "Add  `remove_columns` to `IterableDataset ` ",
    "dateCreated": "2021-09-20T04:01:00Z",
    "dateModified": "2021-09-20T04:01:00Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is.\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"c4\", 'realnewslike', streaming =True, split='train')\r\ndataset = dataset.remove_columns('url')\r\n```\r\n```\r\nAttributeError: 'IterableDataset' object has no attribute 'remove_columns'\r\n```\r\n\r\n**Describe the solution you'd like**\r\n\r\nIt would be nice to have `.remove_columns()` to match the `Datasets` api. \r\n\r\n\r\n**Describe alternatives you've considered**\r\n\r\nThis can be done with a single call to `.map()`, \r\n\r\nI can try to help add this. \ud83e\udd17",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 1003999481,
    "title": "Backwards compatibility broken for cached datasets that use `.filter()`",
    "dateCreated": "2021-09-19T16:16:37Z",
    "dateModified": "2021-09-19T16:16:37Z",
    "description": "## Describe the bug\r\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \r\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\r\n\r\nRelated feature: https://github.com/huggingface/datasets/pull/2836\r\n\r\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \r\n\r\n## Workaround\r\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\r\n\r\n## Steps to reproduce the bug\r\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\r\n\r\n2. `pip install datasets==1.11.0` and run the following snippet:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nids = [\"1272-141231-0000\"]\r\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\nds = ds.filter(lambda x: x[\"id\"] in ids)\r\n```\r\n3. `pip install datasets==1.12.1` and re-run the code again\r\n\r\n## Expected results\r\nSame result as with the previous `datasets` version.\r\n\r\n## Actual results\r\n```bash\r\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\r\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\r\nTraceback (most recent call last):\r\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\r\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\r\n    indices = self.map(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\r\n    return self._map_single(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\r\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\r\n    return cls(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\r\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\r\n    return Features(recursive_reorder(self, other))\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\r\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\r\nValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 1003999482,
    "title": "Add SEDE dataset",
    "dateCreated": "2021-09-19T13:11:24Z",
    "dateModified": "2021-09-19T13:11:24Z",
    "description": "This PR adds the SEDE dataset for the task of realistic Text-to-SQL, following the instructions of how to add a database and a dataset card.\r\n\r\nPlease see our paper for more details: https://arxiv.org/abs/2106.05006",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 1003999483,
    "title": "OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError",
    "dateCreated": "2021-09-18T10:39:13Z",
    "dateModified": "2021-09-18T10:39:13Z",
    "description": "## Describe the bug\r\n\r\nCannot download OSCAR `unshuffled_original_ko` due to `NonMatchingSplitsSizesError`.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n>>> dataset = datasets.load_dataset('oscar', 'unshuffled_original_ko')\r\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=25292102197, num_examples=7345075, dataset_name='oscar'), 'recorded': SplitInfo(name='train', num_bytes=25284578514, num_examples=7344907, dataset_name='oscar')}]\r\n```\r\n\r\n## Expected results\r\n\r\nLoading is successful.\r\n\r\n## Actual results\r\n\r\nLoading throws above error.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.4.0-81-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 1003999484,
    "title": "add swedish_medical_ner dataset",
    "dateCreated": "2021-09-17T20:03:05Z",
    "dateModified": "2021-09-17T20:03:05Z",
    "description": "Adding the Swedish Medical NER dataset, listed in \"Biomedical Datasets - BigScience Workshop 2021\"",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 1003999485,
    "title": "MENYO-20k repo has moved, updating URL",
    "dateCreated": "2021-09-17T19:01:54Z",
    "dateModified": "2021-09-17T19:01:54Z",
    "description": "Dataset repo moved to https://github.com/uds-lsv/menyo-20k_MT, now editing URL to match.\r\n\r\nhttps://github.com/uds-lsv/menyo-20k_MT/blob/master/data/train.tsv is the file we're looking for",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 1003999486,
    "title": "Take namespace into account in caching",
    "dateCreated": "2021-09-17T16:57:33Z",
    "dateModified": "2021-09-17T16:57:33Z",
    "description": "Loading a dataset \"username/dataset_name\" hosted by a user on the hub used to cache the dataset only taking into account the dataset name, and ignorign the username. Because of this, if a user later loads \"dataset_name\" without specifying the username, it would reload the dataset from the cache instead of failing.\r\n\r\nI changed the dataset cache and module cache mechanism to include the username in the name of the cache directory that is used:\r\n<s>\r\n`~/.cache/huggingface/datasets/username/dataset_name`  for the data\r\n`~/.cache/huggingface/modules/datasets_modules/datasets/username/dataset_name` for the python files\r\n</s>\r\nEDIT: actually using three underscores:\r\n`~/.cache/huggingface/datasets/username___dataset_name` for the data\r\n`~/.cache/huggingface/modules/datasets_modules/datasets/username___dataset_name` for the python files\r\n\r\nThis PR should fix the issue https://github.com/huggingface/datasets/issues/2842\r\n\r\ncc @stas00 ",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 1003999487,
    "title": "load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied",
    "dateCreated": "2021-09-17T16:52:10Z",
    "dateModified": "2021-09-17T16:52:10Z",
    "description": "## Describe the bug\r\nStandard process to download and load the wiki_bio dataset causes PermissionError in Windows 10 and 11.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset('wiki_bio')\r\n```\r\n\r\n## Expected results\r\nIt is expected that the dataset downloads without any errors.\r\n\r\n## Actual results\r\nPermissionError see trace below:\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\builder.py\", line 644, in download_and_prepare\r\n    self._save_info()\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\contextlib.py\", line 120, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\builder.py\", line 598, in incomplete_dir\r\n    os.rename(tmp_dir, dirname)\r\nPermissionError: [WinError 5] Access is denied: 'C:\\\\Users\\\\username\\\\.cache\\\\huggingface\\\\datasets\\\\wiki_bio\\\\default\\\\1.1.0\\\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9.incomplete' -> 'C:\\\\Users\\\\username\\\\.cache\\\\huggingface\\\\datasets\\\\wiki_bio\\\\default\\\\1.1.0\\\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9'\r\n```\r\nBy commenting out the os.rename() [L604](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L604) and the shutil.rmtree() [L607](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L607) lines, in my virtual environment, I was able to get the load process to complete, rename the directory manually and then rerun the `load_dataset('wiki_bio')` to get what I needed.\r\n\r\nIt seems that os.rename() in the `incomplete_dir` content manager is the culprit. Here's another project [Conan](https://github.com/conan-io/conan/issues/6560) with similar issue with os.rename() if it helps debug this issue.\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Windows-10-10.0.22449-SP0\r\n- Python version: 3.8.12\r\n- PyArrow version: 5.0.0\r\n",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 1003999488,
    "title": "Check that array is not Float as nan != nan",
    "dateCreated": "2021-09-17T16:16:41Z",
    "dateModified": "2021-09-17T16:16:41Z",
    "description": "The Exception wants to check for issues with StructArrays/ListArrays but catches FloatArrays with value nan as nan != nan.\r\nPass on FloatArrays as we should not raise an Exception for them.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 1003999489,
    "title": "Add Jigsaw unintended Bias",
    "dateCreated": "2021-09-17T16:12:31Z",
    "dateModified": "2021-09-17T16:12:31Z",
    "description": "Hi,\r\n\r\nHere's a first attempt at this dataset. Would be great if it could be merged relatively quickly as it is needed for Bigscience-related stuff. \r\n\r\nThis requires manual download, and I had some trouble generating dummy_data in this setting, so welcoming feedback there.",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 1003999490,
    "title": "to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows",
    "dateCreated": "2021-09-17T15:26:53Z",
    "dateModified": "2021-09-17T15:26:53Z",
    "description": "To reproduce:\r\n```python\r\nimport datasets as ds\r\nimport weakref\r\nimport gc\r\n\r\nd = ds.load_dataset(\"mnist\", split=\"train\")\r\nref = weakref.ref(d._data.table)\r\ntfd = d.to_tf_dataset(\"image\", batch_size=1, shuffle=False, label_cols=\"label\")\r\ndel tfd, d\r\ngc.collect()\r\nassert ref() is None, \"Error: there is at least one reference left\"\r\n```\r\n\r\nThis causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.\r\n\r\nMoreover the CI test of the `to_tf_dataset` method isn't able to clean up the temporary arrow files because of this.\r\n\r\ncc @Rocketknight1 ",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 1003999491,
    "title": "Replace script_version with revision",
    "dateCreated": "2021-09-17T14:04:39Z",
    "dateModified": "2021-09-17T14:04:39Z",
    "description": "As discussed in https://github.com/huggingface/datasets/pull/2718#discussion_r707013278, the parameter name `script_version` is no longer applicable to datasets without loading script (i.e., datasets only with raw data files).\r\n\r\nThis PR replaces the parameter name `script_version` with `revision`.\r\n\r\nThis way, we are also aligned with:\r\n- Transformers: `AutoTokenizer.from_pretrained(..., revision=...)`\r\n- Hub: `HfApi.dataset_info(..., revision=...)`, `HfApi.upload_file(..., revision=...)`",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 1003999492,
    "title": "Conda build fails",
    "dateCreated": "2021-09-17T12:49:22Z",
    "dateModified": "2021-09-17T12:49:22Z",
    "description": "## Describe the bug\r\nCurrent `datasets` version in conda is 1.9 instead of 1.12.\r\n\r\nThe build of the conda package fails.\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 1003999493,
    "title": "Fix bug in to_tf_dataset",
    "dateCreated": "2021-09-16T15:08:03Z",
    "dateModified": "2021-09-16T15:08:03Z",
    "description": "Replace `set_format()` to `with_format()` so that we don't alter the original dataset in `to_tf_dataset()`",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 1003999494,
    "title": "Mutable columns argument breaks set_format",
    "dateCreated": "2021-09-16T12:27:22Z",
    "dateModified": "2021-09-16T12:27:22Z",
    "description": "## Describe the bug\r\nIf you pass a mutable list to the `columns` argument of `set_format` and then change the list afterwards, the returned columns also change.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"glue\", \"cola\")\r\n\r\ncolumn_list = [\"idx\", \"label\"]\r\ndataset.set_format(\"python\", columns=column_list)\r\ncolumn_list[1] = \"foo\"  # Change the list after we call `set_format`\r\ndataset['train'][:4].keys()\r\n```\r\n\r\n## Expected results\r\n```python\r\ndict_keys(['idx', 'label'])\r\n```\r\n\r\n## Actual results\r\n```python\r\ndict_keys(['idx'])\r\n```",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 1003999495,
    "title": "Add regression test for null Sequence",
    "dateCreated": "2021-09-16T08:58:33Z",
    "dateModified": "2021-09-16T08:58:33Z",
    "description": "Relates to #2892 and #2900.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 1003999496,
    "title": "Update BibTeX entry",
    "dateCreated": "2021-09-16T08:39:20Z",
    "dateModified": "2021-09-16T08:39:20Z",
    "description": "Update BibTeX entry.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 1003999497,
    "title": "Datasets 1.12 dataset.filter TypeError: get_indices_from_mask_function() got an unexpected keyword argument",
    "dateCreated": "2021-09-16T01:14:02Z",
    "dateModified": "2021-09-16T01:14:02Z",
    "description": "## Describe the bug\r\nUpgrading to 1.12 caused `dataset.filter` call to fail with \r\n\r\n> get_indices_from_mask_function() got an unexpected keyword argument valid_rel_labels\r\n\r\n\r\n## Steps to reproduce the bug\r\n```pythondef \r\n\r\nfilter_good_rows(\r\n    ex: Dict,\r\n    valid_rel_labels: Set[str],\r\n    valid_ner_labels: Set[str],\r\n    tokenizer: PreTrainedTokenizerFast,\r\n) -> bool:\r\n    \"\"\"Get the good rows\"\"\"\r\n    encoding = get_encoding_for_text(text=ex[\"text\"], tokenizer=tokenizer)\r\n    ex[\"encoding\"] = encoding\r\n    for relation in ex[\"relations\"]:\r\n        if not is_valid_relation(relation, valid_rel_labels):\r\n            return False\r\n    for span in ex[\"spans\"]:\r\n        if not is_valid_span(span, valid_ner_labels, encoding):\r\n            return False\r\n    return True\r\n    \r\ndef get_dataset():    \r\n    loader_path = str(Path(__file__).parent / \"prodigy_dataset_builder.py\")\r\n    ds = load_dataset(\r\n        loader_path,\r\n        name=\"prodigy-dataset\",\r\n        data_files=sorted(file_paths),\r\n        cache_dir=cache_dir,\r\n    )[\"train\"]\r\n\r\n    valid_ner_labels = set(vocab.ner_category)\r\n    valid_relations = set(vocab.relation_types.keys())\r\n    ds = ds.filter(\r\n        filter_good_rows,\r\n        fn_kwargs=dict(\r\n            valid_rel_labels=valid_relations,\r\n            valid_ner_labels=valid_ner_labels,\r\n            tokenizer=vocab.tokenizer,\r\n        ),\r\n        keep_in_memory=True,\r\n        num_proc=num_proc,\r\n    )\r\n\r\n```\r\n\r\n`ds` is a `DatasetDict` produced by a jsonl dataset.\r\nThis runs fine on 1.11 but fails on 1.12\r\n\r\n**Stack Trace**\r\n\r\n\r\n\r\n## Expected results\r\n\r\nI expect 1.12 datasets filter to filter the dataset without raising as it does on 1.11\r\n\r\n## Actual results\r\n```\r\ntf_ner_rel_lib/dataset.py:695: in load_prodigy_arrow_datasets_from_jsonl\r\n    ds = ds.filter(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper\r\n    out = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2169: in filter\r\n    indices = self.map(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1686: in map\r\n    return self._map_single(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper\r\n    out = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2048: in _map_single\r\n    batch = apply_function_on_filtered_inputs(\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\ninputs = {'_input_hash': [2108817714, 1477695082, -1021597032, 2130671338, -1260483858, -1203431639, ...], '_task_hash': [18070...ons', 'relations', 'relations', ...], 'answer': ['accept', 'accept', 'accept', 'accept', 'accept', 'accept', ...], ...}\r\nindices = [0, 1, 2, 3, 4, 5, ...], check_same_num_examples = False, offset = 0\r\n\r\n    def apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples=False, offset=0):\r\n        \"\"\"Utility to apply the function on a selection of columns.\"\"\"\r\n        nonlocal update_data\r\n        fn_args = [inputs] if input_columns is None else [inputs[col] for col in input_columns]\r\n        if offset == 0:\r\n            effective_indices = indices\r\n        else:\r\n            effective_indices = [i + offset for i in indices] if isinstance(indices, list) else indices + offset\r\n        processed_inputs = (\r\n>           function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n        )\r\nE       TypeError: get_indices_from_mask_function() got an unexpected keyword argument 'valid_rel_labels'\r\n\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1939: TypeError\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: Mac\r\n- Python version: 3.8.9\r\n- PyArrow version: pyarrow==5.0.0\r\n\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 1003999498,
    "title": "Error when downloading datasets to non-traditional cache directories",
    "dateCreated": "2021-09-15T19:59:46Z",
    "dateModified": "2021-09-15T19:59:46Z",
    "description": "## Describe the bug\r\nWhen the cache directory is linked (soft link) to a directory on a NetApp device, the download fails. \r\n\r\n## Steps to reproduce the bug\r\n```bash\r\nln -s /path/to/netapp/.cache ~/.cache\r\n```\r\n\r\n```python\r\nload_dataset(\"imdb\")\r\n```\r\n\r\n## Expected results\r\nSuccessfully loading IMDB dataset\r\n\r\n## Actual results\r\n```\r\ndatasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=33432835, \r\nnum_examples=25000, dataset_name='imdb'), 'recorded': SplitInfo(name='train', num_bytes=0, num_examples=0,\r\n dataset_name='imdb')}, {'expected': SplitInfo(name='test', num_bytes=32650697, num_examples=25000, dataset_name='imdb'),\r\n 'recorded': SplitInfo(name='test', num_bytes=659932, num_examples=503, dataset_name='imdb')}, {'expected':\r\n SplitInfo(name='unsupervised', num_bytes=67106814, num_examples=50000, dataset_name='imdb'), 'recorded':\r\n SplitInfo(name='unsupervised', num_bytes=0, num_examples=0, dataset_name='imdb')}]\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.1.2\r\n- Platform: Ubuntu \r\n- Python version: 3.8\r\n\r\n## Extra notes\r\nStranger yet, trying to debug the phenomenon, I found the range of results to vary a lot without clear direction:\r\n - With `cache_dir=\"/path/to/netapp/.cache\"` the same thing happens.\r\n - However, when linking `~/netapp/` to `/path/to/netapp` *and* setting `cache_dir=\"~/netapp/.cache/huggingface/datasets\"` - it does work\r\n - On the other hand, when linking `~/.cache` to `~/netapp/.cache` without using `cache_dir`, it does work anymore.\r\n\r\nWhile I could test it only for a NetApp device, it might have to do with any other mounted FS.\r\n\r\nThanks :)\r\n\r\n",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 1003999499,
    "title": "Add tutorial for no-code dataset upload",
    "dateCreated": "2021-09-15T18:54:42Z",
    "dateModified": "2021-09-15T18:54:42Z",
    "description": "This PR is for a tutorial for uploading a dataset to the Hub. It relies on the Hub UI elements to upload a dataset, introduces the online tagging tool for creating tags, and the Dataset card template to get a head start on filling it out. The addition of this tutorial should make it easier for beginners to upload a dataset without accessing the terminal or knowing Git.",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 1003999500,
    "title": "\"File name too long\" error for file locks",
    "dateCreated": "2021-09-15T18:16:50Z",
    "dateModified": "2021-09-15T18:16:50Z",
    "description": "## Describe the bug\r\n\r\nGetting the following error when calling `load_dataset(\"gar1t/test\")`:\r\n\r\n```\r\nOSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'\r\n```\r\n\r\n## Steps to reproduce the bug\r\n\r\nWhere the user cache dir (e.g. `~/.cache`) is on a file system that limits filenames to 255 chars (e.g. ext4):\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"gar1t/test\")\r\n```\r\n\r\n## Expected results\r\n\r\nExpect the function to return without an error.\r\n\r\n## Actual results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/builder.py\", line 644, in download_and_prepare\r\n    self._save_info()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/builder.py\", line 765, in _save_info\r\n    with FileLock(lock_path):\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 323, in __enter__\r\n    self.acquire()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 272, in acquire\r\n    self._acquire()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 403, in _acquire\r\n    fd = os.open(self._lock_file, open_mode)\r\nOSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-27-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- PyArrow version: 5.0.0\r\n",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 1003999501,
    "title": "Loading an autonlp dataset raises in normal mode but not in streaming mode",
    "dateCreated": "2021-09-15T17:44:38Z",
    "dateModified": "2021-09-15T17:44:38Z",
    "description": "## Describe the bug\r\n\r\nThe same dataset (from autonlp) raises an error in normal mode, but does not raise in streaming mode\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(\"severo/autonlp-data-sentiment_detection-3c8bcd36\", split=\"train\", streaming=False)\r\n## raises an error\r\n\r\nload_dataset(\"severo/autonlp-data-sentiment_detection-3c8bcd36\", split=\"train\", streaming=True)\r\n## does not raise an error\r\n```\r\n\r\n## Expected results\r\n\r\nBoth calls should raise the same error\r\n\r\n## Actual results\r\n\r\nCall with streaming=False:\r\n\r\n```\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 5825.42it/s]\r\nUsing custom data configuration autonlp-data-sentiment_detection-3c8bcd36-fe30267462d1d42b\r\nDownloading and preparing dataset json/autonlp-data-sentiment_detection-3c8bcd36 to /home/slesage/.cache/huggingface/datasets/json/autonlp-data-sentiment_detection-3c8bcd36-fe30267462d1d42b/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 15923.71it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 3346.88it/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 636, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 726, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 1187, in _prepare_split\r\n    writer.write_table(table)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 418, in write_table\r\n    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 418, in <listcomp>\r\n    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\r\n  File \"pyarrow/table.pxi\", line 1249, in pyarrow.lib.Table.__getitem__\r\n  File \"pyarrow/table.pxi\", line 1825, in pyarrow.lib.Table.column\r\n  File \"pyarrow/table.pxi\", line 1800, in pyarrow.lib.Table._ensure_integer_index\r\nKeyError: 'Field \"splits\" does not exist in table schema'\r\n```\r\n\r\nCall with `streaming=False`:\r\n\r\n```\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6000.43it/s]\r\nUsing custom data configuration autonlp-data-sentiment_detection-3c8bcd36-fe30267462d1d42b\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 46916.15it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 148734.18it/s]\r\n```\r\n\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.1.dev0\r\n- Platform: Linux-5.11.0-1017-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 1003999502,
    "title": "Fix conversion of multidim arrays in list to arrow",
    "dateCreated": "2021-09-15T17:21:36Z",
    "dateModified": "2021-09-15T17:21:36Z",
    "description": "Arrow only supports 1-dim arrays. Previously we were converting all the numpy arrays to python list before instantiating arrow arrays to workaround this limitation.\r\nHowever in #2361 we started to keep numpy arrays in order to keep their dtypes.\r\nIt works when we pass any multi-dim numpy array (the conversion to arrow has been added on our side), but not for lists of multi-dim numpy arrays.\r\n\r\nIn this PR I added two strategies:\r\n- one that takes a list of multi-dim numpy arrays on returns an arrow array in an optimized way (more common case)\r\n- one that takes a list of possibly very nested data (lists, dicts, tuples) containing multi-dim arrays. This one is less optimized since it converts all the multi-dim numpy arrays into lists of 1-d arrays for compatibility with arrow. This strategy is simpler that just trying to create the arrow array from a possibly very nested data structure, but in the future we can improve it if needed.\r\n\r\nFix https://github.com/huggingface/datasets/issues/2921",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 1003999503,
    "title": "Using a list of multi-dim numpy arrays raises an error \"can only convert 1-dimensional array values\"",
    "dateCreated": "2021-09-15T17:12:11Z",
    "dateModified": "2021-09-15T17:12:11Z",
    "description": "This error has been introduced in https://github.com/huggingface/datasets/pull/2361\r\n\r\nTo reproduce:\r\n```python\r\nimport numpy as np\r\nfrom datasets import Dataset\r\n\r\nd = Dataset.from_dict({\"a\": [np.zeros((2, 2))]})\r\n```\r\nraises\r\n```python\r\nTraceback (most recent call last):\r\n  File \"playground/ttest.py\", line 5, in <module>\r\n    d = Dataset.from_dict({\"a\": [np.zeros((2, 2))]}).with_format(\"torch\")\r\n  File \"/Users/quentinlhoest/Desktop/hf/nlp/src/datasets/arrow_dataset.py\", line 458, in from_dict\r\n    pa_table = InMemoryTable.from_pydict(mapping=mapping)\r\n  File \"/Users/quentinlhoest/Desktop/hf/nlp/src/datasets/table.py\", line 365, in from_pydict\r\n    return cls(pa.Table.from_pydict(*args, **kwargs))\r\n  File \"pyarrow/table.pxi\", line 1639, in pyarrow.lib.Table.from_pydict\r\n  File \"pyarrow/array.pxi\", line 332, in pyarrow.lib.asarray\r\n  File \"pyarrow/array.pxi\", line 223, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/Users/quentinlhoest/Desktop/hf/nlp/src/datasets/arrow_writer.py\", line 107, in __arrow_array__\r\n    out = pa.array(self.data, type=type)\r\n  File \"pyarrow/array.pxi\", line 306, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 39, in pyarrow.lib._sequence_to_array\r\n  File \"pyarrow/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Can only convert 1-dimensional array values",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 1003999504,
    "title": "Fix unwanted tqdm bar when accessing examples",
    "dateCreated": "2021-09-15T17:09:11Z",
    "dateModified": "2021-09-15T17:09:11Z",
    "description": "A change in #2814 added bad progress bars in `map_nested`. Now they're disabled by default\r\n\r\nFix #2919 ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 1003999505,
    "title": "Unwanted progress bars when accessing examples",
    "dateCreated": "2021-09-15T14:05:10Z",
    "dateModified": "2021-09-15T14:05:10Z",
    "description": "When accessing examples from a dataset formatted for pytorch, some progress bars appear when accessing examples:\r\n```python\r\nIn [1]: import datasets as ds                                        \r\n\r\nIn [2]: d = ds.Dataset.from_dict({\"a\": [0, 1, 2]}).with_format(\"torch\")                                                           \r\n\r\nIn [3]: d[0]                                                         \r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 3172.70it/s]\r\nOut[3]: {'a': tensor(0)}\r\n```\r\n\r\nThis is because the pytorch formatter calls `map_nested` that uses progress bars\r\n\r\ncc @sgugger ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 1003999506,
    "title": "`Can not decode content-encoding: gzip` when loading `scitldr` dataset with streaming",
    "dateCreated": "2021-09-15T13:06:07Z",
    "dateModified": "2021-09-15T13:06:07Z",
    "description": "## Describe the bug\r\n\r\nTrying to load the `\"FullText\"` config of the `\"scitldr\"` dataset with `streaming=True` raises an error from `aiohttp`:\r\n```python\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\ncc @lhoestq \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\niter_dset = iter(\r\n    load_dataset(\"scitldr\", name=\"FullText\", split=\"test\", streaming=True)\r\n)\r\n\r\nnext(iter_dset)\r\n```\r\n\r\n## Expected results\r\nReturns the first sample of the dataset\r\n\r\n## Actual results\r\nCalling `__next__` crashes with the following Traceback:\r\n\r\n```python\r\n----> 1 next(dset_iter)\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n    339\r\n    340     def __iter__(self):\r\n--> 341         for key, example in self._iter():\r\n    342             if self.features:\r\n    343                 # we encode the example for ClassLabel feature types for example\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in _iter(self)\r\n    336         else:\r\n    337             ex_iterable = self._ex_iterable\r\n--> 338         yield from ex_iterable\r\n    339\r\n    340     def __iter__(self):\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n     76\r\n     77     def __iter__(self):\r\n---> 78         for key, example in self.generate_examples_fn(**self.kwargs):\r\n     79             yield key, example\r\n     80\r\n\r\n~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\scitldr\\72d6e2195786c57e1d343066fb2cc4f93ea39c5e381e53e6ae7c44bbfd1f05ef\\scitldr.py in _generate_examples(self, filepath, split)\r\n    162\r\n    163         with open(filepath, encoding=\"utf-8\") as f:\r\n--> 164             for id_, row in enumerate(f):\r\n    165                 data = json.loads(row)\r\n    166                 if self.config.name == \"AIC\":\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\implementations\\http.py in read(self, length)\r\n    496         else:\r\n    497             length = min(self.size - self.loc, length)\r\n--> 498         return super().read(length)\r\n    499\r\n    500     async def async_fetch_all(self):\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\spec.py in read(self, length)\r\n   1481             # don't even bother calling fetch\r\n   1482             return b\"\"\r\n-> 1483         out = self.cache._fetch(self.loc, self.loc + length)\r\n   1484         self.loc += len(out)\r\n   1485         return out\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\caching.py in _fetch(self, start, end)\r\n    378         elif start < self.start:\r\n    379             if self.end - end > self.blocksize:\r\n--> 380                 self.cache = self.fetcher(start, bend)\r\n    381                 self.start = start\r\n    382             else:\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in wrapper(*args, **kwargs)\r\n     86     def wrapper(*args, **kwargs):\r\n     87         self = obj or args[0]\r\n---> 88         return sync(self.loop, func, *args, **kwargs)\r\n     89\r\n     90     return wrapper\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in sync(loop, func, timeout, *args, **kwargs)\r\n     67         raise FSTimeoutError\r\n     68     if isinstance(result[0], BaseException):\r\n---> 69         raise result[0]\r\n     70     return result[0]\r\n     71\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in _runner(event, coro, result, timeout)\r\n     23         coro = asyncio.wait_for(coro, timeout=timeout)\r\n     24     try:\r\n---> 25         result[0] = await coro\r\n     26     except Exception as ex:\r\n     27         result[0] = ex\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\implementations\\http.py in async_fetch_range(self, start, end)\r\n    538             if r.status == 206:\r\n    539                 # partial content, as expected\r\n--> 540                 out = await r.read()\r\n    541             elif \"Content-Length\" in r.headers:\r\n    542                 cl = int(r.headers[\"Content-Length\"])\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\aiohttp\\client_reqrep.py in read(self)\r\n   1030         if self._body is None:\r\n   1031             try:\r\n-> 1032                 self._body = await self.content.read()\r\n   1033                 for trace in self._traces:\r\n   1034                     await trace.send_response_chunk_received(\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\aiohttp\\streams.py in read(self, n)\r\n    342     async def read(self, n: int = -1) -> bytes:\r\n    343         if self._exception is not None:\r\n--> 344             raise self._exception\r\n    345\r\n    346         # migration problem; with DataQueue you have to catch\r\n\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.8.5\r\n- PyArrow version: 2.0.0\r\n- aiohttp version: 3.7.4.post0\r\n",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 1003999507,
    "title": "windows download abnormal",
    "dateCreated": "2021-09-15T12:45:35Z",
    "dateModified": "2021-09-15T12:45:35Z",
    "description": "## Describe the bug\r\nThe script clearly exists (accessible from the browser), but the script download fails on windows. Then I tried it again and it can be downloaded normally on linux. why??\r\n## Steps to reproduce the bug\r\n```python3.7 + windows\r\n![image](https://user-images.githubusercontent.com/52347799/133436174-4303f847-55d5-434f-a749-08da3bb9b654.png)\r\n\r\n\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\nIt can be downloaded normally.\r\n\r\n## Actual results\r\nit cann't\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:1.11.0\r\n- Platform:windows\r\n- Python version:3.7\r\n- PyArrow version:\r\n",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 1003999508,
    "title": "Add OpenAI's pass@k code evaluation metric",
    "dateCreated": "2021-09-15T12:05:43Z",
    "dateModified": "2021-09-15T12:05:43Z",
    "description": "This PR introduces the `code_eval` metric which implements [OpenAI's code evaluation harness](https://github.com/openai/human-eval) introduced in the [Codex paper](https://arxiv.org/abs/2107.03374). It is heavily based on the original implementation and just adapts the interface to follow the `predictions`/`references` convention.\r\n\r\nThe addition of this metric should enable the evaluation against the code evaluation datasets added in #2897 and #2893.\r\n\r\nA few open questions:\r\n\r\n- The implementation makes heavy use of multiprocessing which this PR does not touch. Is this conflicting with multiprocessing natively integrated in `datasets`?\r\n- This metric executes generated Python code and as such it poses dangers of executing malicious code. OpenAI addresses this issue by 1) commenting the `exec` call in the code so the user has to actively uncomment it and read the warning and 2) suggests using a sandbox environment (gVisor container). Should we add a similar safeguard? E.g. a prompt that needs to be answered when initialising the metric? Or at least a warning message?\r\n- Naming: the implementation sticks to the `predictions`/`references` naming, however, the references are not reference solutions but unittest to test the solution. While reference solutions are also available they are not used. Should the naming be adapted?",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 1003999509,
    "title": "Fix fsspec AbstractFileSystem access",
    "dateCreated": "2021-09-15T09:39:20Z",
    "dateModified": "2021-09-15T09:39:20Z",
    "description": "This addresses the issue from #2914 by changing the way fsspec's AbstractFileSystem is accessed.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 1003999510,
    "title": "Having a dependency defining fsspec entrypoint raises an AttributeError when importing datasets",
    "dateCreated": "2021-09-15T07:54:06Z",
    "dateModified": "2021-09-15T07:54:06Z",
    "description": "## Describe the bug\r\nIn one of my project, I defined a custom fsspec filesystem with an entrypoint.\r\nMy guess is that by doing so, a variable named `spec` is created in the module `fsspec` (created by entering a for loop as there are entrypoints defined, see the loop in question [here](https://github.com/intake/filesystem_spec/blob/0589358d8a029ed6b60d031018f52be2eb721291/fsspec/__init__.py#L55)).\r\nSo that `fsspec.spec`, that was previously referring to the `spec` submodule, is now referring to that `spec` variable.\r\nThis make the import of datasets failing as it is using that `fsspec.spec`.\r\n\r\n## Steps to reproduce the bug\r\nI could reproduce the bug with a dummy poetry project.\r\n\r\nHere is the pyproject.toml:\r\n```toml\r\n[tool.poetry]\r\nname = \"debug-datasets\"\r\nversion = \"0.1.0\"\r\ndescription = \"\"\r\nauthors = [\"Pierre Godard\"]\r\n\r\n[tool.poetry.dependencies]\r\npython = \"^3.8\"\r\ndatasets = \"^1.11.0\"\r\n\r\n[tool.poetry.dev-dependencies]\r\n\r\n[build-system]\r\nrequires = [\"poetry-core>=1.0.0\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n\r\n[tool.poetry.plugins.\"fsspec.specs\"]\r\n\"file2\" = \"fsspec.implementations.local.LocalFileSystem\"\r\n```\r\n\r\nThe only other file being a `debug_datasets/__init__.py` empty file.\r\n\r\nThe overall structure of the project is as follows:\r\n```\r\n.\r\n\u251c\u2500\u2500 pyproject.toml\r\n\u2514\u2500\u2500 debug_datasets\r\n    \u2514\u2500\u2500 __init__.py\r\n```\r\n\r\nThen, within the project folder run:\r\n\r\n```\r\npoetry install\r\npoetry run python\r\n```\r\n\r\nAnd in the python interpreter, try to import `datasets`:\r\n\r\n```\r\nimport datasets\r\n```\r\n\r\n## Expected results\r\nThe import should run successfully.\r\n\r\n## Actual results\r\n\r\nHere is the trace of the error I get:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/__init__.py\", line 33, in <module>\r\n    from .arrow_dataset import Dataset, concatenate_datasets\r\n  File \"/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 48, in <module>\r\n    from .filesystems import extract_path_from_uri, is_remote_filesystem\r\n  File \"/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/filesystems/__init__.py\", line 30, in <module>\r\n    def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:\r\nAttributeError: 'EntryPoint' object has no attribute 'AbstractFileSystem'\r\n```\r\n\r\n## Suggested fix\r\n\r\n`datasets/filesystems/__init__.py`, line 30, replace:\r\n```\r\n    def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:\r\n```\r\nby:\r\n```\r\n    def is_remote_filesystem(fs: fsspec.AbstractFileSystem) -> bool:\r\n```\r\n\r\nI will come up with a PR soon if this effectively solves the issue.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: WSL2 (Ubuntu 20.04.1 LTS)\r\n- Python version: 3.8.5\r\n- PyArrow version: 5.0.0\r\n- `fsspec` version: 2021.8.1\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 1003999511,
    "title": "timit_asr dataset only includes one text phrase",
    "dateCreated": "2021-09-14T21:06:07Z",
    "dateModified": "2021-09-14T21:06:07Z",
    "description": "## Describe the bug\r\nThe dataset 'timit_asr' only includes one text phrase. It only includes the transcription \"Would such an act of refusal be useful?\" multiple times rather than different phrases.\r\n\r\n## Steps to reproduce the bug\r\nNote: I am following the tutorial https://huggingface.co/blog/fine-tune-wav2vec2-english\r\n\r\n1. Install the dataset and other packages\r\n```python\r\n!pip install datasets>=1.5.0\r\n!pip install transformers==4.4.0\r\n!pip install soundfile\r\n!pip install jiwer\r\n```\r\n2. Load the dataset\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\n\r\ntimit = load_dataset(\"timit_asr\")\r\n```\r\n3. Remove columns that we don't want\r\n```python\r\ntimit = timit.remove_columns([\"phonetic_detail\", \"word_detail\", \"dialect_region\", \"id\", \"sentence_type\", \"speaker_id\"])\r\n```\r\n4. Write a short function to display some random samples of the dataset.\r\n```python\r\nfrom datasets import ClassLabel\r\nimport random\r\nimport pandas as pd\r\nfrom IPython.display import display, HTML\r\n\r\ndef show_random_elements(dataset, num_examples=10):\r\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\r\n    picks = []\r\n    for _ in range(num_examples):\r\n        pick = random.randint(0, len(dataset)-1)\r\n        while pick in picks:\r\n            pick = random.randint(0, len(dataset)-1)\r\n        picks.append(pick)\r\n    \r\n    df = pd.DataFrame(dataset[picks])\r\n    display(HTML(df.to_html()))\r\n\r\nshow_random_elements(timit[\"train\"].remove_columns([\"file\"]))\r\n```\r\n\r\n## Expected results\r\n10 random different transcription phrases.\r\n\r\n## Actual results\r\n10 of the same transcription phrase \"Would such an act of refusal be useful?\"\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.4.1\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyArrow version: not listed\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 1003999512,
    "title": "Update link to Blog in docs footer",
    "dateCreated": "2021-09-14T17:23:14Z",
    "dateModified": "2021-09-14T17:23:14Z",
    "description": "Update link.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 1003999513,
    "title": "Fix exception chaining",
    "dateCreated": "2021-09-14T16:19:29Z",
    "dateModified": "2021-09-14T16:19:29Z",
    "description": "Fix exception chaining to avoid tracebacks with message: `During handling of the above exception, another exception occurred:`",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 1003999514,
    "title": "feat: \ud83c\udfb8 pass additional arguments to get private configs + info",
    "dateCreated": "2021-09-14T15:24:19Z",
    "dateModified": "2021-09-14T15:24:19Z",
    "description": "`use_auth_token` can now be passed to the functions to get the configs\r\nor infos of private datasets on the hub",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 1003999515,
    "title": "fix anli splits",
    "dateCreated": "2021-09-14T13:10:35Z",
    "dateModified": "2021-09-14T13:10:35Z",
    "description": "I can't run the tests for dummy data, facing this error \r\n\r\n`ImportError while loading conftest '/home/zaid/tmp/fix_anli_splits/datasets/tests/conftest.py'.\r\ntests/conftest.py:10: in <module>\r\n    from datasets import config\r\nE   ImportError: cannot import name 'config' from 'datasets' (unknown location)`",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 1003999516,
    "title": "Update Zenodo metadata with creator names and affiliation",
    "dateCreated": "2021-09-14T12:39:37Z",
    "dateModified": "2021-09-14T12:39:37Z",
    "description": "This PR helps in prefilling author data when automatically generating the DOI after each release.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 1003999517,
    "title": "add story_cloze dataset",
    "dateCreated": "2021-09-14T12:36:53Z",
    "dateModified": "2021-09-14T12:36:53Z",
    "description": "@lhoestq I have spent some time but I still I can't succeed in correctly testing the dummy_data.",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 1003999518,
    "title": "feat: \ud83c\udfb8 add a function to get a dataset config's split names",
    "dateCreated": "2021-09-14T12:31:22Z",
    "dateModified": "2021-09-14T12:31:22Z",
    "description": "Also: pass additional arguments (use_auth_token) to get private configs + info of private datasets on the hub\r\n\r\nQuestions:\r\n\r\n- <strike>I'm not sure how the versions work: I changed 1.12.1.dev0 to 1.12.1.dev1, was it correct?</strike> no -> reverted\r\n- Should I add a section in https://github.com/huggingface/datasets/blob/master/docs/source/load_hub.rst? (there is no section for get_dataset_infos)",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 1003999519,
    "title": "Update BibTeX entry",
    "dateCreated": "2021-09-14T10:16:17Z",
    "dateModified": "2021-09-14T10:16:17Z",
    "description": "Update BibTeX entry.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 1003999520,
    "title": "FORCE_REDOWNLOAD does not work",
    "dateCreated": "2021-09-14T09:45:26Z",
    "dateModified": "2021-09-14T09:45:26Z",
    "description": "## Describe the bug\r\nWith GenerateMode.FORCE_REDOWNLOAD, the documentation says \r\n    +------------------------------------+-----------+---------+\r\n    |                                    | Downloads | Dataset |\r\n    +====================================+===========+=========+\r\n    | `REUSE_DATASET_IF_EXISTS` (default)| Reuse     | Reuse   |\r\n    +------------------------------------+-----------+---------+\r\n    | `REUSE_CACHE_IF_EXISTS`            | Reuse     | Fresh   |\r\n    +------------------------------------+-----------+---------+\r\n    | `FORCE_REDOWNLOAD`                 | Fresh     | Fresh   |\r\n    +------------------------------------+-----------+---------+\r\n\r\nHowever, the old dataset is loaded even when FORCE_REDOWNLOAD is chosen.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n\r\nimport pandas as pd\r\nfrom datasets import load_dataset, GenerateMode\r\npd.DataFrame(range(5), columns=['numbers']).to_csv('/tmp/test.tsv.gz', index=False)\r\nee = load_dataset('csv', data_files=['/tmp/test.tsv.gz'], delimiter='\\t', split='train', download_mode=GenerateMode.FORCE_REDOWNLOAD)\r\nprint(ee)\r\npd.DataFrame(range(10), columns=['numerals']).to_csv('/tmp/test.tsv.gz', index=False)\r\nee = load_dataset('csv', data_files=['/tmp/test.tsv.gz'], delimiter='\\t', split='train', download_mode=GenerateMode.FORCE_REDOWNLOAD)\r\nprint(ee)\r\n\r\n```\r\n\r\n## Expected results\r\nDataset({\r\n    features: ['numbers'],\r\n    num_rows: 5\r\n})\r\nDataset({\r\n    features: ['numerals'],\r\n    num_rows: 10\r\n})\r\n\r\n## Actual results\r\nDataset({\r\n    features: ['numbers'],\r\n    num_rows: 5\r\n})\r\nDataset({\r\n    features: ['numbers'],\r\n    num_rows: 5\r\n})\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-4.14.181-108.257.amzn1.x86_64-x86_64-with-glibc2.10\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 1003999521,
    "title": "Fix xpathopen to accept positional arguments",
    "dateCreated": "2021-09-14T08:02:50Z",
    "dateModified": "2021-09-14T08:02:50Z",
    "description": "Fix `xpathopen()` so that it also accepts positional arguments.\r\n\r\nFix #2901.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 1003999522,
    "title": "Add WIT Dataset",
    "dateCreated": "2021-09-13T19:38:49Z",
    "dateModified": "2021-09-13T19:38:49Z",
    "description": "## Adding a Dataset\r\n- **Name:** *WIT*\r\n- **Description:** *Wikipedia-based Image Text Dataset*\r\n- **Paper:** *[WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning\r\n](https://arxiv.org/abs/2103.01913)*\r\n- **Data:** *https://github.com/google-research-datasets/wit*\r\n- **Motivation:**  (excerpt from their Github README.md)\r\n\r\n> - The largest multimodal dataset (publicly available at the time of this writing) by the number of image-text examples.\r\n> - A massively multilingual dataset (first of its kind) with coverage for over 100+ languages.\r\n> - A collection of diverse set of concepts and real world entities.\r\n> - Brings forth challenging real-world test sets.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 1003999523,
    "title": "Incompatibility with pytest",
    "dateCreated": "2021-09-13T19:12:17Z",
    "dateModified": "2021-09-13T19:12:17Z",
    "description": "## Describe the bug\r\n\r\npytest complains about xpathopen / path.open(\"w\")\r\n\r\n## Steps to reproduce the bug\r\n\r\nCreate a test file, `test.py`:\r\n\r\n```python\r\nimport datasets as ds\r\ndef load_dataset():\r\n    ds.load_dataset(\"counter\", split=\"train\", streaming=True)\r\n```\r\n\r\nAnd launch it with pytest:\r\n\r\n```bash\r\npython -m pytest test.py\r\n```\r\n\r\n## Expected results\r\n\r\nIt should give something like:\r\n\r\n```\r\ncollected 1 item\r\n\r\ntest.py .                                                                                                                                                                                                                                             [100%]\r\n\r\n======= 1 passed in 3.15s =======\r\n```\r\n\r\n## Actual results\r\n\r\n```\r\n============================================================================================================================= test session starts ==============================================================================================================================\r\nplatform linux -- Python 3.8.11, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\r\nrootdir: /home/slesage/hf/datasets-preview-backend, configfile: pyproject.toml\r\nplugins: anyio-3.3.1\r\ncollected 1 item\r\n\r\ntests/queries/test_rows.py .                                                                                                                                                                                                                                             [100%]Traceback (most recent call last):\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pytest/__main__.py\", line 5, in <module>\r\n    raise SystemExit(pytest.console_main())\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/config/__init__.py\", line 185, in console_main\r\n    code = main()\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/config/__init__.py\", line 162, in main\r\n    ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_hooks.py\", line 265, in __call__\r\n    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_manager.py\", line 80, in _hookexec\r\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 60, in _multicall\r\n    return outcome.get_result()\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_result.py\", line 60, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 39, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/main.py\", line 316, in pytest_cmdline_main\r\n    return wrap_session(config, _main)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/main.py\", line 304, in wrap_session\r\n    config.hook.pytest_sessionfinish(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_hooks.py\", line 265, in __call__\r\n    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_manager.py\", line 80, in _hookexec\r\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 55, in _multicall\r\n    gen.send(outcome)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/terminal.py\", line 803, in pytest_sessionfinish\r\n    outcome.get_result()\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_result.py\", line 60, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 39, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/cacheprovider.py\", line 428, in pytest_sessionfinish\r\n    config.cache.set(\"cache/nodeids\", sorted(self.cached_nodeids))\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/cacheprovider.py\", line 188, in set\r\n    f = path.open(\"w\")\r\nTypeError: xpathopen() takes 1 positional argument but 2 were given\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.0\r\n- Platform: Linux-5.11.0-1017-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 1003999524,
    "title": "Fix null sequence encoding",
    "dateCreated": "2021-09-13T13:55:08Z",
    "dateModified": "2021-09-13T13:55:08Z",
    "description": "The Sequence feature encoding was failing when a `None` sequence was used in a dataset.\r\n\r\nFix https://github.com/huggingface/datasets/issues/2892",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 1003999525,
    "title": "Dataset",
    "dateCreated": "2021-09-12T07:38:53Z",
    "dateModified": "2021-09-12T07:38:53Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 1003999526,
    "title": "Hug emoji",
    "dateCreated": "2021-09-12T03:27:51Z",
    "dateModified": "2021-09-12T03:27:51Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 1003999527,
    "title": "Add OpenAI's HumanEval dataset",
    "dateCreated": "2021-09-11T09:37:47Z",
    "dateModified": "2021-09-11T09:37:47Z",
    "description": "This PR adds OpenAI's [HumanEval](https://github.com/openai/human-eval) dataset. The dataset consists of 164 handcrafted programming problems with solutions and unittests to verify solution. This dataset is useful to evaluate code generation models.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 1003999528,
    "title": "add multi-proc in `to_csv`",
    "dateCreated": "2021-09-10T21:35:09Z",
    "dateModified": "2021-09-10T21:35:09Z",
    "description": "This PR extends the multi-proc method used in #2747 for`to_json` to `to_csv` as well. \r\n\r\nResults on my machine post benchmarking on `ascent_kb` dataset (giving ~45% improvement when compared to num_proc = 1):\r\n```\r\nTime taken on 1 num_proc, 10000 batch_size  674.2055702209473\r\nTime taken on 4 num_proc, 10000 batch_size  425.6553490161896\r\n\r\nTime taken on 1 num_proc, 50000 batch_size  623.5897650718689\r\nTime taken on 4 num_proc, 50000 batch_size  380.0402421951294\r\n\r\nTime taken on 4 num_proc, 100000 batch_size  361.7168130874634\r\n```\r\nThis is a WIP as writing tests is pending for this PR. \r\n\r\nI'm also exploring [this](https://arrow.apache.org/docs/python/csv.html#incremental-writing) approach for which I'm using `pyarrow-5.0.0`.\r\n",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 1003999529,
    "title": "Use pyarrow.Table.replace_schema_metadata instead of pyarrow.Table.cast",
    "dateCreated": "2021-09-10T17:56:57Z",
    "dateModified": "2021-09-10T17:56:57Z",
    "description": "This PR partially addresses #2252.\r\n\r\n``update_metadata_with_features`` uses ``Table.cast`` which slows down ``load_from_disk`` (and possibly other methods that use it) for very large datasets. Since ``update_metadata_with_features`` is only updating the schema metadata, it makes more sense to use ``pyarrow.Table.replace_schema_metadata`` which is much faster. This PR adds a ``replace_schema_metadata`` method to all table classes, and modifies  ``update_metadata_with_features`` to use it instead of ``cast``.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 1003999530,
    "title": "Fix COUNTER dataset",
    "dateCreated": "2021-09-10T16:07:29Z",
    "dateModified": "2021-09-10T16:07:29Z",
    "description": "Fix filename generating `FileNotFoundError`.\r\n\r\nRelated to #2866.\r\nCC: @severo.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 1003999531,
    "title": "add mbpp dataset",
    "dateCreated": "2021-09-10T15:27:30Z",
    "dateModified": "2021-09-10T15:27:30Z",
    "description": "This PR adds the mbpp dataset introduced by Google [here](https://github.com/google-research/google-research/tree/master/mbpp) as mentioned in #2816.\r\n\r\nThe dataset contain two versions: a full and a sanitized one. They have a slightly different schema and it is current state the loading preserves the original schema. An open question is whether to harmonize the two schemas when loading the dataset or to preserve the original one. Since not all fields are overlapping the schema will not be exactly the same.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 1003999532,
    "title": "Error when encoding a dataset with None objects with a Sequence feature",
    "dateCreated": "2021-09-10T14:11:43Z",
    "dateModified": "2021-09-10T14:11:43Z",
    "description": "There is an error when encoding a dataset with None objects with a Sequence feature\r\n\r\nTo reproduce:\r\n```python\r\nfrom datasets import Dataset, Features, Value, Sequence\r\ndata = {\"a\": [[0], None]}\r\nfeatures = Features({\"a\": Sequence(Value(\"int32\"))})\r\ndataset = Dataset.from_dict(data, features=features)\r\n```\r\nraises\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-24-40add67f8751> in <module>\r\n      2 data = {\"a\": [[0], None]}\r\n      3 features = Features({\"a\": Sequence(Value(\"int32\"))})\r\n----> 4 dataset = Dataset.from_dict(data, features=features)\r\n[...]\r\n~/datasets/features.py in encode_nested_example(schema, obj)\r\n    888         if isinstance(obj, str):  # don't interpret a string as a list\r\n    889             raise ValueError(\"Got a string but expected a list instead: '{}'\".format(obj))\r\n--> 890         return [encode_nested_example(schema.feature, o) for o in obj]\r\n    891     # Object with special encoding:\r\n    892     # ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\r\n\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\n\r\nInstead, if should run without error, as if the `features` were not passed",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 1003999533,
    "title": "[WIP] Allow dynamic first dimension for ArrayXD",
    "dateCreated": "2021-09-10T11:52:52Z",
    "dateModified": "2021-09-10T11:52:52Z",
    "description": "Add support for dynamic first dimension for ArrayXD features. See issue [#887](https://github.com/huggingface/datasets/issues/887).\r\nFollowing changes allow for `to_pylist` method of `ArrayExtensionArray` to return a list of numpy arrays where fist dimension can vary.\r\n\r\n@lhoestq Could you suggest how you want to extend test suit. For now I added only very limited testing.",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 1003999534,
    "title": "0x290B112ED1280537B24Ee6C268a004994a16e6CE",
    "dateCreated": "2021-09-10T09:51:17Z",
    "dateModified": "2021-09-10T09:51:17Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 1003999535,
    "title": "Coc",
    "dateCreated": "2021-09-10T07:32:07Z",
    "dateModified": "2021-09-10T07:32:07Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 1003999536,
    "title": "v1.11.1 release date",
    "dateCreated": "2021-09-09T21:53:15Z",
    "dateModified": "2021-09-09T21:53:15Z",
    "description": "Hello, i need to use latest features in one of my packages but there have been no new datasets release since 2 months ago.\r\n\r\nWhen do you plan to publush v1.11.1 release?",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 1003999537,
    "title": "#2837 Use cache folder for lockfile",
    "dateCreated": "2021-09-09T19:55:56Z",
    "dateModified": "2021-09-09T19:55:56Z",
    "description": "Fixes #2837 \r\n\r\nUse a cache folder directory to store the FileLock.\r\n\r\nThe issue was that the lock file was in a readonly folder.\r\n",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 1003999538,
    "title": "Hj",
    "dateCreated": "2021-09-09T18:58:52Z",
    "dateModified": "2021-09-09T18:58:52Z",
    "description": null,
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 1003999539,
    "title": "Adding an Elastic Search index to a Dataset",
    "dateCreated": "2021-09-09T12:21:39Z",
    "dateModified": "2021-09-09T12:21:39Z",
    "description": "## Describe the bug\r\nWhen trying to index documents from the squad dataset, the connection to ElasticSearch seems to break:\r\n\r\nReusing dataset squad (/Users/andreasmotz/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\r\n 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589     | 9501/10570 [00:01<00:00, 6335.61docs/s]\r\n\r\nNo error is thrown, but the indexing breaks ~90%.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom datasets import load_dataset\r\nfrom elasticsearch import Elasticsearch\r\nes = Elasticsearch()\r\nsquad = load_dataset('squad', split='validation')\r\nindex_name = \"corpus\"\r\nes_config = {\r\n    \"settings\": {\r\n        \"number_of_shards\": 1,\r\n        \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\r\n    },\r\n    \"mappings\": {\r\n        \"properties\": {\r\n            \"idx\" : {\"type\" : \"keyword\"},\r\n            \"title\" : {\"type\" : \"keyword\"},\r\n            \"text\": {\r\n                \"type\": \"text\",\r\n                \"analyzer\": \"standard\",\r\n                \"similarity\": \"BM25\"\r\n            },\r\n        }\r\n    },\r\n}\r\nclass IndexBuilder:\r\n    \"\"\"\r\n    Elastic search indexing of a corpus\r\n    \"\"\"\r\n    def __init__(\r\n        self,\r\n        *args,\r\n        #corpus : None,\r\n        dataset : squad,\r\n        index_name = str,\r\n        query = str,\r\n        config = dict,\r\n        **kwargs,\r\n    ):\r\n        #instantiate HuggingFace dataset\r\n        self.dataset = dataset\r\n        #instantiate ElasticSearch config\r\n        self.config = config\r\n        self.es = Elasticsearch()\r\n        self.index_name = index_name\r\n        self.query = query\r\n    def elastic_index(self):\r\n        print(self.es.info)\r\n        self.es.indices.delete(index=self.index_name, ignore=[400, 404])\r\n        search_index = self.dataset.add_elasticsearch_index(column='context', host='localhost', port='9200', es_index_name=self.index_name, es_index_config=self.config)\r\n        return search_index\r\n    def exact_match_method(self, index):\r\n        scores, retrieved_examples = index.get_nearest_examples('context', query=self.query, k=1)\r\n        return scores, retrieved_examples\r\nif __name__ == \"__main__\":\r\n    print(type(squad))\r\n    Index = IndexBuilder(dataset=squad, index_name='corpus_index', query='Where was Chopin born?', config=es_config)\r\n    search_index = Index.elastic_index()\r\n    scores, examples = Index.exact_match_method(search_index)\r\n    print(scores, examples)\r\n    for name in squad.column_names:\r\n        print(type(squad[name]))\r\n```\r\n\r\n## Environment info\r\nWe run the code in Poetry. This might be the issue, since the script runs successfully in our local environment.\r\n\r\nPoetry:\r\n- Python version: 3.8\r\n- PyArrow: 4.0.1\r\n- Elasticsearch: 7.13.4\r\n- datasets: 1.10.2\r\n\r\nLocal:\r\n- Python version: 3.8\r\n- PyArrow: 3.0.0\r\n- Elasticsearch: 7.7.1\r\n- datasets: 1.7.0\r\n",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 1003999540,
    "title": "Add IC, SI, ER tasks to SUPERB",
    "dateCreated": "2021-09-09T11:56:03Z",
    "dateModified": "2021-09-09T11:56:03Z",
    "description": "This PR adds 3 additional classification tasks to SUPERB\r\n\r\n#### Intent Classification\r\nDataset URL seems to be down at the moment :( See the note below.\r\nS3PRL source: https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/fluent_commands/dataset.py\r\nInstructions: https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#ic-intent-classification---fluent-speech-commands\r\n\r\n#### Speaker Identification\r\nManual download script:\r\n```\r\nmkdir VoxCeleb1\r\ncd VoxCeleb1\r\n            \r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partaa\r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partab\r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partac\r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partad\r\ncat vox1_dev* > vox1_dev_wav.zip\r\nunzip vox1_dev_wav.zip\r\n            \r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip\r\nunzip vox1_test_wav.zip\r\n            \r\n# download the official SUPERB train-dev-test split\r\nwget https://raw.githubusercontent.com/s3prl/s3prl/master/s3prl/downstream/voxceleb1/veri_test_class.txt\r\n```\r\nS3PRL source: https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/voxceleb1/dataset.py\r\nInstructions: https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#sid-speaker-identification\r\n\r\n#### Intent Classification\r\nManual download requires going through a slow application process, see the note below.\r\nS3PRL source: https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/emotion/IEMOCAP_preprocess.py\r\nInstructions: https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#er-emotion-recognition\r\n\r\n#### :warning:  Note\r\nThese datasets either require manual downloads or have broken/unstable links. You can get all necessary archives in this repo: https://huggingface.co/datasets/anton-l/superb_source_data_dumps/tree/main",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 1003999541,
    "title": "Fix data URLs and metadata in DocRED dataset",
    "dateCreated": "2021-09-09T08:55:34Z",
    "dateModified": "2021-09-09T08:55:34Z",
    "description": "The host of `docred` dataset has updated the `dev` data file. This PR:\r\n- Updates the dev URL\r\n- Updates dataset metadata\r\n\r\nThis PR also fixes the URL of the `train_distant` split, which was wrong.\r\n\r\nFix #2882.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 1003999542,
    "title": "`load_dataset('docred')` results in a `NonMatchingChecksumError` ",
    "dateCreated": "2021-09-09T05:55:02Z",
    "dateModified": "2021-09-09T05:55:02Z",
    "description": "## Describe the bug\r\nI get consistent `NonMatchingChecksumError: Checksums didn't match for dataset source files` errors when trying to execute `datasets.load_dataset('docred')`.\r\n\r\n## Steps to reproduce the bug\r\nIt is quasi only this code:\r\n```python\r\nimport datasets\r\ndata = datasets.load_dataset('docred')\r\n```\r\n\r\n## Expected results\r\nThe DocRED dataset should be loaded without any problems.\r\n\r\n## Actual results\r\n```\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-4-b1b83f25a16c> in <module>\r\n----> 1 d = datasets.load_dataset('docred')\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\r\n    845 \r\n    846     # Download and prepare data\r\n--> 847     builder_instance.download_and_prepare(\r\n    848         download_config=download_config,\r\n    849         download_mode=download_mode,\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    613                             logger.warning(\"HF google storage unreachable. Downloading and preparing it from source\")\r\n    614                     if not downloaded_from_gcs:\r\n--> 615                         self._download_and_prepare(\r\n    616                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    617                         )\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    673         # Checksums verification\r\n    674         if verify_infos:\r\n--> 675             verify_checksums(\r\n    676                 self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n    677             )\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     38     if len(bad_urls) > 0:\r\n     39         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 40         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     41     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     42 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/uc?export=download&id=1fDmfUUo5G7gfaoqWWvK81u08m71TK2g7']\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: Linux-5.11.0-7633-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.5\r\n- PyArrow version: 5.0.0\r\n\r\nThis error also happened on my Windows-partition, after freshly installing python 3.9 and `datasets`.\r\n\r\n## Remarks\r\n\r\n- I have already called `rm -rf /home/<user>/.cache/huggingface`, i.e., I have tried clearing the cache.\r\n- The problem does not exist for other datasets, i.e., it seems to be DocRED-specific.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 1003999543,
    "title": "Add BIOSSES dataset",
    "dateCreated": "2021-09-09T00:35:36Z",
    "dateModified": "2021-09-09T00:35:36Z",
    "description": "Adding the biomedical semantic sentence similarity dataset, BIOSSES, listed in \"Biomedical Datasets - BigScience Workshop 2021\"",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 1003999544,
    "title": "Extend support for streaming datasets that use pathlib.Path stem/suffix",
    "dateCreated": "2021-09-08T08:42:43Z",
    "dateModified": "2021-09-08T08:42:43Z",
    "description": "This PR extends the support in streaming mode for datasets that use `pathlib`, by patching the properties `pathlib.Path.stem` and `pathlib.Path.suffix`.\r\n\r\nRelated to #2876, #2874, #2866.\r\n\r\nCC: @severo",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 1003999545,
    "title": "In v1.4.1, all TIMIT train transcripts are \"Would such an act of refusal be useful?\"",
    "dateCreated": "2021-09-07T18:53:45Z",
    "dateModified": "2021-09-07T18:53:45Z",
    "description": "## Describe the bug\r\nUsing version 1.4.1 of `datasets`, TIMIT transcripts are all the same.\r\n\r\n## Steps to reproduce the bug\r\nI was following this tutorial\r\n- https://huggingface.co/blog/fine-tune-wav2vec2-english\r\n\r\nBut here's a distilled repro:\r\n```python\r\n!pip install datasets==1.4.1\r\nfrom datasets import load_dataset\r\ntimit = load_dataset(\"timit_asr\", cache_dir=\"./temp\")\r\nunique_transcripts = set(timit[\"train\"][\"text\"])\r\nprint(unique_transcripts)\r\nassert len(unique_transcripts) > 1\r\n```\r\n## Expected results\r\nExpected the correct TIMIT data. Or an error saying that this version of `datasets` can't produce it.\r\n\r\n## Actual results\r\nEvery train transcript was \"Would such an act of refusal be useful?\" Every test transcript was \"The bungalow was pleasantly situated near the shore.\"\r\n\r\n## Environment info\r\n- `datasets` version: 1.4.1\r\n- Platform: Darwin-18.7.0-x86_64-i386-64bit\r\n- Python version: 3.7.9\r\n- PyTorch version (GPU?): 1.9.0 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: tried both\r\n- Using distributed or parallel set-up in script?: no\r\n- \r\n\r\n",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 1003999546,
    "title": "NotADirectoryError: [WinError 267] During load_from_disk",
    "dateCreated": "2021-09-07T15:15:05Z",
    "dateModified": "2021-09-07T15:15:05Z",
    "description": "## Describe the bug\r\nTrying to load saved dataset or dataset directory from Amazon S3 on a Windows machine fails.\r\nPerforming the same operation succeeds on non-windows environment (AWS Sagemaker).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Followed https://huggingface.co/docs/datasets/filesystems.html#loading-a-processed-dataset-from-s3\r\n\r\nfrom datasets import load_from_disk\r\nfrom datasets.filesystems import S3FileSystem\r\n\r\n\r\ns3_file = \"output of save_to_disk\"\r\n\r\ns3_filesystem = S3FileSystem()\r\n\r\nload_from_disk(s3_file, fs=s3_filesystem)\r\n```\r\n\r\n## Expected results\r\nload_from_disk succeeds without error\r\n\r\n## Actual results\r\nSeems like it succeeds in pulling the file into a windows temp directory, as it exists in my system, but fails to process it.\r\n```\r\nException ignored in: <finalize object at 0x26409231ce0; dead>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\weakref.py\", line 566, in __call__\r\n    return info.func(*info.args, **(info.kwargs or {}))\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 817, in _cleanup\r\n    cls._rmtree(name)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 813, in _rmtree\r\n    _shutil.rmtree(name, onerror=onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 740, in rmtree\r\n    return _rmtree_unsafe(path, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 613, in _rmtree_unsafe\r\n    _rmtree_unsafe(fullname, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 613, in _rmtree_unsafe\r\n    _rmtree_unsafe(fullname, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 613, in _rmtree_unsafe\r\n    _rmtree_unsafe(fullname, onerror)\r\n  [Previous line repeated 2 more times]\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 618, in _rmtree_unsafe\r\n    onerror(os.unlink, fullname, sys.exc_info())\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 805, in onerror\r\n    cls._rmtree(path)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 813, in _rmtree\r\n    _shutil.rmtree(name, onerror=onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 740, in rmtree\r\n    return _rmtree_unsafe(path, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 599, in _rmtree_unsafe\r\n    onerror(os.scandir, path, sys.exc_info())\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 596, in _rmtree_unsafe\r\n    with os.scandir(path) as scandir_it:\r\nNotADirectoryError: [WinError 267] The directory name is invalid: 'C:\\\\Users\\\\grassycup\\\\AppData\\\\Local\\\\Temp\\\\tmp45f_qbma\\\\tests3bucket\\\\output\\\\test_output\\\\train\\\\dataset.arrow'\r\nException ignored in: <finalize object at 0x264091c7880; dead>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\weakref.py\", line 566, in __call__\r\n    return info.func(*info.args, **(info.kwargs or {}))\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 817, in _cleanup\r\n    cls._rmtree(name)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 813, in _rmtree\r\n    _shutil.rmtree(name, onerror=onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 740, in rmtree\r\n    return _rmtree_unsafe(path, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 613, in _rmtree_unsafe\r\n    _rmtree_unsafe(fullname, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 613, in _rmtree_unsafe\r\n    _rmtree_unsafe(fullname, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 613, in _rmtree_unsafe\r\n    _rmtree_unsafe(fullname, onerror)\r\n  [Previous line repeated 2 more times]\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 618, in _rmtree_unsafe\r\n    onerror(os.unlink, fullname, sys.exc_info())\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 805, in onerror\r\n    cls._rmtree(path)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\tempfile.py\", line 813, in _rmtree\r\n    _shutil.rmtree(name, onerror=onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 740, in rmtree\r\n    return _rmtree_unsafe(path, onerror)\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 599, in _rmtree_unsafe\r\n    onerror(os.scandir, path, sys.exc_info())\r\n  File \"C:\\Users\\grassycup\\Anaconda3\\envs\\hello.world\\lib\\shutil.py\", line 596, in _rmtree_unsafe\r\n    with os.scandir(path) as scandir_it:\r\nNotADirectoryError: [WinError 267] The directory name is invalid:\r\n'C:\\\\Users\\\\grassycup\\\\AppData\\\\Local\\\\Temp\\\\tmp45f_qbma\\\\tests3bucket\\\\output\\\\test_output\\\\train\\\\dataset.arrow'\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: Windows-10-10.0.19042-SP0\r\n- Python version: 3.8.11\r\n- PyArrow version: 3.0.0\r\n",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 1003999547,
    "title": "Don't keep the dummy data folder or dataset_infos.json when resolving data files",
    "dateCreated": "2021-09-07T14:09:04Z",
    "dateModified": "2021-09-07T14:09:04Z",
    "description": "When there's no dataset script, all the data files of a folder or a repository on the Hub are loaded as data files.\r\n\r\nThere are already a few exceptions:\r\n- files starting with \".\" are ignored\r\n- the dataset card \"README.md\" is ignored\r\n- any file named \"config.json\" is ignored (currently it isn't used anywhere, but it could be used in the future to define splits or configs for example, but not 100% sure)\r\n\r\nHowever any data files in a folder named \"dummy\" should be ignored as well as they should only be used to test the dataset.\r\nSame for \"dataset_infos.json\" which should only be used to get the `dataset.info`",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 1003999548,
    "title": "Extend support for streaming datasets that use pathlib.Path.glob",
    "dateCreated": "2021-09-07T13:43:45Z",
    "dateModified": "2021-09-07T13:43:45Z",
    "description": "This PR extends the support in streaming mode for datasets that use `pathlib`, by patching the method `pathlib.Path.glob`.\r\n\r\nRelated to #2874, #2866.\r\n\r\nCC: @severo",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 1003999549,
    "title": "Add Congolese Swahili speech datasets",
    "dateCreated": "2021-09-07T12:13:50Z",
    "dateModified": "2021-09-07T12:13:50Z",
    "description": "## Adding a Dataset\r\n- **Name:** Congolese Swahili speech corpora\r\n- **Data:** https://gamayun.translatorswb.org/data/\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n\r\nAlso related: https://mobile.twitter.com/OktemAlp/status/1435196393631764482",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 1003999550,
    "title": "Support streaming datasets that use pathlib",
    "dateCreated": "2021-09-07T07:35:49Z",
    "dateModified": "2021-09-07T07:35:49Z",
    "description": "This PR extends the support in streaming mode for datasets that use `pathlib.Path`.\r\n\r\nRelated to: #2866.\r\nCC: @severo ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 1003999551,
    "title": "adding swedish_medical_ner",
    "dateCreated": "2021-09-07T04:44:53Z",
    "dateModified": "2021-09-07T04:44:53Z",
    "description": "Adding the Swedish Medical NER dataset, listed in \"Biomedical Datasets - BigScience Workshop 2021\"\r\n\r\nCode refactored ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 1003999552,
    "title": "adding swedish_medical_ner",
    "dateCreated": "2021-09-06T22:00:52Z",
    "dateModified": "2021-09-06T22:00:52Z",
    "description": "Adding the Swedish Medical NER dataset, listed in \"Biomedical Datasets - BigScience Workshop 2021\"",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 1003999553,
    "title": "datasets.config.PYARROW_VERSION has no attribute 'major'",
    "dateCreated": "2021-09-06T21:06:57Z",
    "dateModified": "2021-09-06T21:06:57Z",
    "description": "In the test_dataset_common.py script, line 288-289\r\n\r\n```\r\nif datasets.config.PYARROW_VERSION.major < 3:\r\n   packaged_datasets = [pd for pd in packaged_datasets if pd[\"dataset_name\"] != \"parquet\"]\r\n```\r\n\r\nwhich throws the error below. `datasets.config.PYARROW_VERSION` itself return the string '4.0.1'. I have tested this on both datasets.__version_=='1.11.0' and '1.9.0'. I am using Mac OS.\r\n\r\n```\r\nimport datasets\r\ndatasets.config.PYARROW_VERSION.major\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/var/folders/1f/0wqmlgp90qjd5mpj53fnjq440000gn/T/ipykernel_73361/2547517336.py in <module>\r\n      1 import datasets\r\n----> 2 datasets.config.PYARROW_VERSION.major\r\n\r\nAttributeError: 'str' object has no attribute 'major'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: Darwin-20.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.11\r\n- PyArrow version: 4.0.1\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 1003999554,
    "title": "Fix three typos in two files for documentation",
    "dateCreated": "2021-09-04T11:49:43Z",
    "dateModified": "2021-09-04T11:49:43Z",
    "description": "Changed \"bacth_size\" to \"batch_size\" (2x)\r\nChanged \"intsructions\" to \"instructions\"",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 1003999555,
    "title": "TypeError: 'NoneType' object is not callable",
    "dateCreated": "2021-09-03T11:27:39Z",
    "dateModified": "2021-09-03T11:27:39Z",
    "description": "## Describe the bug\r\n\r\nTypeError: 'NoneType' object is not callable\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\ndataset = datasets.load_dataset(\"glue\", 'cola')\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform:\r\n- Python version: 3.7\r\n- PyArrow version:\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 1003999556,
    "title": "Add Common Objects in 3D (CO3D)",
    "dateCreated": "2021-09-02T20:36:12Z",
    "dateModified": "2021-09-02T20:36:12Z",
    "description": "## Adding a Dataset\r\n- **Name:** *Common Objects in 3D (CO3D)*\r\n- **Description:** *See blog post [here](https://ai.facebook.com/blog/common-objects-in-3d-dataset-for-3d-reconstruction)*\r\n- **Paper:** *[link to paper](https://arxiv.org/abs/2109.00512)*\r\n- **Data:** *[link to data](https://ai.facebook.com/datasets/co3d-downloads/)*\r\n- **Motivation:** *excerpt from above blog post:*\r\n\r\n> As the first data set of its kind, CO3D will aptly enable reconstruction of real-life 3D objects. Indeed, CO3D already provides training data to enable our NeRFormer to tackle the new-view synthesis (NVS) task. Here, photorealistic NVS is a major step on the path to fully immersive AR/VR effects, where objects can be virtually transported across different environments, which will allow connecting users by sharing or recollecting their experiences.\r\n> \r\n> Besides practical applications in AR/VR, we hope that the data set will become a standard testbed for the recent proliferation of methods (including NeRFormer, Implicit Differentiable Renderer, NeRF, and others) that reconstruct 3D scenes by means of an implicit shape model.\r\n> \r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 1003999557,
    "title": "Add CaSiNo dataset",
    "dateCreated": "2021-09-02T17:06:23Z",
    "dateModified": "2021-09-02T17:06:23Z",
    "description": "Hi. I request you to add our dataset to the repository. \r\n\r\nThis data was recently published at NAACL 2021: https://aclanthology.org/2021.naacl-main.254.pdf",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 1003999558,
    "title": "\"counter\" dataset raises an error in normal mode, but not in streaming mode",
    "dateCreated": "2021-09-02T13:10:53Z",
    "dateModified": "2021-09-02T13:10:53Z",
    "description": "## Describe the bug\r\n\r\n`counter` dataset raises an error on `load_dataset()`, but simply returns an empty iterator in streaming mode.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n>>> import datasets as ds\r\n>>> a = ds.load_dataset('counter', split=\"train\", streaming=False)\r\nUsing custom data configuration default\r\nDownloading and preparing dataset counter/default (download: 1.29 MiB, generated: 2.48 MiB, post-processed: Unknown size, total: 3.77 MiB) to /home/slesage/.cache/huggingface/datasets/counter/default/1.0.0/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9...\r\nTraceback (most recent call last):\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 726, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 1124, in _prepare_split\r\n    for key, record in utils.tqdm(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/tqdm/std.py\", line 1185, in __iter__\r\n    for obj in iterable:\r\n  File \"/home/slesage/.cache/huggingface/modules/datasets_modules/datasets/counter/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9/counter.py\", line 161, in _generate_examples\r\n    with derived_file.open(encoding=\"utf-8\") as f:\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py\", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py\", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 636, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 728, in _download_and_prepare\r\n    raise OSError(\r\nOSError: Cannot find data file.\r\nOriginal error:\r\n[Errno 2] No such file or directory: '/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml'\r\n```\r\n\r\n```python\r\n>>> import datasets as ds\r\n>>> b = ds.load_dataset('counter', split=\"train\", streaming=True)\r\nUsing custom data configuration default\r\n>>> list(b)\r\n[]\r\n```\r\n\r\n## Expected results\r\n\r\nAn exception should be raised in streaming mode\r\n\r\n## Actual results\r\n\r\nNo exception is raised in streaming mode: there is no way to tell if something has broken or if the dataset is simply empty.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.11.1.dev0\r\n- Platform: Linux-5.11.0-1016-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 1003999559,
    "title": "Add MultiEURLEX dataset",
    "dateCreated": "2021-09-02T09:42:24Z",
    "dateModified": "2021-09-02T09:42:24Z",
    "description": "**Add new MultiEURLEX Dataset**\r\n\r\nMultiEURLEX comprises 65k EU laws in 23 official EU languages (some low-ish resource). Each EU law has been annotated with EUROVOC concepts (labels) by the Publication Office of EU. As with the English EURLEX, the goal is to predict the relevant EUROVOC concepts (labels); this is multi-label classification task (given the text, predict multiple labels).",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 1003999560,
    "title": "Fix data URL in ToTTo dataset",
    "dateCreated": "2021-09-02T05:25:08Z",
    "dateModified": "2021-09-02T05:25:08Z",
    "description": "Data source host changed their data URL: google-research-datasets/ToTTo@cebeb43.\r\n\r\nFix #2860.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 1003999561,
    "title": "Update dataset URL",
    "dateCreated": "2021-09-02T05:22:18Z",
    "dateModified": "2021-09-02T05:22:18Z",
    "description": null,
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 1003999562,
    "title": "Only retain relevant statistics in certain metrics",
    "dateCreated": "2021-09-01T22:18:10Z",
    "dateModified": "2021-09-01T22:18:10Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nAs I understand, in the `add_batch()` function, the raw predictions and references are kept (in memory?) until `compute()` is called.\r\nhttps://github.com/huggingface/datasets/blob/e248247518140d5b0527ce2843a1a327e2902059/src/datasets/metric.py#L423-L442\r\n\r\nThis takes O(n) memory. However, for many (most?) metrics, this is not necessary. E.g., for accuracy, only the # correct and # total need to be recorded.\r\n\r\n**Describe the solution you'd like**\r\nProbably an inheritance hierarchy where `\"predictions\"` and `\"references\"` are not always the two keys for the final metric computation. Each metric should create and maintain its own relevant statistics, again for example, `\"n_correct\"` and `\"n_total\"` for accuracy.\r\n\r\nI believe the metrics in AllenNLP (https://github.com/allenai/allennlp/tree/39c40fe38cd2fd36b3465b0b3c031f54ec824160/allennlp/training/metrics) can be used as a good reference.\r\n\r\n**Describe alternatives you've considered**\r\nAt least `Metric.compute()` shouldn't hard-code `\"predictions\"` and `\"references\"` so that custom subclasses may override this behavior.\r\nhttps://github.com/huggingface/datasets/blob/e248247518140d5b0527ce2843a1a327e2902059/src/datasets/metric.py#L399-L400",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 1003999563,
    "title": "fix: \ud83d\udc1b be more specific when catching exceptions",
    "dateCreated": "2021-09-01T12:18:12Z",
    "dateModified": "2021-09-01T12:18:12Z",
    "description": "The same specific exception is catched in other parts of the same\r\nfunction.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 1003999564,
    "title": "Cannot download TOTTO dataset",
    "dateCreated": "2021-09-01T11:04:10Z",
    "dateModified": "2021-09-01T11:04:10Z",
    "description": "Error: Couldn't find file at https://storage.googleapis.com/totto/totto_data.zip\r\n\r\n`datasets version: 1.11.0`\r\n# How to reproduce:\r\n\r\n```py\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('totto')\r\n```\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 1003999565,
    "title": "Loading allenai/c4 in streaming mode does too many HEAD requests",
    "dateCreated": "2021-08-31T21:11:04Z",
    "dateModified": "2021-08-31T21:11:04Z",
    "description": "This does 60,000+ HEAD requests to get all the ETags of all the data files:\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"allenai/c4\", streaming=True)\r\n```\r\nIt makes loading the dataset completely impractical.\r\n\r\nThe ETags are used to compute the config id (it must depend on the data files being used).\r\nInstead of using the ETags, we could simply use the commit hash of the dataset repository on the hub, as well and the glob pattern used to resolve the files (here it's `*` by default, to load all the files of the repository)",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 1003999566,
    "title": "Fix s3fs version in CI",
    "dateCreated": "2021-08-31T18:05:43Z",
    "dateModified": "2021-08-31T18:05:43Z",
    "description": "The latest s3fs version has new constrains on aiobotocore, and therefore on boto3 and botocore\r\n\r\nThis PR changes the constrains to avoid the new conflicts\r\n\r\nIn particular it pins the version of s3fs.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 1003999567,
    "title": "Update: Openwebtext - update size",
    "dateCreated": "2021-08-31T17:11:03Z",
    "dateModified": "2021-08-31T17:11:03Z",
    "description": "Update the size of the Openwebtext dataset\r\n\r\nI also regenerated the dataset_infos.json but the data file checksum didn't change, and the number of examples either (8013769 examples)\r\n\r\nrelated to #2839 ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 1003999568,
    "title": "fix: \ud83d\udc1b remove URL's query string only if it's ?dl=1",
    "dateCreated": "2021-08-31T13:40:07Z",
    "dateModified": "2021-08-31T13:40:07Z",
    "description": "A lot of URL use the query strings, for example\r\nhttp://opus.nlpl.eu/download.php?f=Bianet/v1/moses/en-ku.txt.zip, we\r\nmust not remove it when trying to detect the protocol. We thus remove it\r\nonly in the case of the query string being ?dl=1 which occurs on dropbox\r\nand dl.orangedox.com. Also: add unit tests.\r\n\r\nSee https://github.com/huggingface/datasets/pull/2843 for the original\r\ndiscussion.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 1003999569,
    "title": "Fix windows CI CondaError",
    "dateCreated": "2021-08-31T13:22:02Z",
    "dateModified": "2021-08-31T13:22:02Z",
    "description": "From this thread: https://github.com/conda/conda/issues/6057\r\n\r\nWe can fix the conda error\r\n```\r\nCondaError: Cannot link a source that does not exist.\r\nC:\\Users\\...\\Anaconda3\\Scripts\\conda.exe\r\n```\r\n\r\nby doing\r\n```bash\r\nconda update conda\r\n```\r\n\r\nbefore doing any install in the windows CI",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 983726084,
    "title": "Fix caching when moving script",
    "dateCreated": "2021-08-31T10:58:35Z",
    "dateModified": "2021-08-31T10:58:35Z",
    "description": "When caching the result of a `map` function, the hash that is computed depends on many properties of this function, such as all the python objects it uses, its code and also the location of this code.\r\n\r\nUsing the full path of the python script for the location of the code makes the hash change if a script like `run_mlm.py` is moved.\r\n\r\nI changed this by simply using the base name of the script instead of the full path.\r\n\r\nNote that this change also affects the hash of the code used from imported modules, but I think it's fine. Indeed it hashes the code of the imported modules anyway, so the location of the python files of the imported modules doesn't matter when computing the hash.\r\n\r\nClose https://github.com/huggingface/datasets/issues/2825",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 983692026,
    "title": "Add AMI dataset",
    "dateCreated": "2021-08-31T10:19:01Z",
    "dateModified": "2021-08-31T10:19:01Z",
    "description": "This is an initial commit for AMI dataset",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 983609352,
    "title": "Fix: linnaeus - fix url",
    "dateCreated": "2021-08-31T08:51:13Z",
    "dateModified": "2021-08-31T08:51:13Z",
    "description": "The url was causing a `ConnectionError` because of the \"/\" at the end\r\n\r\nClose https://github.com/huggingface/datasets/issues/2821",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 982789593,
    "title": "Update `column_names` showed as `:func:` in exploring.st",
    "dateCreated": "2021-08-30T13:21:46Z",
    "dateModified": "2021-08-30T13:21:46Z",
    "description": "Hi,  \r\n\r\nOne mention of `column_names` in exploring.st was showing it as `:func:` instead of `:attr:`.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 982654644,
    "title": "Wound segmentation datasets",
    "dateCreated": "2021-08-30T10:44:32Z",
    "dateModified": "2021-08-30T10:44:32Z",
    "description": "## Adding a Dataset\r\n- **Name:** Wound segmentation datasets\r\n- **Description:** annotated wound image dataset \r\n- **Paper:** https://www.nature.com/articles/s41598-020-78799-w\r\n- **Data:** https://github.com/uwm-bigdata/wound-segmentation\r\n- **Motivation:** Interesting simple image dataset, useful for segmentation, with visibility due to http://www.miccai.org/special-interest-groups/challenges/ and https://fusc.grand-challenge.org/\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 982631420,
    "title": "Add Open Catalyst Project Dataset",
    "dateCreated": "2021-08-30T10:14:39Z",
    "dateModified": "2021-08-30T10:14:39Z",
    "description": "## Adding a Dataset\r\n- **Name:**  Open Catalyst 2020 (OC20) Dataset\r\n- **Website:** https://opencatalystproject.org/\r\n- **Data:** https://github.com/Open-Catalyst-Project/ocp/blob/master/DATASET.md\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 981953908,
    "title": "Update README.md",
    "dateCreated": "2021-08-28T23:58:26Z",
    "dateModified": "2021-08-28T23:58:26Z",
    "description": "Changed 'Tain' to 'Train'.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 981589693,
    "title": "fix regex to accept negative timezone",
    "dateCreated": "2021-08-27T20:54:05Z",
    "dateModified": "2021-08-27T20:54:05Z",
    "description": "fix #2846",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 981587590,
    "title": "Negative timezone",
    "dateCreated": "2021-08-27T20:50:33Z",
    "dateModified": "2021-08-27T20:50:33Z",
    "description": "## Describe the bug\r\nThe load_dataset method do not accept a parquet file with a negative timezone, as it has the following regex:\r\n```\r\n\"^(s|ms|us|ns),\\s*tz=([a-zA-Z0-9/_+:]*)$\"\r\n```\r\nSo a valid timestap ```timestamp[us, tz=-03:00]``` returns an error when loading parquet files.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Where the timestamp column has a tz of -03:00\r\ndatasets = load_dataset('parquet', data_files={'train': train_files, 'validation': validation_files,\r\n                                        'test': test_files}, cache_dir=\"./cache_teste/\")\r\n```\r\n\r\n## Expected results\r\nThe -03:00 is a valid tz so the regex should accept this without raising an error.\r\n\r\n## Actual results\r\nAs this regex disaproves a valid tz it raises the following error:\r\n```python\r\nraise ValueError(\r\n                f\"{datasets_dtype} is not a validly formatted string representation of a pyarrow timestamp.\"\r\n                f\"Examples include timestamp[us] or timestamp[us, tz=America/New_York]\"\r\n                f\"See: https://arrow.apache.org/docs/python/generated/pyarrow.timestamp.html#pyarrow.timestamp\"\r\n            )\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: Ubuntu 20.04\r\n- Python version: 3.8\r\n- PyArrow version: 5.0.0\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 981487861,
    "title": "[feature request] adding easy to remember `datasets.cache_dataset()` + `datasets.is_dataset_cached()`",
    "dateCreated": "2021-08-27T18:21:51Z",
    "dateModified": "2021-08-27T18:21:51Z",
    "description": "Often, there is a need to prepare a dataset but not use it immediately, e.g. think tests suite setup, so it'd be really useful to be able to do:\r\n\r\n``` \r\nif not datasets.is_dataset_cached(ds): datasets.cache_dataset(ds)\r\n```\r\n\r\nThis can already be done with:\r\n```\r\nbuilder = load_dataset_builder(ds)\r\nif not os.path.idsir(builder.cache_dir):\r\n    builder.download_and_prepare()\r\n```\r\n\r\nbut the current way is a way less intuitive and much harder to remember than the proposed API, IMHO. \r\n\r\nOne more way is to do:\r\n\r\n```\r\n_ = load_dataset(ds)\r\n```\r\nbut it wastes resources loading the dataset when it's not needed.\r\n\r\nthis has been discussed at https://huggingface.slack.com/archives/C01229B19EX/p1630021912025800\r\n\r\nThank you!\r\n\r\n@lhoestq \r\n",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 981382806,
    "title": "Fix: wikicorpus - fix keys",
    "dateCreated": "2021-08-27T15:56:06Z",
    "dateModified": "2021-08-27T15:56:06Z",
    "description": "As mentioned in https://github.com/huggingface/datasets/issues/2552, there is a duplicate keys error in `wikicorpus`.\r\n\r\nI fixed that by taking into account the file index in the keys",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 981317775,
    "title": "Fix extraction protocol inference from urls with params",
    "dateCreated": "2021-08-27T14:40:57Z",
    "dateModified": "2021-08-27T14:40:57Z",
    "description": "Previously it was unable to infer the compression protocol for files at URLs like\r\n```\r\nhttps://foo.bar/train.json.gz?dl=1\r\n```\r\nbecause of the query parameters.\r\n\r\nI fixed that, this should allow 10+ datasets to work in streaming mode:\r\n```\r\n  \"discovery\",\r\n  \"emotion\",\r\n  \"grail_qa\",\r\n  \"guardian_authorship\",\r\n  \"pragmeval\",\r\n  \"simple_questions_v2\",\r\n  \"versae/adobo\",\r\n  \"w-nicole/childes_data\",\r\n  \"w-nicole/childes_data_no_tags_\",\r\n  \"w-nicole/childes_data_with_tags\",\r\n  \"w-nicole/childes_data_with_tags_\"\r\n```\r\n\r\ncc @severo ",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 980725899,
    "title": "always requiring the username in the dataset name when there is one",
    "dateCreated": "2021-08-26T23:31:53Z",
    "dateModified": "2021-08-26T23:31:53Z",
    "description": "Me and now another person have been bitten by the `datasets`'s non-strictness on requiring a dataset creator's username when it's due.\r\n\r\nSo both of us started with `stas/openwebtext-10k`, somewhere along the lines lost `stas/` and continued using `openwebtext-10k` and it all was good until we published the software and things broke, since there is no `openwebtext-10k`\r\n\r\nSo this feature request is asking to tighten the checking and not allow dataset loading if it was downloaded with the user prefix, but then attempted to be used w/o it.\r\n\r\nThe same in code:\r\n\r\n```\r\n# first run\r\npython -c \"from datasets import load_dataset; load_dataset('stas/openwebtext-10k')\"\r\n# now run immediately\r\npython -c \"from datasets import load_dataset; load_dataset('openwebtext-10k')\"\r\n# the second command should fail, but it doesn't fail now.\r\n```\r\n\r\nPlease let me know if I explained myself clearly.\r\n\r\nThank you!",
    "status": "open",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 980497321,
    "title": "Adding GLUECoS Hinglish and Spanglish code-switching bemchmark",
    "dateCreated": "2021-08-26T17:47:39Z",
    "dateModified": "2021-08-26T17:47:39Z",
    "description": "## Adding a Dataset\r\n- **Name:** GLUECoS\r\n- **Description:** a Microsoft Benchmark to evaluate code-switching for only two language pairs but a variety of tasks\r\n- **Paper:** https://aclanthology.org/2020.acl-main.329/\r\n- **Data:** https://github.com/microsoft/GLUECoS\r\n- **Motivation:** We currently only have [one other](https://huggingface.co/datasets/lince) dataset for code-switching\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 980489074,
    "title": "How can I compute BLEU-4 score use `load_metric` ?",
    "dateCreated": "2021-08-26T17:36:37Z",
    "dateModified": "2021-08-26T17:36:37Z",
    "description": "I have found the sacrebleu metric. But, I do not know the difference between it and BLEU-4.\r\nIf I want to compute BLEU-4 score, what can i do?",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 980271715,
    "title": "OpenWebText: NonMatchingSplitsSizesError",
    "dateCreated": "2021-08-26T13:50:26Z",
    "dateModified": "2021-08-26T13:50:26Z",
    "description": "## Describe the bug\r\n\r\nWhen downloading `openwebtext`, I'm getting:\r\n```\r\ndatasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=39769494896, num_examples=8013769, dataset_name='openwebtext'), 'recorded': SplitInfo(name='train', num_bytes=39611023912, num_examples=7982430, dataset_name='openwebtext')}]\r\n```\r\n\r\nI suspect that the file we download from has changed since the size doesn't look like to match with documentation\r\n\r\n`Downloading:   0%|          | 0.00/12.9G [00:00<?, ?B/s]` This suggest the total size is 12.9GB, whereas the one documented mentions `Size of downloaded dataset files: 12283.35 MB`.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(\"openwebtext\", download_mode=\"force_redownload\")\r\n```\r\n\r\n## Expected results\r\n\r\nLoading is successful\r\n\r\n## Actual results\r\n\r\nLoading throws above error.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.10.2\r\n- Platform: linux (Redhat version 8.1)\r\n- Python version: 3.8\r\n- PyArrow version: 4.0.1\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 980067186,
    "title": "Add error_bad_chunk to the JSON loader",
    "dateCreated": "2021-08-26T10:07:32Z",
    "dateModified": "2021-08-26T10:07:32Z",
    "description": "Add the `error_bad_chunk` parameter to the JSON loader.\r\n\r\nSetting `error_bad_chunk=False` allows to skip an unparsable chunk of JSON data without raising an error.\r\n\r\nAdditional note:\r\n\r\nIn case of an unparsable JSON chunk, the JSON loader no longer tries to load the full JSON (which could take a lot of time in streaming mode) to get the JSON fields that the user may have forgotten to pass. Ex : for squad-like data, the user has to pass `field=\"data\"` to tell the loader to get the list of examples from this field.\r\n\r\nTODO: update docs\r\n\r\ncc @lvwerra ",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 979298297,
    "title": "prepare_module issue when loading from read-only fs",
    "dateCreated": "2021-08-25T15:21:26Z",
    "dateModified": "2021-08-25T15:21:26Z",
    "description": "## Describe the bug\r\n\r\nWhen we use prepare_module from a readonly file system, we create a FileLock using the `local_path`.\r\nThis path is not necessarily writable.\r\n\r\n`lock_path = local_path + \".lock\"`\r\n\r\n\r\n## Steps to reproduce the bug\r\n\r\nRun `load_dataset` on a readonly python loader file.\r\n```python\r\nds = load_dataset(\r\n        python_loader, data_files={\"train\": train_path, \"test\": test_path}\r\n    )\r\n```\r\n\r\nwhere `python_loader` is a path to a file located in a readonly folder.\r\n\r\n## Expected results\r\nThis should work I think?\r\n\r\n## Actual results\r\n\r\n```python\r\n    return load_dataset(\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/load.py\", line 711, in load_dataset\r\n    module_path, hash, resolved_file_path = prepare_module(\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/load.py\", line 465, in prepare_module\r\n    with FileLock(lock_path):\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/filelock.py\", line 314, in __enter__\r\n    self.acquire()\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/filelock.py\", line 263, in acquire\r\n    self._acquire()\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/filelock.py\", line 378, in _acquire\r\n    fd = os.open(self._lock_file, open_mode)\r\nOSError: [Errno 30] Read-only file system: 'YOUR_FILE.py.lock'\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.7.0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.8\r\n- PyArrow version: 3.0.0\r\n",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 979230142,
    "title": "Optimize Dataset.filter to only compute the indices to keep",
    "dateCreated": "2021-08-25T14:41:22Z",
    "dateModified": "2021-08-25T14:41:22Z",
    "description": "Optimize `Dataset.filter` to only compute the indices of the rows to keep, instead of creating a new Arrow table with the rows to keep. Creating a new table was an issue because it could take a lot of disk space.\r\n\r\nThis will be useful to process audio datasets for example cc @patrickvonplaten ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 979209394,
    "title": "Update: timit_asr - make the dataset streamable",
    "dateCreated": "2021-08-25T14:22:49Z",
    "dateModified": "2021-08-25T14:22:49Z",
    "description": "The TIMIT ASR dataset had two issues that was preventing it from being streamable:\r\n\r\n1. it was missing a call to `open` before `pd.read_csv`\r\n2. it was using `os.path.dirname` which is not supported for streaming\r\n\r\nI made the dataset streamable by using `open` to load the CSV, and by adding the support for `os.path.dirname` in dataset scripts to stream data\r\n\r\nYou can now do\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ntimit_asr = load_dataset(\"timit_asr\", streaming=True)\r\nprint(next(iter(timit_asr[\"train\"])))\r\n```\r\nprints:\r\n\r\n```json\r\n{\"file\": \"zip://data/TRAIN/DR4/MMDM0/SI681.WAV::https://data.deepai.org/timit.zip\",\r\n\"phonetic_detail\": {\"start\": [0, 1960, 2466, 3480, 4000, 5960, 7480, 7880, 9400, 9960, 10680, 13480, 15680, 15880, 16920, 18297, 18882, 19480, 21723, 22516, 24040, 25190, 27080, 28160, 28560, 30120, 31832, 33240, 34640, 35968, 37720],\r\n\"utterance\": [\"h#\", \"w\", \"ix\", \"dcl\", \"s\", \"ah\", \"tcl\", \"ch\", \"ix\", \"n\", \"ae\", \"kcl\", \"t\", \"ix\", \"v\", \"r\", \"ix\", \"f\", \"y\", \"ux\", \"zh\", \"el\", \"bcl\", \"b\", \"iy\", \"y\", \"ux\", \"s\", \"f\", \"el\", \"h#\"],\r\n\"stop\": [1960, 2466, 3480, 4000, 5960, 7480, 7880, 9400, 9960, 10680, 13480, 15680, 15880, 16920, 18297, 18882, 19480, 21723, 22516, 24040, 25190, 27080, 28160, 28560, 30120, 31832, 33240, 34640, 35968, 37720, 39920]},\r\n\"sentence_type\": \"SI\", \"id\": \"SI681\",\r\n\"speaker_id\": \"MMDM0\",\r\n\"dialect_region\": \"DR4\",\r\n\"text\": \"Would such an act of refusal be useful?\",\r\n\"word_detail\": {\r\n    \"start\": [1960, 4000, 9400, 10680, 15880, 18297, 27080, 30120],\r\n    \"utterance\": [\"would\", \"such\", \"an\", \"act\", \"of\", \"refusal\", \"be\", \"useful\"],\r\n    \"stop\": [4000, 9400, 10680, 15880, 18297, 27080, 30120, 37720]\r\n}}\r\n```\r\n\r\ncc @patrickvonplaten @vrindaprabhu",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 978309749,
    "title": "Fix IndexError by ignoring empty RecordBatch",
    "dateCreated": "2021-08-24T17:06:13Z",
    "dateModified": "2021-08-24T17:06:13Z",
    "description": "We need to ignore the empty record batches for the interpolation search to work correctly when querying arrow tables\r\n\r\nClose #2833\r\n\r\ncc @SaulLu ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 978296140,
    "title": "IndexError when accessing first element of a Dataset if first RecordBatch is empty",
    "dateCreated": "2021-08-24T16:49:20Z",
    "dateModified": "2021-08-24T16:49:20Z",
    "description": "The computation of the offsets of the underlying Table of a Dataset has some issues if the first RecordBatch is empty.\r\n\r\n```python\r\nfrom datasets import Dataset\r\nimport pyarrow as pa\r\n\r\npa_table = pa.Table.from_pydict({\"a\": [1]})\r\npa_table2 = pa.Table.from_pydict({\"a\": []}, schema=pa_table.schema)\r\nds_table = pa.concat_tables([pa_table2, pa_table])\r\n\r\ndataset = Dataset(ds_table)\r\n\r\nprint([len(b) for b in dataset.data._batches])\r\n# [0, 1]\r\n\r\nprint(dataset.data._offsets)\r\n# [0 0 1] (should be [0, 1])\r\n\r\ndataset[0]\r\n```\r\nraises\r\n```python\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/datasets/table.py in _interpolation_search(arr, x)\r\n     90         else:\r\n     91             i, j = i, k\r\n---> 92     raise IndexError(f\"Invalid query '{x}' for size {arr[-1] if len(arr) else 'none'}.\")\r\n     93 \r\n     94 \r\n\r\nIndexError: Invalid query '0' for size 1.\r\n```\r\n\r\nThis can be fixed by ignoring empty batches when computing `table._batches` and `table._offsets`\r\n\r\ncc @SaulLu ",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 978012800,
    "title": "Logging levels not taken into account",
    "dateCreated": "2021-08-24T11:50:41Z",
    "dateModified": "2021-08-24T11:50:41Z",
    "description": "## Describe the bug\r\n\r\nThe `logging` module isn't working as intended relative to the levels to set.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import logging\r\n\r\nlogging.set_verbosity_debug()\r\nlogger = logging.get_logger()\r\n\r\nlogger.error(\"ERROR\")\r\nlogger.warning(\"WARNING\")\r\nlogger.info(\"INFO\")\r\nlogger.debug(\"DEBUG\"\r\n```\r\n\r\n## Expected results\r\n\r\nI expect all logs to be output since I'm putting a `debug` level.\r\n\r\n## Actual results\r\n\r\nOnly the two first logs are output.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.11.0\r\n- Platform: Linux-5.13.9-arch1-1-x86_64-with-glibc2.33\r\n- Python version: 3.9.6\r\n- PyArrow version: 5.0.0\r\n\r\n## To go further\r\n\r\nThis logging issue appears in `datasets` but not in `transformers`. It happens because there is no handler defined for the logger. When no handler is defined, the `logging` library will output a one-off error to stderr, using a `StderrHandler` with level `WARNING`.\r\n\r\n`transformers` sets a default `StreamHandler` [here](https://github.com/huggingface/transformers/blob/5c6eca71a983bae2589eed01e5c04fcf88ba5690/src/transformers/utils/logging.py#L86)",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 977864600,
    "title": "ArrowInvalid when mapping dataset with missing values",
    "dateCreated": "2021-08-24T08:50:42Z",
    "dateModified": "2021-08-24T08:50:42Z",
    "description": "## Describe the bug\r\nI encountered an `ArrowInvalid` when mapping dataset with missing values. \r\nHere are the files for a minimal example. The exception is only thrown when the first line in the csv has a missing value (if you move the last line to the top it isn't thrown).\r\n[data_small.csv](https://github.com/huggingface/datasets/files/7037838/data_small.csv)\r\n[data.csv](https://github.com/huggingface/datasets/files/7037842/data.csv)\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndatasets = load_dataset(\"csv\", data_files=['data_small.csv'])\r\n\r\ndatasets = datasets.map(lambda e: {'labels': e['match']},\r\n                        remove_columns=['id'])\r\n```\r\n\r\n## Expected results\r\nNo error\r\n\r\n## Actual results\r\n```\r\nFile \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Invalid null value\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.5.0\r\n- Platform: Linux-5.11.0-25-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyTorch version (GPU?): 1.7.1+cpu (False)\r\n- Tensorflow version (GPU?): 2.4.1 (False)\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 977563947,
    "title": "Add imagefolder dataset",
    "dateCreated": "2021-08-23T23:34:06Z",
    "dateModified": "2021-08-23T23:34:06Z",
    "description": "A generic imagefolder dataset inspired by `torchvision.datasets.ImageFolder`. \r\n\r\nResolves #2508 \r\n\r\n---\r\n\r\nExample Usage:\r\n\r\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/nateraw/954fa8cba4ff806f6147a782fa9efd1a/imagefolder-official-example.ipynb)",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 977233360,
    "title": "Optimize streaming from TAR archives",
    "dateCreated": "2021-08-23T16:56:40Z",
    "dateModified": "2021-08-23T16:56:40Z",
    "description": "Hi ! As you know TAR has some constraints for data streaming. While it is optimized for buffering, the files in the TAR archive **need to be streamed in order**. It means that we can't choose which file to stream from, and this notation is to be avoided for TAR archives:\r\n```\r\ntar://books_large_p1.txt::https://storage.googleapis.com/huggingface-nlp/datasets/bookcorpus/bookcorpus.tar.bz2\r\n```\r\nInstead, I suggest we implement `iter_archive` for the `StreamingDownloadManager`.\r\nThe regular `DownloadManager` already has it.\r\n\r\nThen we will have to update the json/txt/csv/etc. loaders to make them use `iter_archive` on TAR archives.\r\n\r\nThat's also what Tensorflow Datasets is doing in this case.\r\nSee this [dataset](https://github.com/tensorflow/datasets/blob/93895059c80a9e05805e8f32a2e310f66a23fc98/tensorflow_datasets/image_classification/flowers.py) for example.\r\n\r\nTherefore instead of doing\r\n```python\r\nuncompressed = dl_manager.extract(tar_archive)\r\nfilename = \"books_large_p1.txt\"\r\nwith open(os.path.join(uncompressed, filename)) as f:\r\n    for line in f:\r\n        ...\r\n```\r\nwe'll do\r\n```python\r\nfor filename, f in dl_manager.iter_archive(tar_archive):\r\n    for line in f:\r\n        ...\r\n```",
    "status": "open",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 977181517,
    "title": "Add code-mixed Kannada Hope speech dataset",
    "dateCreated": "2021-08-23T15:55:09Z",
    "dateModified": "2021-08-23T15:55:09Z",
    "description": "## Adding a Dataset\r\n- **Name:** *KanHope*\r\n- **Description:** *A code-mixed English-Kannada dataset for Hope speech detection*\r\n- **Paper:** *https://arxiv.org/abs/2108.04616* \r\n- **Data:** *https://github.com/adeepH/KanHope/tree/main/dataset*\r\n- **Motivation:** *The dataset is amongst the very few resources available for code-mixed low-resourced Dravidian languages of India*",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 976976552,
    "title": "add a text classification dataset",
    "dateCreated": "2021-08-23T12:24:41Z",
    "dateModified": "2021-08-23T12:24:41Z",
    "description": null,
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 976974254,
    "title": "Add a Text Classification dataset: KanHope",
    "dateCreated": "2021-08-23T12:21:58Z",
    "dateModified": "2021-08-23T12:21:58Z",
    "description": "## Adding a Dataset\r\n- **Name:** *KanHope*\r\n- **Description:** *A code-mixed English-Kannada dataset for Hope speech detection*\r\n- **Paper:** *https://arxiv.org/abs/2108.04616* (I am the author of the paper}\r\n- **Author:** *[AdeepH](https://github.com/adeepH)*\r\n- **Data:** *https://github.com/adeepH/KanHope/tree/main/dataset*\r\n- **Motivation:** *The dataset is amongst the very few resources available for code-mixed Dravidian languages*\r\n\r\n- I tried following the steps as per the instructions. However, could not resolve an error. Any help would be appreciated.\r\n\r\n- The dataset card and the scripts for the dataset *https://github.com/adeepH/datasets/tree/multilingual-hope-speech/datasets/mhs_eval*\r\n\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset bn_hate_speech/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/bn_hate_speech/default/0.0.0/5f417ddc89777278abd29988f909f39495f0ec802090f7d8fa63b5bffb121762...\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-114-4a9cdb519e4c> in <module>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 data = load_dataset('/content/bn')\r\n\r\n9 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\r\n    850         ignore_verifications=ignore_verifications,\r\n    851         try_from_hf_gcs=try_from_hf_gcs,\r\n--> 852         use_auth_token=use_auth_token,\r\n    853     )\r\n    854 \r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    614                     if not downloaded_from_gcs:\r\n    615                         self._download_and_prepare(\r\n--> 616                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    617                         )\r\n    618                     # Sync info\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    691             try:\r\n    692                 # Prepare split will record examples associated to the split\r\n--> 693                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    694             except OSError as e:\r\n    695                 raise OSError(\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _prepare_split(self, split_generator)\r\n   1107                     disable=bool(logging.get_verbosity() == logging.NOTSET),\r\n   1108                 ):\r\n-> 1109                     example = self.info.features.encode_example(record)\r\n   1110                     writer.write(example, key)\r\n   1111             finally:\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/features.py in encode_example(self, example)\r\n   1015         \"\"\"\r\n   1016         example = cast_to_python_objects(example)\r\n-> 1017         return encode_nested_example(self, example)\r\n   1018 \r\n   1019     def encode_batch(self, batch):\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/features.py in encode_nested_example(schema, obj)\r\n    863     if isinstance(schema, dict):\r\n    864         return {\r\n--> 865             k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)\r\n    866         }\r\n    867     elif isinstance(schema, (list, tuple)):\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/features.py in <dictcomp>(.0)\r\n    863     if isinstance(schema, dict):\r\n    864         return {\r\n--> 865             k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)\r\n    866         }\r\n    867     elif isinstance(schema, (list, tuple)):\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/features.py in encode_nested_example(schema, obj)\r\n    890     # ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\r\n    891     elif isinstance(schema, (ClassLabel, TranslationVariableLanguages, Value, _ArrayXD)):\r\n--> 892         return schema.encode_example(obj)\r\n    893     # Other object should be directly convertible to a native Arrow type (like Translation and Translation)\r\n    894     return obj\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/features.py in encode_example(self, example_data)\r\n    665         # If a string is given, convert to associated integer\r\n    666         if isinstance(example_data, str):\r\n--> 667             example_data = self.str2int(example_data)\r\n    668 \r\n    669         # Allowing -1 to mean no label.\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/features.py in str2int(self, values)\r\n    623                 if value not in self._str2int:\r\n    624                     value = str(value).strip()\r\n--> 625                 output.append(self._str2int[str(value)])\r\n    626             else:\r\n    627                 # No names provided, try to integerize\r\n\r\nKeyError: ' '\r\n```",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 976584926,
    "title": "The datasets.map function does not load cached dataset after moving python script",
    "dateCreated": "2021-08-23T03:23:37Z",
    "dateModified": "2021-08-23T03:23:37Z",
    "description": "## Describe the bug\r\nThe datasets.map function caches the processed data to a certain directory. When the map function is called another time with totally the same parameters, the cached data are supposed to be reloaded instead of re-processing. However, it doesn't reuse cached data sometimes. I use the common data processing in different tasks, the datasets are processed again, the only difference is that I run them in different files.\r\n\r\n## Steps to reproduce the bug\r\nJust run the following codes in different .py files.\r\n```python\r\nif __name__ == '__main__':\r\n    from datasets import load_dataset\r\n    from transformers import AutoTokenizer\r\n    raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\r\n\r\n\r\n    def tokenize_function(examples):\r\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\r\n\r\n\r\n    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\r\n```\r\n\r\n## Expected results\r\nThe map function should reload data in the second or any later runs.\r\n\r\n## Actual results\r\nThe processing happens in each run.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.0\r\n- Platform: linux\r\n- Python version: 3.7.6\r\n- PyArrow version: 3.0.0\r\n\r\nThis is the first time I report a bug. If there is any problem or confusing description, please let me know \ud83d\ude04.\r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 976394721,
    "title": "Fix defaults in cache_dir docstring in load.py",
    "dateCreated": "2021-08-22T14:48:37Z",
    "dateModified": "2021-08-22T14:48:37Z",
    "description": "Fix defaults in the `cache_dir` docstring.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 976135355,
    "title": "HF_DATASETS_CACHE variable in Windows",
    "dateCreated": "2021-08-21T13:17:44Z",
    "dateModified": "2021-08-21T13:17:44Z",
    "description": "I can't seem to use a custom Cache directory in Windows. I have tried:\r\n\r\nset HF_DATASETS_CACHE = \"C:\\Datasets\"\r\nset HF_DATASETS_CACHE = \"C:/Datasets\"\r\nset HF_DATASETS_CACHE = \"C:\\\\Datasets\"\r\nset HF_DATASETS_CACHE = \"r'C:\\Datasets'\"\r\nset HF_DATASETS_CACHE = \"\\Datasets\"\r\nset HF_DATASETS_CACHE = \"/Datasets\"\r\n\r\nIn each instance I get the \"[WinError 123] The filename, directory name, or volume label syntax is incorrect\" error when attempting to load a dataset",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 975744463,
    "title": "Add url prefix convention for many compression formats",
    "dateCreated": "2021-08-20T16:11:23Z",
    "dateModified": "2021-08-20T16:11:23Z",
    "description": "## Intro\r\n\r\nWhen doing dataset streaming, the uncompression of compressed files is done on the fly using `fsspec`.\r\n\r\nIn particular, the download manager method `download_and_extract` doesn't return a path to the local download and extracted file, but instead a chained URL so that the uncompression can be done when the file is opened. A few examples of chained URLS:\r\n- `gz://file.txt::https://foo.bar/file.txt.gz`\r\n- `bz2://file.txt::https://foo.bar/file.txt.bz2`\r\n- `zip://::https://foo.bar/archive.zip`\r\n- `tar://::https://foo.bar/archive.tar.gz` (the TAR uncompression includes gz, bz2 etc. uncompression in `fsspec`)\r\n\r\nThis syntax is highly inspired by the `fsspec` URL chaining syntax from https://filesystem-spec.readthedocs.io/en/latest/features.html#url-chaining\r\n\r\nThis url prefixing allows `open` to know what kind of uncompression to do in a dataset script when doing\r\n```python\r\ndef _generate_examples(self, urlpath):\r\n    with open(urlpath) as f:\r\n        ....\r\n```\r\n\r\n## What it changes\r\n\r\nThis changes the previous behavior from https://github.com/huggingface/datasets/pull/2786 , in which `open` was trying to infer the compression automatically. Infering the compression made it impossible to know whether the user wanted `open` to return compressed data (as the default behavior of the buitin open), or the uncompressed data. By adding uncompression prefixes to the URL, `open` know directly if it has to uncompress or not, and also which protocol to use.\r\n\r\n## Additional notes\r\n\r\nThis PR should close https://github.com/huggingface/datasets/issues/2813\r\n\r\nIt should also close this PR https://github.com/huggingface/datasets/pull/2811 since the oscar dataset script won't try to uncompress twice anymore\r\n\r\nNote that I had to temporarily remove the support for passing tar and zip files to `data_files` for streaming to make it work, since it makes it ambiguous whether a zip file passed as `data_files` should be uncompressed or not. IMO we can make it work again by changing the syntax to make the glob explicit:\r\n```python\r\nload_dataset(\"json\", data_files=\"zip://*.jsonl::https://foo.bar/archive.zip\")\r\n```\r\nThis is the exact same convention as fsspec and it removes all ambiguities\r\n\r\ncc @albertvillanova @lewtun ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 975556032,
    "title": "Cannot load linnaeus dataset",
    "dateCreated": "2021-08-20T12:15:15Z",
    "dateModified": "2021-08-20T12:15:15Z",
    "description": "## Describe the bug\r\nThe [linnaeus](https://huggingface.co/datasets/linnaeus) dataset cannot be loaded. To reproduce:\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndatasets = load_dataset(\"linnaeus\")\r\n```\r\nThis results in:\r\n```\r\nDownloading and preparing dataset linnaeus/linnaeus (download: 17.36 MiB, generated: 8.74 MiB, post-processed: Unknown size, total: 26.10 MiB) to /root/.cache/huggingface/datasets/linnaeus/linnaeus/1.0.0/2ff05dbc256108233262f596e09e322dbc3db067202de14286913607cd9cb704...\r\n---------------------------------------------------------------------------\r\nConnectionError                           Traceback (most recent call last)\r\n<ipython-input-4-7ef3a88f6276> in <module>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 datasets = load_dataset(\"linnaeus\")\r\n\r\n11 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\r\n    603             raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\n    604         _raise_if_offline_mode_is_enabled(f\"Tried to reach {url}\")\r\n--> 605         raise ConnectionError(\"Couldn't reach {}\".format(url))\r\n    606 \r\n    607     # Try a second time\r\n\r\nConnectionError: Couldn't reach https://drive.google.com/u/0/uc?id=1OletxmPYNkz2ltOr9pyT0b0iBtUWxslh&export=download/\r\n```",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 975210712,
    "title": "Downloading \u201creddit\u201d dataset keeps timing out.",
    "dateCreated": "2021-08-20T02:52:36Z",
    "dateModified": "2021-08-20T02:52:36Z",
    "description": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\nEverytime I try and download the reddit dataset it times out before finishing and I have to try again.\r\n\r\nThere is some timeout error that I will post once it happens again.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"reddit\", ignore_verifications=True, cache_dir=\"/Volumes/My Passport for Mac/og-chat-data\")\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\nI would expect the download to finish, or at least provide a parameter to extend the read timeout window.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\nShown below in error message.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:  1.11.0\r\n- Platform: macOS \r\n- Python version: 3.9.6 (conda env)\r\n- PyArrow version: N/A\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 974683155,
    "title": "Added XL-Sum dataset",
    "dateCreated": "2021-08-19T13:47:45Z",
    "dateModified": "2021-08-19T13:47:45Z",
    "description": "Added XL-Sum dataset published in ACL-IJCNLP 2021. (https://aclanthology.org/2021.findings-acl.413/). The default timeout values in `src/datasets/utils/file_utls.py` were increased to enable downloading from the original google drive links.",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 974552009,
    "title": "cannot load data from my loacal path",
    "dateCreated": "2021-08-19T11:13:30Z",
    "dateModified": "2021-08-19T11:13:30Z",
    "description": "## Describe the bug\r\nI just want to directly load data from my local path,but find a bug.And I compare it with pandas to provide my local path is real.\r\n\r\nhere is my code\r\n```python3\r\n# print my local path\r\nprint(config.train_path)\r\n# read data and print data length\r\ntarin=pd.read_csv(config.train_path)\r\nprint(len(tarin))\r\n\r\n# loading data by load_dataset \r\ndata = load_dataset('csv',data_files=config.train_path)\r\n\r\nprint(len(data))\r\n```\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nC:\\Users\\wie\\Documents\\\u9879\u76ee\\\u6587\u672c\u5206\u7c7b\\data\\train.csv\r\n7613\r\nTraceback (most recent call last):\r\n  File \"c:/Users/wie/Documents/\u9879\u76ee/\u6587\u672c\u5206\u7c7b/lib/DataPrecess.py\", line 17, in <module>\r\n    data = load_dataset('csv',data_files=config.train_path)\r\n  File \"C:\\Users\\wie\\Miniconda3\\lib\\site-packages\\datasets\\load.py\", line 830, in load_dataset\r\n    **config_kwargs,\r\n  File \"C:\\Users\\wie\\Miniconda3\\lib\\site-packages\\datasets\\load.py\", line 710, in load_dataset_builder\r\n    **config_kwargs,\r\n  File \"C:\\Users\\wie\\Miniconda3\\lib\\site-packages\\datasets\\builder.py\", line 271, in __init__\r\n    **config_kwargs,\r\n  File \"C:\\Users\\wie\\Miniconda3\\lib\\site-packages\\datasets\\builder.py\", line 386, in _create_builder_config\r\n    config_kwargs, custom_features=custom_features, use_auth_token=self.use_auth_token\r\n  File \"C:\\Users\\wie\\Miniconda3\\lib\\site-packages\\datasets\\builder.py\", line 156, in create_config_id\r\n    raise ValueError(\"Please provide a valid `data_files` in `DatasetBuilder`\")\r\nValueError: Please provide a valid `data_files` in `DatasetBuilder`\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:  1.11.0\r\n- Platform: win10\r\n- Python version: 3.7.9\r\n- PyArrow version: 5.0.0\r\n",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 974486051,
    "title": "Rename The Pile subsets",
    "dateCreated": "2021-08-19T09:56:22Z",
    "dateModified": "2021-08-19T09:56:22Z",
    "description": "After discussing with @yjernite we think it's better to have the subsets of The Pile explicitly have \"the_pile\" in their names.\r\n\r\nI'm doing the changes for the subsets that @richarddwang added:\r\n- [x] books3 -> the_pile_books3 https://github.com/huggingface/datasets/pull/2801\r\n- [x] stack_exchange -> the_pile_stack_exchange https://github.com/huggingface/datasets/pull/2803\r\n- [x] openwebtext2 -> the_pile_openwebtext2 https://github.com/huggingface/datasets/pull/2802\r\n\r\nFor consistency we should also rename `bookcorpusopen` to `the_pile_bookcorpus` IMO, but let me know what you think.\r\n(we can just add a deprecation message to `bookcorpusopen` for now and add `the_pile_bookcorpus`)",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 974031404,
    "title": "Add Mostly Basic Python Problems Dataset",
    "dateCreated": "2021-08-18T20:28:39Z",
    "dateModified": "2021-08-18T20:28:39Z",
    "description": "## Adding a Dataset\r\n- **Name:** Mostly Basic Python Problems Dataset\r\n- **Description:** The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases.\r\n- **Paper:** *link to the dataset paper if available*\r\n- **Data:** https://github.com/google-research/google-research/tree/master/mbpp\r\n- **Motivation:** Simple, small dataset related to coding problems.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 973862024,
    "title": "Tiny typo fixes of \"fo\" -> \"of\"",
    "dateCreated": "2021-08-18T16:36:11Z",
    "dateModified": "2021-08-18T16:36:11Z",
    "description": "Noticed a few of these when reading docs- feel free to ignore the PR and just fix on some main contributor branch if more helpful. Thanks for the great library! :)",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 973632645,
    "title": "Bump tqdm version",
    "dateCreated": "2021-08-18T12:51:29Z",
    "dateModified": "2021-08-18T12:51:29Z",
    "description": "The recently released tqdm 4.62.1 includes a fix for PermissionError on Windows (submitted by me in https://github.com/tqdm/tqdm/pull/1207), which means we can remove expensive `gc.collect` calls by bumping tqdm to that version. This PR does exactly that and, additionally, fixes a `disable_tqdm` definition that would previously, if used, raise a PermissionError on Windows.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 973470580,
    "title": "Remove compression from xopen",
    "dateCreated": "2021-08-18T09:35:59Z",
    "dateModified": "2021-08-18T09:35:59Z",
    "description": "We implemented support for streaming with 2 requirements:\r\n- transparent use for the end user: just needs to pass the parameter `streaming=True`\r\n- no additional work for the contributors: previous loading scripts should also work in streaming mode with no (or minor) changes; and new loading scripts should not involve additional code to support streaming\r\n\r\nIn order to fulfill these requirements, streaming implementation patched some Python functions:\r\n- the `open(urlpath)` function was patched with `fsspec.open(urlpath)`\r\n- the `os.path.join(urlpath, *others)` function was patched in order to add to `urlpath` hops (`::`) and extractor protocols (`zip://`), which are required by `fsspec.open`\r\n\r\nRecently, we implemented support for streaming all archive+compression formats: zip, tar, gz, bz2, lz4, xz, zst; tar.gz, tar.bz2,...\r\nUnder the hood, the implementation:\r\n- passes an additional parameter `compression` to `fsspec.open`, so that it performs the decompression on the fly: `fsspec.open(urlpath, compression=...)`\r\n\r\nSome concerns have been raised about passing the parameter `compression` to `fsspec.open`:\r\n- https://github.com/huggingface/datasets/pull/2786#discussion_r689550254\r\n- #2811 \r\n\r\nThe main argument is that if `open` decompresses the file and afterwards we call `gzip.open` on it, that will raise an error in `oscar` dataset:\r\n```python\r\ngzip.open(open(urlpath\r\n```\r\nWhile this is true:\r\n- it is not natural/usual to call `open` inside `gzip.open` (never seen this before)\r\n- indeed, this was recently (2 months ago) coded that way in `datasets` in order to allow streaming support (with previous implementation of streaming)\r\n\r\nIn this particular case, there is a natural fix solution: #2811:\r\n- Revert the `open` inside the `gzip.open` (change done 2 months ago): `gzip.open(open(urlpath` => `gzip.open(urlpath`\r\n- Patch `gzip.open(urlpath` with `fsspec.open(urlpath, compression=\"gzip\"` \r\n\r\nAre there other issues apart from this?\r\n\r\nNote that there is an issue just because the open inside of the gzip.open. There is no issue in the other cases where datasets loading scripts use just\r\n- `gzip.open` \r\n- `open` (after having called dl_manager.download_and_extract)\r\n\r\nTODO:\r\n- [ ] Is this really an issue? Please enumerate the `datasets` loading scripts where this is problematic.\r\n  - For the moment, there are only 3 datasets where we have an `open` inside a `gzip.open`:\r\n    - oscar (since 23 June), mc4 (since 2 July) and c4 (since 2 July)\r\n  - In the 3 datasets, the only reason to put an open inside a gzip.open was indeed to force supporting streaming\r\n- [ ] If this is indeed an issue, which are the possible alternatives? Pros/cons?",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 972936889,
    "title": "arXiv Dataset verification problem",
    "dateCreated": "2021-08-17T18:01:48Z",
    "dateModified": "2021-08-17T18:01:48Z",
    "description": "## Describe the bug\r\n`dataset_infos.json` for `arxiv_dataset` contains a fixed number of training examples, however the data (downloaded from an external source) is updated every week with additional examples.\r\nTherefore, loading the dataset without `ignore_verifications=True` results in a verification error.",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 972522480,
    "title": "Fix stream oscar",
    "dateCreated": "2021-08-17T10:10:59Z",
    "dateModified": "2021-08-17T10:10:59Z",
    "description": "Previously, an additional `open` was added to oscar to make it stream-compatible: 587bbb94e891b22863b312b99696e32708c379f4.\r\n\r\nThis was argued that might be problematic: https://github.com/huggingface/datasets/pull/2786#discussion_r690045921\r\n\r\nThis PR:\r\n- removes that additional `open`\r\n- patches `gzip.open` with `xopen` + `compression=\"gzip\"`",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 972040022,
    "title": "Add WIT Dataset",
    "dateCreated": "2021-08-16T19:34:09Z",
    "dateModified": "2021-08-16T19:34:09Z",
    "description": "Adds Google's [WIT](https://github.com/google-research-datasets/wit) dataset.",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 971902613,
    "title": "Add Beans Dataset",
    "dateCreated": "2021-08-16T16:22:33Z",
    "dateModified": "2021-08-16T16:22:33Z",
    "description": "Adds the [beans](https://github.com/AI-Lab-Makerere/ibean/) image classification dataset.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 971882320,
    "title": "Enable streaming for Wikipedia corpora",
    "dateCreated": "2021-08-16T15:59:12Z",
    "dateModified": "2021-08-16T15:59:12Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nSeveral of the [Wikipedia corpora](https://huggingface.co/datasets?search=wiki) on the Hub involve quite large files that would be a good candidate for streaming. Currently it is not possible to stream these corpora:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n# Throws ValueError: Builder wikipedia is not streamable.\r\nwiki_dataset_streamed = load_dataset(\"wikipedia\", \"20200501.en\", split=\"train\", streaming=True)\r\n```\r\n\r\nGiven that these corpora are derived from Wikipedia dumps in XML format which are then processed with Apache Beam, I am not sure whether streaming is possible in principle. The goal of this issue is to discuss whether this feature even makes sense :)\r\n\r\n**Describe the solution you'd like**\r\nIt would be nice to be able to stream Wikipedia corpora from the Hub with something like\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nwiki_dataset_streamed = load_dataset(\"wikipedia\", \"20200501.en\", split=\"train\", streaming=True)\r\n```",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 971849863,
    "title": "Add cats_vs_dogs dataset",
    "dateCreated": "2021-08-16T15:21:11Z",
    "dateModified": "2021-08-16T15:21:11Z",
    "description": "Adds Microsoft's [Cats vs. Dogs](https://www.microsoft.com/en-us/download/details.aspx?id=54765) dataset.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 971625449,
    "title": "Fix streaming tar files from canonical datasets",
    "dateCreated": "2021-08-16T11:10:28Z",
    "dateModified": "2021-08-16T11:10:28Z",
    "description": "Previous PR #2800 implemented support to stream remote tar files when passing the parameter `data_files`: they required a glob string `\"*\"`.\r\n\r\nHowever, this glob string creates an error when streaming canonical datasets (with a `join` after the `open`).\r\n\r\nThis PR fixes this issue and allows streaming tar files both from:\r\n- canonical datasets scripts and\r\n- data files.\r\n\r\nThis PR also adds support for compressed tar files: `.tar.gz`, `.tar.bz2`,...\r\n",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 971436456,
    "title": "Fix streaming zip files from canonical datasets",
    "dateCreated": "2021-08-16T07:11:40Z",
    "dateModified": "2021-08-16T07:11:40Z",
    "description": "Previous PR #2798 fixed streaming remote zip files when passing the parameter `data_files`.\r\n\r\nHowever, that broke streaming zip files used in canonical `datasets` scripts, which normally have a subsequent `join()` (patched with `xjoin()`) after the `StreamingDownloadManager.download_and_extract()` is called.\r\n\r\nThis PR fixes this issue and allows streaming zip files both from:\r\n- canonical datasets scripts and\r\n- data files.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 971353437,
    "title": "Add Food-101",
    "dateCreated": "2021-08-16T04:26:15Z",
    "dateModified": "2021-08-16T04:26:15Z",
    "description": "Adds image classification dataset [Food-101](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/).",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 970858928,
    "title": "add stack exchange",
    "dateCreated": "2021-08-14T08:11:02Z",
    "dateModified": "2021-08-14T08:11:02Z",
    "description": "stack exchange is part of EleutherAI/The Pile, but AFAIK, The Pile dataset blend all sub datasets together thus we are not able to use just one of its sub dataset from The Pile data. So I create an independent dataset using The Pile preliminary components.\r\n\r\nI also change default `timeout` to 100 seconds instead of 10 seconds, otherwise I keep getting read time out when downloading source data of stack exchange and cc100 dataset.\r\n\r\nWhen I was creating dataset card. I found there is room for creating / editing dataset card. I've made it an issue. #2797\r\n\r\nAlso I am wondering whether the import of The Pile dataset is actively undertaken (because I may need it recently)? #1675",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 970848302,
    "title": "add openwebtext2",
    "dateCreated": "2021-08-14T07:09:03Z",
    "dateModified": "2021-08-14T07:09:03Z",
    "description": "openwebtext2 is part of EleutherAI/The Pile, but AFAIK, The Pile dataset blend all sub datasets together thus we are not able to use just one of its sub dataset from The Pile data. So I create an independent dataset using The Pile preliminary components.\r\n\r\nWhen I was creating dataset card. I found there is room for creating / editing dataset card. I've made it an issue. #2797\r\n\r\nAlso I am wondering whether the import of The Pile dataset is actively undertaken (because I may need it recently)? #1675",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 970844617,
    "title": "add books3",
    "dateCreated": "2021-08-14T07:04:25Z",
    "dateModified": "2021-08-14T07:04:25Z",
    "description": "books3 is part of EleutherAI/The Pile, but AFAIK, The Pile dataset blend all sub datasets together thus we are not able to use just one of its sub dataset from The Pile data. So I create an independent dataset using The Pile preliminary components.\r\n\r\nWhen I was creating dataset card. I found there is room for creating / editing dataset card. I've made it an issue. #2797 \r\n\r\nAlso I am wondering whether the import of The Pile dataset is actively undertaken (because I may need it recently)? #1675",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 970819988,
    "title": "Support streaming tar files",
    "dateCreated": "2021-08-14T04:40:17Z",
    "dateModified": "2021-08-14T04:40:17Z",
    "description": "This PR adds support to stream tar files by using the `fsspec` tar protocol.\r\n\r\nIt also uses the custom `readline` implemented in PR #2786.\r\n\r\nThe corresponding test is implemented in PR #2786.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 970507351,
    "title": "Loading JSON throws ArrowNotImplementedError",
    "dateCreated": "2021-08-13T15:31:48Z",
    "dateModified": "2021-08-13T15:31:48Z",
    "description": "## Describe the bug\r\nI have created a [dataset](https://huggingface.co/datasets/lewtun/github-issues-test) of GitHub issues in line-separated JSON format and am finding that I cannot load it with the `json` loading script (see stack trace below).\r\n\r\nCuriously, there is no problem loading the dataset with `pandas` which suggests some incorrect type inference is being made on the `datasets` side. For example, the stack trace indicates that some URL fields are being parsed as timestamps.\r\n\r\nYou can find a Colab notebook which reproduces the error [here](https://colab.research.google.com/drive/1YUCM0j1vx5ZrouQbYSzal6RwB4-Aoh4o?usp=sharing).\r\n\r\n**Edit:** If one repeatedly tries to load the dataset, it _eventually_ works but I think it would still be good to understand why it fails in the first place :)\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nfrom huggingface_hub import hf_hub_url\r\nimport pandas as pd\r\n\r\n# returns https://huggingface.co/datasets/lewtun/github-issues-test/resolve/main/issues-datasets.jsonl\r\ndata_files = hf_hub_url(repo_id=\"lewtun/github-issues-test\", filename=\"issues-datasets.jsonl\", repo_type=\"dataset\")\r\n# throws ArrowNotImplementedError\r\ndset = load_dataset(\"json\", data_files=data_files, split=\"test\")\r\n# no problem with pandas ...\r\ndf = pd.read_json(data_files, orient=\"records\", lines=True)\r\ndf.head()\r\n```\r\n\r\n## Expected results\r\nI can load any line-separated JSON file, similar to `pandas`.\r\n\r\n## Actual results\r\n```\r\n---------------------------------------------------------------------------\r\nArrowNotImplementedError                  Traceback (most recent call last)\r\n<ipython-input-7-5b8e82b6c3a2> in <module>()\r\n----> 1 dset = load_dataset(\"json\", data_files=data_files, split=\"test\")\r\n\r\n9 frames\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowNotImplementedError: JSON conversion to struct<url: timestamp[s], html_url: timestamp[s], labels_url: timestamp[s], id: int64, node_id: timestamp[s], number: int64, title: timestamp[s], description: timestamp[s], creator: struct<login: timestamp[s], id: int64, node_id: timestamp[s], avatar_url: timestamp[s], gravatar_id: timestamp[s], url: timestamp[s], html_url: timestamp[s], followers_url: timestamp[s], following_url: timestamp[s], gists_url: timestamp[s], starred_url: timestamp[s], subscriptions_url: timestamp[s], organizations_url: timestamp[s], repos_url: timestamp[s], events_url: timestamp[s], received_events_url: timestamp[s], type: timestamp[s], site_admin: bool>, open_issues: int64, closed_issues: int64, state: timestamp[s], created_at: timestamp[s], updated_at: timestamp[s], due_on: timestamp[s], closed_at: timestamp[s]> is not supported\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.11\r\n- PyArrow version: 3.0.0\r\n",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 970493126,
    "title": "Fix streaming zip files",
    "dateCreated": "2021-08-13T15:17:01Z",
    "dateModified": "2021-08-13T15:17:01Z",
    "description": "Currently, streaming remote zip data files gives `FileNotFoundError` message:\r\n```python\r\ndata_files = f\"https://huggingface.co/datasets/albertvillanova/datasets-tests-compression/resolve/main/sample.zip\"\r\nds = load_dataset(\"json\", split=\"train\", data_files=data_files, streaming=True)\r\nnext(iter(ds))\r\n```\r\n\r\nThis PR fixes it by adding a glob string.\r\n\r\nThe corresponding test is implemented in PR #2786.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 970331634,
    "title": "Make creating/editing dataset cards easier, by editing on site and dumping info from test command.",
    "dateCreated": "2021-08-13T11:54:49Z",
    "dateModified": "2021-08-13T11:54:49Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\n\r\nCreating and editing dataset cards should be but not that easy\r\n- If other else know Some information I don't know (bias of dataset, dataset curation, supported dataset, ...), he/she should know the description on hf.co comes from README.md under github huggingface/datasets/datasets/the dataset, and willing to make a pr to add or fix information.\r\n- Many information is also saved in `dataset_info.json` (citaion, description), but still need to write it down to README.md again.\r\n- Contributor need to pip install and start a local server just for tagging the dataset's size. And contributor may be creating the dataset on lab's server, which can't open a browser.  \r\n- if any one proposes a new tag, it doesn't show in the list that another creator see. (a stackoverflow way may be ideal)\r\n- dataset card generator web app doesn't generate the necessary subsecion `Contributions` for us.\r\n\r\n**Describe the solution you'd like**\r\n- Everyone (or at least the author/contributor) can edit the description, information, tags of the dataset, on hf.co website. Just like wikipedia+stackoverflow\r\n-  We can infer the actual data size, citation, data instance, ... from `dataset_info.json` and `dataset.arrow`  via `dataset-cli test`\r\n",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 970235846,
    "title": "add cedr dataset",
    "dateCreated": "2021-08-13T09:37:35Z",
    "dateModified": "2021-08-13T09:37:35Z",
    "description": null,
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 969728545,
    "title": "Warnings and documentation about pickling incorrect",
    "dateCreated": "2021-08-12T23:09:13Z",
    "dateModified": "2021-08-12T23:09:13Z",
    "description": "## Describe the bug\r\n\r\nI have a docs bug and a closely related docs enhancement suggestion!\r\n\r\n### Bug\r\n\r\nThe warning and documentation say \"either `dill` or `pickle`\" for fingerprinting. But it seems that `dill`, which is installed by `datasets` by default, _must_ work, or else the fingerprinting fails.\r\n\r\nWarning:\r\n\r\nhttps://github.com/huggingface/datasets/blob/450b9174765374111e5c6daab0ed294bc3d9b639/src/datasets/fingerprint.py#L262\r\n\r\nDocs:\r\n\r\n> For a transform to be hashable, it needs to be pickleable using dill or pickle.\r\n> \u2013 [docs](https://huggingface.co/docs/datasets/processing.html#fingerprinting)\r\n\r\nFor my code, `pickle` works, but `dill` fails. The `dill` failure has already been reported in https://github.com/huggingface/datasets/issues/2643. However, the `dill` failure causes a hashing failure in the datasets library, without any backing off to `pickle`. This implies that it's not the case that either `dill` **or** `pickle` can work, but that `dill` must work if it is installed. I think this is more accurate wording, since it is installed and used by default:\r\n\r\nhttps://github.com/huggingface/datasets/blob/c93525dc291346e54212567fa72d7d607befe937/setup.py#L83\r\n\r\n... and the hashing will fail if it fails.\r\n\r\n### Enhancement\r\n\r\nI think it'd be very helpful to add to the documentation how to debug hashing failures. It took me a while to figure out how to diagnose this. There is a very nice two-liner by @lhoestq in https://github.com/huggingface/datasets/issues/2516#issuecomment-865173139:\r\n\r\n```python\r\nfrom datasets.fingerprint import Hasher\r\nHasher.hash(my_object)\r\n```\r\n\r\nI think add this to the docs will help future users quickly debug any hashing troubles of their own :-)\r\n\r\n## Steps to reproduce the bug\r\n\r\n`dill` but not `pickle` hashing failure in https://github.com/huggingface/datasets/issues/2643\r\n\r\n## Expected results\r\nIf either `dill` or `pickle` can successfully hash, the hashing will succeed.\r\n\r\n## Actual results\r\nIf `dill` or `pickle` cannot hash, the hashing fails.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.9.0\r\n- Platform: Linux-5.8.0-1038-gcp-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- PyArrow version: 4.0.1\r\n",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 968967773,
    "title": "Fix type hint for data_files",
    "dateCreated": "2021-08-12T14:42:37Z",
    "dateModified": "2021-08-12T14:42:37Z",
    "description": "Fix type hint for `data_files` in signatures and docstrings.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 968650274,
    "title": "Update: GooAQ - add train/val/test splits",
    "dateCreated": "2021-08-12T11:40:18Z",
    "dateModified": "2021-08-12T11:40:18Z",
    "description": "[GooAQ](https://github.com/allenai/gooaq) dataset was recently updated after splits were added for the same. This PR contains new updated GooAQ with train/val/test splits and updated README as well.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 968360314,
    "title": "Fix typo in cnn_dailymail",
    "dateCreated": "2021-08-12T08:38:42Z",
    "dateModified": "2021-08-12T08:38:42Z",
    "description": null,
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 967772181,
    "title": "Fix typo in test_dataset_common",
    "dateCreated": "2021-08-12T01:10:29Z",
    "dateModified": "2021-08-12T01:10:29Z",
    "description": null,
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 967361934,
    "title": "Updated dataset description of DaNE",
    "dateCreated": "2021-08-11T19:58:48Z",
    "dateModified": "2021-08-11T19:58:48Z",
    "description": null,
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 967149389,
    "title": "How to sample every file in a list of files making up a split in a dataset when loading?",
    "dateCreated": "2021-08-11T17:43:21Z",
    "dateModified": "2021-08-11T17:43:21Z",
    "description": "I am loading a dataset with multiple train, test, and validation files like this:\r\n\r\n```\r\ndata_files_dict = {\r\n    \"train\": [train_file1, train_file2],\r\n    \"test\": [test_file1, test_file2],\r\n    \"val\": [val_file1, val_file2]\r\n}\r\ndataset = datasets.load_dataset(\r\n    \"csv\",\r\n    data_files=data_files_dict,\r\n    split=['train[:8]', 'test[:8]', 'val[:8]']\r\n)\r\n\r\n```\r\n\r\nHowever, this only selects the first 8 rows from train_file1, test_file1, val_file1, since they are the first files in the lists.\r\n\r\nI'm trying to formulate a split argument that can sample from each file specified in my list of files that make up each split.\r\n\r\nIs this type of splitting supported? If so, how can I do it?",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 967018406,
    "title": "ConnectionError: Couldn't reach https://raw.githubusercontent.com",
    "dateCreated": "2021-08-11T16:19:01Z",
    "dateModified": "2021-08-11T16:19:01Z",
    "description": "Hello,\r\nI am trying to run run_glue.py and it gives me this error -\r\n\r\nTraceback (most recent call last):\r\n  File \"E:/BERT/pytorch_hugging/transformers/examples/pytorch/text-classification/run_glue.py\", line 546, in <module>\r\n    main()\r\n  File \"E:/BERT/pytorch_hugging/transformers/examples/pytorch/text-classification/run_glue.py\", line 250, in main\r\n    datasets = load_dataset(\"glue\", data_args.task_name, cache_dir=model_args.cache_dir)\r\n  File \"C:\\install\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\datasets\\load.py\", line 718, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"C:\\install\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\datasets\\load.py\", line 320, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"C:\\install\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\datasets\\utils\\file_utils.py\", line 291, in cached_path\r\n    use_auth_token=download_config.use_auth_token,\r\n  File \"C:\\install\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\datasets\\utils\\file_utils.py\", line 623, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py\r\n\r\nTrying to do python run_glue.py  --model_name_or_path\r\nbert-base-cased\r\n--task_name\r\nmrpc\r\n--do_train\r\n--do_eval\r\n--max_seq_length\r\n128\r\n--per_device_train_batch_size\r\n32\r\n--learning_rate\r\n2e-5\r\n--num_train_epochs\r\n3\r\n--output_dir\r\n./tmp/mrpc/\r\n\r\nIs this something on my end? From what I can tell, this was re-fixeded by @fullyz a few months ago.\r\nThank you!\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 966282934,
    "title": "Support streaming compressed files",
    "dateCreated": "2021-08-11T09:02:06Z",
    "dateModified": "2021-08-11T09:02:06Z",
    "description": "Add support to stream compressed files (current options in fsspec):\r\n- bz2\r\n- lz4\r\n- xz\r\n- zstd\r\n\r\ncc: @lewtun ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 965461382,
    "title": "Add KS task to SUPERB",
    "dateCreated": "2021-08-10T22:14:07Z",
    "dateModified": "2021-08-10T22:14:07Z",
    "description": "Add the KS (keyword spotting) task as described in the [SUPERB paper](https://arxiv.org/abs/2105.01051).\r\n\r\n- [s3prl instructions](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/README.md#ks-keyword-spotting)\r\n- [s3prl implementation](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/speech_commands/dataset.py)\r\n- [TFDS implementation](https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/audio/speech_commands.py)\r\n\r\nSome notable quirks:\r\n- The dataset is originally single-archive (train+val+test all in one), but the test set has a \"canonical\" distribution in a separate archive, which is also used here (see `_split_ks_files()`). \r\n- The `_background_noise_`/`_silence_` audio files are much longer than others, so they require some sort of slicing for downstream training. I decided to leave the implementation of that up to the users, since TFDS and s3prl take different approaches (either slicing wavs deterministically, or subsampling randomly at runtime)\r\n\r\nRelated to #2619.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 964858439,
    "title": "Fix renaming of corpus_bleu args",
    "dateCreated": "2021-08-10T11:02:34Z",
    "dateModified": "2021-08-10T11:02:34Z",
    "description": "Last `sacrebleu` release (v2.0.0) has renamed `sacrebleu.corpus_bleu` args from `(sys_stream, ref_streams)` to `(hipotheses, references)`: https://github.com/mjpost/sacrebleu/pull/152/files#diff-2553a315bb1f7e68c9c1b00d56eaeb74f5205aeb3a189bc3e527b122c6078795L17-R15\r\n\r\nThis PR passes the args without parameter names, so that it is valid for all versions of `sacrebleu`.\r\n\r\nThis is a partial hotfix of #2781.\r\n\r\nClose #2781.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 964805351,
    "title": "Latest v2.0.0 release of sacrebleu has broken some metrics",
    "dateCreated": "2021-08-10T09:59:41Z",
    "dateModified": "2021-08-10T09:59:41Z",
    "description": "## Describe the bug\r\nAfter `sacrebleu` v2.0.0 release (see changes here: https://github.com/mjpost/sacrebleu/pull/152/files#diff-2553a315bb1f7e68c9c1b00d56eaeb74f5205aeb3a189bc3e527b122c6078795L17-R15), some of `datasets` metrics are broken:\r\n- Default tokenizer `sacrebleu.DEFAULT_TOKENIZER` no longer exists:\r\n  - #2739\r\n  - #2778\r\n- Bleu tokenizers are no longer accessible with `sacrebleu.TOKENIZERS`:\r\n  - #2779\r\n- `corpus_bleu` args have been renamed from `(sys_stream, ref_streams)` to `(hipotheses, references)`: \r\n  - #2782 ",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 964794764,
    "title": "VIVOS dataset for Vietnamese ASR",
    "dateCreated": "2021-08-10T09:47:36Z",
    "dateModified": "2021-08-10T09:47:36Z",
    "description": null,
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 964775085,
    "title": "Fix sacrebleu tokenizers",
    "dateCreated": "2021-08-10T09:24:27Z",
    "dateModified": "2021-08-10T09:24:27Z",
    "description": "Last `sacrebleu` release (v2.0.0) has removed `sacrebleu.TOKENIZERS`: https://github.com/mjpost/sacrebleu/pull/152/files#diff-2553a315bb1f7e68c9c1b00d56eaeb74f5205aeb3a189bc3e527b122c6078795L17-R15\r\n\r\nThis PR makes a hot fix of the bug by using a private function in `sacrebleu`: `sacrebleu.metrics.bleu._get_tokenizer()`.\r\n\r\nEventually, this should be further fixed in order to use only public functions.\r\n\r\nThis is a partial hotfix of #2781.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 964737422,
    "title": "Do not pass tokenize to sacrebleu",
    "dateCreated": "2021-08-10T08:40:37Z",
    "dateModified": "2021-08-10T08:40:37Z",
    "description": "Last `sacrebleu` release (v2.0.0) has removed `sacrebleu.DEFAULT_TOKENIZER`: https://github.com/mjpost/sacrebleu/pull/152/files#diff-2553a315bb1f7e68c9c1b00d56eaeb74f5205aeb3a189bc3e527b122c6078795L17-R15\r\n\r\nThis PR does not pass `tokenize` to `sacrebleu` (note that the user cannot pass it anyway) and `sacrebleu` will use its default, no matter where it is and how it is called.\r\n\r\nRelated to #2739.\r\n\r\nThis is a partial hotfix of #2781.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 964696380,
    "title": "Use packaging to handle versions",
    "dateCreated": "2021-08-10T07:51:39Z",
    "dateModified": "2021-08-10T07:51:39Z",
    "description": "Use packaging module to handle/validate/check versions of Python packages.\r\n\r\nRelated to #2769.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 964400596,
    "title": "document `config.HF_DATASETS_OFFLINE` and precedence",
    "dateCreated": "2021-08-09T21:23:17Z",
    "dateModified": "2021-08-09T21:23:17Z",
    "description": "https://github.com/huggingface/datasets/pull/1976 implemented `HF_DATASETS_OFFLINE`, but:\r\n1. `config.HF_DATASETS_OFFLINE` is not documented\r\n2. the precedence is not documented (env, config)\r\n\r\nI'm thinking it probably should be similar to what it says https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub about `datasets.config.IN_MEMORY_MAX_SIZE`:\r\n\r\nQuote:\r\n> The default in \ud83e\udd17 Datasets is to memory-map the dataset on disk unless you set datasets.config.IN_MEMORY_MAX_SIZE different from 0 bytes (default). In that case, the dataset will be copied in-memory if its size is smaller than datasets.config.IN_MEMORY_MAX_SIZE bytes, and memory-mapped otherwise. This behavior can be enabled by setting either the configuration option datasets.config.IN_MEMORY_MAX_SIZE (higher precedence) or the environment variable HF_DATASETS_IN_MEMORY_MAX_SIZE (lower precedence) to nonzero.\r\n\r\nContext: trying to use `config.HF_DATASETS_OFFLINE` here:\r\nhttps://github.com/bigscience-workshop/Megatron-DeepSpeed/pull/48\r\nbut are uncertain if it's safe, since it's not documented as a public API.\r\n\r\nThank you!\r\n\r\n@lhoestq, @albertvillanova ",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 964303626,
    "title": "`generate_random_fingerprint()` deterministic with \ud83e\udd17Transformers' `set_seed()`",
    "dateCreated": "2021-08-09T19:28:51Z",
    "dateModified": "2021-08-09T19:28:51Z",
    "description": "## Describe the bug\r\n\r\n**Update:** I dug into this to try to reproduce the underlying issue, and I believe it's that `set_seed()` from the `transformers` library makes the \"random\" fingerprint identical each time. I believe this is still a bug, because `datasets` is used exactly this way in `transformers` after `set_seed()` has been called, and I think that using `set_seed()` is a standard procedure to aid reproducibility. I've added more details to reproduce this below.\r\n\r\nHi there! I'm using my own local dataset and custom preprocessing function. My preprocessing function seems to be unpickle-able, perhaps because it is from a closure (will debug this separately). I get this warning, which is expected:\r\n\r\nhttps://github.com/huggingface/datasets/blob/450b9174765374111e5c6daab0ed294bc3d9b639/src/datasets/fingerprint.py#L260-L265\r\n\r\nHowever, what's not expected is that the `datasets` actually _does_ seem to cache and reuse this dataset between runs! After that line, the next thing that's logged looks like:\r\n\r\n```text\r\n Loading cached processed dataset at /home/xxx/.cache/huggingface/datasets/csv/default-xxx/0.0.0/xxx/cache-xxx.arrow\r\n```\r\n\r\nThe path is exactly the same each run (e.g., last 26 runs).\r\n\r\nThis becomes a problem because I'll pass in the `--max_eval_samples` flag to the HuggingFace example script I'm running off of ([run_swag.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/multiple-choice/run_swag.py)).  The fact that the cached dataset is reused means this flag gets ignored. I'll try to load 100 examples, and it will load the full cached 1,000,000.\r\n\r\nI think that\r\n\r\nhttps://github.com/huggingface/datasets/blob/450b9174765374111e5c6daab0ed294bc3d9b639/src/datasets/fingerprint.py#L248\r\n\r\n... is actually consistent because randomness is being controlled in HuggingFace/Transformers for reproducibility. I've added a demo of this below.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n# Contents of print_fingerprint.py\r\nfrom transformers import set_seed\r\nfrom datasets.fingerprint import generate_random_fingerprint\r\nset_seed(42)\r\nprint(generate_random_fingerprint())\r\n```\r\n\r\n```bash\r\nfor i in {0..10}; do\r\n    python print_fingerprint.py\r\ndone\r\n\r\n1c80317fa3b1799d\r\n1c80317fa3b1799d\r\n1c80317fa3b1799d\r\n1c80317fa3b1799d\r\n1c80317fa3b1799d\r\n1c80317fa3b1799d\r\n1c80317fa3b1799d\r\n1c80317fa3b1799d\r\n1c80317fa3b1799d\r\n1c80317fa3b1799d\r\n1c80317fa3b1799d\r\n```\r\n\r\n## Expected results\r\nAfter the \"random hash\" warning is emitted, a random hash is generated, and no outdated cached datasets are reused.\r\n\r\n## Actual results\r\nAfter the \"random hash\" warning is emitted, an identical hash is generated each time, and an outdated cached dataset is reused each run.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n\r\n- `datasets` version: 1.9.0\r\n- Platform: Linux-5.8.0-1038-gcp-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- PyArrow version: 4.0.1",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 963932199,
    "title": "Prevent .map from using multiprocessing when loading from cache",
    "dateCreated": "2021-08-09T12:11:38Z",
    "dateModified": "2021-08-09T12:11:38Z",
    "description": "## Context\r\n\r\nOn our setup, we use different setup to train vs proprocessing datasets. Usually we are able to obtain a high number of cpus to preprocess, which allows us to use `num_proc` however we can't use as many during training phase. Currently if we use `num_proc={whatever the preprocessing value was}` we load from cache, but we get:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"lib/python3.8/site-packages/multiprocess/pool.py\", line 131, in worker\r\n    put((job, i, result))\r\n  File \"lib/python3.8/site-packages/multiprocess/queues.py\", line 371, in put\r\n    self._writer.send_bytes(obj)\r\n  File \"lib/python3.8/site-packages/multiprocess/connection.py\", line 203, in send_bytes\r\n    self._send_bytes(m[offset:offset + size])\r\n  File \"lib/python3.8/site-packages/multiprocess/connection.py\", line 414, in _send_bytes\r\n    self._send(header + buf)\r\n  File \"lib/python3.8/site-packages/multiprocess/connection.py\", line 371, in _send\r\n    n = write(self._handle, buf)\r\nBrokenPipeError: [Errno 32] Broken pipe\r\n```\r\n\r\nOur current guess, is that we're spawning too many processes compared to the number of cpus available, and it's running OOM.  Also we're loading this in DDP setting which means that for each gpu, I need to spawn a high number of processes to match the preprocessing fingerprint.\r\n\r\nInstead what we suggest:\r\n - Allow loading shard sequentially, sharing the same fingerprint as the multiprocessed one, in order to leverage multiprocessing when we actually generate the cache, and remove it when loading from cache.\r\n\r\n## Current issues\r\n\r\n~I'm having a hard time making fingerprints match. For some reason, the multiprocessing and the sequential version generate two different hash.~\r\n\r\n**EDIT**: Turns out multiprocessing and sequential have different `transform` value for fingerprinting (check `fingerprint_transform`) when running `_map_single`:\r\n - sequential : `datasets.arrow_dataset.Dataset._map_single`\r\n - multiprocessing: `datasets.arrow_dataset._map_single`\r\n \r\n This discrepancy is caused by multiprocessing pickling the transformer function, it doesn't seem to keep the `Dataset` hierarchy. I'm still unclear on why `func.__qual_name__` isn't handled correctly in multiprocessing. But replacing `__qualname__` by `__name__` fixes the issue.\r\n\r\n## What was done\r\n\r\n~We try to prevent the usage of multiprocessing when loading a dataset. Instead we load all cached shards sequentially.~\r\n\r\nI couldn't find a nice way to obtain the cached_file_name and check they all exist before deciding to use the multiprocessing flow or not. Instead I expose an optional boolean `sequential` in `map` method.\r\n\r\n## TODO\r\n - [x] Check that the multiprocessed version and the sequential version output the same output\r\n - [x] Check that sequential can load multiprocessed\r\n - [x] Check that multiprocessed can load sequential\r\n \r\n ## Test\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nfrom multiprocessing import Pool\r\nimport random\r\n\r\ndef process(batch, rng):\r\n  length = len(batch[\"text\"])\r\n  return {**batch, \"processed_text\": [f\"PROCESSED {rng.random()}\" for _ in range(length)]}\r\n\r\ndataset = load_dataset(\"stas/openwebtext-10k\", split=\"train\")\r\nprint(dataset.column_names)\r\nprint(type(dataset))\r\n\r\nrng = random.Random(42)\r\ndataset1 = dataset.map(process, batched=True, batch_size=50, num_proc=4, fn_kwargs={\"rng\": rng})\r\n\r\n# This one should be loaded from cache\r\nrng = random.Random(42)\r\ndataset2 = dataset.map(process, batched=True, batch_size=50, num_proc=4, fn_kwargs={\"rng\": rng}, sequential=True)\r\n\r\n# Just to check that the random generator was correct\r\nprint(dataset1[-1][\"processed_text\"])\r\nprint(dataset2[-1][\"processed_text\"])\r\n```\r\n \r\n ## Other solutions\r\n\r\nI chose to load everything sequentially, but we can probably find a way to load shards in parallel using another number of workers (essentially this would be an argument not used for fingerprinting, allowing to allow `m` shards using `n` processes, which would be very useful when same dataset have to be loaded on two different setup, and we still want to leverage cache).\r\n\r\nAlso we can use a env variable similarly to `TOKENIZERS_PARALLELISM` as this seems generally setup related (though this changes slightly if we use multiprocessing).\r\ncc @lhoestq (since I had asked you previously on `num_proc` being used for fingerprinting). Don't know if this is acceptable.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 963730497,
    "title": "Remove dataset_infos.json",
    "dateCreated": "2021-08-09T07:43:19Z",
    "dateModified": "2021-08-09T07:43:19Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nAs discussed, there are infos in the `dataset_infos.json` which are redundant and we could have them only in the README file.\r\n\r\nOthers could be migrated to the README, like: \"dataset_size\", \"size_in_bytes\", \"download_size\", \"splits.split_name.[num_bytes, num_examples]\",...\r\n\r\nHowever, there are others that do not seem too meaningful in the README, like the checksums.\r\n\r\n**Describe the solution you'd like**\r\nOpen a discussion to decide what to do with the `dataset_infos.json` files: which information to be migrated and/or which information to be kept.\r\n\r\ncc: @julien-c @lhoestq ",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 963348834,
    "title": "Remove returned feature constrain",
    "dateCreated": "2021-08-08T04:01:30Z",
    "dateModified": "2021-08-08T04:01:30Z",
    "description": "In the current version, the returned value of the map function has to be list or ndarray. However, this makes it unsuitable for many tasks. In NLP, many features are sparse like verb words, noun chunks, if we want to assign different values to different words, which will result in a large sparse matrix if we only score useful words like verb words.  \r\n\r\nMostly, when using it on large scale, saving it as a whole takes a lot of disk storage and making it hard to read,  the normal method is saving it in sparse form. However, the NumPy does not support sparse, therefore I have to use PyTorch or scipy to transform a matrix into special sparse form, which is not a form that can be transformed into list or ndarry. This violates the feature constraints of the map function. \r\n\r\nI do appreciate the convenience of Datasets package, but I do not think the compulsory datatype constrain is necessary, in some cases, we just cannot transform it into a list or ndarray due to some reasons.  Any way to fix this? Or what I can do to disable the compulsory datatype constrain?\r\n",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 963257036,
    "title": "[WIP][Common Voice 7] Add common voice 7.0",
    "dateCreated": "2021-08-07T16:01:10Z",
    "dateModified": "2021-08-07T16:01:10Z",
    "description": "This PR allows to load the new common voice dataset manually as explained when doing: \r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"./datasets/datasets/common_voice_7\", \"ab\")\r\n```\r\n\r\n=>\r\n\r\n```\r\n                   Please follow the manual download instructions:\r\n\r\n                     You need to manually the dataset from `https://commonvoice.mozilla.org/en/datasets`.\r\n                     Make sure you choose the version `Common Voice Corpus 7.0`.\r\n                     Choose a language of your choice and find the corresponding language-id, *e.g.*, `Abkhaz` with language-id `ab`. The following language-ids are available:\r\n\r\n                     ['ab', 'ar', 'as', 'az', 'ba', 'bas', 'be', 'bg', 'br', 'ca', 'cnh', 'cs', 'cv', 'cy', 'de', 'dv', 'el', 'en', 'eo', 'es', 'et', 'eu', 'fa', 'fi', 'fr', 'fy-NL', 'ga-IE', 'gl', 'gn', 'ha', 'hi', 'hsb', 'hu', 'hy-AM', 'ia', 'id', 'it', 'ja', 'ka', 'kab', 'kk', 'kmr', 'ky', 'lg', 'lt', 'lv', 'mn', 'mt', 'nl', 'or', 'pa-IN', 'pl', 'pt', 'rm-sursilv', 'rm-vallader', 'ro', 'ru', 'rw', 'sah', 'sk', 'sl', 'sr', 'sv-SE', 'ta', 'th', 'tr', 'tt', 'ug', 'uk', 'ur', 'uz', 'vi', 'vot', 'zh-CN', 'zh-HK', 'zh-TW']\r\n\r\n                     Next, you will have to enter your email address to download the dataset in the `tar.gz` format. Save the file under <path-to-file>.\r\n                     The file should then be extracted with: ``tar -xvzf <path-to-file>`` which will extract a folder called ``cv-corpus-7.0-2021-07-21``.\r\n                     The dataset can then be loaded with `datasets.load_dataset(\"common_voice\", <language-id>, data_dir=\"<path-to-'cv-corpus-7.0-2021-07-21'-folder>\", ignore_verifications=True).\r\n```\r\n\r\nHaving followed those instructions one can then download the data as follows: \r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"./datasets/datasets/common_voice_7\", \"ab\", data_dir=\"./cv-corpus-7.0-2021-07-21/\", ignore_verifications=True)\r\n```\r\n\r\n## TODO\r\n- [ ] Discuss naming. Is the name ok here \"common_voice_7\"? The dataset script differs only really in one point from `common_voice.py` in that all the metadata is different (more hours etc...) and that it has to use manual data dir for now\r\n- [ ] Ideally we should get a bundled download link. For `common_voice.py` there is a bundled download link: `https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-6.1-2020-12-11/{}.tar.gz` that allows one to directly download the data. However such a link is missing for Common Voice 7. I guess we should try to contact common voice about it and ask whether we could host the data or help otherwise somehow. See: https://github.com/common-voice/common-voice-bundler/issues/15 cc @yjernite \r\n- [ ] I did not compute the dataset.json and it would mean that I'd have to download 76 datasets totalling around 1TB manually before running the checksum command. This just takes too much time. For now the user will have to add a `ignore_verifications=True` to download the data. This step would also be much easier if we could get a bundled link\r\n- [ ] Add dummy data",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 963246512,
    "title": "Add support for fast tokenizer in BertScore",
    "dateCreated": "2021-08-07T15:00:03Z",
    "dateModified": "2021-08-07T15:00:03Z",
    "description": "This PR adds support for a fast tokenizer in BertScore, which has been added recently to the lib.\r\nFixes #2765 ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 963240802,
    "title": "Allow PyArrow from source",
    "dateCreated": "2021-08-07T14:26:44Z",
    "dateModified": "2021-08-07T14:26:44Z",
    "description": "When installing pyarrow from source the version is:\r\n\r\n```python\r\n>>> import pyarrow; pyarrow.__version__\r\n'2.1.0.dev612'\r\n```\r\n\r\n-> however this breaks the install check at init of `datasets`. This PR makes sure that everything coming after the last `'.'` is removed.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 963229173,
    "title": "`ArrowInvalid: Added column's length must match table's length.` after using `select`",
    "dateCreated": "2021-08-07T13:17:29Z",
    "dateModified": "2021-08-07T13:17:29Z",
    "description": "## Describe the bug\r\nI would like to add a column to a downsampled dataset. However I get an error message saying the length don't match with the length of the unsampled dataset indicated. I suspect that the dataset size is not updated when calling `select`.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"tweets_hate_speech_detection\")['train'].select(range(128))\r\nds = ds.add_column('ones', [1]*128)\r\n```\r\n\r\n## Expected results\r\nI would expect a new column named `ones` filled with `1`. When I check the length of `ds` it says `128`. Interestingly, it works when calling `ds = ds.map(lambda x: x)` before adding the column.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n```python\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\n/var/folders/l4/2905jygx4tx5jv8_kn03vxsw0000gn/T/ipykernel_6301/868709636.py in <module>\r\n      1 from datasets import load_dataset\r\n      2 ds = load_dataset(\"tweets_hate_speech_detection\")['train'].select(range(128))\r\n----> 3 ds = ds.add_column('ones', [0]*128)\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    183         }\r\n    184         # apply actual function\r\n--> 185         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    186         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    187         # re-apply format to the output\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    395             # Call actual function\r\n    396 \r\n--> 397             out = func(self, *args, **kwargs)\r\n    398 \r\n    399             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/arrow_dataset.py in add_column(self, name, column, new_fingerprint)\r\n   2965         column_table = InMemoryTable.from_pydict({name: column})\r\n   2966         # Concatenate tables horizontally\r\n-> 2967         table = ConcatenationTable.from_tables([self._data, column_table], axis=1)\r\n   2968         # Update features\r\n   2969         info = self.info.copy()\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/table.py in from_tables(cls, tables, axis)\r\n    715             table_blocks = to_blocks(table)\r\n    716             blocks = _extend_blocks(blocks, table_blocks, axis=axis)\r\n--> 717         return cls.from_blocks(blocks)\r\n    718 \r\n    719     @property\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/table.py in from_blocks(cls, blocks)\r\n    663             return cls(table, blocks)\r\n    664         else:\r\n--> 665             table = cls._concat_blocks_horizontally_and_vertically(blocks)\r\n    666             return cls(table, blocks)\r\n    667 \r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/table.py in _concat_blocks_horizontally_and_vertically(cls, blocks)\r\n    623             if not tables:\r\n    624                 continue\r\n--> 625             pa_table_horizontally_concatenated = cls._concat_blocks(tables, axis=1)\r\n    626             pa_tables_to_concat_vertically.append(pa_table_horizontally_concatenated)\r\n    627         return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/table.py in _concat_blocks(blocks, axis)\r\n    612                 else:\r\n    613                     for name, col in zip(table.column_names, table.columns):\r\n--> 614                         pa_table = pa_table.append_column(name, col)\r\n    615             return pa_table\r\n    616         else:\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.append_column()\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.add_column()\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Added column's length must match table's length. Expected length 31962 but got length 128\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyArrow version: 5.0.0\r\n",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 963002120,
    "title": "equal operation to perform unbatch for huggingface datasets ",
    "dateCreated": "2021-08-06T19:45:52Z",
    "dateModified": "2021-08-06T19:45:52Z",
    "description": "Hi\r\nI need to use \"unbatch\" operation in tensorflow on a huggingface dataset, I could not find this operation, could you kindly direct me how I can do it, here is the problem I am trying to solve:\r\n\r\nI am considering \"record\" dataset in SuperGlue and I need to replicate each entery of the dataset for each answer, to make it similar to what T5 originally did:\r\n\r\nhttps://github.com/google-research/text-to-text-transfer-transformer/blob/3c58859b8fe72c2dbca6a43bc775aa510ba7e706/t5/data/preprocessors.py#L925\r\n\r\nHere please find an example:\r\n\r\n  For example, a typical example from ReCoRD might look like\r\n  {\r\n      'passsage': 'This is the passage.',\r\n      'query': 'A @placeholder is a bird.',\r\n      'entities': ['penguin', 'potato', 'pigeon'],\r\n      'answers': ['penguin', 'pigeon'],\r\n  }\r\n  and I need a prosessor which would turn this example into the following two examples:\r\n  {\r\n      'inputs': 'record query: A @placeholder is a bird. entities: penguin, '\r\n                'potato, pigeon passage: This is the passage.',\r\n      'targets': 'penguin',\r\n  }\r\n  and\r\n  {\r\n      'inputs': 'record query: A @placeholder is a bird. entities: penguin, '\r\n                'potato, pigeon passage: This is the passage.',\r\n      'targets': 'pigeon',\r\n  }\r\n\r\n\r\nFor doing this, one need unbatch, as each entry can map to multiple samples depending on the number of answers, I am not sure how to perform this operation with  huggingface datasets library and greatly appreciate your help\r\n\r\n@lhoestq \r\n\r\nThank you very much.\r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 962994198,
    "title": "fix typo (ShuffingConfig -> ShufflingConfig)",
    "dateCreated": "2021-08-06T19:31:40Z",
    "dateModified": "2021-08-06T19:31:40Z",
    "description": "pretty straightforward, it should be Shuffling instead of Shuffing",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 962861395,
    "title": "BERTScore Error",
    "dateCreated": "2021-08-06T15:58:57Z",
    "dateModified": "2021-08-06T15:58:57Z",
    "description": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\npredictions = [\"hello there\", \"general kenobi\"]\r\nreferences = [\"hello there\", \"general kenobi\"]\r\nbert = load_metric('bertscore')\r\nbert.compute(predictions=predictions, references=references,lang='en')\r\n```\r\n\r\n# Bug\r\n`TypeError: get_hash() missing 1 required positional argument: 'use_fast_tokenizer'`\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform: Colab \r\n- Python version:\r\n- PyArrow version:\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 962554799,
    "title": "Add DER metric for SUPERB speaker diarization task",
    "dateCreated": "2021-08-06T09:12:36Z",
    "dateModified": "2021-08-06T09:12:36Z",
    "description": null,
    "status": "open",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 961895523,
    "title": "English wikipedia datasets is not clean",
    "dateCreated": "2021-08-05T14:37:24Z",
    "dateModified": "2021-08-05T14:37:24Z",
    "description": "## Describe the bug\r\nWikipedia english dumps contain many wikipedia paragraphs like \"References\", \"Category:\" and \"See Also\" that should not be used for training.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom datasets import load_dataset\r\nw = load_dataset('wikipedia', '20200501.en')\r\nprint(w['train'][0]['text'])\r\n```\r\n\r\n> 'Yangliuqing () is a market town in Xiqing District, in the western suburbs of Tianjin, People\\'s Republic of China. Despite its relatively small size, it has been named since 2006 in the \"famous historical and cultural market towns in China\".\\n\\nIt is best known in China for creating nianhua or Yangliuqing nianhua. For more than 400 years, Yangliuqing has in effect specialised in the creation of these woodcuts for the New Year.  wood block prints using vivid colourschemes to portray traditional scenes of children\\'s games often interwoven with auspiciouse objects.\\n\\n, it had 27 residential communities () and 25 villages under its administration.\\n\\nShi Family Grand Courtyard\\n\\nShi Family Grand Courtyard (Ti\u0101nj\u012bn Sh\u00ed Ji\u0101 D\u00e0 Yu\u00e0n, \u5929\u6d25\u77f3\u5bb6\u5927\u9662) is situated in Yangliuqing Town of Xiqing District, which is the former residence of wealthy merchant Shi Yuanshi - the 4th son of Shi Wancheng, one of the eight great masters in Tianjin. First built in 1875, it covers over 6,000 square meters, including large and small yards and over 200 folk houses, a theater and over 275 rooms that served as apartments and places of business and worship for this powerful family. Shifu Garden, which finished its expansion in October 2003, covers 1,200 square meters, incorporates the elegance of imperial garden and delicacy of south garden. Now the courtyard of Shi family covers about 10,000 square meters, which is called the first mansion in North China. Now it serves as the folk custom museum in Yangliuqing, which has a large collection of folk custom museum in Yanliuqing, which has a large collection of folk art pieces like Yanliuqing New Year pictures, brick sculpture.\\n\\nShi\\'s ancestor came from Dong\\'e County in Shandong Province, engaged in water transport of grain. As the wealth gradually accumulated, the Shi Family moved to Yangliuqing and bought large tracts of land and set up their residence. Shi Yuanshi came from the fourth generation of the family, who was a successful businessman and a good household manager, and the residence was thus enlarged for several times until it acquired the present scale. It is believed to be the first mansion in the west of Tianjin.\\n\\nThe residence is symmetric based on the axis formed by a passageway in the middle, on which there are four archways. On the east side of the courtyard, there are traditional single-story houses with rows of rooms around the four sides, which was once the living area for the Shi Family. The rooms on north side were the accountants\\' office. On the west are the major constructions including the family hall for worshipping Buddha, theater and the south reception room. On both sides of the residence are side yard rooms for maids and servants.\\n\\nToday, the Shi mansion, located in the township of Yangliuqing to the west of central Tianjin, stands as a surprisingly well-preserved monument to China\\'s pre-revolution mercantile spirit. It also serves as an on-location shoot for many of China\\'s popular historical dramas. Many of the rooms feature period furniture, paintings and calligraphy, and the extensive Shifu Garden.\\n\\nPart of the complex has been turned into the Yangliuqing Museum, which includes displays focused on symbolic aspects of the courtyards\\'  construction, local folk art and customs, and traditional period furnishings and crafts.\\n\\n**See also \\n\\nList of township-level divisions of Tianjin\\n\\nReferences \\n\\n http://arts.cultural-china.com/en/65Arts4795.html\\n\\nCategory:Towns in Tianjin'**\r\n\r\n## Expected results\r\nI expect no junk in the data.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n- `datasets` version: 1.10.2\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyArrow version: 3.0.0\r\n",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 961652046,
    "title": "Add RVL-CDIP dataset",
    "dateCreated": "2021-08-05T09:57:05Z",
    "dateModified": "2021-08-05T09:57:05Z",
    "description": "## Adding a Dataset\r\n- **Name:** RVL-CDIP\r\n- **Description:** The RVL-CDIP (Ryerson Vision Lab Complex Document Information Processing) dataset consists of 400,000 grayscale images in 16 classes, with 25,000 images per class. There are 320,000 training images, 40,000 validation images, and 40,000 test images. The images are sized so their largest dimension does not exceed 1000 pixels.\r\n- **Paper:** https://www.cs.cmu.edu/~aharley/icdar15/\r\n- **Data:** https://www.cs.cmu.edu/~aharley/rvl-cdip/\r\n- **Motivation:** I'm currently adding LayoutLMv2 and LayoutXLM to HuggingFace Transformers. LayoutLM (v1) already exists in the library. This dataset has a large value for document image classification (i.e. classifying scanned documents). LayoutLM models obtain SOTA on this dataset, so would be great to directly use it in notebooks.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 961568287,
    "title": "Error loading C4 realnewslike dataset",
    "dateCreated": "2021-08-05T08:16:58Z",
    "dateModified": "2021-08-05T08:16:58Z",
    "description": "## Describe the bug\r\nError loading C4 realnewslike dataset. Validation part mismatch\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n raw_datasets = load_dataset('c4', 'realnewslike', cache_dir=model_args.cache_dir)\r\n## Expected results\r\nsuccess on data loading\r\n## Actual results\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.3M/15.3M [00:00<00:00, 28.1MB/s]Traceback (most recent call last):                                                                                                                                                                                                                                             \r\n  File \"run_mlm_tf.py\", line 794, in <module>                                                                                                                                                                                                                                  \r\n    main()                                                                                                                                                                                                                                                                     \r\n  File \"run_mlm_tf.py\", line 425, in main                                                                                                                                                                                                                                      \r\n    raw_datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)                                                                                                                                                           File \"/home/dshirron/.local/lib/python3.8/site-packages/datasets/load.py\", line 843, in load_dataset                                                                                                                                                                         \r\n    builder_instance.download_and_prepare(                                                                                                                                                                                                                                     \r\n  File \"/home/dshirron/.local/lib/python3.8/site-packages/datasets/builder.py\", line 608, in download_and_prepare                                                                                                                                                              \r\n    self._download_and_prepare(                                                                                                                                                                                                                                                \r\n  File \"/home/dshirron/.local/lib/python3.8/site-packages/datasets/builder.py\", line 698, in _download_and_prepare                                                                                                                                                                 verify_splits(self.info.splits, split_dict)                                                                                                                                                                                                                                  File \"/home/dshirron/.local/lib/python3.8/site-packages/datasets/utils/info_utils.py\", line 74, in verify_splits                                                                                                                                                             \r\n    raise NonMatchingSplitsSizesError(str(bad_splits))                                                                                                                                                                                                                         \r\ndatasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='validation', num_bytes=38165657946, num_examples=13799838, dataset_name='c4'), 'recorded': SplitInfo(name='validation', num_bytes=37875873, num_examples=13863, dataset_name='c4')}] \r\n\r\n## Environment info\r\n- `datasets` version: 1.10.2\r\n- Platform: Linux-5.4.0-58-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 4.0.1",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 961372667,
    "title": "Add Nuswide dataset",
    "dateCreated": "2021-08-05T03:00:41Z",
    "dateModified": "2021-08-05T03:00:41Z",
    "description": "## Adding a Dataset\r\n- **Name:** *NUSWIDE*\r\n- **Description:** *[A Real-World Web Image Dataset from National University of Singapore](https://lms.comp.nus.edu.sg/wp-content/uploads/2019/research/nuswide/NUS-WIDE.html)*\r\n- **Paper:** *[here](https://lms.comp.nus.edu.sg/wp-content/uploads/2019/research/nuswide/nuswide-civr2009.pdf)*\r\n- **Data:** *[here](https://github.com/wenting-zhao/nuswide)*\r\n- **Motivation:** *This dataset is a benchmark in the Text Retrieval task.*\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 960636572,
    "title": "the meteor metric seems not consist with the official version",
    "dateCreated": "2021-08-04T15:33:17Z",
    "dateModified": "2021-08-04T15:33:17Z",
    "description": "## Describe the bug\r\nThe computed meteor score seems strange because the value is very different from the scores computed by other tools. For example, I use the meteor score computed by [NLGeval](https://github.com/Maluuba/nlg-eval) as the reference (which reuses the official jar file for the computation)\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_metric\r\nfrom nlgeval import NLGEval, compute_individual_metrics\r\n\r\nmeteor = load_metric('meteor')\r\npredictions = [\"It is a guide to action which ensures that the military always obeys the commands of the party\"]\r\nreferences = [\"It is a guide to action that ensures that the military will forever heed Party commands\"]\r\nresults = meteor.compute(predictions=predictions, references=references)\r\n# print the actual result\r\nprint(round(results[\"meteor\"], 4))\r\nmetrics_dict = compute_individual_metrics(references, predictions[0])\r\n# print the expected result\r\nprint(round(metrics_dict[\"METEOR\"], 4))\r\n```\r\nBy the way, you need to install the `nlg-eval` library first. Please check the installation guide [here](https://github.com/Maluuba/nlg-eval#setup), thanks!\r\n\r\n## Expected results\r\n`0.4474`\r\n\r\n## Actual results\r\n`0.7398`\r\n\r\n## Environment info\r\n- `datasets` version: 1.10.2\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyArrow version: 4.0.1\r\n",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 960206575,
    "title": "Raise ManualDownloadError when loading a dataset that requires previous manual download",
    "dateCreated": "2021-08-04T10:19:55Z",
    "dateModified": "2021-08-04T10:19:55Z",
    "description": "This PR implements the raising of a `ManualDownloadError` when loading a dataset that requires previous manual download, and this is missing.\r\n\r\nThe `ManualDownloadError` is raised whether the dataset is loaded in normal or streaming mode.\r\n\r\nClose #2749.\r\n\r\ncc: @severo ",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 959984081,
    "title": "Unexpected type after `concatenate_datasets`",
    "dateCreated": "2021-08-04T07:10:39Z",
    "dateModified": "2021-08-04T07:10:39Z",
    "description": "## Describe the bug\r\nI am trying to concatenate two `Dataset` using `concatenate_datasets` but it turns out that after concatenation the features are casted from `torch.Tensor` to `list`. \r\nIt then leads to a weird tensors when trying to convert it to a `DataLoader`. However, if I use each `Dataset` separately everything behave as expected.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n>>> featurized_teacher\r\nDataset({\r\n    features: ['t_labels', 't_input_ids', 't_token_type_ids', 't_attention_mask'],\r\n    num_rows: 502\r\n})\r\n>>> for f in featurized_teacher.features:\r\n     print(featurized_teacher[f].shape)\r\ntorch.Size([502])\r\ntorch.Size([502, 300])\r\ntorch.Size([502, 300])\r\ntorch.Size([502, 300])\r\n\r\n>>> featurized_student\r\nDataset({\r\n    features: ['s_features', 's_labels'],\r\n    num_rows: 502\r\n})\r\n>>> for f in featurized_student.features:\r\n     print(featurized_student[f].shape)\r\ntorch.Size([502, 64])\r\ntorch.Size([502])\r\n```\r\nThe shapes seem alright to me. Then the results after concatenation are as follow:\r\n```python\r\n>>> concat_dataset = datasets.concatenate_datasets([featurized_student, featurized_teacher], axis=1)\r\n>>> type(concat_dataset[\"t_labels\"])\r\n<class 'list'>\r\n```\r\nOne would expect to obtain the same type as the one before concatenation.\r\n\r\nAm I doing something wrong here? Any idea on how to fix this unexpected behavior?\r\n\r\n## Environment info\r\n- `datasets` version: 1.9.0\r\n- Platform: macOS-10.14.6-x86_64-i386-64bit\r\n- Python version: 3.9.5\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 959255646,
    "title": "Fix metadata JSON for ubuntu_dialogs_corpus dataset",
    "dateCreated": "2021-08-03T15:48:59Z",
    "dateModified": "2021-08-03T15:48:59Z",
    "description": "Related to #2743.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 959115888,
    "title": "Fix metadata JSON for turkish_movie_sentiment dataset",
    "dateCreated": "2021-08-03T13:25:44Z",
    "dateModified": "2021-08-03T13:25:44Z",
    "description": "Related to #2743.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 959105577,
    "title": "Generate metadata JSON for telugu_books dataset",
    "dateCreated": "2021-08-03T13:14:52Z",
    "dateModified": "2021-08-03T13:14:52Z",
    "description": "Related to #2743.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 959036995,
    "title": "Generate metadata JSON for reclor dataset",
    "dateCreated": "2021-08-03T11:52:29Z",
    "dateModified": "2021-08-03T11:52:29Z",
    "description": "Related to #2743.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 959023608,
    "title": "Generate metadata JSON for lm1b dataset",
    "dateCreated": "2021-08-03T11:34:56Z",
    "dateModified": "2021-08-03T11:34:56Z",
    "description": "Related to #2743.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 959021262,
    "title": "Update metadata for wikihow dataset",
    "dateCreated": "2021-08-03T11:31:57Z",
    "dateModified": "2021-08-03T11:31:57Z",
    "description": "Update metadata for wikihow dataset:\r\n- Remove leading new line character in description and citation\r\n- Update metadata JSON\r\n- Remove no longer necessary `urls_checksums/checksums.txt` file\r\n\r\nRelated to #2748.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 958984730,
    "title": "Second concatenation of datasets produces errors",
    "dateCreated": "2021-08-03T10:47:04Z",
    "dateModified": "2021-08-03T10:47:04Z",
    "description": "Hi,\r\n\r\nI am need to concatenate my dataset with others several times, and after I concatenate it for the second time, the features of features (e.g. tags names) are collapsed. This hinders, for instance, the usage of tokenize function with `data.map`.\r\n\r\n```\r\nfrom datasets import load_dataset, concatenate_datasets\r\n\r\ndata = load_dataset('trec')['train']\r\nconcatenated = concatenate_datasets([data, data])\r\nconcatenated_2 = concatenate_datasets([concatenated, concatenated])\r\nprint('True features of features:', concatenated.features)\r\nprint('\\nProduced features of features:', concatenated_2.features)\r\n```\r\noutputs \r\n\r\n```\r\nTrue features of features: {'label-coarse': ClassLabel(num_classes=6, names=['DESC', 'ENTY', 'ABBR', 'HUM', 'NUM', 'LOC'], names_file=None, id=None), 'label-fine': ClassLabel(num_classes=47, names=['manner', 'cremat', 'animal', 'exp', 'ind', 'gr', 'title', 'def', 'date', 'reason', 'event', 'state', 'desc', 'count', 'other', 'letter', 'religion', 'food', 'country', 'color', 'termeq', 'city', 'body', 'dismed', 'mount', 'money', 'product', 'period', 'substance', 'sport', 'plant', 'techmeth', 'volsize', 'instru', 'abb', 'speed', 'word', 'lang', 'perc', 'code', 'dist', 'temp', 'symbol', 'ord', 'veh', 'weight', 'currency'], names_file=None, id=None), 'text': Value(dtype='string', id=None)}\r\n\r\nProduced features of features: {'label-coarse': Value(dtype='int64', id=None), 'label-fine': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}\r\n```\r\n\r\nI am using `datasets` v.1.11.0",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 958968748,
    "title": "Raise a proper exception when trying to stream a dataset that requires to manually download files",
    "dateCreated": "2021-08-03T10:26:27Z",
    "dateModified": "2021-08-03T10:26:27Z",
    "description": "## Describe the bug\r\n\r\nAt least for 'reclor', 'telugu_books', 'turkish_movie_sentiment', 'ubuntu_dialogs_corpus', 'wikihow', trying to `load_dataset` in streaming mode raises a `TypeError` without any detail about why it fails.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"reclor\", streaming=True)\r\n```\r\n\r\n## Expected results\r\n\r\nIdeally: raise a specific exception, something like `ManualDownloadError`.\r\n\r\nOr at least give the reason in the message, as when we load in normal mode:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"reclor\")\r\n```\r\n\r\n```\r\nAssertionError: The dataset reclor with config default requires manual data.\r\n Please follow the manual download instructions:   to use ReClor you need to download it manually. Please go to its homepage (http://whyu.me/reclor/) fill the google\r\n  form and you will receive a download link and a password to extract it.Please extract all files in one folder and use the path folder in datasets.load_dataset('reclor', data_dir='path/to/folder/folder_name')\r\n  .\r\n Manual data can be loaded with `datasets.load_dataset(reclor, data_dir='<path/to/manual/data>')\r\n```\r\n\r\n## Actual results\r\n\r\n```\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.11.0\r\n- Platform: macOS-11.5-x86_64-i386-64bit\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 958889041,
    "title": "Generate metadata JSON for wikihow dataset",
    "dateCreated": "2021-08-03T08:55:40Z",
    "dateModified": "2021-08-03T08:55:40Z",
    "description": "Related to #2743.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 958867627,
    "title": "add multi-proc in `to_json`",
    "dateCreated": "2021-08-03T08:30:13Z",
    "dateModified": "2021-08-03T08:30:13Z",
    "description": "Closes #2663. I've tried adding multiprocessing in `to_json`. Here's some benchmarking I did to compare the timings of current version (say v1) and multi-proc version (say v2). I did this with `cpu_count` 4 (2015 Macbook Air)\r\n\r\n1. Dataset name: `ascent_kb` - 8.9M samples (all samples were used, reporting this for a single run)\r\nv1- ~225 seconds for converting whole dataset to json\r\nv2- ~200 seconds for converting whole dataset to json\r\n\r\n2. Dataset name: `lama` - 1.3M samples (all samples were used, reporting this for 2 runs)\r\nv1- ~26 seconds for converting whole dataset to json\r\nv2- ~23.6 seconds for converting whole dataset to json\r\n\r\nI think it's safe to say that v2 is 10% faster as compared to v1. Timings may improve further with better configuration.\r\n\r\nThe only bottleneck I feel is writing to file from the output list. If we can improve that aspect then timings may improve further. \r\n\r\nLet me know if any changes/improvements can be done in this @stas00, @lhoestq, @albertvillanova. @lhoestq even suggested to extend this work with other export methods as well like `csv` or `parquet`.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 958551619,
    "title": "Cannot load `few-nerd` dataset",
    "dateCreated": "2021-08-02T22:18:57Z",
    "dateModified": "2021-08-02T22:18:57Z",
    "description": "## Describe the bug\r\n\r\nCannot load `few-nerd` dataset.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset('few-nerd', 'supervised')\r\n```\r\n\r\n## Actual results\r\n\r\nExecuting above code will give the following error:\r\n\r\n```\r\nUsing the latest cached version of the module from /Users/Mehrad/.cache/huggingface/modules/datasets_modules/datasets/few-nerd/62464ace912a40a0f33a11a8310f9041c9dc3590ff2b3c77c14d83ca53cfec53 (last modified on Wed Jun  2 11:34:25 2021) since it couldn't be found locally at /Users/Mehrad/Documents/GitHub/genienlp/few-nerd/few-nerd.py, or remotely (FileNotFoundError).\r\nDownloading and preparing dataset few_nerd/supervised (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/Mehrad/.cache/huggingface/datasets/few_nerd/supervised/0.0.0/62464ace912a40a0f33a11a8310f9041c9dc3590ff2b3c77c14d83ca53cfec53...\r\nTraceback (most recent call last):\r\n  File \"/Users/Mehrad/opt/anaconda3/lib/python3.7/site-packages/datasets/builder.py\", line 693, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/Users/Mehrad/opt/anaconda3/lib/python3.7/site-packages/datasets/builder.py\", line 1107, in _prepare_split\r\n    disable=bool(logging.get_verbosity() == logging.NOTSET),\r\n  File \"/Users/Mehrad/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py\", line 1133, in __iter__\r\n    for obj in iterable:\r\n  File \"/Users/Mehrad/.cache/huggingface/modules/datasets_modules/datasets/few-nerd/62464ace912a40a0f33a11a8310f9041c9dc3590ff2b3c77c14d83ca53cfec53/few-nerd.py\", line 196, in _generate_examples\r\n    with open(filepath, encoding=\"utf-8\") as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/Mehrad/.cache/huggingface/datasets/downloads/supervised/train.json'\r\n```\r\nThe bug is probably in identifying and downloading the dataset. If I download the json splits directly from [link](https://github.com/nbroad1881/few-nerd/tree/main/uncompressed) and put them under the downloads directory, they will be processed into arrow format correctly. \r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Python version: 3.8\r\n- PyArrow version: 1.0.1\r\n",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 958269579,
    "title": "added semeval18_emotion_classification dataset",
    "dateCreated": "2021-08-02T15:39:55Z",
    "dateModified": "2021-08-02T15:39:55Z",
    "description": "I added the data set of SemEval 2018 Task 1 (Subtask 5) for emotion detection in three languages.\r\n\r\n```\r\ndatasets-cli test datasets/semeval18_emotion_classification/ --save_infos --all_configs\r\n\r\nRUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_real_dataset_semeval18_emotion_classification\r\n```\r\nBoth commands ran successfully.\r\n\r\nI couldn't create the dummy data (the files are tsvs but have .txt ending, maybe that's the problem?) and therefore the test on the dummy data fails, maybe someone can help here.\r\n\r\nI also formatted the code:\r\n```\r\nblack --line-length 119 --target-version py36 datasets/semeval18_emotion_classification/\r\nisort datasets/semeval18_emotion_classification/\r\nflake8 datasets/semeval18_emotion_classification/\r\n```\r\nThat's the publication for reference:\r\n\r\nMohammad, S., Bravo-Marquez, F., Salameh, M., & Kiritchenko, S. (2018). SemEval-2018 task 1: Affect in tweets. Proceedings of the 12th International Workshop on Semantic Evaluation, 1\u201317. https://doi.org/10.18653/v1/S18-1001",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 958146637,
    "title": "Fix key by recreating metadata JSON for journalists_questions dataset",
    "dateCreated": "2021-08-02T13:27:53Z",
    "dateModified": "2021-08-02T13:27:53Z",
    "description": "Close #2743.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 958119251,
    "title": "Dataset JSON is incorrect",
    "dateCreated": "2021-08-02T13:01:26Z",
    "dateModified": "2021-08-02T13:01:26Z",
    "description": "## Describe the bug\r\n\r\nThe JSON file generated for https://github.com/huggingface/datasets/blob/573f3d35081cee239d1b962878206e9abe6cde91/datasets/journalists_questions/journalists_questions.py is https://github.com/huggingface/datasets/blob/573f3d35081cee239d1b962878206e9abe6cde91/datasets/journalists_questions/dataset_infos.json.\r\n\r\nThe only config should be `plain_text`, but the first key in the JSON is `journalists_questions` (the dataset id) instead.\r\n\r\n```json\r\n{\r\n  \"journalists_questions\": {\r\n    \"description\": \"The journalists_questions corpus (version 1.0) is a collection of 10K human-written Arabic\\ntweets manually labeled for question identification over Arabic tweets posted by journalists.\\n\",\r\n    ...\r\n```\r\n\r\n## Steps to reproduce the bug\r\n\r\nLook at the files.\r\n\r\n## Expected results\r\n\r\nThe first key should be `plain_text`:\r\n\r\n```json\r\n{\r\n  \"plain_text\": {\r\n    \"description\": \"The journalists_questions corpus (version 1.0) is a collection of 10K human-written Arabic\\ntweets manually labeled for question identification over Arabic tweets posted by journalists.\\n\",\r\n    ...\r\n```\r\n\r\n## Actual results\r\n\r\n```json\r\n{\r\n  \"journalists_questions\": {\r\n    \"description\": \"The journalists_questions corpus (version 1.0) is a collection of 10K human-written Arabic\\ntweets manually labeled for question identification over Arabic tweets posted by journalists.\\n\",\r\n    ...\r\n```\r\n\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 958114064,
    "title": "Improve detection of streamable file types",
    "dateCreated": "2021-08-02T12:55:09Z",
    "dateModified": "2021-08-02T12:55:09Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\nfrom datasets.utils.streaming_download_manager import StreamingDownloadManager\r\nbuilder = load_dataset_builder(\"journalists_questions\", name=\"plain_text\")\r\nbuilder._split_generators(StreamingDownloadManager(base_path=builder.base_path))\r\n```\r\n\r\nraises\r\n\r\n```\r\nNotImplementedError: Extraction protocol for file at https://drive.google.com/uc?export=download&id=1CBrh-9OrSpKmPQBxTK_ji6mq6WTN_U9U is not implemented yet\r\n```\r\n\r\nBut the file at https://drive.google.com/uc?export=download&id=1CBrh-9OrSpKmPQBxTK_ji6mq6WTN_U9U is a text file and it can be streamed:\r\n\r\n```bash\r\ncurl --header \"Range: bytes=0-100\" -L https://drive.google.com/uc\\?export\\=download\\&id\\=1CBrh-9OrSpKmPQBxTK_ji6mq6WTN_U9U\r\n506938088174940160      yes     1\r\n302221719412830209      yes     1\r\n289761704907268096      yes     1\r\n513820885032378369      yes     %\r\n```\r\n\r\nYet, it's wrongly categorized as a file type that cannot be streamed because the test is currently based on 1. the presence of a file extension at the end of the URL (here: no extension), and 2. the inclusion of this extension in a list of supported formats.\r\n\r\n**Describe the solution you'd like**\r\n\r\nIn the case of an URL (instead of a local path), ask for the MIME type, and decide on that value? Note that it would not work in that case, because the value of `content_type` is `text/html; charset=UTF-8`.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nAdd a variable in the dataset script to set the data format by hand.\r\n",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 957979559,
    "title": "Add Hypersim dataset",
    "dateCreated": "2021-08-02T10:06:50Z",
    "dateModified": "2021-08-02T10:06:50Z",
    "description": "## Adding a Dataset\r\n- **Name:** Hypersim\r\n- **Description:** photorealistic synthetic dataset for holistic indoor scene understanding\r\n- **Paper:** *link to the dataset paper if available*\r\n- **Data:** https://github.com/apple/ml-hypersim\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 957911035,
    "title": "Update release instructions",
    "dateCreated": "2021-08-02T08:46:00Z",
    "dateModified": "2021-08-02T08:46:00Z",
    "description": "Update release instructions.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 957751260,
    "title": "Pass tokenize to sacrebleu only if explicitly passed by user",
    "dateCreated": "2021-08-02T05:09:05Z",
    "dateModified": "2021-08-02T05:09:05Z",
    "description": "Next `sacrebleu` release (v2.0.0) will remove `sacrebleu.DEFAULT_TOKENIZER`: https://github.com/mjpost/sacrebleu/pull/152/files#diff-2553a315bb1f7e68c9c1b00d56eaeb74f5205aeb3a189bc3e527b122c6078795L17-R15\r\n\r\nThis PR passes `tokenize` to `sacrebleu` only if explicitly passed by the user, otherwise it will not pass it (and `sacrebleu` will use its default, no matter where it is and how it is called).\r\n\r\nClose: #2737.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 957517746,
    "title": "Sunbird AI Ugandan low resource language dataset",
    "dateCreated": "2021-08-01T15:18:00Z",
    "dateModified": "2021-08-01T15:18:00Z",
    "description": "Multi-way parallel text corpus of 5 key Ugandan languages for the task of machine translation. ",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 957124881,
    "title": "SacreBLEU update",
    "dateCreated": "2021-07-30T23:53:08Z",
    "dateModified": "2021-07-30T23:53:08Z",
    "description": "With the latest release of [sacrebleu](https://github.com/mjpost/sacrebleu), `datasets.metrics.sacrebleu` is broken, and getting error.\r\n\r\n    AttributeError: module 'sacrebleu' has no attribute 'DEFAULT_TOKENIZER'\r\n\r\nthis happens since in new version of sacrebleu there is no `DEFAULT_TOKENIZER`, but sacrebleu.py tries to import it anyways. This can be fixed currently with fixing `sacrebleu==1.5.0`\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nsacrebleu= datasets.load_metric('sacrebleu')\r\npredictions = [\"It is a guide to action which ensures that the military always obeys the commands of the party\"]\r\nreferences = [\"It is a guide to action that ensures that the military will forever heed Party commands\"]\r\nresults = sacrebleu.compute(predictions=predictions, references=references)\r\nprint(results)\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: Python 3.8.0\r\n- PyArrow version: 5.0.0\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 956895199,
    "title": "Add Microsoft Building Footprints dataset",
    "dateCreated": "2021-07-30T16:17:08Z",
    "dateModified": "2021-07-30T16:17:08Z",
    "description": "## Adding a Dataset\r\n- **Name:** Microsoft Building Footprints\r\n- **Description:** With the goal to increase the coverage of building footprint data available as open data for OpenStreetMap and humanitarian efforts, we have released millions of building footprints as open data available to download free of charge.\r\n- **Paper:** *link to the dataset paper if available*\r\n- **Data:** https://www.microsoft.com/en-us/maps/building-footprints\r\n- **Motivation:** this can be a useful dataset for researchers working on climate change adaptation, urban studies, geography, etc.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n\r\nReported by: @sashavor",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 956889365,
    "title": "Add Open Buildings dataset",
    "dateCreated": "2021-07-30T16:08:39Z",
    "dateModified": "2021-07-30T16:08:39Z",
    "description": "## Adding a Dataset\r\n- **Name:** Open Buildings\r\n- **Description:** A dataset of building footprints to support social good applications.\r\n\r\n  Building footprints are useful for a range of important applications, from population estimation, urban planning and humanitarian response, to environmental and climate science. This large-scale open dataset contains the outlines of buildings derived from high-resolution satellite imagery in order to support these types of uses. The project being based in Ghana, the current focus is on the continent of Africa.\r\n\r\n  See: \"Mapping Africa's Buildings with Satellite Imagery\" https://ai.googleblog.com/2021/07/mapping-africas-buildings-with.html\r\n- **Paper:** https://arxiv.org/abs/2107.12283\r\n- **Data:** https://sites.research.google/open-buildings/\r\n- **Motivation:** *what are some good reasons to have this dataset*\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n\r\nReported by: @osanseviero ",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 956844874,
    "title": "Update BibTeX entry",
    "dateCreated": "2021-07-30T15:22:51Z",
    "dateModified": "2021-07-30T15:22:51Z",
    "description": "Update BibTeX entry.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 956725476,
    "title": "Add missing parquet known extension",
    "dateCreated": "2021-07-30T13:01:20Z",
    "dateModified": "2021-07-30T13:01:20Z",
    "description": "This code was failing because the parquet extension wasn't recognized:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nbase_url = \"https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/\"\r\ndata_files = {\"train\": base_url + \"wikipedia-train.parquet\"}\r\nwiki = load_dataset(\"parquet\", data_files=data_files, split=\"train\", streaming=True)\r\n```\r\n\r\nIt raises\r\n```python\r\nNotImplementedError: Extraction protocol for file at https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/wikipedia-train.parquet is not implemented yet\r\n```\r\n\r\nI added `parquet` to the list of known extensions\r\n\r\nEDIT: added pickle, conllu, xml extensions as well",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 956676360,
    "title": "Updated TTC4900 Dataset",
    "dateCreated": "2021-07-30T11:52:14Z",
    "dateModified": "2021-07-30T11:52:14Z",
    "description": "- The source address of the TTC4900 dataset of [@savasy](https://github.com/savasy) has been updated for direct download.\r\n- Updated readme.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 956087452,
    "title": "Adding to_tf_dataset method",
    "dateCreated": "2021-07-29T18:10:25Z",
    "dateModified": "2021-07-29T18:10:25Z",
    "description": "Oh my **god** do not merge this yet, it's just a draft.\r\n\r\nI've added a method (via a mixin) to the `arrow_dataset.Dataset` class that automatically converts our Dataset classes to TF Dataset classes ready for training. It hopefully has most of the features we want, including streaming from disk (no need to load the whole dataset in memory!), correct shuffling, variable-length batches to reduce compute, and correct support for unusual padding. It achieves that by calling the tokenizer `pad` method in the middle of a TF compute graph via a very hacky call to `tf.py_function`, which is heretical but seems to work.\r\n\r\nA number of issues need to be resolved before it's ready to merge, though:\r\n\r\n1) Is a MixIn the right way to do this? Do other classes besides `arrow_dataset.Dataset` need this method too?\r\n2) Needs an argument to support constant-length batches for TPU training - this is easy to add and I'll do it soon.\r\n3) Needs the user to supply the list of columns to drop from the arrow `Dataset`. Is there some automatic way to get the columns we want, or see which columns were added by the tokenizer?\r\n4) Assumes the label column is always present and always called \"label\" - this is probably not great, but I'm not sure what the 'correct' thing to do here is.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 955987834,
    "title": "Update CommonVoice with new release",
    "dateCreated": "2021-07-29T15:59:59Z",
    "dateModified": "2021-07-29T15:59:59Z",
    "description": "## Adding a Dataset\r\n- **Name:** CommonVoice mid-2021 release\r\n- **Description:** more data in CommonVoice: Languages that have increased the most by percentage are Thai (almost 20x growth, from 12 hours to 250 hours), Luganda (almost 9x growth, from 8 to 80), Esperanto (7x growth, from 100 to 840), and Tamil (almost 8x, from 24 to 220).\r\n- **Paper:** https://discourse.mozilla.org/t/common-voice-2021-mid-year-dataset-release/83812\r\n- **Data:** https://commonvoice.mozilla.org/en/datasets\r\n- **Motivation:** More data and more varied. I think we just need to add configs in the existing dataset script.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 955920489,
    "title": "Fix IndexError while loading Arabic Billion Words dataset",
    "dateCreated": "2021-07-29T14:47:02Z",
    "dateModified": "2021-07-29T14:47:02Z",
    "description": "Catch `IndexError` and ignore that record.\r\n\r\nClose #2727.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 955892970,
    "title": "Concurrent use of same dataset (already downloaded)",
    "dateCreated": "2021-07-29T14:18:38Z",
    "dateModified": "2021-07-29T14:18:38Z",
    "description": "## Describe the bug\r\nWhen launching several jobs at the same time loading the same dataset trigger some errors see (last comments).\r\n\r\n## Steps to reproduce the bug\r\nexport HF_DATASETS_CACHE=/gpfswork/rech/toto/datasets\r\nfor MODEL in \"bert-base-uncased\" \"roberta-base\" \"distilbert-base-cased\"; do #  \"bert-base-uncased\" \"bert-large-cased\" \"roberta-large\" \"albert-base-v1\" \"albert-large-v1\"; do\r\n  for TASK_NAME in \"mrpc\" \"rte\" 'imdb' \"paws\" \"mnli\"; do\r\n    export OUTPUT_DIR=${MODEL}_${TASK_NAME}\r\n    sbatch --job-name=${OUTPUT_DIR} \\\r\n      --gres=gpu:1 \\\r\n      --no-requeue \\\r\n      --cpus-per-task=10 \\\r\n      --hint=nomultithread \\\r\n      --time=1:00:00 \\\r\n      --output=jobinfo/${OUTPUT_DIR}_%j.out \\\r\n      --error=jobinfo/${OUTPUT_DIR}_%j.err \\\r\n      --qos=qos_gpu-t4 \\\r\n      --wrap=\"module purge; module load pytorch-gpu/py3/1.7.0 ; export HF_DATASETS_OFFLINE=1; export HF_DATASETS_CACHE=/gpfswork/rech/toto/datasets;  python compute_measures.py --seed=$SEED --saving_path=results --batch_size=$BATCH_SIZE --task_name=$TASK_NAME --model_name=/gpfswork/rech/toto/transformers_models/$MODEL\"\r\n\r\n  done\r\ndone\r\n\r\n\r\n\r\n```python\r\n# Sample code to reproduce the bug\r\n        dataset_train = load_dataset('imdb', split='train', download_mode=\"reuse_cache_if_exists\")\r\n        dataset_train = dataset_train.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length'),\r\n                                          batched=True).select(list(range(args.filter)))\r\n\r\n        dataset_val = load_dataset('imdb', split='train', download_mode=\"reuse_cache_if_exists\")\r\n        dataset_val = dataset_val.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length'),\r\n                                      batched=True).select(list(range(args.filter, args.filter + 5000)))\r\n\r\n        dataset_test = load_dataset('imdb', split='test', download_mode=\"reuse_cache_if_exists\")\r\n        dataset_test = dataset_test.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length'),\r\n                                        batched=True)\r\n```\r\n\r\n## Expected results\r\nI believe I am doing something wrong with the objects. \r\n\r\n## Actual results\r\nTraceback (most recent call last):\r\n  File \"/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/builder.py\", line 652, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/builder.py\", line 983, in _prepare_split\r\n    check_duplicates=True,\r\n  File \"/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/arrow_writer.py\", line 192, in __init__\r\n    self.stream = pa.OSFile(self._path, \"wb\")\r\n  File \"pyarrow/io.pxi\", line 829, in pyarrow.lib.OSFile.__cinit__\r\n  File \"pyarrow/io.pxi\", line 844, in pyarrow.lib.OSFile._open_writable\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 97, in pyarrow.lib.check_status\r\nFileNotFoundError: [Errno 2] Failed to open local file '/gpfswork/rech/tts/unm25jp/datasets/paws/labeled_final/1.1.0/09d8fae989bb569009a8f5b879ccf2924d3e5cd55bfe2e89e6dab1c0b50ecd34.incomplete/paws-test.arrow'. Detail: [errno 2] No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"compute_measures.py\", line 181, in <module>\r\n    train_loader, val_loader, test_loader = get_dataloader(args)\r\n  File \"/gpfsdswork/projects/rech/toto/intRAOcular/dataset_utils.py\", line 69, in get_dataloader\r\n    dataset_train = load_dataset('paws', \"labeled_final\", split='train', download_mode=\"reuse_cache_if_exists\")\r\n  File \"/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/load.py\", line 748, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/builder.py\", line 575, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/builder.py\", line 658, in _download_and_prepare\r\n    + str(e)\r\nOSError: Cannot find data file.\r\nOriginal error:\r\n[Errno 2] Failed to open local file '/gpfswork/rech/toto/datasets/paws/labeled_final/1.1.0/09d8fae989bb569009a8f5b879ccf2924d3e5cd55bfe2e89e6dab1c0b50ecd34.incomplete/paws-test.arrow'. Detail: [errno 2] No such file or directory\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: datasets==1.8.0\r\n- Platform: linux (jeanzay)\r\n- Python version: pyarrow==2.0.0\r\n- PyArrow version: 3.7.8\r\n",
    "status": "open",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 955812149,
    "title": "Error in loading the Arabic Billion Words Corpus",
    "dateCreated": "2021-07-29T12:53:09Z",
    "dateModified": "2021-07-29T12:53:09Z",
    "description": "## Describe the bug\r\nI get `IndexError: list index out of range` when trying to load the `Techreen` and `Almustaqbal` configs of the dataset.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset(\"arabic_billion_words\", \"Techreen\")\r\nload_dataset(\"arabic_billion_words\", \"Almustaqbal\")\r\n```\r\n\r\n## Expected results\r\nThe datasets load succefully.\r\n\r\n## Actual results\r\n```python\r\n_extract_tags(self, sample, tag)\r\n    139             if len(out) > 0:\r\n    140                 break\r\n--> 141         return out[0]\r\n    142 \r\n    143     def _clean_text(self, text):\r\n\r\nIndexError: list index out of range\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.10.2\r\n- Platform: Ubuntu 18.04.5 LTS\r\n- Python version: 3.7.11\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 955674388,
    "title": "Typo fix `tokenize_exemple`",
    "dateCreated": "2021-07-29T10:03:37Z",
    "dateModified": "2021-07-29T10:03:37Z",
    "description": "There is a small typo in the main README.md",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 955020776,
    "title": "Pass use_auth_token to request_etags",
    "dateCreated": "2021-07-28T16:13:29Z",
    "dateModified": "2021-07-28T16:13:29Z",
    "description": "Fix #2724.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 954919607,
    "title": "404 Error when loading remote data files from private repo",
    "dateCreated": "2021-07-28T14:24:23Z",
    "dateModified": "2021-07-28T14:24:23Z",
    "description": "## Describe the bug\r\nWhen loading remote data files from a private repo, a 404 error is raised.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nurl = hf_hub_url(\"lewtun/asr-preds-test\", \"preds.jsonl\", repo_type=\"dataset\")\r\ndset = load_dataset(\"json\", data_files=url, use_auth_token=True)\r\n# HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/datasets/lewtun/asr-preds-test/resolve/main/preds.jsonl\r\n```\r\n\r\n## Expected results\r\nLoad dataset.\r\n\r\n## Actual results\r\n404 Error.\r\n\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 954864104,
    "title": "Fix en subset by modifying dataset_info with correct validation infos",
    "dateCreated": "2021-07-28T13:36:19Z",
    "dateModified": "2021-07-28T13:36:19Z",
    "description": "- Related to: #2682 \r\n\r\nWe correct the values of `en` subset concerning the expected validation values (both `num_bytes` and `num_examples`.\r\n\r\nInstead of having:\r\n\r\n`{\"name\": \"validation\", \"num_bytes\": 828589180707, \"num_examples\": 364868892, \"dataset_name\": \"c4\"}`\r\n\r\nWe replace with correct values:\r\n\r\n`{\"name\": \"validation\", \"num_bytes\": 825767266, \"num_examples\": 364608, \"dataset_name\": \"c4\"}`\r\n\r\nThere are still issues with validation with other subsets, but I can't download all the files, unzip to check for the correct number of bytes. (If you have a fast way to obtain those values for other subsets, I can do this in this PR ... otherwise I can't spend those resources)\r\n",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 954446053,
    "title": "Missing cache file",
    "dateCreated": "2021-07-28T03:52:07Z",
    "dateModified": "2021-07-28T03:52:07Z",
    "description": "Strangely missing cache file after I  restart my program again.\r\n\r\n`glue_dataset = datasets.load_dataset('glue', 'sst2')`\r\n\r\n`FileNotFoundError: [Errno 2] No such file or directory: /Users/chris/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96d6053ad/dataset_info.json'`\r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 954238230,
    "title": "Deal with the bad check in test_load.py",
    "dateCreated": "2021-07-27T20:23:23Z",
    "dateModified": "2021-07-27T20:23:23Z",
    "description": "This PR removes a check that's been added in #2684. My intention with this check was to capture an URL in the error message, but instead, it captures a substring of the previous regex match in the test function. Another option would be to replace this check with:\r\n```python\r\nm_paths = re.findall(r\"\\S*_dummy/_dummy.py\\b\", str(exc_info.value))  # on Linux this will match an URL as well as a local_path due to different os.sep, so take the last element (an URL always comes last in the list)\r\nassert len(m_paths) > 0 and is_remote_url(m_paths[-1])  # is_remote_url comes from datasets.utils.file_utils\r\n```\r\n\r\n@lhoestq Let me know which one of these two approaches (delete or replace) do you prefer?",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 954024426,
    "title": "fix: \ud83d\udc1b fix two typos",
    "dateCreated": "2021-07-27T15:50:17Z",
    "dateModified": "2021-07-27T15:50:17Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 953932416,
    "title": "Use ETag in streaming mode to detect resource updates",
    "dateCreated": "2021-07-27T14:17:09Z",
    "dateModified": "2021-07-27T14:17:09Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\n\r\nI want to cache data I generate from processing a dataset I've loaded in streaming mode, but I've currently no way to know if the remote data has been updated or not, thus I don't know when to invalidate my cache.\r\n\r\n**Describe the solution you'd like**\r\n\r\nTake the ETag of the data files into account and provide it (directly or through a hash) to give a signal that I can invalidate my cache.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nNone\r\n\r\n",
    "status": "open",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 953360663,
    "title": "New documentation structure",
    "dateCreated": "2021-07-26T23:15:13Z",
    "dateModified": "2021-07-26T23:15:13Z",
    "description": "Organize Datasets documentation into four documentation types to improve clarity and discoverability of content.\r\n\r\n**Content to add in the very short term (feel free to add anything I'm missing):**\r\n- A discussion on why Datasets uses Arrow that includes some context and background about why we use Arrow. Would also be great to talk about Datasets speed and performance here, and if you can share any benchmarking/tests you did, that would be awesome! Finally, a discussion about how memory-mapping frees the user from RAM constraints would be very helpful.\r\n- Explain why you would want to disable or override verifications when loading a dataset.\r\n- If possible, include a code sample of when the number of elements in the field of an output dictionary aren\u2019t the same as the other fields in the output dictionary (taken from the [note](https://huggingface.co/docs/datasets/processing.html#augmenting-the-dataset) here).",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 952979976,
    "title": "Fix shuffle on IterableDataset that disables batching in case any functions were mapped",
    "dateCreated": "2021-07-26T14:42:22Z",
    "dateModified": "2021-07-26T14:42:22Z",
    "description": "Made a very minor change to fix the issue#2716. Added the missing argument in the constructor call.\r\n\r\nAs discussed in the bug report, the change is made to prevent the `shuffle` method call from resetting the value of `batched` attribute in `MappedExamplesIterable`\r\n\r\nFix #2716.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 952902778,
    "title": "Calling shuffle on IterableDataset will disable batching in case any functions were mapped",
    "dateCreated": "2021-07-26T13:24:59Z",
    "dateModified": "2021-07-26T13:24:59Z",
    "description": "When using dataset in streaming mode, if one applies `shuffle` method on the dataset and `map` method for which `batched=True` than the batching operation will not happen, instead `batched` will be set to `False`\r\n\r\nI did RCA on the dataset codebase, the problem is emerging from [this line of code](https://github.com/huggingface/datasets/blob/d25a0bf94d9f9a9aa6cabdf5b450b9c327d19729/src/datasets/iterable_dataset.py#L197) here as it is\r\n`self.ex_iterable.shuffle_data_sources(seed), function=self.function, batch_size=self.batch_size`, as one can see it is missing batched argument, which means that the iterator fallsback to default constructor value, which in this case is `False`.\r\nTo remedy the problem we can change this line to\r\n`self.ex_iterable.shuffle_data_sources(seed), function=self.function, batched=self.batched, batch_size=self.batch_size`\r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 952845229,
    "title": "Update PAN-X data URL in XTREME dataset",
    "dateCreated": "2021-07-26T12:21:17Z",
    "dateModified": "2021-07-26T12:21:17Z",
    "description": "Related to #2710, #2691.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 952580820,
    "title": "add more precise information for size",
    "dateCreated": "2021-07-26T07:11:03Z",
    "dateModified": "2021-07-26T07:11:03Z",
    "description": "For the import into ELG, we would like a more precise description of the size of the dataset, instead of the current size categories. The size can be expressed in bytes, or any other preferred size unit. As suggested in the slack channel, perhaps this could be computed with a regex for existing datasets.",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 952515256,
    "title": "Enumerate all ner_tags values in WNUT 17 dataset",
    "dateCreated": "2021-07-26T05:22:16Z",
    "dateModified": "2021-07-26T05:22:16Z",
    "description": "This PR does:\r\n- Enumerate all ner_tags in dataset card Data Fields section\r\n- Add all metadata tags to dataset card\r\n\r\nClose #2709.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 951723326,
    "title": "Update WikiANN data URL",
    "dateCreated": "2021-07-23T16:29:21Z",
    "dateModified": "2021-07-23T16:29:21Z",
    "description": "WikiANN data source URL is no longer accessible: 404 error from Dropbox.\r\n\r\nWe have decided to host it at Hugging Face. This PR updates the data source URL, the metadata JSON file and the dataset card.\r\n\r\nClose #2691.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 951534757,
    "title": "Missing documentation for wnut_17 (ner_tags)",
    "dateCreated": "2021-07-23T12:25:32Z",
    "dateModified": "2021-07-23T12:25:32Z",
    "description": "On the info page of the wnut_17 data set (https://huggingface.co/datasets/wnut_17), the model output of ner-tags is only documented for these 5 cases:\r\n\r\n`ner_tags: a list of classification labels, with possible values including O (0), B-corporation (1), I-corporation (2), B-creative-work (3), I-creative-work (4).`\r\n\r\nI trained a model with the data and it gives me 13 classes:\r\n\r\n```\r\n\"id2label\": {\r\n    \"0\": 0,\r\n    \"1\": 1,\r\n    \"2\": 2,\r\n    \"3\": 3,\r\n    \"4\": 4,\r\n    \"5\": 5,\r\n    \"6\": 6,\r\n    \"7\": 7,\r\n    \"8\": 8,\r\n    \"9\": 9,\r\n    \"10\": 10,\r\n    \"11\": 11,\r\n    \"12\": 12\r\n  }\r\n\r\n  \"label2id\": {\r\n    \"0\": 0,\r\n    \"1\": 1,\r\n    \"10\": 10,\r\n    \"11\": 11,\r\n    \"12\": 12,\r\n    \"2\": 2,\r\n    \"3\": 3,\r\n    \"4\": 4,\r\n    \"5\": 5,\r\n    \"6\": 6,\r\n    \"7\": 7,\r\n    \"8\": 8,\r\n    \"9\": 9\r\n  }\r\n```\r\nThe paper (https://www.aclweb.org/anthology/W17-4418.pdf) explains those 6 categories, but the ordering does not match:\r\n\r\n```\r\n1. person\r\n2. location (including GPE, facility)\r\n3. corporation\r\n4. product (tangible goods, or well-defined\r\nservices)\r\n5. creative-work (song, movie, book and\r\nso on)\r\n6. group (subsuming music band, sports team,\r\nand non-corporate organisations)\r\n```\r\nI would be very helpful for me, if somebody could clarify the model ouputs and explain the \"B-\" and \"I-\" prefixes to me.\r\n\r\nReally great work with that and the other packages, I couldn't believe that training the model with that data was basically a one-liner!",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 951092660,
    "title": "QASC: incomplete training set ",
    "dateCreated": "2021-07-22T21:59:44Z",
    "dateModified": "2021-07-22T21:59:44Z",
    "description": "## Describe the bug\r\nThe training instances are not loaded properly. \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"qasc\", script_version='1.10.2')\r\n \r\ndef load_instances(split): \r\n    instances = dataset[split]\r\n    print(f\"split: {split} - size: {len(instances)}\")\r\n    for x in instances:\r\n        print(json.dumps(x))\r\n\r\n\r\nload_instances('test')\r\nload_instances('validation')\r\nload_instances('train')\r\n```\r\n\r\n##  results\r\nFor test and validation, we can see the examples in the output (which is good!): \r\n```\r\nsplit: test - size: 920\r\n{\"answerKey\": \"\", \"choices\": {\"label\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"], \"text\": [\"Anthax\", \"under water\", \"uterus\", \"wombs\", \"two\", \"moles\", \"live\", \"embryo\"]}, \"combinedfact\": \"\", \"fact1\": \"\", \"fact2\": \"\", \"formatted_question\": \"What type of birth do therian mammals have? (A) Anthax (B) under water (C) uterus (D) wombs (E) two (F) moles (G) live (H) embryo\", \"id\": \"3C44YUNSI1OBFBB8D36GODNOZN9DPA\", \"question\": \"What type of birth do therian mammals have?\"}\r\n{\"answerKey\": \"\", \"choices\": {\"label\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"], \"text\": [\"Corvidae\", \"arthropods\", \"birds\", \"backbones\", \"keratin\", \"Jurassic\", \"front paws\", \"Parakeets.\"]}, \"combinedfact\": \"\", \"fact1\": \"\", \"fact2\": \"\", \"formatted_question\": \"By what time had mouse-sized viviparous mammals evolved? (A) Corvidae (B) arthropods (C) birds (D) backbones (E) keratin (F) Jurassic (G) front paws (H) Parakeets.\", \"id\": \"3B1NLC6UGZVERVLZFT7OUYQLD1SGPZ\", \"question\": \"By what time had mouse-sized viviparous mammals evolved?\"}\r\n{\"answerKey\": \"\", \"choices\": {\"label\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"], \"text\": [\"Reduced friction\", \"causes infection\", \"vital to a good life\", \"prevents water loss\", \"camouflage from consumers\", \"Protection against predators\", \"spur the growth of the plant\", \"a smooth surface\"]}, \"combinedfact\": \"\", \"fact1\": \"\", \"fact2\": \"\", \"formatted_question\": \"What does a plant's skin do? (A) Reduced friction (B) causes infection (C) vital to a good life (D) prevents water loss (E) camouflage from consumers (F) Protection against predators (G) spur the growth of the plant (H) a smooth surface\", \"id\": \"3QRYMNZ7FYGITFVSJET3PS0F4S0NT9\", \"question\": \"What does a plant's skin do?\"}\r\n...\r\n```\r\nHowever, only a few instances are loaded for the training split, which is not correct. \r\n\r\n## Environment info\r\n- `datasets` version: '1.10.2' \r\n- Platform: MaxOS \r\n- Python version:3.7\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 950812945,
    "title": "404 Not Found Error when loading LAMA dataset",
    "dateCreated": "2021-07-22T15:52:33Z",
    "dateModified": "2021-07-22T15:52:33Z",
    "description": "The [LAMA](https://huggingface.co/datasets/viewer/?dataset=lama) probing dataset is not available for download:  \r\n\r\nSteps to Reproduce: \r\n\r\n1. `from datasets import load_dataset`\r\n2. `dataset = load_dataset('lama', 'trex')`. \r\n\r\n\r\nResults:  \r\n`FileNotFoundError: Couldn't find file locally at lama/lama.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/lama/lama.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/lama/lama.py`",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 950606561,
    "title": "Update BibTeX entry",
    "dateCreated": "2021-07-22T12:29:29Z",
    "dateModified": "2021-07-22T12:29:29Z",
    "description": "Update BibTeX entry.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 950488583,
    "title": "404 not found error on loading WIKIANN dataset",
    "dateCreated": "2021-07-22T09:55:50Z",
    "dateModified": "2021-07-22T09:55:50Z",
    "description": "## Describe the bug\r\nUnable to retreive wikiann English dataset\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import list_datasets, load_dataset, list_metrics, load_metric\r\nWIKIANN = load_dataset(\"wikiann\",\"en\")\r\n```\r\n\r\n## Expected results\r\nColab notebook should display successful download status\r\n\r\n## Actual results\r\nFileNotFoundError: Couldn't find file at https://www.dropbox.com/s/12h3qqog6q4bjve/panx_dataset.tar?dl=1\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.10.1\r\n- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.11\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 950483980,
    "title": "Fix pick default config name message",
    "dateCreated": "2021-07-22T09:49:43Z",
    "dateModified": "2021-07-22T09:49:43Z",
    "description": "The error message to tell which config name to load is not displayed. \r\n\r\nThis is because in the code it was considering the config kwargs to be non-empty, which is a special case for custom configs created on the fly. It appears after this change: https://github.com/huggingface/datasets/pull/2659\r\n\r\nI fixed that by making the config kwargs empty by default, even if default parameters are passed\r\n\r\nFix https://github.com/huggingface/datasets/issues/2703",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 950482284,
    "title": "Bad message when config name is missing",
    "dateCreated": "2021-07-22T09:47:23Z",
    "dateModified": "2021-07-22T09:47:23Z",
    "description": "When loading a dataset that have several configurations, we expect to see an error message if the user doesn't specify a config name.\r\n\r\nHowever in `datasets` 1.10.0 and 1.10.1 it doesn't show the right message:\r\n\r\n```python\r\nimport datasets\r\n\r\ndatasets.load_dataset(\"glue\")\r\n```\r\nraises\r\n```python\r\nAttributeError: 'BuilderConfig' object has no attribute 'text_features'\r\n```\r\ninstead of\r\n```python\r\nValueError: Config name is missing.\r\nPlease pick one among the available configs: ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'mnli_mismatched', 'mnli_matched', 'qnli', 'rte', 'wnli', 'ax']\r\nExample of usage:\r\n        `load_dataset('glue', 'cola')`\r\n```",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 950448159,
    "title": "Update BibTeX entry",
    "dateCreated": "2021-07-22T09:04:39Z",
    "dateModified": "2021-07-22T09:04:39Z",
    "description": "Update BibTeX entry.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 950422403,
    "title": "Fix download_mode docstrings",
    "dateCreated": "2021-07-22T08:30:25Z",
    "dateModified": "2021-07-22T08:30:25Z",
    "description": "Fix `download_mode` docstrings.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 950276325,
    "title": "from datasets import Dataset is failing ",
    "dateCreated": "2021-07-22T03:51:23Z",
    "dateModified": "2021-07-22T03:51:23Z",
    "description": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom datasets import Dataset\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in <module>()\r\n     25 import posixpath\r\n     26 import requests\r\n---> 27 from tqdm.contrib.concurrent import thread_map\r\n     28 \r\n     29 from .. import __version__, config, utils\r\n\r\nModuleNotFoundError: No module named 'tqdm.contrib.concurrent'\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: If your import is failing due to a missing package, you can\r\nmanually install dependencies using either !pip or !apt.\r\n\r\nTo view examples of installing some common dependencies, click the\r\n\"Open Examples\" button below.\r\n---------------------------------------------------------------------------\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: latest version as of 07/21/2021\r\n- Platform: Google Colab\r\n- Python version: 3.7\r\n- PyArrow version:\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 950221226,
    "title": "cannot combine splits merging and streaming?",
    "dateCreated": "2021-07-22T01:13:25Z",
    "dateModified": "2021-07-22T01:13:25Z",
    "description": "this does not work:\r\n`dataset = datasets.load_dataset('mc4','iw',split='train+validation',streaming=True)`\r\nwith error:\r\n`ValueError: Bad split: train+validation. Available splits: ['train', 'validation']`\r\n\r\nthese work:\r\n`dataset = datasets.load_dataset('mc4','iw',split='train+validation')`\r\n`dataset = datasets.load_dataset('mc4','iw',split='train',streaming=True)`\r\n`dataset = datasets.load_dataset('mc4','iw',split='validation',streaming=True)`\r\n\r\ni could not find a reference to this in the documentation and the error message is confusing. also would be nice to allow streaming for the merged splits",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 950159867,
    "title": "Ignore empty batch when writing",
    "dateCreated": "2021-07-21T22:35:30Z",
    "dateModified": "2021-07-21T22:35:30Z",
    "description": "This prevents an schema update with unknown column types, as reported in #2644.\r\n\r\nThis is my first attempt at fixing the issue. I tested the following:\r\n- First batch returned by a batched map operation is empty.\r\n- An intermediate batch is empty.\r\n- `python -m unittest tests.test_arrow_writer` passes.\r\n\r\nHowever, `arrow_writer` looks like a pretty generic interface, I'm not sure if there are other uses I may have overlooked. Let me know if that's the case, or if a better approach would be preferable.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 950021623,
    "title": "Fix import on Colab",
    "dateCreated": "2021-07-21T19:03:38Z",
    "dateModified": "2021-07-21T19:03:38Z",
    "description": "Fix #2695, fix #2700. ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 949901726,
    "title": "Add support for disable_progress_bar on Windows",
    "dateCreated": "2021-07-21T16:34:53Z",
    "dateModified": "2021-07-21T16:34:53Z",
    "description": "This PR is a continuation of #2667 and adds support for `utils.disable_progress_bar()` on Windows when using multiprocessing. This [answer](https://stackoverflow.com/a/6596695/14095927) on SO explains it nicely why the current approach (with calling `utils.is_progress_bar_enabled()` inside `Dataset._map_single`) would not work on Windows.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 949864823,
    "title": "Cannot import load_dataset on Colab",
    "dateCreated": "2021-07-21T15:52:51Z",
    "dateModified": "2021-07-21T15:52:51Z",
    "description": "## Describe the bug\r\nGot tqdm concurrent module not found error during importing load_dataset from datasets.\r\n\r\n## Steps to reproduce the bug\r\nHere [colab notebook](https://colab.research.google.com/drive/1pErWWnVP4P4mVHjSFUtkePd8Na_Qirg4?usp=sharing) to reproduce the error\r\n\r\nOn colab:\r\n```python\r\n!pip install datasets\r\nfrom datasets import load_dataset\r\n```\r\n\r\n## Expected results\r\nWorks without error\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n```\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-2-8cc7de4c69eb> in <module>()\r\n----> 1 from datasets import load_dataset, load_metric, Metric, MetricInfo, Features, Value\r\n      2 from sklearn.metrics import mean_squared_error\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/__init__.py in <module>()\r\n     31     )\r\n     32 \r\n---> 33 from .arrow_dataset import Dataset, concatenate_datasets\r\n     34 from .arrow_reader import ArrowReader, ReadInstruction\r\n     35 from .arrow_writer import ArrowWriter\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in <module>()\r\n     40 from tqdm.auto import tqdm\r\n     41 \r\n---> 42 from datasets.tasks.text_classification import TextClassification\r\n     43 \r\n     44 from . import config, utils\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/tasks/__init__.py in <module>()\r\n      1 from typing import Optional\r\n      2 \r\n----> 3 from ..utils.logging import get_logger\r\n      4 from .automatic_speech_recognition import AutomaticSpeechRecognition\r\n      5 from .base import TaskTemplate\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/__init__.py in <module>()\r\n     19 \r\n     20 from . import logging\r\n---> 21 from .download_manager import DownloadManager, GenerateMode\r\n     22 from .file_utils import DownloadConfig, cached_path, hf_bucket_url, is_remote_url, temp_seed\r\n     23 from .mock_download_manager import MockDownloadManager\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/download_manager.py in <module>()\r\n     24 \r\n     25 from .. import config\r\n---> 26 from .file_utils import (\r\n     27     DownloadConfig,\r\n     28     cached_path,\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in <module>()\r\n     25 import posixpath\r\n     26 import requests\r\n---> 27 from tqdm.contrib.concurrent import thread_map\r\n     28 \r\n     29 from .. import __version__, config, utils\r\n\r\nModuleNotFoundError: No module named 'tqdm.contrib.concurrent'\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.10.0\r\n- Platform: Colab\r\n- Python version: 3.7.11\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 949844722,
    "title": "fix: \ud83d\udc1b change string format to allow copy/paste to work in bash",
    "dateCreated": "2021-07-21T15:30:40Z",
    "dateModified": "2021-07-21T15:30:40Z",
    "description": "Before: copy/paste resulted in an error because the square bracket\r\ncharacters `[]` are special characters in bash",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 949797014,
    "title": "Fix OSCAR Esperanto",
    "dateCreated": "2021-07-21T14:43:50Z",
    "dateModified": "2021-07-21T14:43:50Z",
    "description": "The Esperanto part (original) of OSCAR has the wrong number of examples:\r\n```python\r\nfrom datasets import load_dataset\r\nraw_datasets = load_dataset(\"oscar\", \"unshuffled_original_eo\")\r\n```\r\nraises\r\n```python\r\nNonMatchingSplitsSizesError:\r\n[{'expected': SplitInfo(name='train', num_bytes=314188336, num_examples=121171, dataset_name='oscar'),\r\n'recorded': SplitInfo(name='train', num_bytes=314064514, num_examples=121168, dataset_name='oscar')}]\r\n```\r\n\r\nI updated the number of expected examples in dataset_infos.json\r\n\r\ncc @sgugger ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 949765484,
    "title": "Update BibTeX entry",
    "dateCreated": "2021-07-21T14:23:35Z",
    "dateModified": "2021-07-21T14:23:35Z",
    "description": "Update BibTeX entry",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 949758379,
    "title": "xtreme / pan-x cannot be downloaded",
    "dateCreated": "2021-07-21T14:18:05Z",
    "dateModified": "2021-07-21T14:18:05Z",
    "description": "## Describe the bug\r\n\r\nDataset xtreme / pan-x cannot be loaded\r\n\r\nSeems related to https://github.com/huggingface/datasets/pull/2326\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\ndataset = load_dataset(\"xtreme\", \"PAN-X.fr\")\r\n```\r\n\r\n## Expected results\r\n\r\nLoad the dataset\r\n\r\n## Actual results\r\n\r\n```\r\nFileNotFoundError: Couldn't find file at https://www.dropbox.com/s/12h3qqog6q4bjve/panx_dataset.tar?dl=1\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.9.0\r\n- Platform: macOS-11.4-x86_64-i386-64bit\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 949574500,
    "title": "Docs details",
    "dateCreated": "2021-07-21T10:43:14Z",
    "dateModified": "2021-07-21T10:43:14Z",
    "description": "Some comments here:\r\n\r\n- the code samples assume the expected libraries have already been installed. Maybe add a section at start, or add it to every code sample. Something like `pip install datasets transformers torch  'datasets[streaming]'` (maybe just link to https://huggingface.co/docs/datasets/installation.html + a one-liner that installs all the requirements / alternatively a requirements.txt file)\r\n- \"If you\u2019d like to play with the examples, you must install it from source.\" in https://huggingface.co/docs/datasets/installation.html: it's not clear to me what this means (what are these \"examples\"?)\r\n- in https://huggingface.co/docs/datasets/loading_datasets.html: \"or AWS bucket if it\u2019s not already stored in the library\". It's the only place in the doc (aside from the docstring https://huggingface.co/docs/datasets/package_reference/loading_methods.html?highlight=aws bucket#datasets.list_datasets) where the \"AWS bucket\" is mentioned. It's not easy to understand what this means. Maybe explain more, and link to https://s3.amazonaws.com/datasets.huggingface.co and/or https://huggingface.co/docs/datasets/filesystems.html.\r\n- example in https://huggingface.co/docs/datasets/loading_datasets.html#manually-downloading-files is obsoleted by https://github.com/huggingface/datasets/pull/2326. Also: see https://github.com/huggingface/datasets/issues/2691 for a bug on this specific dataset.\r\n- in https://huggingface.co/docs/datasets/loading_datasets.html#manually-downloading-files the doc says \"After you\u2019ve downloaded the files, you can point to the folder hosting them locally with the data_dir argument as follows:\", but the following example does not show how to use `data_dir`\r\n- in https://huggingface.co/docs/datasets/loading_datasets.html#csv-files, it would be nice to have an URL to the csv loader reference (but I'm not sure there is one in the API reference). This comment applies in many places in the doc: I would want the API reference to contain doc for all the code/functions/classes... and I would want a lot more links inside the doc pointing to the API entries.\r\n- in the API reference (docstrings) I would prefer \"SOURCE\" to link to github instead of a copy of the code inside the docs site (eg. https://github.com/huggingface/datasets/blob/master/src/datasets/load.py#L711 instead of https://huggingface.co/docs/datasets/_modules/datasets/load.html#load_dataset)\r\n- it seems like not all the API is exposed in the doc. For example, there is no doc for [`disable_progress_bar`](https://github.com/huggingface/datasets/search?q=disable_progress_bar), see https://huggingface.co/docs/datasets/search.html?q=disable_progress_bar, even if the code contains docstrings. Does it mean that the function is not officially supported? (otherwise, maybe it also deserves a mention in https://huggingface.co/docs/datasets/package_reference/logging_methods.html)\r\n- in https://huggingface.co/docs/datasets/loading_datasets.html?highlight=most%20efficient%20format%20have%20json%20files%20consisting%20multiple%20json%20objects#json-files, \"The most efficient format is to have JSON files consisting of multiple JSON objects, one per line, representing individual data rows:\", maybe link to https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON and give it a name (\"line-delimited JSON\"? \"JSON Lines\" as in https://huggingface.co/docs/datasets/processing.html#exporting-a-dataset-to-csv-json-parquet-or-to-python-objects ?)\r\n- in https://huggingface.co/docs/datasets/loading_datasets.html, for the local files sections, it would be nice to provide sample csv / json / text files to download, so that it's easier for the reader to try to load them (instead: they won't try)\r\n- the doc explains how to shard a dataset, but does not explain why and when a dataset should be sharded (I have no idea... for [parallelizing](https://huggingface.co/docs/datasets/processing.html#multiprocessing)?). It does neither give an idea of the number of shards a dataset typically should have and why.\r\n- the code example in https://huggingface.co/docs/datasets/processing.html#mapping-in-a-distributed-setting does not work, because `training_args` has not been defined before in the doc.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 949447104,
    "title": "cannot save the dataset to disk after rename_column",
    "dateCreated": "2021-07-21T08:13:40Z",
    "dateModified": "2021-07-21T08:13:40Z",
    "description": "## Describe the bug\r\nIf you use `rename_column` and do no other modification, you will be unable to save the dataset using `save_to_disk`\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nIn [1]: from datasets import Dataset, load_from_disk\r\nIn [5]: dataset=Dataset.from_dict({'foo': [0]})\r\nIn [7]: dataset.save_to_disk('foo')\r\nIn [8]: dataset=load_from_disk('foo')\r\nIn [10]: dataset=dataset.rename_column('foo', 'bar')\r\nIn [11]: dataset.save_to_disk('foo')\r\n---------------------------------------------------------------------------\r\nPermissionError                           Traceback (most recent call last)\r\n<ipython-input-11-a3bc0d4fc339> in <module>\r\n----> 1 dataset.save_to_disk('foo')\r\n\r\n/mnt/beegfs/projects/meerqat/anaconda3/envs/meerqat/lib/python3.7/site-packages/datasets/arrow_dataset.py in save_to_disk(self, dataset_path\r\n, fs)\r\n    597             if Path(dataset_path, config.DATASET_ARROW_FILENAME) in cache_files_paths:\r\n    598                 raise PermissionError(\r\n--> 599                     f\"Tried to overwrite {Path(dataset_path, config.DATASET_ARROW_FILENAME)} but a dataset can't overwrite itself.\"\r\n    600                 )\r\n    601             if Path(dataset_path, config.DATASET_INDICES_FILENAME) in cache_files_paths:\r\n\r\nPermissionError: Tried to overwrite foo/dataset.arrow but a dataset can't overwrite itself.\r\n```\r\n\r\nN. B. I created the dataset from dict to enable easy reproduction but the same happens if you load an existing dataset (e.g. starting from `In [8]`)\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-centos-7.9.2009-Core\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 949182074,
    "title": "hebrew language codes he and iw should be treated as aliases",
    "dateCreated": "2021-07-20T23:13:52Z",
    "dateModified": "2021-07-20T23:13:52Z",
    "description": "https://huggingface.co/datasets/mc4 not listed when searching for hebrew datasets (he) as it uses the older language code iw, preventing discoverability. ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 948890481,
    "title": "Minor documentation fix",
    "dateCreated": "2021-07-20T17:43:23Z",
    "dateModified": "2021-07-20T17:43:23Z",
    "description": "Currently, [Writing a dataset loading script](https://huggingface.co/docs/datasets/add_dataset.html) page has a small error. A link to `matinf` dataset in [_Dataset scripts of reference_](https://huggingface.co/docs/datasets/add_dataset.html#dataset-scripts-of-reference) section actually leads to `xsquad`, instead. This PR fixes that. ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 948811669,
    "title": "Fix bad config ids that name cache directories",
    "dateCreated": "2021-07-20T16:00:45Z",
    "dateModified": "2021-07-20T16:00:45Z",
    "description": "`data_dir=None` was considered a dataset config parameter, hence creating a special config_id for all dataset being loaded.\r\nSince the config_id is used to name the cache directories, this leaded to datasets being regenerated for users.\r\n\r\nI fixed this by ignoring the value of `data_dir` when it's `None` when computing the config_id.\r\nI also added a test to make sure the cache directories are not unexpectedly renamed in the future.\r\n\r\nFix https://github.com/huggingface/datasets/issues/2683",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 948791572,
    "title": "Fix Blog Authorship Corpus dataset",
    "dateCreated": "2021-07-20T15:44:50Z",
    "dateModified": "2021-07-20T15:44:50Z",
    "description": "This PR:\r\n- Update the JSON metadata file, which previously was raising a `NonMatchingSplitsSizesError`\r\n- Fix the codec of the data files (`latin_1` instead of `utf-8`), which previously was raising ` UnicodeDecodeError` for some files\r\n\r\nClose #2679.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 948771753,
    "title": "Print absolute local paths in load_dataset error messages",
    "dateCreated": "2021-07-20T15:28:28Z",
    "dateModified": "2021-07-20T15:28:28Z",
    "description": "Use absolute local paths in the error messages of `load_dataset` as per @stas00's suggestion in https://github.com/huggingface/datasets/pull/2500#issuecomment-874891223 ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 948721379,
    "title": "Cache directories changed due to recent changes in how config kwargs are handled",
    "dateCreated": "2021-07-20T14:37:57Z",
    "dateModified": "2021-07-20T14:37:57Z",
    "description": "Since #2659 I can see weird cache directory names with hashes in the config id, even though no additional config kwargs are passed. For example:\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\n\r\nc4_builder = load_dataset_builder(\"c4\", \"en\")\r\nprint(c4_builder.cache_dir)\r\n# /Users/quentinlhoest/.cache/huggingface/datasets/c4/en-174d3b7155eb68db/0.0.0/...\r\n\r\n# instead of \r\n# /Users/quentinlhoest/.cache/huggingface/datasets/c4/en/0.0.0/...\r\n```\r\nThis issue could be annoying since it would simply ignore old cache directories for users, and regenerate datasets\r\n\r\ncc @stas00 this is what you experienced a few days ago\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 948713137,
    "title": "Fix c4 expected files",
    "dateCreated": "2021-07-20T14:29:31Z",
    "dateModified": "2021-07-20T14:29:31Z",
    "description": "Some files were not registered in the list of expected files to download\r\n\r\nFix https://github.com/huggingface/datasets/issues/2677",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 948708645,
    "title": "5 duplicate datasets",
    "dateCreated": "2021-07-20T14:25:00Z",
    "dateModified": "2021-07-20T14:25:00Z",
    "description": "## Describe the bug\r\n\r\nIn 5 cases, I could find a dataset on Paperswithcode which references two Hugging Face datasets as dataset loaders. They are:\r\n\r\n- https://paperswithcode.com/dataset/multinli -> https://huggingface.co/datasets/multi_nli and https://huggingface.co/datasets/multi_nli_mismatch\r\n  \r\n  <img width=\"838\" alt=\"Capture d\u2019e\u0301cran 2021-07-20 a\u0300 16 33 58\" src=\"https://user-images.githubusercontent.com/1676121/126342757-4625522a-f788-41a3-bd1f-2a8b9817bbf5.png\">\r\n\r\n- https://paperswithcode.com/dataset/squad -> https://huggingface.co/datasets/squad and https://huggingface.co/datasets/squad_v2\r\n- https://paperswithcode.com/dataset/narrativeqa -> https://huggingface.co/datasets/narrativeqa and https://huggingface.co/datasets/narrativeqa_manual\r\n- https://paperswithcode.com/dataset/hate-speech-and-offensive-language -> https://huggingface.co/datasets/hate_offensive and https://huggingface.co/datasets/hate_speech_offensive\r\n- https://paperswithcode.com/dataset/newsph-nli -> https://huggingface.co/datasets/newsph and https://huggingface.co/datasets/newsph_nli\r\n\r\nPossible solutions:\r\n- don't fix (it works)\r\n- for each pair of duplicate datasets, remove one, and create an alias to the other.\r\n\r\n## Steps to reproduce the bug\r\n\r\nVisit the Paperswithcode links, and look at the \"Dataset Loaders\" section\r\n\r\n## Expected results\r\n\r\nThere should only be one reference to a Hugging Face dataset loader\r\n\r\n## Actual results\r\n\r\nTwo Hugging Face dataset loaders\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 948649716,
    "title": "feat: \ud83c\udfb8 add paperswithcode id for qasper dataset",
    "dateCreated": "2021-07-20T13:22:29Z",
    "dateModified": "2021-07-20T13:22:29Z",
    "description": "The reverse reference exists on paperswithcode:\r\nhttps://paperswithcode.com/dataset/qasper",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 948506638,
    "title": "Cannot load the blog_authorship_corpus due to codec errors",
    "dateCreated": "2021-07-20T10:13:20Z",
    "dateModified": "2021-07-20T10:13:20Z",
    "description": "## Describe the bug\r\nA codec error is raised while loading the blog_authorship_corpus. \r\n\r\n## Steps to reproduce the bug\r\n```\r\nfrom datasets import load_dataset\r\nraw_datasets = load_dataset(\"blog_authorship_corpus\")\r\n```\r\n\r\n\r\n## Expected results\r\nLoading the dataset without errors.\r\n\r\n## Actual results\r\nAn error similar to the one below was raised for (what seems like) every XML file.\r\n/home/izaskr/.cache/huggingface/datasets/downloads/extracted/7cf52524f6517e168604b41c6719292e8f97abbe8f731e638b13423f4212359a/blogs/788358.male.24.Arts.Libra.xml cannot be loaded. Error message: 'utf-8' codec can't decode byte 0xe7 in position 7551: invalid continuation byte\r\n\r\nTraceback (most recent call last):         \r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/izaskr/anaconda3/envs/local_vae_older/lib/python3.8/site-packages/datasets/load.py\", line 856, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/izaskr/anaconda3/envs/local_vae_older/lib/python3.8/site-packages/datasets/builder.py\", line 583, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/izaskr/anaconda3/envs/local_vae_older/lib/python3.8/site-packages/datasets/builder.py\", line 671, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File \"/home/izaskr/anaconda3/envs/local_vae_older/lib/python3.8/site-packages/datasets/utils/info_utils.py\", line 74, in verify_splits\r\n    raise NonMatchingSplitsSizesError(str(bad_splits))\r\ndatasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=610252351, num_examples=532812, dataset_name='blog_authorship_corpus'), 'recorded': SplitInfo(name='train', num_bytes=614706451, num_examples=535568, dataset_name='blog_authorship_corpus')}, {'expected': SplitInfo(name='validation', num_bytes=37500394, num_examples=31277, dataset_name='blog_authorship_corpus'), 'recorded': SplitInfo(name='validation', num_bytes=32553710, num_examples=28521, dataset_name='blog_authorship_corpus')}]\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.9.0\r\n- Platform: Linux-4.15.0-132-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.8\r\n- PyArrow version: 4.0.1\r\n\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 948471222,
    "title": "Import Error in Kaggle notebook",
    "dateCreated": "2021-07-20T09:28:38Z",
    "dateModified": "2021-07-20T09:28:38Z",
    "description": "## Describe the bug\r\nNot able to import datasets library in kaggle notebooks\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n!pip install datasets\r\nimport datasets\r\n```\r\n\r\n## Expected results\r\nNo such error\r\n\r\n## Actual results\r\n```\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-9-652e886d387f> in <module>\r\n----> 1 import datasets\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/__init__.py in <module>\r\n     31     )\r\n     32 \r\n---> 33 from .arrow_dataset import Dataset, concatenate_datasets\r\n     34 from .arrow_reader import ArrowReader, ReadInstruction\r\n     35 from .arrow_writer import ArrowWriter\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py in <module>\r\n     36 import pandas as pd\r\n     37 import pyarrow as pa\r\n---> 38 import pyarrow.compute as pc\r\n     39 from multiprocess import Pool, RLock\r\n     40 from tqdm.auto import tqdm\r\n\r\n/opt/conda/lib/python3.7/site-packages/pyarrow/compute.py in <module>\r\n     16 # under the License.\r\n     17 \r\n---> 18 from pyarrow._compute import (  # noqa\r\n     19     Function,\r\n     20     FunctionOptions,\r\n\r\nImportError: /opt/conda/lib/python3.7/site-packages/pyarrow/_compute.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZNK5arrow7compute15KernelSignature8ToStringEv\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.9.0\r\n- Platform: Kaggle\r\n- Python version: 3.7.10\r\n- PyArrow version: 4.0.1\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 948429788,
    "title": "Error when downloading C4",
    "dateCreated": "2021-07-20T08:37:30Z",
    "dateModified": "2021-07-20T08:37:30Z",
    "description": "Hi,\r\nI am trying to download `en` corpus from C4 dataset. However, I get an error caused by validation files download (see image). My code is very primitive:\r\n`datasets.load_dataset('c4', 'en')`\r\n\r\nIs this a bug or do I have some configurations missing on my server? \r\nThanks!\r\n\r\n\r\n<img width=\"1014\" alt=\"\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 2021-07-20 \u0432 11 37 17\" src=\"https://user-images.githubusercontent.com/36672861/126289448-6e0db402-5f3f-485a-bf74-eb6e0271fc25.png\">",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 947734909,
    "title": "Increase json reader block_size automatically",
    "dateCreated": "2021-07-19T14:51:14Z",
    "dateModified": "2021-07-19T14:51:14Z",
    "description": "Currently some files can't be read with the default parameters of the JSON lines reader.\r\nFor example this one:\r\nhttps://huggingface.co/datasets/thomwolf/codeparrot/resolve/main/file-000000000006.json.gz\r\n\r\nraises a pyarrow error:\r\n```python\r\nArrowInvalid: straddling object straddles two block boundaries (try to increase block size?)\r\n```\r\n\r\nThe block size that is used is the default one by pyarrow (related to this [jira issue](https://issues.apache.org/jira/browse/ARROW-9612)).\r\n\r\nTo fix this issue I changed the block_size to increase automatically if there is a straddling issue when parsing a batch of json lines.\r\n\r\nBy default the value is `chunksize // 32` in order to leverage multithreading, and it doubles every time a straddling issue occurs. The block_size is then reset for each file.\r\n\r\ncc @thomwolf @albertvillanova ",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 947657732,
    "title": "Parallelize ETag requests",
    "dateCreated": "2021-07-19T13:30:42Z",
    "dateModified": "2021-07-19T13:30:42Z",
    "description": "Since https://github.com/huggingface/datasets/pull/2628 we use the ETag or the remote data files to compute the directory in the cache where a dataset is saved. This is useful in order to reload the dataset from the cache only if the remote files haven't changed.\r\n\r\nIn this I made the ETag requests parallel using multithreading. There is also a tqdm progress bar that shows up if there are more than 16 data files.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 947338202,
    "title": "Fix sacrebleu parameter name",
    "dateCreated": "2021-07-19T07:07:26Z",
    "dateModified": "2021-07-19T07:07:26Z",
    "description": "DONE:\r\n- Fix parameter name: `smooth` to `smooth_method`.\r\n- Improve kwargs description.\r\n- Align docs on using a metric.\r\n- Add example of passing additional arguments in using metrics.\r\n\r\nRelated to #2669.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 947300008,
    "title": "Fix potential DuplicatedKeysError in SQuAD",
    "dateCreated": "2021-07-19T06:08:00Z",
    "dateModified": "2021-07-19T06:08:00Z",
    "description": "DONE:\r\n- Fix potential DiplicatedKeysError by ensuring keys are unique.\r\n- Align examples in the docs with SQuAD code.\r\n\r\nWe should promote as a good practice, that the keys should be programmatically generated as unique, instead of read from data (which might be not unique).",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 947294605,
    "title": "Fix potential DuplicatedKeysError in LibriSpeech",
    "dateCreated": "2021-07-19T06:00:49Z",
    "dateModified": "2021-07-19T06:00:49Z",
    "description": "DONE:\r\n- Fix unnecessary path join.\r\n- Fix potential DiplicatedKeysError by ensuring keys are unique.\r\n\r\nWe should promote as a good practice, that the keys should be programmatically generated as unique, instead of read from data (which might be not unique).",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 947273875,
    "title": "Mesinesp development and training data sets have been added.",
    "dateCreated": "2021-07-19T05:14:38Z",
    "dateModified": "2021-07-19T05:14:38Z",
    "description": "https://zenodo.org/search?page=1&size=20&q=mesinesp, Mesinesp has Medical Semantic Indexed records in Spanish.  Indexing is done using DeCS codes, a sort of Spanish equivalent to MeSH terms.\r\nThe Mesinesp (Spanish BioASQ track, see https://temu.bsc.es/mesinesp) development set has a total of 750 records.\r\nThe Mesinesp (Spanish BioASQ track, see https://temu.bsc.es/mesinesp) training set has a total of 369,368 records. \r\n\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 947120709,
    "title": "Using sharding to parallelize indexing",
    "dateCreated": "2021-07-18T21:26:26Z",
    "dateModified": "2021-07-18T21:26:26Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nCreating an elasticsearch index on large dataset could be quite and cannot be parallelized on shard (the index creation is colliding)\r\n\r\n**Describe the solution you'd like**\r\nWhen working on dataset shards, if an index already exists, its mapping should be checked and if compatible, the indexing process should continue with the shard data. \r\n\r\nAdditionally, at the end of the process, the `_indexes` dict should be send back to the original dataset object (from which the shards have been created) to allow to use the index for later filtering on the whole dataset.\r\n\r\n**Describe alternatives you've considered**\r\nEach dataset shard could created independent partial indices. then on the whole dataset level, indices should be all referred in `_indexes` dict and be used in querying through `get_nearest_examples()`. The drawback is that the scores will be computed independently on the partial indices leading to inconsistent values for most scoring based on corpus level statistics (tf/idf, BM25).\r\n\r\n**Additional context**\r\nThe objectives is to parallelize the index creation to speed-up the process (ie surcharging the ES server which is fine to handle large load) while later enabling search on the whole dataset.",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 946982998,
    "title": "Metric kwargs are not passed to underlying external metric f1_score",
    "dateCreated": "2021-07-18T08:32:31Z",
    "dateModified": "2021-07-18T08:32:31Z",
    "description": "## Describe the bug\r\nWhen I want to use F1 score with average=\"min\", this keyword argument does not seem to be passed through to the underlying sklearn metric. This is evident because [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) throws an error telling me so.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nf1 = datasets.load_metric(\"f1\", keep_in_memory=True, average=\"min\")\r\nf1.add_batch(predictions=[0,2,3], references=[1, 2, 3])\r\nf1.compute()\r\n```\r\n\r\n## Expected results\r\nNo error, because `average=\"min\"` should be passed correctly to f1_score in sklearn.\r\n\r\n## Actual results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\datasets\\metric.py\", line 402, in compute\r\n    output = self._compute(predictions=predictions, references=references, **kwargs)\r\n  File \"C:\\Users\\bramv\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\f1\\82177930a325d4c28342bba0f116d73f6d92fb0c44cd67be32a07c1262b61cfe\\f1.py\", line 97, in _compute\r\n    \"f1\": f1_score(\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\r\n    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\r\n    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\r\n    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1294, in _check_set_wise_labels\r\n    raise ValueError(\"Target is %s but average='binary'. Please \"\r\nValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.9.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.9.2\r\n- PyArrow version: 4.0.1",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 946867622,
    "title": "Add Russian SuperGLUE",
    "dateCreated": "2021-07-17T17:41:28Z",
    "dateModified": "2021-07-17T17:41:28Z",
    "description": "Hi,\r\n\r\nThis adds the [Russian SuperGLUE](https://russiansuperglue.com/) dataset. For the most part I reused the code for the original SuperGLUE, although there are some relatively minor differences in the structure that I accounted for.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 946861908,
    "title": "Use tqdm from tqdm_utils",
    "dateCreated": "2021-07-17T17:06:35Z",
    "dateModified": "2021-07-17T17:06:35Z",
    "description": "This PR replaces `tqdm` from the `tqdm` lib with `tqdm` from `datasets.utils.tqdm_utils`. With this change, it's possible to disable progress bars just by calling `disable_progress_bar`. Note this doesn't work on Windows when using multiprocessing due to how global variables are shared between processes. Currently, there is no easy way to disable progress bars in a multiprocess setting on Windows (patching logging with `datasets.utils.logging.get_verbosity = lambda: datasets.utils.logging.NOTSET` doesn't seem to work as well), so adding support for this is a future goal. Additionally, this PR adds a unit (\"ba\" for batches) to the bar printed by `Dataset.to_json` (this change is motivated by https://github.com/huggingface/datasets/issues/2657).",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 946825140,
    "title": "Adds CodeClippy dataset [WIP]",
    "dateCreated": "2021-07-17T13:32:04Z",
    "dateModified": "2021-07-17T13:32:04Z",
    "description": "CodeClippy is an opensource code dataset scrapped from github during flax-jax-community-week\r\nhttps://the-eye.eu/public/AI/training_data/code_clippy_data/",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 946822036,
    "title": "Adds APPS dataset to the hub [WIP]",
    "dateCreated": "2021-07-17T13:13:17Z",
    "dateModified": "2021-07-17T13:13:17Z",
    "description": "A loading script for [APPS dataset](https://github.com/hendrycks/apps) ",
    "status": "open",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 946552273,
    "title": "[`to_json`] add multi-proc sharding support",
    "dateCreated": "2021-07-16T19:41:50Z",
    "dateModified": "2021-07-16T19:41:50Z",
    "description": "As discussed on slack it appears that `to_json` is quite slow on huge datasets like OSCAR.\r\n\r\nI implemented sharded saving, which is much much faster - but the tqdm bars all overwrite each other, so it's hard to make sense of the progress, so if possible ideally this multi-proc support could be implemented internally in `to_json` via `num_proc` argument. I guess `num_proc` will be the number of shards?\r\n\r\nI think the user will need to use this feature wisely, since too many processes writing to say normal style HD is likely to be slower than one process.\r\n\r\nI'm not sure whether the user should be responsible to concatenate the shards at the end  or `datasets`, either way works for my needs.\r\n\r\nThe code I was using:\r\n\r\n```\r\nfrom multiprocessing import cpu_count, Process, Queue\r\n\r\n[...]\r\n\r\nfiltered_dataset = concat_dataset.map(filter_short_documents, batched=True, batch_size=256, num_proc=cpu_count())\r\n\r\nDATASET_NAME = \"oscar\"\r\nSHARDS = 10\r\ndef process_shard(idx):\r\n    print(f\"Sharding {idx}\")\r\n    ds_shard = filtered_dataset.shard(SHARDS, idx, contiguous=True)\r\n    # ds_shard = ds_shard.shuffle() # remove contiguous=True above if shuffling\r\n    print(f\"Saving {DATASET_NAME}-{idx}.jsonl\")\r\n    ds_shard.to_json(f\"{DATASET_NAME}-{idx}.jsonl\", orient=\"records\", lines=True, force_ascii=False)\r\n\r\nqueue = Queue()\r\nprocesses = [Process(target=process_shard, args=(idx,)) for idx in range(SHARDS)]\r\nfor p in processes:\r\n    p.start()\r\n\r\nfor p in processes:\r\n    p.join()\r\n```\r\n\r\nThank you!\r\n\r\n@lhoestq ",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 946470815,
    "title": "Load Dataset from the Hub (NO DATASET SCRIPT)",
    "dateCreated": "2021-07-16T17:21:58Z",
    "dateModified": "2021-07-16T17:21:58Z",
    "description": "## Load the data from any Dataset repository on the Hub\r\n\r\nThis PR adds support for loading datasets from any dataset repository on the hub, without requiring any dataset script.\r\n\r\nAs a user it's now possible to create a repo and upload some csv/json/text/parquet files, and then be able to load the data in one line. Here is an example with the `allenai/c4` repository that contains a lot of compressed json lines files:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndata_files = {\"train\": \"en/c4-train.*.json.gz\"}\r\nc4 = load_dataset(\"allenai/c4\", data_files=data_files, split=\"train\", streaming=True)\r\n\r\nprint(c4.n_shards)\r\n# 1024\r\nprint(next(iter(c4)))\r\n# {'text': 'Beginners BBQ Class Takin...'}\r\n```\r\n\r\nBy default it loads all the files, but as shown in the example you can choose the ones you want with unix style patterns.\r\n\r\nOf course it's still possible to use dataset scripts since they offer the most flexibility.\r\n\r\n## Implementation details\r\n\r\nIt uses `huggingface_hub` to list the files in a dataset repository.\r\n\r\nIf you provide a path to a local directory instead of a repository name, it works the same way but it uses `glob`.\r\n\r\nDepending on the data files available, or passed in the `data_files` parameter, one of the available builders will be used among the csv, json, text and parquet builders.\r\n\r\nBecause of this, it's not possible to load both csv and json files at once. In this case you have to load them separately and then concatenate the two datasets for example.\r\n\r\n## TODO\r\n\r\n- [x] tests\r\n- [x] docs\r\n- [x] when huggingface_hub gets a new release, update the CI and the setup.py\r\n\r\nClose https://github.com/huggingface/datasets/issues/2629",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 946446967,
    "title": "Add SD task for SUPERB",
    "dateCreated": "2021-07-16T16:43:21Z",
    "dateModified": "2021-07-16T16:43:21Z",
    "description": "Include the SD (Speaker Diarization) task as described in the [SUPERB paper](https://arxiv.org/abs/2105.01051) and `s3prl` [instructions](https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#sd-speaker-diarization).\r\n\r\nTODO:\r\n- [x] Generate the LibriMix corpus\r\n- [x] Prepare the corpus for diarization\r\n- [x] Upload these files to the superb-data repo\r\n- [x] Transcribe the corresponding s3prl processing of these files into our superb loading script\r\n- [x] README: tags + description sections\r\n- ~~Add DER metric~~ (we leave the DER metric for a follow-up PR)\r\n\r\nRelated to #2619.\r\n\r\nClose #2653.\r\n\r\ncc: @lewtun ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 946316180,
    "title": "Move checks from _map_single to map",
    "dateCreated": "2021-07-16T13:53:33Z",
    "dateModified": "2021-07-16T13:53:33Z",
    "description": "The goal of this PR is to remove duplicated checks in the `map` logic to execute them only once whenever possible (`fn_kwargs`, `input_columns`, ...). Additionally, this PR improves the consistency (to align it with `input_columns`) of the `remove_columns` check by adding support for a single string value, which is then wrapped into a list. ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 946155407,
    "title": "Allow dataset config kwargs to be None",
    "dateCreated": "2021-07-16T10:25:38Z",
    "dateModified": "2021-07-16T10:25:38Z",
    "description": "Close https://github.com/huggingface/datasets/issues/2658\r\n\r\nThe dataset config kwargs that were set to None we simply ignored.\r\nThis was an issue when None has some meaning for certain parameters of certain builders, like the `sep` parameter of the \"csv\" builder that allows to infer to separator.\r\n\r\ncc @SBrandeis ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 946139532,
    "title": "Can't pass `sep=None` to load_dataset(\"csv\", ...) to infer the separator via pandas.read_csv",
    "dateCreated": "2021-07-16T10:05:44Z",
    "dateModified": "2021-07-16T10:05:44Z",
    "description": "When doing `load_dataset(\"csv\", sep=None)`, the `sep` passed to `pd.read_csv` is still the default `sep=\",\"` instead, which makes it impossible to make the csv loader infer the separator.\r\n\r\nRelated to https://github.com/huggingface/datasets/pull/2656\r\n\r\ncc @SBrandeis ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 945822829,
    "title": "`to_json` reporting enhancements",
    "dateCreated": "2021-07-15T23:32:18Z",
    "dateModified": "2021-07-15T23:32:18Z",
    "description": "While using `to_json` 2 things came to mind that would have made the experience easier on the user:\r\n\r\n1. Could we have a `desc` arg for the tqdm use and a fallback to just `to_json` so that it'd be clear to the user what's happening? Surely, one can just print the description before calling json, but I thought perhaps it'd help to have it self-identify like you did for other progress bars recently.\r\n\r\n2. It took me a while to make sense of the reported numbers:\r\n```\r\n 22%|\u2588\u2588\u258f       | 1536/7076 [12:30:57<44:09:42, 28.70s/it]\r\n```\r\nSo iteration here happens to be 10K samples, and the total is 70M records. But the user does't know that, so the progress bar is perfect, but the numbers it reports are meaningless until one discovers that 1it=10K samples. And one still has to convert these in the head - so it's not quick. Not exactly sure what's the best way to approach this, perhaps it can be part of `desc`? or report M or K, so it'd be built-in if it were to print, e.g.:\r\n```\r\n 22%|\u2588\u2588\u258f       | 15360K/70760K [12:30:57<44:09:42, 28.70s/it]\r\n```\r\nor \r\n```\r\n 22%|\u2588\u2588\u258f       | 15.36M/70.76M [12:30:57<44:09:42, 28.70s/it]\r\n```\r\n(while of course remaining friendly to small datasets)\r\n\r\nI forget if tqdm lets you add a magnitude identifier to the running count.\r\n\r\nThank you!",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 945421790,
    "title": "Change `from_csv` default arguments",
    "dateCreated": "2021-07-15T14:09:06Z",
    "dateModified": "2021-07-15T14:09:06Z",
    "description": "Passing `sep=None` to pandas's `read_csv` lets pandas guess the CSV file's separator\r\n\r\nThis PR allows users to use this pandas's feature by passing `sep=None` to `Dataset.from_csv`:\r\n\r\n```python\r\nDataset.from_csv(\r\n    ...,\r\n    sep=None\r\n)\r\n```",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 945382723,
    "title": "Allow the selection of multiple columns at once",
    "dateCreated": "2021-07-15T13:30:45Z",
    "dateModified": "2021-07-15T13:30:45Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\n\r\nSimilar to pandas, it would be great if we could select multiple columns at once.\r\n\r\n\r\n**Describe the solution you'd like**\r\n```python\r\nmy_dataset = ...  # Has columns ['idx', 'sentence', 'label']\r\nidx, label = my_dataset[['idx', 'label']]\r\n```\r\n\r\n**Describe alternatives you've considered**\r\nwe can do `[dataset[col] for col in ('idx', 'label')]`\r\n\r\n**Additional context**\r\nThis is of course very minor.\r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 945167231,
    "title": "Give a user feedback if the dataset he loads is streamable or not",
    "dateCreated": "2021-07-15T09:07:27Z",
    "dateModified": "2021-07-15T09:07:27Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nI would love to know if a `dataset` is with the current implementation streamable or not. \r\n\r\n**Describe the solution you'd like**\r\nWe could show a warning when a dataset is loaded with `load_dataset('...',streaming=True)` when its lot streamable, e.g. if it is an archive. \r\n\r\n**Describe alternatives you've considered**\r\nAdd a new metadata tag for \"streaming\"\r\n",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 945102321,
    "title": "Add SD task for SUPERB",
    "dateCreated": "2021-07-15T07:51:40Z",
    "dateModified": "2021-07-15T07:51:40Z",
    "description": "Include the SD (Speaker Diarization) task as described in the [SUPERB paper](https://arxiv.org/abs/2105.01051) and `s3prl` [instructions](https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#sd-speaker-diarization).\r\n\r\nSteps:\r\n- [x] Generate the LibriMix corpus\r\n- [x] Prepare the corpus for diarization\r\n- [x] Upload these files to the superb-data repo\r\n- [x] Transcribe the corresponding s3prl processing of these files into our superb loading script\r\n- [ ] README: tags + description sections\r\n\r\nRelated to #2619.\r\n\r\ncc: @lewtun \r\n",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 944865924,
    "title": "Fix logging docstring",
    "dateCreated": "2021-07-14T23:19:58Z",
    "dateModified": "2021-07-14T23:19:58Z",
    "description": "Remove \"no tqdm bars\" from the docstring in the logging module to align it with the changes introduced in #2534.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 944796961,
    "title": "Setting log level higher than warning does not suppress progress bar",
    "dateCreated": "2021-07-14T21:06:51Z",
    "dateModified": "2021-07-14T21:06:51Z",
    "description": "## Describe the bug\r\nI would like to disable progress bars for `.map` method (and other methods like `.filter` and `load_dataset` as well).\r\nAccording to #1627 one can suppress it by setting log level higher than `warning`, however doing so doesn't suppress it with version 1.9.0.\r\n\r\nI also tried to set `DATASETS_VERBOSITY` environment variable to `error` or `critical` but it also didn't work.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\n\r\nfrom datasets.utils.logging import set_verbosity_error\r\n\r\nset_verbosity_error()\r\n\r\ndef dummy_map(batch):\r\n    return batch\r\n\r\ncommon_voice_train = datasets.load_dataset(\"common_voice\", \"de\", split=\"train\")\r\ncommon_voice_test = datasets.load_dataset(\"common_voice\", \"de\", split=\"test\")\r\n\r\ncommon_voice_train.map(dummy_map)\r\n```\r\n\r\n## Expected results\r\n- The progress bar for `.map` call won't be shown\r\n\r\n## Actual results\r\n- The progress bar for `.map` is still shown \r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.9.0\r\n- Platform: Linux-5.4.0-1045-aws-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.5\r\n- PyArrow version: 4.0.1\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 944672565,
    "title": "[load_dataset] shard and parallelize the process",
    "dateCreated": "2021-07-14T18:04:58Z",
    "dateModified": "2021-07-14T18:04:58Z",
    "description": "- Some huge datasets take forever to build the first time. (e.g. oscar/en) as it's done in a single cpu core.\r\n- If the build crashes, everything done up to that point gets lost\r\n\r\nRequest: Shard the build over multiple arrow files, which would enable:\r\n- much faster build by parallelizing the build process\r\n- if the process crashed, the completed arrow files don't need to be re-built again\r\n\r\nThank you!\r\n\r\n@lhoestq ",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 944651229,
    "title": "adding progress bar / ETA for `load_dataset`",
    "dateCreated": "2021-07-14T17:34:39Z",
    "dateModified": "2021-07-14T17:34:39Z",
    "description": "Please consider:\r\n```\r\nDownloading and preparing dataset oscar/unshuffled_deduplicated_en (download: 462.40 GiB, generated: 1.18 TiB, post-processed: Unknown size, total: 1.63 TiB) to cache/oscar/unshuffled_deduplicated_en/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2...\r\nHF google storage unreachable. Downloading and preparing it from source\r\n```\r\nand no indication whatsoever of whether things work well or when it'll be done. It's important to have an estimated completion time for when doing slurm jobs since some instances have a cap on run-time.\r\n\r\nI think for this particular job it sat for 30min in total silence and then after 30min it started generating:\r\n```\r\n897850 examples [07:24, 10286.71 examples/s]\r\n```\r\nwhich is already great!\r\n\r\nRequest: \r\n1. ETA - knowing how many hours to allocate for a slurm job\r\n2. progress bar - helps to know things are working and aren't stuck and where we are at.\r\n\r\nThank you!\r\n\r\n@lhoestq \r\n",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 944484522,
    "title": "Add web_split dataset for Paraphase and Rephrase benchmark",
    "dateCreated": "2021-07-14T14:24:36Z",
    "dateModified": "2021-07-14T14:24:36Z",
    "description": "## Describe:\r\nFor getting simple sentences from complex sentence there are dataset and task like wiki_split that is available in hugging face datasets. This web_split is a very similar dataset. There some research paper which states that by combining these two datasets we if we train the model it will yield better results on both tests data.\r\n\r\nThis dataset is made from web NLG data.\r\n\r\nAll the dataset related details are provided in the below repository\r\n\r\nGithub link: https://github.com/shashiongithub/Split-and-Rephrase\r\n\r\n",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 944424941,
    "title": "Fix anchor in README",
    "dateCreated": "2021-07-14T13:22:44Z",
    "dateModified": "2021-07-14T13:22:44Z",
    "description": "I forgot to push this fix in #2611, so I'm sending it now. ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 944379954,
    "title": "downloading of yahoo_answers_topics dataset failed",
    "dateCreated": "2021-07-14T12:31:05Z",
    "dateModified": "2021-07-14T12:31:05Z",
    "description": "## Describe the bug\r\nI get an error datasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files when I try to download the yahoo_answers_topics dataset\r\n\r\n## Steps to reproduce the bug\r\n   self.dataset = load_dataset(\r\n                'yahoo_answers_topics', cache_dir=self.config['yahoo_cache_dir'], split='train[:90%]')\r\n# Sample code to reproduce the bug\r\n   self.dataset = load_dataset(\r\n                'yahoo_answers_topics', cache_dir=self.config['yahoo_cache_dir'], split='train[:90%]')\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files\r\n\r\n",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 944374284,
    "title": "load_dataset processing failed with OS error after downloading a dataset",
    "dateCreated": "2021-07-14T12:23:53Z",
    "dateModified": "2021-07-14T12:23:53Z",
    "description": "## Describe the bug\r\nAfter downloading a dataset like opus100, there is a bug that \r\nOSError: Cannot find data file.\r\nOriginal error:\r\ndlopen: cannot load any more object with static TLS\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nthis_dataset = load_dataset('opus100', 'af-en')\r\n```\r\n\r\n## Expected results\r\nthere is no error when running load_dataset.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/builder.py\", line 652, in _download_and_prep\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/builder.py\", line 989, in _prepare_split\r\n    example = self.info.features.encode_example(record)\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/features.py\", line 952, in encode_example\r\n    example = cast_to_python_objects(example)\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/features.py\", line 219, in cast_to_python_ob\r\n    return _cast_to_python_objects(obj)[0]\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/features.py\", line 165, in _cast_to_python_o\r\n    import torch\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/torch/__init__.py\", line 188, in <module>\r\n    _load_global_deps()\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/torch/__init__.py\", line 141, in _load_global_deps\r\n    ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\r\n  File \"/home/anaconda3/lib/python3.6/ctypes/__init__.py\", line 348, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: dlopen: cannot load any more object with static TLS\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"download_hub_opus100.py\", line 9, in <module>\r\n    this_dataset = load_dataset('opus100', language_pair)\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/load.py\", line 748, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/builder.py\", line 575, in download_and_prepa\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/builder.py\", line 658, in _download_and_prep\r\n    + str(e)\r\nOSError: Cannot find data file.\r\nOriginal error:\r\ndlopen: cannot load any more object with static TLS\r\n\r\n\r\n## Environment info\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-3.13.0-32-generic-x86_64-with-debian-jessie-sid\r\n- Python version: 3.6.6\r\n- PyArrow version: 3.0.0\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 944254748,
    "title": "Batched `map` not allowed to return 0 items",
    "dateCreated": "2021-07-14T09:58:19Z",
    "dateModified": "2021-07-14T09:58:19Z",
    "description": "## Describe the bug\r\nI'm trying to use `map` to filter a large dataset by selecting rows that match an expensive condition (files referenced by one of the columns need to exist in the filesystem, so we have to `stat` them). According to [the documentation](https://huggingface.co/docs/datasets/processing.html#augmenting-the-dataset), `a batch mapped function can take as input a batch of size N and return a batch of size M where M can be greater or less than N and can even be zero`.\r\n\r\nHowever, when the returned batch has a size of zero (neither item in the batch fulfilled the condition), we get an `index out of bounds` error. I think that `arrow_writer.py` is [trying to infer the returned types using the first element returned](https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_writer.py#L100), but no elements were returned in this case.\r\n\r\nFor this error to happen, I'm returning a dictionary that contains empty lists for the keys I want to keep, see below. If I return an empty dictionary instead (no keys), then a different error eventually occurs.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndef select_rows(examples):\r\n    # `key` is a column name that exists in the original dataset\r\n    # The following line simulates no matches found, so we return an empty batch\r\n    result = {'key': []}\r\n    return result\r\n\r\nfiltered_dataset = dataset.map(\r\n    select_rows,\r\n    remove_columns = dataset.column_names,\r\n    batched = True,\r\n    num_proc = 1,\r\n    desc = \"Selecting rows with images that exist\"\r\n)\r\n```\r\n\r\nThe code above immediately triggers the exception. If we use the following instead:\r\n\r\n```python\r\ndef select_rows(examples):\r\n    # `key` is a column name that exists in the original dataset\r\n    result = {'key': []}   # or defaultdict or whatever\r\n    \r\n    # code to check for condition and append elements to result\r\n    # some_items_found will be set to True if there were any matching elements in the batch\r\n    \r\n    return result if some_items_found else {}\r\n```\r\n\r\nThen it _seems_ to work, but it eventually fails with some sort of schema error. I believe it may happen when an empty batch is followed by a non-empty one, but haven't set up a test to verify it.\r\n\r\nIn my opinion, returning a dictionary with empty lists and valid column names should be accepted as a valid result with zero items.\r\n\r\n## Expected results\r\nThe dataset would be filtered and only the matching fields would be returned.\r\n\r\n## Actual results\r\nAn exception is encountered, as described. Using a workaround makes it fail further along the line.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.9.1.dev0\r\n- Platform: Linux-5.4.0-53-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.10\r\n- PyArrow version: 4.0.1\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 944220273,
    "title": "Enum used in map functions will raise a RecursionError with dill.",
    "dateCreated": "2021-07-14T09:16:08Z",
    "dateModified": "2021-07-14T09:16:08Z",
    "description": "## Describe the bug\r\n\r\nEnums used in functions pass to `map` will fail at pickling with a maximum recursion exception as described here: https://github.com/uqfoundation/dill/issues/250#issuecomment-852566284\r\n\r\nIn my particular case, I use an enum to define an argument with fixed options using the `TraininigArguments` dataclass as base class and the `HfArgumentParser`. In the same file I use a `ds.map` that tries to pickle the content of the module including the definition of the enum that runs into the dill bug described above.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nfrom enum import Enum\r\n\r\nclass A(Enum):\r\n    a = 'a'\r\n\r\ndef main():\r\n    a = A.a\r\n    \r\n    def f(x):\r\n        return {} if a == a.a else x\r\n    \r\n    ds = load_dataset('cnn_dailymail', '3.0.0')['test']\r\n    ds = ds.map(f, num_proc=15)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n## Expected results\r\nThe known problem with dill could be prevented as explained in the link above (workaround.) Since `HFArgumentParser` nicely uses the enum class for choices it makes sense to also deal with this bug under the hood.\r\n\r\n## Actual results\r\n\r\n```python\r\n  File \"/home/xxxx/miniconda3/lib/python3.8/site-packages/dill/_dill.py\", line 1373, in save_type\r\n    pickler.save_reduce(_create_type, (type(obj), obj.__name__,\r\n  File \"/home/xxxx/miniconda3/lib/python3.8/pickle.py\", line 690, in save_reduce\r\n    save(args)\r\n  File \"/home/xxxx/miniconda3/lib/python3.8/pickle.py\", line 558, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"/home/xxxx/miniconda3/lib/python3.8/pickle.py\", line 899, in save_tuple\r\n    save(element)\r\n  File \"/home/xxxx/miniconda3/lib/python3.8/pickle.py\", line 534, in save\r\n    self.framer.commit_frame()\r\n  File \"/home/xxxx/miniconda3/lib/python3.8/pickle.py\", line 220, in commit_frame\r\n    if f.tell() >= self._FRAME_SIZE_TARGET or force:\r\nRecursionError: maximum recursion depth exceeded while calling a Python object\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-5.9.0-4-amd64-x86_64-with-glibc2.10\r\n- Python version: 3.8.5\r\n- PyArrow version: 3.0.0\r\n",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 944175697,
    "title": "Support multi-worker with streaming dataset (IterableDataset).",
    "dateCreated": "2021-07-14T08:22:58Z",
    "dateModified": "2021-07-14T08:22:58Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nThe current `.map` does not support multi-process, CPU can become bottleneck if the pre-processing is complex (e.g. t5 span masking).\r\n\r\n**Describe the solution you'd like**\r\nIdeally `.map` should support multi-worker like tfds, with `AUTOTUNE`.\r\n\r\n**Describe alternatives you've considered**\r\nA simpler solution is to shard the dataset and process it in parallel with pytorch dataloader. The shard does not need to be of equal size.\r\n* https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset\r\n\r\n**Additional context**\r\n",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 943838085,
    "title": "load_dataset(\"financial_phrasebank\") NonMatchingChecksumError",
    "dateCreated": "2021-07-13T21:21:49Z",
    "dateModified": "2021-07-13T21:21:49Z",
    "description": "## Describe the bug\r\nAttempting to download the financial_phrasebank dataset results in a NonMatchingChecksumError\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"financial_phrasebank\", 'sentences_allagree')\r\n```\r\n\r\n## Expected results\r\nI expect to see the financial_phrasebank dataset downloaded successfully\r\n\r\n## Actual results\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip']\r\n\r\n## Environment info\r\n- `datasets` version: 1.9.0\r\n- Platform: Linux-4.14.232-177.418.amzn2.x86_64-x86_64-with-debian-10.6\r\n- Python version: 3.7.10\r\n- PyArrow version: 4.0.1\r\n",
    "status": "open",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 943591055,
    "title": "Fix docstrings",
    "dateCreated": "2021-07-13T16:09:14Z",
    "dateModified": "2021-07-13T16:09:14Z",
    "description": "Fix rendering of some docstrings.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 943527463,
    "title": "Refactor patching to specific submodule",
    "dateCreated": "2021-07-13T15:08:45Z",
    "dateModified": "2021-07-13T15:08:45Z",
    "description": "Minor reorganization of the code, so that additional patching functions (not related to streaming) might be created.\r\n\r\nIn relation with the initial approach followed in #2631.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 943484913,
    "title": "Streaming for the Json loader",
    "dateCreated": "2021-07-13T14:37:06Z",
    "dateModified": "2021-07-13T14:37:06Z",
    "description": "It was not using `open` in the builder. Therefore `pyarrow.json.read_json` was downloading the full file to start yielding rows.\r\n\r\nMoreover, it appeared that `pyarrow.json.read_json` was not really suited for streaming as it was downloading too much data and failing if `block_size` was not properly configured (related to #2573).\r\n\r\nSo I switched to using `open` which is extended to support reading from remote file progressively, and I removed the pyarrow json reader which was not practical.\r\nInstead, I'm using the classical `json.loads` from the standard library.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 943290736,
    "title": "Add the CIDEr metric?",
    "dateCreated": "2021-07-13T12:22:51Z",
    "dateModified": "2021-07-13T12:22:51Z",
    "description": "Hi,\r\nI find the api in https://huggingface.co/metrics  quite useful.\r\nI am playing around with video/image captioning task, where CIDEr is a popular metric.\r\nDo you plan to add this into the HF ```datasets``` library?\r\nThanks.",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 943044514,
    "title": "Streaming for the Pandas loader",
    "dateCreated": "2021-07-13T09:18:21Z",
    "dateModified": "2021-07-13T09:18:21Z",
    "description": "It was not using open in the builder. Therefore pd.read_pickle could fail when streaming from a private repo for example.\r\n\r\nIndeed, when streaming, open is extended to support reading from remote files and handles authentication to the HF Hub",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 943030999,
    "title": "Streaming for the CSV loader",
    "dateCreated": "2021-07-13T09:08:58Z",
    "dateModified": "2021-07-13T09:08:58Z",
    "description": "It was not using `open` in the builder. Therefore `pd.read_csv` was downloading the full file to start yielding rows.\r\n\r\nIndeed, when streaming, `open` is extended to support reading from remote file progressively.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 942805621,
    "title": "Inject ASR template for lj_speech dataset",
    "dateCreated": "2021-07-13T06:04:54Z",
    "dateModified": "2021-07-13T06:04:54Z",
    "description": "Related to: #2565, #2633.\r\n\r\ncc: @lewtun ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 942396414,
    "title": "Update ASR tags",
    "dateCreated": "2021-07-12T19:58:31Z",
    "dateModified": "2021-07-12T19:58:31Z",
    "description": "This PR updates the ASR tags of the 5 datasets added in #2565 following the change of task categories in #2620 ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 942293727,
    "title": "add image-classification task template",
    "dateCreated": "2021-07-12T17:41:03Z",
    "dateModified": "2021-07-12T17:41:03Z",
    "description": "Snippet below is the tl;dr, but you can try it out directly here:\r\n\r\n[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/nateraw/005c025d41f0e48ae3d4ee61c0f20b70/image-classification-task-template-demo.ipynb)\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset('nateraw/image-folder', data_files='PetImages/')\r\n# DatasetDict({\r\n#     train: Dataset({\r\n#         features: ['file', 'labels'],\r\n#         num_rows: 23410\r\n#     })\r\n# })\r\n\r\nds = ds.prepare_for_task('image-classification')\r\n# DatasetDict({\r\n#     train: Dataset({\r\n#         features: ['image_file_path', 'labels'],\r\n#         num_rows: 23410\r\n#     })\r\n# })\r\n```",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 942242271,
    "title": "Delete extracted files when loading dataset",
    "dateCreated": "2021-07-12T16:39:33Z",
    "dateModified": "2021-07-12T16:39:33Z",
    "description": "Close #2481, close #2604, close #2591.\r\n\r\ncc: @stas00, @thomwolf, @BirgerMoell ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 942102956,
    "title": "Progress bars are not properly rendered in Jupyter notebook",
    "dateCreated": "2021-07-12T14:07:13Z",
    "dateModified": "2021-07-12T14:07:13Z",
    "description": "## Describe the bug\r\nThe progress bars are not Jupyter widgets; regular progress bars appear (like in a terminal).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nds.map(tokenize, num_proc=10)\r\n```\r\n\r\n## Expected results\r\nJupyter widgets displaying the progress bars.\r\n\r\n## Actual results\r\nSimple plane progress bars.\r\n\r\ncc: Reported by @thomwolf ",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 941819205,
    "title": "Load datasets from the Hub without requiring a dataset script",
    "dateCreated": "2021-07-12T08:45:17Z",
    "dateModified": "2021-07-12T08:45:17Z",
    "description": "As a user I would like to be able to upload my csv/json/text/parquet/etc. files in a dataset repository on the Hugging Face Hub and be able to load this dataset with `load_dataset` without having to implement a dataset script.\r\n\r\nMoreover I would like to be able to specify which file goes into which split using the `data_files` argument.\r\n\r\nThis feature should be compatible with private repositories and dataset streaming.\r\n\r\nThis can be implemented by checking the extension of the files in the dataset repository and then by using the right dataset builder that is already packaged in the library (csv/json/text/parquet/etc.)",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 941676404,
    "title": "Use ETag of remote data files",
    "dateCreated": "2021-07-12T05:10:10Z",
    "dateModified": "2021-07-12T05:10:10Z",
    "description": "Use ETag of remote data files to create config ID.\r\n\r\nRelated to #2616.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 941503349,
    "title": "Minor fix tests with Windows paths",
    "dateCreated": "2021-07-11T17:55:48Z",
    "dateModified": "2021-07-11T17:55:48Z",
    "description": "Minor fix tests with Windows paths.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 941497830,
    "title": "Use correct logger in metrics.py",
    "dateCreated": "2021-07-11T17:22:30Z",
    "dateModified": "2021-07-11T17:22:30Z",
    "description": "Fixes #2624 ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 941439922,
    "title": "\u269b\ufe0f\ud83d\ude07\u2699\ufe0f\ud83d\udd11",
    "dateCreated": "2021-07-11T12:14:34Z",
    "dateModified": "2021-07-11T12:14:34Z",
    "description": "",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 941318247,
    "title": "can't set verbosity for `metric.py`",
    "dateCreated": "2021-07-10T20:23:45Z",
    "dateModified": "2021-07-10T20:23:45Z",
    "description": "## Describe the bug\r\n```\r\n[2021-07-10 20:13:11,528][datasets.utils.filelock][INFO] - Lock 139705371374976 acquired on /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.lock\r\n[2021-07-10 20:13:11,529][datasets.arrow_writer][INFO] - Done writing 32 examples in 6100 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\r\n[2021-07-10 20:13:11,531][datasets.arrow_dataset][INFO] - Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\r\n[2021-07-10 20:13:11,543][/conda/envs/myenv/lib/python3.8/site-packages/datasets/metric.py][INFO] - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\r\n```\r\nAs you can see, `datasets` logging come from different places. \r\n`filelock`, `arrow_writer` & `arrow_dataset` comes from `datasets.*` which are expected \r\nHowever, `metric.py` logging comes from `/conda/envs/myenv/lib/python3.8/site-packages/datasets/`\r\n\r\nSo when setting `datasets.utils.logging.set_verbosity_error()`,  it still logs the last message which is annoying during evaluation. \r\n\r\nI had to do \r\n```\r\nlogging.getLogger(\"/conda/envs/myenv/lib/python3.8/site-packages/datasets/metric\").setLevel(logging.ERROR)\r\n``` \r\nto fully mute these messages\r\n\r\n## Expected results\r\nit shouldn't log these messages when setting `datasets.utils.logging.set_verbosity_error()`\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: tried both 1.8.0 & 1.9.0\r\n- Platform: Ubuntu 18.04.5 LTS \r\n- Python version: 3.8.10\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 941265342,
    "title": "[Metrics] added wiki_split metrics",
    "dateCreated": "2021-07-10T14:51:50Z",
    "dateModified": "2021-07-10T14:51:50Z",
    "description": "Fixes: #2606\r\n\r\nThis pull request adds combine metrics for the wikisplit or English sentence split task\r\n\r\nReviewer: @patrickvonplaten ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 941127785,
    "title": "Integration with AugLy",
    "dateCreated": "2021-07-10T00:03:09Z",
    "dateModified": "2021-07-10T00:03:09Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nFacebook recently launched a library, [AugLy](https://github.com/facebookresearch/AugLy) , that has a unified API for augmentations for image, video and text.\r\n\r\nIt would be pretty exciting to have it hooked up to HF libraries so that we can make NLP models robust to misspellings or to punctuation, or emojis etc. Plus, with Transformers supporting more CV use cases, having augmentations support becomes crucial.\r\n\r\n**Describe the solution you'd like**\r\nThe biggest difference between augmentations and preprocessing is that preprocessing happens only once, but you are running augmentations once per epoch. AugLy operates on text directly, so this breaks the typical workflow where we would run the tokenizer once, set format to pt tensors and be ready for the Dataloader.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nOne possible way of implementing these is to make a custom Dataset class where getitem(i) runs the augmentation and the tokenizer every time, though this would slow training down considerably given we wouldn't even run the tokenizer in batches.\r\n",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 940916446,
    "title": "Use prefix to allow exceed Windows MAX_PATH",
    "dateCreated": "2021-07-09T16:39:53Z",
    "dateModified": "2021-07-09T16:39:53Z",
    "description": "By using this prefix, you can exceed the Windows MAX_PATH limit.\r\n\r\nSee: https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file?redirectedfrom=MSDN#win32-file-namespaces\r\n\r\nRelated to #2524, #2220.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 940893389,
    "title": "Add speech processing tasks",
    "dateCreated": "2021-07-09T16:07:29Z",
    "dateModified": "2021-07-09T16:07:29Z",
    "description": "This PR replaces the `automatic-speech-recognition` task category with a broader `speech-processing` category. \r\n\r\nThe tasks associated with this category are derived from the [SUPERB benchmark](https://arxiv.org/abs/2105.01051), and ASR is included in this set.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 940858236,
    "title": "Add ASR task for SUPERB",
    "dateCreated": "2021-07-09T15:19:45Z",
    "dateModified": "2021-07-09T15:19:45Z",
    "description": "This PR starts building up the SUPERB benchmark by including the ASR task as described in the [SUPERB paper](https://arxiv.org/abs/2105.01051) and `s3prl` [instructions](https://github.com/s3prl/s3prl/tree/v0.2.0/downstream#asr-automatic-speech-recognition).\r\n\r\nUsage:\r\n\r\n```python\r\nfrom datasets import load_dataset \r\n\r\nasr = load_dataset(\"superb\", \"asr\")\r\n# DatasetDict({\r\n#     train: Dataset({\r\n#         features: ['file', 'text', 'speaker_id', 'chapter_id', 'id'],\r\n#         num_rows: 28539\r\n#     })\r\n#     validation: Dataset({\r\n#         features: ['file', 'text', 'speaker_id', 'chapter_id', 'id'],\r\n#         num_rows: 2703\r\n#     })\r\n#     test: Dataset({\r\n#         features: ['file', 'text', 'speaker_id', 'chapter_id', 'id'],\r\n#         num_rows: 2620\r\n#     })\r\n# })\r\n```\r\n\r\nI've used the GLUE benchmark as a guide for filling out the README.\r\n\r\nTo move fast during the evaluation PoC I propose to merge one task at a time, so we can continue building the training / evaluation framework in parallel.\r\n\r\nNote: codewise this PR is ready for review - I'll add the missing YAML tags once #2620 is merged :)",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 940852640,
    "title": "`filelock.py` Error",
    "dateCreated": "2021-07-09T15:12:49Z",
    "dateModified": "2021-07-09T15:12:49Z",
    "description": "## Describe the bug\r\n\r\nIt seems that the `filelock.py` went error. \r\n\r\n```\r\n>>> ds=load_dataset('xsum')\r\n\r\n^CTraceback (most recent call last):\r\n  File \"/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/utils/filelock.py\", line 402, in _acquire\r\n    fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\r\nOSError: [Errno 37] No locks available\r\n```\r\n\r\nAccording to error log, it is OSError, but there is an `except` in the `_acquire` function.\r\n\r\n```\r\n    def _acquire(self):\r\n        open_mode = os.O_WRONLY | os.O_CREAT | os.O_EXCL | os.O_TRUNC\r\n        try:\r\n            fd = os.open(self._lock_file, open_mode)\r\n        except (IOError, OSError):\r\n            pass\r\n        else:\r\n            self._lock_file_fd = fd\r\n        return None\r\n```\r\n\r\nI don't know why it stucked rather than `pass` directly.\r\n\r\nI am not quite familiar with filelock operation, so any help is highly appriciated.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n\r\nds = load_dataset('xsum')\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\n```\r\n>>> ds=load_dataset('xsum')\r\n\r\n^CTraceback (most recent call last):\r\n  File \"/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/utils/filelock.py\", line 402, in _acquire\r\n    fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\r\nOSError: [Errno 37] No locks available\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/load.py\", line 818, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/load.py\", line 470, in prepare_module\r\n    with FileLock(lock_path):\r\n  File \"/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/utils/filelock.py\", line 323, in __enter__\r\n    self.acquire()\r\n  File \"/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/utils/filelock.py\", line 272, in acquire\r\n    self._acquire()\r\n  File \"/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/utils/filelock.py\", line 402, in _acquire\r\n    fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\r\nKeyboardInterrupt\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n\r\n- `datasets` version: 1.9.0\r\n- Platform: Linux-4.15.0-135-generic-x86_64-with-debian-buster-sid\r\n- Python version: 3.6.13\r\n- PyArrow version: 4.0.1\r\n",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 940846847,
    "title": "Fix missing EOL issue in to_json for old versions of pandas",
    "dateCreated": "2021-07-09T15:05:45Z",
    "dateModified": "2021-07-09T15:05:45Z",
    "description": "Some versions of pandas don't add an EOL at the end of the output of `to_json`.\r\nTherefore users could end up having two samples in the same line\r\n\r\nClose https://github.com/huggingface/datasets/issues/2615",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 940799038,
    "title": "Support remote data files",
    "dateCreated": "2021-07-09T14:07:38Z",
    "dateModified": "2021-07-09T14:07:38Z",
    "description": "Add support for (streaming) remote data files:\r\n\r\n```python\r\ndata_files = f\"https://huggingface.co/datasets/{repo_id}/resolve/main/{relative_file_path}\"\r\nds = load_dataset(\"json\", split=\"train\", data_files=data_files, streaming=True)\r\n```\r\n\r\ncc: @thomwolf ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 940794339,
    "title": "Jsonlines export error",
    "dateCreated": "2021-07-09T14:02:05Z",
    "dateModified": "2021-07-09T14:02:05Z",
    "description": "## Describe the bug\r\nWhen exporting large datasets in jsonlines (c4 in my case) the created file has an error every 9999 lines: the 9999th and 10000th are concatenated, thus breaking the jsonlines format. This sounds like it is related to batching, which is by 10000 by default\r\n\r\n## Steps to reproduce the bug\r\nThis what I'm running:\r\n\r\nin python:\r\n\r\n```\r\nfrom datasets import load_dataset\r\nptb = load_dataset(\"ptb_text_only\")\r\nptb[\"train\"].to_json(\"ptb.jsonl\")\r\n```\r\n\r\nthen out of python:\r\n\r\n```\r\nhead -10000 ptb.jsonl\r\n```\r\n\r\n## Expected results\r\nProperly separated lines\r\n\r\n## Actual results\r\nThe last line is a concatenation of two lines\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n\r\n- `datasets` version: 1.9.1.dev0\r\n- Platform: Linux-5.4.0-1046-gcp-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyArrow version: 4.0.1",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 940762427,
    "title": "Convert numpy scalar to python float in Pearsonr output",
    "dateCreated": "2021-07-09T13:22:55Z",
    "dateModified": "2021-07-09T13:22:55Z",
    "description": "Following of https://github.com/huggingface/datasets/pull/2612",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 940759852,
    "title": "Use ndarray.item instead of ndarray.tolist",
    "dateCreated": "2021-07-09T13:19:35Z",
    "dateModified": "2021-07-09T13:19:35Z",
    "description": "This PR follows up on #2612 to use `numpy.ndarray.item` instead of `numpy.ndarray.tolist` as the latter is somewhat confusing to the developer (even though it works).\r\n\r\nJudging from the `numpy` docs, `ndarray.item` is closer to what we want: https://numpy.org/doc/stable/reference/generated/numpy.ndarray.item.html#numpy-ndarray-item\r\n\r\nPS. Sorry for the duplicate work here. I should have read the numpy docs more carefully in #2612 \r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 940604512,
    "title": "Return Python float instead of numpy.float64 in sklearn metrics",
    "dateCreated": "2021-07-09T09:48:09Z",
    "dateModified": "2021-07-09T09:48:09Z",
    "description": "This PR converts the return type of all `sklearn` metrics to be Python `float` instead of `numpy.float64`.\r\n\r\nThe reason behind this is that our Hub evaluation framework relies on converting benchmark-specific metrics to YAML ([example](https://huggingface.co/datasets/autonlp/autonlp-benchmark-raft-neelalex__raft-test-neelalex__raft-predictions-3/blob/main/README.md#L11)) and the `numpy.float64` format produces garbage like:\r\n\r\n```python\r\nimport yaml\r\nfrom datasets import load_metric\r\n\r\nmetric = load_metric(\"accuracy\")\r\nscore = metric.compute(predictions=[0,1], references=[0,1])\r\nprint(yaml.dump(score[\"accuracy\"])) # output below\r\n# !!python/object/apply:numpy.core.multiarray.scalar\r\n# - !!python/object/apply:numpy.dtype\r\n#   args:\r\n#   - f8\r\n#   - false\r\n#   - true\r\n#   state: !!python/tuple\r\n#   - 3\r\n#   - <\r\n#   - null\r\n#   - null\r\n#   - null\r\n#   - -1\r\n#   - -1\r\n#   - 0\r\n# - !!binary |\r\n#   AAAAAAAA8D8=\r\n```",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 940307053,
    "title": "More consistent naming",
    "dateCreated": "2021-07-09T00:09:17Z",
    "dateModified": "2021-07-09T00:09:17Z",
    "description": "As per @stas00's suggestion in #2500, this PR inserts a space between the logo and the lib name (`\ud83e\udd17Datasets` -> `\ud83e\udd17 Datasets`) for consistency with the Transformers lib. Additionally, more consistent names are used for Datasets Hub, etc.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 939899829,
    "title": "Add missing WikiANN language tags",
    "dateCreated": "2021-07-08T14:08:01Z",
    "dateModified": "2021-07-08T14:08:01Z",
    "description": "Add missing language tags for WikiANN datasets.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 939616682,
    "title": "Fix potential DuplicatedKeysError",
    "dateCreated": "2021-07-08T08:38:04Z",
    "dateModified": "2021-07-08T08:38:04Z",
    "description": "Fix potential DiplicatedKeysError by ensuring keys are unique.\r\n\r\nWe should promote as a good practice, that the keys should be programmatically generated as unique, instead of read from data (which might be not unique).",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 938897626,
    "title": "Support streaming JSON files ",
    "dateCreated": "2021-07-07T13:30:22Z",
    "dateModified": "2021-07-07T13:30:22Z",
    "description": "Use open in JSON dataset builder, so that it can be patched with xopen for streaming.\r\n\r\nClose #2607.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 938796902,
    "title": "Streaming local gzip compressed JSON line files is not working",
    "dateCreated": "2021-07-07T11:36:33Z",
    "dateModified": "2021-07-07T11:36:33Z",
    "description": "## Describe the bug\r\nUsing streaming to iterate on local gzip compressed JSON files raise a file not exist error\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nstreamed_dataset = load_dataset('json', split='train', data_files=data_files, streaming=True)\r\n\r\nnext(iter(streamed_dataset))\r\n```\r\n\r\n## Actual results\r\n```\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-6-27a664e29784> in <module>\r\n----> 1 next(iter(streamed_dataset))\r\n\r\n~/Documents/GitHub/datasets/src/datasets/iterable_dataset.py in __iter__(self)\r\n    336 \r\n    337     def __iter__(self):\r\n--> 338         for key, example in self._iter():\r\n    339             if self.features:\r\n    340                 # we encode the example for ClassLabel feature types for example\r\n\r\n~/Documents/GitHub/datasets/src/datasets/iterable_dataset.py in _iter(self)\r\n    333         else:\r\n    334             ex_iterable = self._ex_iterable\r\n--> 335         yield from ex_iterable\r\n    336 \r\n    337     def __iter__(self):\r\n\r\n~/Documents/GitHub/datasets/src/datasets/iterable_dataset.py in __iter__(self)\r\n     76 \r\n     77     def __iter__(self):\r\n---> 78         for key, example in self.generate_examples_fn(**self.kwargs):\r\n     79             yield key, example\r\n     80 \r\n\r\n~/Documents/GitHub/datasets/src/datasets/iterable_dataset.py in wrapper(**kwargs)\r\n    282     def wrapper(**kwargs):\r\n    283         python_formatter = PythonFormatter()\r\n--> 284         for key, table in generate_tables_fn(**kwargs):\r\n    285             batch = python_formatter.format_batch(table)\r\n    286             for i, example in enumerate(_batch_to_examples(batch)):\r\n\r\n~/Documents/GitHub/datasets/src/datasets/packaged_modules/json/json.py in _generate_tables(self, files, original_files)\r\n     85                         file,\r\n     86                         read_options=self.config.pa_read_options,\r\n---> 87                         parse_options=self.config.pa_parse_options,\r\n     88                     )\r\n     89                 except pa.ArrowInvalid as err:\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/_json.pyx in pyarrow._json.read_json()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/_json.pyx in pyarrow._json._get_reader()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.get_input_stream()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.get_native_file()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.OSFile.__cinit__()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.OSFile._open_readable()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nFileNotFoundError: [Errno 2] Failed to open local file 'gzip://file-000000000000.json::/Users/thomwolf/github-dataset/file-000000000000.json.gz'. Detail: [errno 2] No such file or directory\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.9.1.dev0\r\n- Platform: Darwin-19.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.7\r\n- PyArrow version: 1.0.0",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 938763684,
    "title": "[Metrics] addition of wiki_split metrics",
    "dateCreated": "2021-07-07T10:56:04Z",
    "dateModified": "2021-07-07T10:56:04Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nWhile training the model on sentence split the task in English we require to evaluate the trained model on `Exact Match`, `SARI` and `BLEU` score\r\nlike this \r\n![image](https://user-images.githubusercontent.com/26653468/124746876-ff5a3380-df3e-11eb-9a01-4b48db7a6694.png)\r\nWhile training we require metrics which can give all the output\r\n\r\nCurrently, we don't have an exact match for text normalized data\r\n\r\n**Describe the solution you'd like**\r\nA custom metrics for wiki_split that can calculate these three values and provide it in the form of a single dictionary\r\nFor exact match, we can refer to [this](https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py) \r\n\r\n**Describe alternatives you've considered**\r\nTwo metrics are already present one more can be added for an exact match then we can run all three metrics in training script\r\n\r\n#self-assign",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 938648164,
    "title": "Make any ClientError trigger retry in streaming mode (e.g. ClientOSError)",
    "dateCreated": "2021-07-07T08:47:23Z",
    "dateModified": "2021-07-07T08:47:23Z",
    "description": "During the FLAX sprint some users have this error when streaming datasets:\r\n```python\r\naiohttp.client_exceptions.ClientOSError: [Errno 104] Connection reset by peer\r\n```\r\nThis error must trigger a retry instead of directly crashing\r\n\r\nTherefore I extended the error type that triggers the retry to be the base aiohttp error type: `ClientError`\r\nIn particular both `ClientOSError` and `ServerDisconnectedError` inherit from `ClientError`.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 938602237,
    "title": "Add option to delete temporary files (e.g. extracted files) when loading dataset",
    "dateCreated": "2021-07-07T07:56:16Z",
    "dateModified": "2021-07-07T07:56:16Z",
    "description": "I'm loading a dataset constituted of 44 GB of compressed JSON files.\r\n\r\nWhen loading the dataset with the JSON script, extracting the files create about 200 GB of uncompressed files before creating the 180GB of arrow cache tables\r\n\r\nHaving a simple way to delete the extracted files after usage (or even better, to stream extraction/delete) would be nice to avoid disk cluter.\r\n\r\nI can maybe tackle this one in the JSON script unless you want a more general solution.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 938588149,
    "title": "Fix DuplicatedKeysError in omp",
    "dateCreated": "2021-07-07T07:38:32Z",
    "dateModified": "2021-07-07T07:38:32Z",
    "description": "Close #2598.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 938555712,
    "title": "Remove import of transformers",
    "dateCreated": "2021-07-07T06:58:18Z",
    "dateModified": "2021-07-07T06:58:18Z",
    "description": "When pickling a tokenizer within multiprocessing, check that is instance of transformers PreTrainedTokenizerBase without importing transformers.\r\n\r\nRelated to huggingface/transformers#12549 and #502.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 938096396,
    "title": "Fix `filter` with multiprocessing in case all samples are discarded",
    "dateCreated": "2021-07-06T17:06:28Z",
    "dateModified": "2021-07-06T17:06:28Z",
    "description": "Fixes #2600 \r\n\r\nAlso I moved the check for `num_proc` larger than dataset size added in #2566 up so that multiprocessing is not used with one process.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 938086745,
    "title": "Crash when using multiprocessing (`num_proc` > 1) on `filter` and all samples are discarded",
    "dateCreated": "2021-07-06T16:53:25Z",
    "dateModified": "2021-07-06T16:53:25Z",
    "description": "## Describe the bug\r\nIf `filter` is applied to a dataset using multiprocessing (`num_proc` > 1) and all sharded datasets are empty afterwards (due to all samples being discarded), the program crashes.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import Dataset\r\ndata = Dataset.from_dict({'id': [0,1]})\r\ndata.filter(lambda x: False, num_proc=2)\r\n```\r\n\r\n## Expected results\r\nAn empty table should be returned without crashing.\r\n\r\n## Actual results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/fingerprint.py\", line 397, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2143, in filter\r\n    return self.map(\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1738, in map\r\n    result = concatenate_datasets(transformed_shards)\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3267, in concatenate_datasets\r\n    table = concat_tables(tables_to_concat, axis=axis)\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/table.py\", line 853, in concat_tables\r\n    return ConcatenationTable.from_tables(tables, axis=axis)\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/table.py\", line 713, in from_tables\r\n    blocks = to_blocks(tables[0])\r\nIndexError: list index out of range\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.9.0\r\n- Platform: Linux-5.12.11-300.fc34.x86_64-x86_64-with-glibc2.2.5\r\n- Python version: 3.8.10\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 937980229,
    "title": "Update processing.rst with other export formats",
    "dateCreated": "2021-07-06T14:50:38Z",
    "dateModified": "2021-07-06T14:50:38Z",
    "description": "Add other supported export formats than CSV in the docs.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 937930632,
    "title": "Unable to download omp dataset",
    "dateCreated": "2021-07-06T14:00:52Z",
    "dateModified": "2021-07-06T14:00:52Z",
    "description": "## Describe the bug\r\nThe omp dataset cannot be downloaded because of a DuplicatedKeysError\r\n\r\n## Steps to reproduce the bug\r\nfrom datasets import load_dataset\r\nomp = load_dataset('omp', 'posts_labeled')\r\nprint(omp)\r\n\r\n## Expected results\r\nThis code should download the omp dataset and print the dictionary\r\n\r\n## Actual results\r\nDownloading and preparing dataset omp/posts_labeled (download: 1.27 MiB, generated: 13.31 MiB, post-processed: Unknown size, total: 14.58 MiB) to /home/erika_distefano/.cache/huggingface/datasets/omp/posts_labeled/1.1.0/2fe5b067be3bff1d4588d5b0cbb9b5b22ae1b9d5b026a8ff572cd389f862735b...\r\n0 examples [00:00, ? examples/s]2021-07-06 09:43:55.868815: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\nTraceback (most recent call last):      \r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/builder.py\", line 990, in _prepare_split\r\n    writer.write(example, key)\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/arrow_writer.py\", line 338, in write\r\n    self.check_duplicate_keys()\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/arrow_writer.py\", line 349, in check_duplicate_keys\r\n    raise DuplicatedKeysError(key)\r\ndatasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 3326\r\nKeys should be unique and deterministic in nature\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"hf_datasets.py\", line 32, in <module>\r\n    omp = load_dataset('omp', 'posts_labeled')\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/load.py\", line 748, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/builder.py\", line 575, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/builder.py\", line 652, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/builder.py\", line 992, in _prepare_split\r\n    num_examples, num_bytes = writer.finalize()\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/arrow_writer.py\", line 409, in finalize\r\n    self.check_duplicate_keys()\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/arrow_writer.py\", line 349, in check_duplicate_keys\r\n    raise DuplicatedKeysError(key)\r\ndatasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 3326\r\nKeys should be unique and deterministic in nature\r\n\r\n## Environment info\r\n- `datasets` version: 1.8.0\r\n- Platform: Ubuntu 18.04.4 LTS\r\n- Python version: 3.6.9\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 937917770,
    "title": "Remove redundant prepare_module",
    "dateCreated": "2021-07-06T13:47:45Z",
    "dateModified": "2021-07-06T13:47:45Z",
    "description": "I have noticed that after implementing `load_dataset_builder` (#2500), there is a redundant call to `prepare_module`.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 937598914,
    "title": "Transformer Class on dataset",
    "dateCreated": "2021-07-06T07:27:15Z",
    "dateModified": "2021-07-06T07:27:15Z",
    "description": "Just wondering if you have intenttion to create\r\n\r\nTransformerClass :\r\n    dataset --> dataset\r\n\r\nand make determnistic transformation (ie not fit).\r\n\r\n\r\n\r\n\r\n\r\n",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 937483120,
    "title": "ModuleNotFoundError: No module named 'datasets.tasks' while importing common voice datasets",
    "dateCreated": "2021-07-06T03:20:55Z",
    "dateModified": "2021-07-06T03:20:55Z",
    "description": "Error traceback:\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-8-a7b592d3bca0> in <module>()\r\n      1 from datasets import load_dataset, load_metric\r\n      2 \r\n----> 3 common_voice_train = load_dataset(\"common_voice\", \"pa-IN\", split=\"train+validation\")\r\n      4 common_voice_test = load_dataset(\"common_voice\", \"pa-IN\", split=\"test\")\r\n\r\n9 frames\r\n/root/.cache/huggingface/modules/datasets_modules/datasets/common_voice/078d412587e9efeb0ae2e574da99c31e18844c496008d53dc5c60f4159ed639b/common_voice.py in <module>()\r\n     19 \r\n     20 import datasets\r\n---> 21 from datasets.tasks import AutomaticSpeechRecognition\r\n     22 \r\n     23 \r\n\r\nModuleNotFoundError: No module named 'datasets.tasks'",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 937294772,
    "title": "Fix BibTeX entry",
    "dateCreated": "2021-07-05T18:24:10Z",
    "dateModified": "2021-07-05T18:24:10Z",
    "description": "Fix BibTeX entry.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 937242137,
    "title": "Support pandas 1.3.0 read_csv",
    "dateCreated": "2021-07-05T16:40:04Z",
    "dateModified": "2021-07-05T16:40:04Z",
    "description": "Workaround for this issue in pandas 1.3.0 : https://github.com/pandas-dev/pandas/issues/42387\r\n\r\nThe csv reader raises an error:\r\n```python\r\n/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py in _refine_defaults_read(dialect, delimiter, delim_whitespace, engine, sep, error_bad_lines, warn_bad_lines, on_bad_lines, names, prefix, defaults)\r\n   1304 \r\n   1305     if names is not lib.no_default and prefix is not lib.no_default:\r\n-> 1306         raise ValueError(\"Specified named and prefix; you can only specify one.\")\r\n   1307 \r\n   1308     kwds[\"names\"] = None if names is lib.no_default else names\r\n\r\nValueError: Specified named and prefix; you can only specify one.\r\n```",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 937060559,
    "title": "Add c4.noclean infos",
    "dateCreated": "2021-07-05T12:51:40Z",
    "dateModified": "2021-07-05T12:51:40Z",
    "description": "Adding the data files checksums and the dataset size of the c4.noclean configuration of the C4 dataset",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 936957975,
    "title": "Cached dataset overflowing disk space",
    "dateCreated": "2021-07-05T10:43:19Z",
    "dateModified": "2021-07-05T10:43:19Z",
    "description": "I'm training a Swedish Wav2vec2 model on a Linux GPU and having issues that the huggingface cached dataset folder is completely filling up my disk space (I'm training on a dataset of around 500 gb).\r\n\r\nThe cache folder is 500gb (and now my disk space is full).\r\n\r\nIs there a way to toggle caching or set the caching to be stored on a different device (I have another drive with 4 tb that could hold the caching files).\r\n\r\nThis might not technically be a bug, but I was unsure and I felt that the bug was the closest one.\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/birger/miniconda3/envs/wav2vec2/lib/python3.7/site-packages/multiprocess/pool.py\", line 121, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/birger/miniconda3/envs/wav2vec2/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 186, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/birger/miniconda3/envs/wav2vec2/lib/python3.7/site-packages/datasets/fingerprint.py\", line 397, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/home/birger/miniconda3/envs/wav2vec2/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1983, in _map_single\r\n    writer.finalize()\r\n  File \"/home/birger/miniconda3/envs/wav2vec2/lib/python3.7/site-packages/datasets/arrow_writer.py\", line 418, in finalize\r\n    self.pa_writer.close()\r\n  File \"pyarrow/ipc.pxi\", line 402, in pyarrow.lib._CRecordBatchWriter.close\r\n  File \"pyarrow/error.pxi\", line 97, in pyarrow.lib.check_status\r\nOSError: [Errno 28] Error writing bytes to file. Detail: [errno 28] No space left on device\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 936954348,
    "title": "Add language tags",
    "dateCreated": "2021-07-05T10:39:57Z",
    "dateModified": "2021-07-05T10:39:57Z",
    "description": "This PR adds some missing language tags needed for ASR datasets in #2565 ",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 936825060,
    "title": "Support multilabel metrics",
    "dateCreated": "2021-07-05T08:19:25Z",
    "dateModified": "2021-07-05T08:19:25Z",
    "description": "Currently, multilabel metrics are not supported because `predictions` and `references` are defined as `Value(\"int32\")`.\r\n\r\nThis PR creates a new feature type `OptionalSequence` which can act as either `Value(\"int32\")` or `Sequence(Value(\"int32\"))`, depending on the data passed.\r\n\r\n\r\nClose #2554.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 936795541,
    "title": "Fix test_is_small_dataset",
    "dateCreated": "2021-07-05T07:46:26Z",
    "dateModified": "2021-07-05T07:46:26Z",
    "description": "Remove environment variable fixture `env_max_in_memory_dataset_size`. This fixture does not work because env variable is read in datasets.config when first loading datasets, and it is never reread during tests.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 936771339,
    "title": "Add aiohttp to tests extras require",
    "dateCreated": "2021-07-05T07:14:01Z",
    "dateModified": "2021-07-05T07:14:01Z",
    "description": "Currently, none of the streaming tests are runned within our CI test suite, because the streaming tests require aiohttp and this is missing from our tests extras require dependencies.\r\n\r\nOur CI test suite should be exhaustive and test all the library functionalities.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 936747588,
    "title": "Fix misalignment in SQuAD",
    "dateCreated": "2021-07-05T06:42:20Z",
    "dateModified": "2021-07-05T06:42:20Z",
    "description": "Fix misalignment between:\r\n- the answer text and\r\n- the answer_start within the context\r\n\r\nby keeping original leading blank spaces in the context.\r\n\r\nFix #2585.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 936484419,
    "title": "sqaud_v2 dataset contains misalignment between the answer text and the context value at the answer index",
    "dateCreated": "2021-07-04T15:39:49Z",
    "dateModified": "2021-07-04T15:39:49Z",
    "description": "## Describe the bug\r\nThe built in huggingface squad_v2 dataset that you can access via datasets.load_dataset contains mis-alignment between the answers['text'] and the characters in the context at the location specified by answers['answer_start'].\r\n\r\nFor example:\r\nid = '56d1f453e7d4791d009025bd'\r\nanswers = {'text': ['Pure Land'], 'answer_start': [146]}\r\nHowever the actual text in context at location 146 is 'ure Land,'\r\nWhich is an off-by-one error from the correct answer.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\n\r\ndef check_context_answer_alignment(example):\r\n    for a_idx in range(len(example['answers']['text'])):\r\n        # check raw dataset for answer consistency between context and answer\r\n        answer_text = example['answers']['text'][a_idx]\r\n        a_st_idx = example['answers']['answer_start'][a_idx]\r\n        a_end_idx = a_st_idx + len(example['answers']['text'][a_idx])\r\n        answer_text_from_context = example['context'][a_st_idx:a_end_idx]\r\n        if answer_text != answer_text_from_context:\r\n            #print(example['id'])\r\n            return False\r\n    return True\r\n\r\ndataset = datasets.load_dataset('squad_v2', split='train', keep_in_memory=True)\r\n\r\nstart_len = len(dataset)\r\ndataset = dataset.filter(check_context_answer_alignment,\r\n                                num_proc=1,\r\n                                keep_in_memory=True)\r\nend_len = len(dataset)\r\nprint('{} instances contain mis-alignment between the answer text and answer index.'.format(start_len - end_len))\r\n```\r\n\r\n## Expected results\r\nThis code should result in 0 rows being filtered out from the dataset.\r\n\r\n## Actual results\r\nThis filter command results in 258 rows being flagged as containing a discrepancy between the text contained within answers['text'] and the text in example['context'] at the answers['answer_start'] location.\r\n\r\nThis code will reproduce the problem and produce the following count:\r\n\"258 instances contain mis-alignment between the answer text and answer index.\"\r\n\r\n## Environment info\r\nSteps to rebuilt the Conda environment:\r\n```\r\n# create a virtual environment to stuff all these packages into\r\nconda create -n round8 python=3.8 -y\r\n\r\n# activate the virtual environment\r\nconda activate round8\r\n\r\n# install pytorch (best done through conda to handle cuda dependencies)\r\nconda install pytorch torchvision torchtext cudatoolkit=11.1 -c pytorch-lts -c nvidia\r\n\r\npip install jsonpickle transformers datasets matplotlib\r\n```\r\n\r\nOS: Ubuntu 20.04\r\nPython 3.8\r\n\r\nResult of `conda env export`:\r\n```\r\nname: round8\r\nchannels:\r\n  - pytorch-lts\r\n  - nvidia\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=main\r\n  - _openmp_mutex=4.5=1_gnu\r\n  - blas=1.0=mkl\r\n  - brotlipy=0.7.0=py38h27cfd23_1003\r\n  - bzip2=1.0.8=h7b6447c_0\r\n  - ca-certificates=2021.5.25=h06a4308_1\r\n  - certifi=2021.5.30=py38h06a4308_0\r\n  - cffi=1.14.5=py38h261ae71_0\r\n  - chardet=4.0.0=py38h06a4308_1003\r\n  - cryptography=3.4.7=py38hd23ed53_0\r\n  - cudatoolkit=11.1.74=h6bb024c_0\r\n  - ffmpeg=4.2.2=h20bf706_0\r\n  - freetype=2.10.4=h5ab3b9f_0\r\n  - gmp=6.2.1=h2531618_2\r\n  - gnutls=3.6.15=he1e5248_0\r\n  - idna=2.10=pyhd3eb1b0_0\r\n  - intel-openmp=2021.2.0=h06a4308_610\r\n  - jpeg=9b=h024ee3a_2\r\n  - lame=3.100=h7b6447c_0\r\n  - lcms2=2.12=h3be6417_0\r\n  - ld_impl_linux-64=2.35.1=h7274673_9\r\n  - libffi=3.3=he6710b0_2\r\n  - libgcc-ng=9.3.0=h5101ec6_17\r\n  - libgomp=9.3.0=h5101ec6_17\r\n  - libidn2=2.3.1=h27cfd23_0\r\n  - libopus=1.3.1=h7b6447c_0\r\n  - libpng=1.6.37=hbc83047_0\r\n  - libstdcxx-ng=9.3.0=hd4cf53a_17\r\n  - libtasn1=4.16.0=h27cfd23_0\r\n  - libtiff=4.2.0=h85742a9_0\r\n  - libunistring=0.9.10=h27cfd23_0\r\n  - libuv=1.40.0=h7b6447c_0\r\n  - libvpx=1.7.0=h439df22_0\r\n  - libwebp-base=1.2.0=h27cfd23_0\r\n  - lz4-c=1.9.3=h2531618_0\r\n  - mkl=2021.2.0=h06a4308_296\r\n  - mkl-service=2.3.0=py38h27cfd23_1\r\n  - mkl_fft=1.3.0=py38h42c9631_2\r\n  - mkl_random=1.2.1=py38ha9443f7_2\r\n  - ncurses=6.2=he6710b0_1\r\n  - nettle=3.7.3=hbbd107a_1\r\n  - ninja=1.10.2=hff7bd54_1\r\n  - numpy=1.20.2=py38h2d18471_0\r\n  - numpy-base=1.20.2=py38hfae3a4d_0\r\n  - olefile=0.46=py_0\r\n  - openh264=2.1.0=hd408876_0\r\n  - openssl=1.1.1k=h27cfd23_0\r\n  - pillow=8.2.0=py38he98fc37_0\r\n  - pip=21.1.2=py38h06a4308_0\r\n  - pycparser=2.20=py_2\r\n  - pyopenssl=20.0.1=pyhd3eb1b0_1\r\n  - pysocks=1.7.1=py38h06a4308_0\r\n  - python=3.8.10=h12debd9_8\r\n  - pytorch=1.8.1=py3.8_cuda11.1_cudnn8.0.5_0\r\n  - readline=8.1=h27cfd23_0\r\n  - requests=2.25.1=pyhd3eb1b0_0\r\n  - setuptools=52.0.0=py38h06a4308_0\r\n  - six=1.16.0=pyhd3eb1b0_0\r\n  - sqlite=3.35.4=hdfb4753_0\r\n  - tk=8.6.10=hbc83047_0\r\n  - torchtext=0.9.1=py38\r\n  - torchvision=0.9.1=py38_cu111\r\n  - typing_extensions=3.7.4.3=pyha847dfd_0\r\n  - urllib3=1.26.4=pyhd3eb1b0_0\r\n  - wheel=0.36.2=pyhd3eb1b0_0\r\n  - x264=1!157.20191217=h7b6447c_0\r\n  - xz=5.2.5=h7b6447c_0\r\n  - zlib=1.2.11=h7b6447c_3\r\n  - zstd=1.4.9=haebb681_0\r\n  - pip:\r\n    - click==8.0.1\r\n    - cycler==0.10.0\r\n    - datasets==1.8.0\r\n    - dill==0.3.4\r\n    - filelock==3.0.12\r\n    - fsspec==2021.6.0\r\n    - huggingface-hub==0.0.8\r\n    - joblib==1.0.1\r\n    - jsonpickle==2.0.0\r\n    - kiwisolver==1.3.1\r\n    - matplotlib==3.4.2\r\n    - multiprocess==0.70.12.2\r\n    - packaging==20.9\r\n    - pandas==1.2.4\r\n    - pyarrow==3.0.0\r\n    - pyparsing==2.4.7\r\n    - python-dateutil==2.8.1\r\n    - pytz==2021.1\r\n    - regex==2021.4.4\r\n    - sacremoses==0.0.45\r\n    - tokenizers==0.10.3\r\n    - tqdm==4.49.0\r\n    - transformers==4.6.1\r\n    - xxhash==2.0.2\r\nprefix: /home/mmajurski/anaconda3/envs/round8\r\n```\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 936049736,
    "title": "wi_locness: reference latest leaderboard on codalab",
    "dateCreated": "2021-07-02T20:26:22Z",
    "dateModified": "2021-07-02T20:26:22Z",
    "description": "The dataset's author asked me to put this codalab link into the dataset's README.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 936034976,
    "title": "Error iteration over IterableDataset using Torch DataLoader",
    "dateCreated": "2021-07-02T19:55:58Z",
    "dateModified": "2021-07-02T19:55:58Z",
    "description": "## Describe the bug\r\nI have an IterableDataset (created using streaming=True) and I am trying to create batches using Torch DataLoader class by passing this IterableDataset to it. This throws error which is pasted below. I can do the same by using Torch IterableDataset. One thing I noticed is that in the former case when I look at the dataloader.sampler class I get torch.utils.data.sampler.SequentialSampler while the latter one gives torch.utils.data.dataloader._InfiniteConstantSampler. \r\n\r\nI am not sure if this is how it is meant to be used, but that's what seemed reasonable to me. \r\n\r\n## Steps to reproduce the bug\r\n\r\n1. Does not work.\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True)\r\n>>> dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)\r\n>>> dataloader.sampler\r\n<torch.utils.data.sampler.SequentialSampler object at 0x7f245a510208>\r\n>>> for batch in dataloader:\r\n...     print(batch)\r\n```\r\n\r\n2. Works.\r\n```python\r\nimport torch\r\nfrom torch.utils.data import Dataset, IterableDataset, DataLoader\r\nclass CustomIterableDataset(IterableDataset):\r\n  'Characterizes a dataset for PyTorch'\r\n  def __init__(self, data):\r\n        'Initialization'\r\n        self.data = data\r\n\r\n\r\n  def __iter__(self):\r\n        return iter(self.data)\r\n\r\n\r\ndata = list(range(12))\r\ndataset = CustomIterableDataset(data)\r\ndataloader = DataLoader(dataset, batch_size=4)\r\nprint(\"dataloader: \", dataloader.sampler)\r\nfor batch in dataloader:\r\n    print(batch)\r\n```\r\n\r\n## Expected results\r\nTo get batches of data with the batch size as 4. Output from the latter one (2) though Datasource is different here so actual data is different.\r\ndataloader:  <torch.utils.data.dataloader._InfiniteConstantSampler object at 0x7f1cc29e2c50>\r\ntensor([0, 1, 2, 3])\r\ntensor([4, 5, 6, 7])\r\ntensor([ 8,  9, 10, 11])\r\n\r\n## Actual results\r\n<torch.utils.data.sampler.SequentialSampler object at 0x7f245a510208>\r\n\r\n...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 435, in __next__\r\n    data = self._next_data()\r\n  File \"/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 474, in _next_data\r\n    index = self._next_index()  # may raise StopIteration\r\n  File \"/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 427, in _next_index\r\n    return next(self._sampler_iter)  # may raise StopIteration\r\n  File \"/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/sampler.py\", line 227, in __iter__\r\n    for idx in self.sampler:\r\n  File \"/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/sampler.py\", line 67, in __iter__\r\n    return iter(range(len(self.data_source)))\r\nTypeError: object of type 'IterableDataset' has no len()\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: '1.8.1.dev0'\r\n- Platform: Linux\r\n- Python version: Python 3.6.8\r\n- PyArrow version: '3.0.0'\r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 935859104,
    "title": "Add skip and take",
    "dateCreated": "2021-07-02T15:10:19Z",
    "dateModified": "2021-07-02T15:10:19Z",
    "description": "As discussed in https://github.com/huggingface/datasets/pull/2375#discussion_r657084544 I added the `IterableDataset.skip` and `IterableDataset.take` methods that allows to do basic splitting of iterable datasets.\r\n\r\nYou can create new dataset with the first `n` examples using `IterableDataset.take()`, or you can get a dataset with the rest of the examples by skipping the first `n` examples with `IterableDataset.skip()`\r\n\r\nOne implementation detail:\r\n\r\nUsing `take` (or `skip`) prevents future dataset shuffling from shuffling the dataset shards, otherwise the taken examples could come from other shards. In this case it only uses the shuffle buffer.\r\nI would have loved to allow the shards of the taken examples to be shuffled anyway, but since we don't know in advance the length of each shard we don't know what shards to take or skip.\r\nI think this is ok though since users can shuffle before doing take or skip. I mentioned this in the documentation\r\n\r\ncc @vblagoje @lewtun ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 935783588,
    "title": "Faster search_batch for ElasticsearchIndex due to threading",
    "dateCreated": "2021-07-02T13:42:07Z",
    "dateModified": "2021-07-02T13:42:07Z",
    "description": "Hey, \r\nI think it makes sense to perform search_batch threaded, so ES can perform search in parallel.\r\nCheers!",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 935767421,
    "title": "Fix Counter import",
    "dateCreated": "2021-07-02T13:21:48Z",
    "dateModified": "2021-07-02T13:21:48Z",
    "description": "Import from `collections` instead of `typing`.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 935486894,
    "title": "Fix BibTeX entry",
    "dateCreated": "2021-07-02T07:10:40Z",
    "dateModified": "2021-07-02T07:10:40Z",
    "description": "Add missing contributor to BibTeX entry.\r\n\r\ncc: @abhishekkrthakur @thomwolf ",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 935187497,
    "title": "Support Zstandard compressed files",
    "dateCreated": "2021-07-01T20:22:34Z",
    "dateModified": "2021-07-01T20:22:34Z",
    "description": "Close #2572.\r\n\r\ncc: @thomwolf ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 934986761,
    "title": "Add mC4",
    "dateCreated": "2021-07-01T15:51:25Z",
    "dateModified": "2021-07-01T15:51:25Z",
    "description": "AllenAI is now hosting the processed C4 and mC4 dataset in this repo: https://huggingface.co/datasets/allenai/c4\r\nThanks a lot to them !\r\n\r\nIn this PR I added the mC4 dataset builder. It supports 108 languages\r\n\r\nYou can load it with\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nen_mc4 = load_dataset(\"mc4\", \"en\")\r\nfr_mc4 = load_dataset(\"mc4\", \"fr\")\r\nen_and_fr_mc4 = load_dataset(\"mc4\", languages=[\"en\", \"fr\"])\r\n```\r\n\r\nIt also supports streaming, if you don't want to download hundreds of GB of data:\r\n```python\r\nen_mc4 = load_dataset(\"mc4\", \"en\", streaming=True)\r\n```\r\n\r\nRegarding the dataset_infos.json, I will add them once I have them.\r\n\r\nAlso we can work on the dataset card at that will be at https://huggingface.co/datasets/mc4\r\nFor now I just added a link to https://huggingface.co/datasets/allenai/c4 as well as a few sections",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 934876496,
    "title": "Add C4",
    "dateCreated": "2021-07-01T13:58:08Z",
    "dateModified": "2021-07-01T13:58:08Z",
    "description": "The old code for the C4 dataset was to generate the C4 with Apache Beam, as in Tensorflow Datasets.\r\nHowever AllenAI is now hosting the processed C4 dataset in this repo: https://huggingface.co/datasets/allenai/c4\r\nThanks a lot to them for their amazing work !\r\n\r\nIn this PR I changed the script to download and prepare the data directly from this repo.\r\nIt has 4 variants: en, en.noblocklist, en.noclean, realnewslike\r\n\r\nYou can load it with\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nc4 = load_dataset(\"c4\", \"en\")\r\n```\r\n\r\nIt also supports streaming, if you don't want to download hundreds of GB of data:\r\n```python\r\nc4 = load_dataset(\"c4\", \"en\", streaming=True)\r\n```\r\n\r\nRegarding the dataset_infos.json, I haven't added the infos for en.noclean. I will add them once I have them.\r\n\r\nAlso we can work on the dataset card at https://huggingface.co/datasets/c4\r\nFor now I just added a link to https://huggingface.co/datasets/allenai/c4 as well as a few sections",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 934632378,
    "title": "Add streaming in load a dataset docs",
    "dateCreated": "2021-07-01T09:32:53Z",
    "dateModified": "2021-07-01T09:32:53Z",
    "description": "Mention dataset streaming on the \"loading a dataset\" page of the documentation",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 934584745,
    "title": "Finding right block-size with JSON loading difficult for user",
    "dateCreated": "2021-07-01T08:48:35Z",
    "dateModified": "2021-07-01T08:48:35Z",
    "description": "As reported by @thomwolf, while loading a JSON Lines file with \"json\" loading script, he gets\r\n> json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 383)\r\n",
    "status": "open",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 934573767,
    "title": "Support Zstandard compressed files",
    "dateCreated": "2021-07-01T08:37:04Z",
    "dateModified": "2021-07-01T08:37:04Z",
    "description": "Add support for Zstandard compressed files: https://facebook.github.io/zstd/",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 933791018,
    "title": "Filter expected warning log from transformers",
    "dateCreated": "2021-06-30T14:48:19Z",
    "dateModified": "2021-06-30T14:48:19Z",
    "description": "Close #2569.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 933402521,
    "title": "Minor fix docs format for bertscore",
    "dateCreated": "2021-06-30T07:42:12Z",
    "dateModified": "2021-06-30T07:42:12Z",
    "description": "Minor fix docs format for bertscore:\r\n- link to README\r\n- format of KWARGS_DESCRIPTION",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 933015797,
    "title": "Weights of model checkpoint not initialized for RobertaModel for Bertscore",
    "dateCreated": "2021-06-29T18:55:23Z",
    "dateModified": "2021-06-29T18:55:23Z",
    "description": "When applying bertscore out of the box, \r\n\r\n```Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']```\r\n\r\nFollowing the typical usage from https://huggingface.co/docs/datasets/loading_metrics.html\r\n\r\n```\r\nfrom datasets import load_metric\r\nmetric = load_metric('bertscore')\r\n\r\n# Example of typical usage\r\nfor batch in dataset:\r\n    inputs, references = batch\r\n    predictions = model(inputs)\r\n    metric.add_batch(predictions=predictions, references=references)\r\nscore = metric.compute(lang=\"en\")\r\n#score = metric.compute(model_type=\"roberta-large\") # gives the same error\r\n```\r\n\r\nI am concerned about this because my usage shouldn't require any further fine-tuning and most people would expect to use BertScore out of the box? I realised the huggingface code is a wrapper around https://github.com/Tiiiger/bert_score, but I think this repo is anyway relying on the model code and weights from huggingface repo.... \r\n\r\n## Environment info\r\n- `datasets` version: 1.7.0\r\n- Platform: Linux-5.4.0-1041-aws-x86_64-with-glibc2.27\r\n- Python version:  3.9.5\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 932934795,
    "title": "Add interleave_datasets for map-style datasets",
    "dateCreated": "2021-06-29T17:19:24Z",
    "dateModified": "2021-06-29T17:19:24Z",
    "description": "### Add interleave_datasets for map-style datasets\r\n\r\nAdd support for map-style datasets (i.e. `Dataset` objects) in `interleave_datasets`.\r\nIt was only supporting iterable datasets (i.e. `IterableDataset` objects).\r\n\r\n### Implementation details\r\n\r\nIt works by concatenating the datasets and then re-order the indices to make the new dataset.\r\n\r\n### TODO\r\n- [x] tests\r\n- [x] docs\r\n\r\nClose #2563 ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 932933536,
    "title": "Add ASR task and new languages to resources",
    "dateCreated": "2021-06-29T17:18:01Z",
    "dateModified": "2021-06-29T17:18:01Z",
    "description": "This PR adds a new `automatic-speech-recognition` task to the list of supported tasks in `tasks.json` and also includes a few new languages missing from `common_voice`.\r\n\r\nNote: I used the [Papers with Code list](https://www.paperswithcode.com/area/speech/speech-recognition) as inspiration for the ASR subtasks",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 932804725,
    "title": "fix Dataset.map when num_procs > num rows",
    "dateCreated": "2021-06-29T15:07:07Z",
    "dateModified": "2021-06-29T15:07:07Z",
    "description": "closes #2470\r\n\r\n## Testing notes\r\nTo run updated tests:\r\n```sh\r\npytest tests/test_arrow_dataset.py -k \"BaseDatasetTest and test_map_multiprocessing\" -s\r\n```\r\nWith Python code (to view warning):\r\n```python\r\nfrom datasets import Dataset\r\n\r\n\r\ndataset = Dataset.from_dict({\"x\": [\"sample\"]})\r\nprint(len(dataset))\r\ndataset.map(lambda x: x, num_proc=10)\r\n```",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 932445439,
    "title": "Inject templates for ASR datasets",
    "dateCreated": "2021-06-29T10:02:01Z",
    "dateModified": "2021-06-29T10:02:01Z",
    "description": "This PR adds ASR templates for 5 of the most common speech datasets on the Hub, where \"common\" is defined by the number of models trained on them.\r\n\r\nI also fixed a bunch of the tags in the READMEs \ud83d\ude0e ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 932389639,
    "title": "concatenate_datasets for iterable datasets",
    "dateCreated": "2021-06-29T08:59:41Z",
    "dateModified": "2021-06-29T08:59:41Z",
    "description": "Currently `concatenate_datasets` only works for map-style `Dataset`.\r\n\r\nIt would be nice to have it work for `IterableDataset` objects as well.\r\n\r\nIt would simply chain the iterables of the iterable datasets.",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 932387639,
    "title": "interleave_datasets for map-style datasets",
    "dateCreated": "2021-06-29T08:57:24Z",
    "dateModified": "2021-06-29T08:57:24Z",
    "description": "Currently the `interleave_datasets` functions only works for `IterableDataset`.\r\nLet's make it work for map-style `Dataset` objects as well.\r\n\r\nIt would work the same way: either alternate between the datasets in order or randomly given probabilities specified by the user.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 932333436,
    "title": "Minor fix in loading metrics docs",
    "dateCreated": "2021-06-29T07:55:11Z",
    "dateModified": "2021-06-29T07:55:11Z",
    "description": "Make some minor fixes in \"Loading metrics\" docs.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 932321725,
    "title": "Existing cache for local dataset builder file updates is ignored with `ignore_verifications=True`",
    "dateCreated": "2021-06-29T07:43:03Z",
    "dateModified": "2021-06-29T07:43:03Z",
    "description": "## Describe the bug\r\nIf i have local file defining a dataset builder class and I load it using `load_dataset` functionality, the existing cache is ignored whenever the file is update even with `ignore_verifications=True`. This slows down debugging and cache generator for very large datasets.\r\n\r\n## Steps to reproduce the bug\r\n\r\n- Create a local dataset builder class\r\n- load the local builder class file using `load_dataset` and let the cache build\r\n- update the file's content\r\n- The cache should rebuilt.\r\n\r\n## Expected results\r\n\r\nWith `ignore_verifications=True`, `load_dataset` should pick up existing cache.\r\n\r\n## Actual results\r\n\r\nCreates new cache.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-5.4.0-52-generic-x86_64-with-debian-bullseye-sid\r\n- Python version: 3.7.7\r\n- PyArrow version: 3.0.0\r\n",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 932143634,
    "title": "fix Dataset.map when num_procs > num rows",
    "dateCreated": "2021-06-29T02:24:11Z",
    "dateModified": "2021-06-29T02:24:11Z",
    "description": "closes #2470\r\n\r\n## Testing notes\r\nTo run updated tests:\r\n```sh\r\npytest tests/test_arrow_dataset.py -k \"BaseDatasetTest and test_map_multiprocessing\" -s\r\n```\r\nWith Python code (to view warning):\r\n```python\r\nfrom datasets import Dataset\r\n\r\n\r\ndataset = Dataset.from_dict({\"x\": [\"sample\"]})\r\nprint(len(dataset))\r\ndataset.map(lambda x: x, num_proc=10)\r\n```",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 931849724,
    "title": "Memory usage consistently increases when processing a dataset with `.map`",
    "dateCreated": "2021-06-28T18:31:58Z",
    "dateModified": "2021-06-28T18:31:58Z",
    "description": "## Describe the bug\r\n\r\nI have a HF dataset with image paths stored in it and I am trying to load those image paths using `.map` with `num_proc=80`. I am noticing that the memory usage consistently keeps on increasing with time. I tried using `DEFAULT_WRITER_BATCH_SIZE=10` in the builder to decrease arrow writer's batch size but that doesn't seem to help.\r\n\r\n## Steps to reproduce the bug\r\n\r\nProviding code as it is would be hard. I can provide a MVP if that helps.\r\n\r\n## Expected results\r\n\r\nMemory usage should become consistent after some time following the launch of processing.\r\n\r\n## Actual results\r\n\r\nMemory usage keeps on increasing.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-5.4.0-52-generic-x86_64-with-debian-bullseye-sid\r\n- Python version: 3.7.7\r\n- PyArrow version: 3.0.0",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 931736647,
    "title": "Update: WebNLG - update checksums",
    "dateCreated": "2021-06-28T16:16:37Z",
    "dateModified": "2021-06-28T16:16:37Z",
    "description": "The master branch changed so I computed the new checksums.\r\n\r\nI also pinned a specific revision so that it doesn't happen again in the future.\r\n\r\nFix https://github.com/huggingface/datasets/issues/2553",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 931633823,
    "title": "Fix `fever` keys",
    "dateCreated": "2021-06-28T14:27:02Z",
    "dateModified": "2021-06-28T14:27:02Z",
    "description": "The keys has duplicates since they were reset to 0 after each file.\r\n\r\nI fixed it by taking into account the file index as well.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 931595872,
    "title": "Better DuplicateKeysError error to help the user debug the issue",
    "dateCreated": "2021-06-28T13:50:57Z",
    "dateModified": "2021-06-28T13:50:57Z",
    "description": "As mentioned in https://github.com/huggingface/datasets/issues/2552 it would be nice to improve the error message when a dataset fails to build because there are duplicate example keys.\r\n\r\nThe current one is\r\n```python\r\ndatasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 48\r\nKeys should be unique and deterministic in nature\r\n```\r\n\r\nand we could have something that guides the user to debugging the issue:\r\n```python\r\nDuplicateKeysError: both 42th and 1337th examples have the same keys `48`.\r\nPlease fix the dataset script at <path/to/the/dataset/script>\r\n```",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 931585485,
    "title": "Fix code_search_net keys",
    "dateCreated": "2021-06-28T13:40:23Z",
    "dateModified": "2021-06-28T13:40:23Z",
    "description": "There were duplicate keys in the `code_search_net` dataset, as reported in https://github.com/huggingface/datasets/issues/2552\r\n\r\nI fixed the keys (it was an addition of the file and row indices, which was causing collisions)\r\n\r\nFix #2552.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 931453855,
    "title": "Multilabel metrics not supported",
    "dateCreated": "2021-06-28T11:09:46Z",
    "dateModified": "2021-06-28T11:09:46Z",
    "description": "When I try to use a metric like F1 macro I get the following error:\r\n\r\n```\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'list'\r\n```\r\nThere is an explicit casting here:\r\n\r\nhttps://github.com/huggingface/datasets/blob/fc79f61cbbcfa0e8c68b28c0a8257f17e768a075/src/datasets/features.py#L274\r\n\r\nAnd looks like this is because here\r\n\r\nhttps://github.com/huggingface/datasets/blob/fc79f61cbbcfa0e8c68b28c0a8257f17e768a075/metrics/f1/f1.py#L88\r\n\r\nthe features can only be integers, so we cannot use that F1 for multilabel. Instead, if I create the following F1 (ints replaced with sequence of ints), it will work:\r\n\r\n```python\r\nclass F1(datasets.Metric):\r\n    def _info(self):\r\n        return datasets.MetricInfo(\r\n            description=_DESCRIPTION,\r\n            citation=_CITATION,\r\n            inputs_description=_KWARGS_DESCRIPTION,\r\n            features=datasets.Features(\r\n                {\r\n                    \"predictions\": datasets.Sequence(datasets.Value(\"int32\")),\r\n                    \"references\": datasets.Sequence(datasets.Value(\"int32\")),\r\n                }\r\n            ),\r\n            reference_urls=[\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\"],\r\n        )\r\n\r\n    def _compute(self, predictions, references, labels=None, pos_label=1, average=\"binary\", sample_weight=None):\r\n        return {\r\n            \"f1\": f1_score(\r\n                references,\r\n                predictions,\r\n                labels=labels,\r\n                pos_label=pos_label,\r\n                average=average,\r\n                sample_weight=sample_weight,\r\n            ),\r\n        }\r\n```\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 931365926,
    "title": "load_dataset(\"web_nlg\") NonMatchingChecksumError",
    "dateCreated": "2021-06-28T09:26:46Z",
    "dateModified": "2021-06-28T09:26:46Z",
    "description": "Hi! It seems the WebNLG dataset gives a NonMatchingChecksumError.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('web_nlg', name=\"release_v3.0_en\", split=\"dev\")\r\n```\r\n\r\nGives\r\n\r\n```\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://gitlab.com/shimorina/webnlg-dataset/-/archive/master/webnlg-dataset-master.zip']\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.8.0\r\n- Platform: macOS-11.3.1-x86_64-i386-64bit\r\n- Python version: 3.9.4\r\n- PyArrow version: 3.0.0\r\n\r\nAlso tested on Linux, with python 3.6.8",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 931354687,
    "title": "Keys should be unique error on code_search_net",
    "dateCreated": "2021-06-28T09:15:20Z",
    "dateModified": "2021-06-28T09:15:20Z",
    "description": "## Describe the bug\r\nLoading `code_search_net` seems not possible at the moment.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n>>> load_dataset('code_search_net')\r\nDownloading: 8.50kB [00:00, 3.09MB/s]                                                                                                                                           \r\nDownloading: 19.1kB [00:00, 10.1MB/s]                                                                                                                                           \r\nNo config specified, defaulting to: code_search_net/all\r\nDownloading and preparing dataset code_search_net/all (download: 4.77 GiB, generated: 5.99 GiB, post-processed: Unknown size, total: 10.76 GiB) to /Users/thomwolf/.cache/huggingface/datasets/code_search_net/all/1.0.0/b3e8278faf5d67da1d06981efbeac3b76a2900693bd2239bbca7a4a3b0d6e52a...\r\nTraceback (most recent call last):         \r\n  File \"/Users/thomwolf/Documents/GitHub/datasets/src/datasets/builder.py\", line 1067, in _prepare_split\r\n    writer.write(example, key)\r\n  File \"/Users/thomwolf/Documents/GitHub/datasets/src/datasets/arrow_writer.py\", line 343, in write\r\n    self.check_duplicate_keys()\r\n  File \"/Users/thomwolf/Documents/GitHub/datasets/src/datasets/arrow_writer.py\", line 354, in check_duplicate_keys\r\n    raise DuplicatedKeysError(key)\r\ndatasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 48\r\nKeys should be unique and deterministic in nature\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.8.1.dev0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyArrow version: 2.0.0\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 930967978,
    "title": "Fix FileSystems documentation",
    "dateCreated": "2021-06-27T16:18:42Z",
    "dateModified": "2021-06-27T16:18:42Z",
    "description": "### What this fixes:\r\nThis PR resolves several issues I discovered in the documentation on the `datasets.filesystems` module ([this page](https://huggingface.co/docs/datasets/filesystems.html)).\r\n\r\n### What were the issues?\r\nWhen I originally tried implementing the code examples I faced several bugs attributed to:\r\n\r\n- out of date [botocore](https://github.com/boto/botocore) call signatures\r\n- capitalization errors in the `S3FileSystem` class name (written as `S3Filesystem` in one place)\r\n- call signature errors for the `S3FileSystem` class constructor (uses parameter `sessions` instead of `session` in some places) (see [`s3fs`](https://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3FileSystem) for where this constructor signature is defined)\r\n\r\n### Testing/reviewing notes\r\nInstructions for generating the documentation locally: [here](https://github.com/huggingface/datasets/tree/master/docs#generating-the-documentation).",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 930951287,
    "title": "Allow for incremental cumulative metric updates in a distributed setup",
    "dateCreated": "2021-06-27T15:00:58Z",
    "dateModified": "2021-06-27T15:00:58Z",
    "description": "Currently, using a metric allows for one of the following:\r\n- Per example/batch metrics\r\n- Cumulative metrics over the whole data\r\n\r\nWhat I'd like is to have an efficient way to get cumulative metrics over the examples/batches added so far, in order to display it as part of the progress bar during training/evaluation.\r\n\r\nSince most metrics are just an average of per-example metrics (which aren't?), an efficient calculation can be done as follows:\r\n`((score_cumulative * n_cumulative) + (score_new * n_new)) / (n_cumulative+ n_new)`\r\nwhere `n` and `score`  refer to number of examples and metric score, `cumulative` refers to the cumulative metric and `new` refers to the addition of new examples.\r\n\r\nIf you don't want to add this capability in the library, a simple solution exists so users can do it themselves:\r\nIt is easy to implement for a single process setup, but in a distributed one there is no way to get the correct `n_new`.\r\nThe solution for this is to return the number of examples that was used to compute the metrics in `.compute()` by adding the following line here:\r\nhttps://github.com/huggingface/datasets/blob/5a3221785311d0ce86c2785b765e86bd6997d516/src/datasets/metric.py#L402-L403\r\n```\r\noutput[\"number_of_examples\"] = len(predictions)\r\n```\r\nand also remove the log message here so it won't spam:\r\nhttps://github.com/huggingface/datasets/blob/3db67f5ff6cbf807b129d2b4d1107af27623b608/src/datasets/metric.py#L411\r\n\r\nIf this change is ok with you, I'll open a pull request.\r\n",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 929819093,
    "title": "Handling unlabeled datasets",
    "dateCreated": "2021-06-25T04:32:23Z",
    "dateModified": "2021-06-25T04:32:23Z",
    "description": "Hi!\r\n\r\nIs there a way for datasets to produce unlabeled instances (e.g., the `ClassLabel` can be nullable).\r\n\r\nFor example, I want to use the MNLI dataset reader ( https://github.com/huggingface/datasets/blob/master/datasets/multi_nli/multi_nli.py ) on a file that doesn't have the `gold_label` field. I tried setting `\"label\": data.get(\"gold_label\")`, but got the following error:\r\n\r\n```\r\n  File \"/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/load.py\", line 748, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/builder.py\", line 575, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/builder.py\", line 652, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/builder.py\", line 989, in _prepare_split\r\n    example = self.info.features.encode_example(record)\r\n  File \"/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/features.py\", line 953, in encode_example\r\n    return encode_nested_example(self, example)\r\n  File \"/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/features.py\", line 848, in encode_nested_example\r\n    k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)\r\n  File \"/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/features.py\", line 848, in <dictcomp>\r\n    k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)\r\n  File \"/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/features.py\", line 875, in encode_nested_example\r\n    return schema.encode_example(obj)\r\n  File \"/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/features.py\", line 653, in encode_example\r\n    if not -1 <= example_data < self.num_classes:\r\nTypeError: '<=' not supported between instances of 'int' and 'NoneType'\r\n```\r\n\r\nWhat's the proper way to handle reading unlabeled datasets, especially for downstream usage with Transformers?",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 929232831,
    "title": "Field order issue in loading json",
    "dateCreated": "2021-06-24T13:29:53Z",
    "dateModified": "2021-06-24T13:29:53Z",
    "description": "## Describe the bug\r\nThe `load_dataset` function expects columns in alphabetical order when loading json files.\r\n\r\nSimilar bug was previously reported for csv in #623 and fixed in #684.\r\n## Steps to reproduce the bug\r\n\r\nFor a json file `j.json`,\r\n```\r\n{\"c\":321, \"a\": 1, \"b\": 2}\r\n```\r\nRunning the following,\r\n```\r\nf= datasets.Features({'a': Value('int32'), 'b': Value('int32'), 'c': Value('int32')})\r\njson_data = datasets.load_dataset('json', data_files='j.json', features=f)\r\n```\r\n\r\n\r\n## Expected results\r\nA successful load.\r\n## Actual results\r\n```\r\nFile \"pyarrow/table.pxi\", line 1409, in pyarrow.lib.Table.cast\r\nValueError: Target schema's field names are not matching the table's field names: ['c', 'a', 'b'], ['a', 'b', 'c']\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-3.10.0-957.1.3.el7.x86_64-x86_64-with-glibc2.10\r\n- Python version: 3.8.8\r\n- PyArrow version: 3.0.0\r\n\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 929192329,
    "title": "Dataset load_from_disk is too slow",
    "dateCreated": "2021-06-24T12:45:44Z",
    "dateModified": "2021-06-24T12:45:44Z",
    "description": "@lhoestq \r\n## Describe the bug\r\nIt's not normal that I have to wait 7-8 hours for a dataset to be loaded from disk, as there are no preprocessing steps, it's only loading it with load_from_disk. I have 96 cpus, however only 1 is used for this, which is inefficient. Moreover, its usage is at 1%... This is happening in the context of a language model training, therefore I'm wasting 100$ each time I have to load the dataset from disk again (because the spot instance was stopped by aws and I need to relaunch it for example). \r\n\r\n## Steps to reproduce the bug\r\nJust get the oscar in  spanish (around 150GGB) and try to first save in disk and then load the processed dataset. It's not dependent on the task you're doing, it just depends on the size of the text dataset.\r\n\r\n## Expected results\r\nI expect the dataset to be loaded in a normal time, by using the whole machine for loading it, I mean if you store the dataset in multiple files (.arrow) and then load it from multiple files, you can use multiprocessing for that and therefore don't waste so much time. \r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.0\r\n- Platform: Ubuntu 18\r\n- Python version: 3.8\r\n\r\n\r\nI've seen you're planning to include a streaming mode for load_dataset, but that only saves the downloading and processing time, that's not being a problem for me, you cannot save the pure loading from disk time, therefore that's not a solution for my use case or for anyone who wants to use your library for training a language model. ",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 929091689,
    "title": "Add license to the Cambridge English Write & Improve + LOCNESS dataset card",
    "dateCreated": "2021-06-24T10:39:29Z",
    "dateModified": "2021-06-24T10:39:29Z",
    "description": "As noticed in https://github.com/huggingface/datasets/pull/2539, the licensing information was missing for this dataset.\r\n\r\nI added it and I also filled a few other empty sections.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 929016580,
    "title": "Fix DuplicatedKeysError in drop dataset",
    "dateCreated": "2021-06-24T09:10:39Z",
    "dateModified": "2021-06-24T09:10:39Z",
    "description": "Close #2542.\r\n\r\ncc: @VictorSanh.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 928900827,
    "title": "Fix logging levels",
    "dateCreated": "2021-06-24T06:41:36Z",
    "dateModified": "2021-06-24T06:41:36Z",
    "description": "Sometimes default `datasets` logging can be too verbose. One approach could be reducing some logging levels, from info to debug, or from warning to info.\r\n\r\nClose #2543.\r\n\r\ncc: @stas00 ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 928571915,
    "title": "switching some low-level log.info's to log.debug?",
    "dateCreated": "2021-06-23T19:26:55Z",
    "dateModified": "2021-06-23T19:26:55Z",
    "description": "In https://github.com/huggingface/transformers/pull/12276 we are now changing the examples to have `datasets` on the same log level as `transformers`, so that one setting can do a consistent logging across all involved components.\r\n\r\nThe trouble is that now we get a ton of these:\r\n\r\n```\r\n06/23/2021 12:15:31 - INFO - datasets.utils.filelock -   Lock 139627640431136 acquired on /home/stas/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow.lock\r\n06/23/2021 12:15:31 - INFO - datasets.arrow_writer -   Done writing 50 examples in 12280 bytes /home/stas/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow.\r\n06/23/2021 12:15:31 - INFO - datasets.arrow_dataset -   Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\r\n06/23/2021 12:15:31 - INFO - datasets.utils.filelock -   Lock 139627640431136 released on /home/stas/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow.lock\r\n```\r\n\r\nMay I suggest that these can be `log.debug` as it's no informative to the user.\r\n\r\nMore examples: these are not informative - too much information:\r\n```\r\n06/23/2021 12:14:26 - INFO - datasets.load -   Checking /home/stas/.cache/huggingface/datasets/downloads/459933f1fe47711fad2f6ff8110014ff189120b45ad159ef5b8e90ea43a174fa.e23e7d1259a8c6274a82a42a8936dd1b87225302c6dc9b7261beb3bc2daac640.py for additional imports.\r\n06/23/2021 12:14:27 - INFO - datasets.builder -   Constructing Dataset for split train, validation, test, from /home/stas/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a\r\n\r\n```\r\n\r\nWhile these are:\r\n```\r\n06/23/2021 12:14:27 - INFO - datasets.info -   Loading Dataset Infos from /home/stas/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a\r\n06/23/2021 12:14:27 - WARNING - datasets.builder -   Reusing dataset wmt16 (/home/stas/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)\r\n```\r\n\r\nI also realize that `transformers` examples don't have do use `info` for `datasets` to let the default `warning` keep logging to less noisy.\r\n\r\nBut I think currently the log levels are slightly misused and skewed by 1 level. Many `warnings` will better be `info`s and most `info`s be `debug`.\r\n\r\ne.g.:\r\n\r\n```\r\n06/23/2021 12:14:27 - WARNING - datasets.builder -   Reusing dataset wmt16 (/home/stas/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)\r\n```\r\n\r\nwhy is this a warning? it is informing me that the cache is used, there is nothing to be worried about. I'd have it as `info`.\r\n\r\nWarnings are typically something that's bordering error or the first thing to check when things don't work as expected.\r\n\r\ninfrequent info is there to inform of the different stages or important events.\r\n\r\nEverything else is debug.\r\n\r\nAt least the way I understand things. \r\n",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 928540382,
    "title": "`datasets.keyhash.DuplicatedKeysError` for `drop` and `adversarial_qa/adversarialQA`",
    "dateCreated": "2021-06-23T18:41:16Z",
    "dateModified": "2021-06-23T18:41:16Z",
    "description": "## Describe the bug\r\nFailure to generate the datasets (`drop` and subset `adversarialQA` from `adversarial_qa`) because of duplicate keys.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"drop\")\r\nload_dataset(\"adversarial_qa\", \"adversarialQA\")\r\n```\r\n\r\n## Expected results\r\nThe examples keys should be unique.\r\n\r\n## Actual results\r\n```bash\r\n>>> load_dataset(\"drop\")\r\nUsing custom data configuration default\r\nDownloading and preparing dataset drop/default (download: 7.92 MiB, generated: 111.88 MiB, post-processed: Unknown size, total: 119.80 MiB) to /home/hf/.cache/huggingface/datasets/drop/default/0.1.0/7a94f1e2bb26c4b5c75f89857c06982967d7416e5af935a9374b9bccf5068026...\r\nTraceback (most recent call last):         \r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/load.py\", line 751, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py\", line 575, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py\", line 652, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py\", line 992, in _prepare_split\r\n    num_examples, num_bytes = writer.finalize()\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/arrow_writer.py\", line 409, in finalize\r\n    self.check_duplicate_keys()\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/arrow_writer.py\", line 349, in check_duplicate_keys\r\n    raise DuplicatedKeysError(key)\r\ndatasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 28553293-d719-441b-8f00-ce3dc6df5398\r\nKeys should be unique and deterministic in nature\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.7.0\r\n- Platform: Linux-5.4.0-1044-gcp-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 928529078,
    "title": "update discofuse link cc @ekQ",
    "dateCreated": "2021-06-23T18:24:58Z",
    "dateModified": "2021-06-23T18:24:58Z",
    "description": "Updating the discofuse link: https://github.com/google-research-datasets/discofuse/commit/fd4b120cb3dd19a417e7f3b5432010b574b5eeee",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 928433892,
    "title": "Remove task templates if required features are removed during `Dataset.map`",
    "dateCreated": "2021-06-23T16:20:25Z",
    "dateModified": "2021-06-23T16:20:25Z",
    "description": "This PR fixes a bug reported by @craffel where removing a dataset's columns during `Dataset.map` triggered a `KeyError` because the `TextClassification` template tried to access the removed columns during `DatasetInfo.__post_init__`:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n# `yelp_polarity` comes with a `TextClassification` template\r\nds = load_dataset(\"yelp_polarity\", split=\"test\")\r\nds\r\n# Dataset({\r\n#     features: ['text', 'label'],\r\n#     num_rows: 38000\r\n# })\r\n\r\n# Triggers KeyError: 'label' - oh noes!\r\nds.map(lambda x: {\"inputs\": 0}, remove_columns=ds.column_names)\r\n```\r\n\r\nI wrote a unit test to make sure I could reproduce the error and then patched a fix.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 927952429,
    "title": "remove wi_locness dataset due to licensing issues",
    "dateCreated": "2021-06-23T07:35:32Z",
    "dateModified": "2021-06-23T07:35:32Z",
    "description": "It was brought to my attention that this dataset's license is not only missing, but also prohibits redistribution. I contacted the original author to apologize for this oversight and asked if we could still use it, but unfortunately we can't and the author kindly asked to take down this dataset.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 927940691,
    "title": "Loading partial dataset when debugging",
    "dateCreated": "2021-06-23T07:19:52Z",
    "dateModified": "2021-06-23T07:19:52Z",
    "description": "I am using PyTorch Lightning along with datasets (thanks for so many datasets already prepared and the great splits). \r\n\r\nEvery time I execute load_dataset  for the imdb dataset it takes some time even if I specify a split involving very few samples. I guess this due to hashing as per the other issues.\r\n\r\nIs there a way to only load part of the dataset on load_dataset? This would really speed up my workflow.\r\nSomething like a debug mode would really help. Thanks!",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 927472659,
    "title": "Add Parquet loader + from_parquet and to_parquet",
    "dateCreated": "2021-06-22T17:28:23Z",
    "dateModified": "2021-06-22T17:28:23Z",
    "description": "Continuation of #2247 \r\n\r\nI added a \"parquet\" dataset builder, as well as the methods `Dataset.from_parquet` and `Dataset.to_parquet`.\r\nAs usual, the data are converted to arrow in a batched way to avoid loading everything in memory.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 927338639,
    "title": "Use `Audio` features for `AutomaticSpeechRecognition` task template",
    "dateCreated": "2021-06-22T15:07:21Z",
    "dateModified": "2021-06-22T15:07:21Z",
    "description": "In #2533 we added a task template for speech recognition that relies on the file paths to the audio files. As pointed out by @SBrandeis this is brittle as it doesn't port easily across different OS'. \r\n\r\nThe solution is to use dedicated `Audio` features when casting the dataset. These features are not yet available in `datasets`, but should be included in the `AutomaticSpeechRecognition` template once they are.",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 927334349,
    "title": "Improve Features docs",
    "dateCreated": "2021-06-22T15:03:27Z",
    "dateModified": "2021-06-22T15:03:27Z",
    "description": "- Fix rendering and cross-references in Features docs\r\n- Add docstrings to Features methods",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 927201435,
    "title": "Sync with transformers disabling NOTSET",
    "dateCreated": "2021-06-22T12:54:21Z",
    "dateModified": "2021-06-22T12:54:21Z",
    "description": "Close #2528.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 927193264,
    "title": "Add task template for automatic speech recognition",
    "dateCreated": "2021-06-22T12:45:02Z",
    "dateModified": "2021-06-22T12:45:02Z",
    "description": "This PR adds a task template for automatic speech recognition. In this task, the input is a path to an audio file which the model consumes to produce a transcription.\r\n\r\nUsage:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nfrom datasets.tasks import AutomaticSpeechRecognition\r\n\r\nds = load_dataset(\"timit_asr\", split=\"train[:10]\")\r\n# Dataset({\r\n#     features: ['file', 'text', 'phonetic_detail', 'word_detail', 'dialect_region', 'sentence_type', 'speaker_id', 'id'],\r\n#     num_rows: 10\r\n# })\r\n\r\ntask = AutomaticSpeechRecognition(audio_file_column=\"file\", transcription_column=\"text\")\r\nds.prepare_for_task(task)\r\n# Dataset({\r\n#     features: ['audio_file', 'transcription'],\r\n#     num_rows: 10\r\n# })\r\n```",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 927063196,
    "title": "Tokenizer's normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task",
    "dateCreated": "2021-06-22T10:08:18Z",
    "dateModified": "2021-06-22T10:08:18Z",
    "description": "[This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\r\n\r\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don't know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\r\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\r\n\r\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\r\n\r\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str('\u30ff')` return '\u30b3\u30c8'.\r\n\r\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\r\n\r\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don't need to include them in their code. But I don't understand the code of tokenizer well that I think I am not able to do this.\r\n\r\np.s.\r\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\r\n`get_dataset `is just a simple wrapping for `load_dataset`\r\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")`",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 927017924,
    "title": "Fix dev version",
    "dateCreated": "2021-06-22T09:17:10Z",
    "dateModified": "2021-06-22T09:17:10Z",
    "description": "The dev version that ends in `.dev0` should be greater than the current version.\r\nHowever it happens that `1.8.0 > 1.8.0.dev0` for example.\r\nTherefore we need to use `1.8.1.dev0` for example in this case.\r\n\r\nI updated the dev version to use `1.8.1.dev0`, and I also added a comment in the setup.py in the release steps about this.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 927013773,
    "title": "Fixed label parsing in the ProductReviews dataset",
    "dateCreated": "2021-06-22T09:12:45Z",
    "dateModified": "2021-06-22T09:12:45Z",
    "description": "Fixed issue with parsing dataset labels. ",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 926378812,
    "title": "Add summarization template",
    "dateCreated": "2021-06-21T16:08:31Z",
    "dateModified": "2021-06-21T16:08:31Z",
    "description": "This PR adds a task template for text summarization. As far as I can tell, we do not need to distinguish between \"extractive\" or \"abstractive\" summarization - both can be handled with this template.\r\n\r\nUsage:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nfrom datasets.tasks import Summarization\r\n\r\nds = load_dataset(\"xsum\", split=\"train\")\r\n# Dataset({\r\n#     features: ['document', 'summary', 'id'],\r\n#     num_rows: 204045\r\n# })\r\n\r\nsummarization = Summarization(text_column=\"document\", summary_column=\"summary\")\r\nds.prepare_for_task(summarization)\r\n# Dataset({\r\n#     features: ['text', 'summary'],\r\n#     num_rows: 204045\r\n# })\r\n```\r\n",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 926314656,
    "title": "Logging cannot be set to NOTSET similar to transformers",
    "dateCreated": "2021-06-21T15:04:54Z",
    "dateModified": "2021-06-21T15:04:54Z",
    "description": "## Describe the bug\r\nIn the transformers library you can set the verbosity level to logging.NOTSET to work around the usage of tqdm and IPywidgets, however in Datasets this is no longer possible. This is because transformers set the verbosity level of tqdm with [this](https://github.com/huggingface/transformers/blob/b53bc55ba9bb10d5ee279eab51a2f0acc5af2a6b/src/transformers/file_utils.py#L1449) \r\n`disable=bool(logging.get_verbosity() == logging.NOTSET)`\r\nand datasets accomplishes this like [so](https://github.com/huggingface/datasets/blob/83554e410e1ab8c6f705cfbb2df7953638ad3ac1/src/datasets/utils/file_utils.py#L493)\r\n`not_verbose = bool(logger.getEffectiveLevel() > WARNING)`\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nimport logging\r\ndatasets.logging.get_verbosity = lambda : logging.NOTSET\r\ndatasets.load_dataset(\"patrickvonplaten/librispeech_asr_dummy\")\r\n```\r\n\r\n## Expected results\r\nThe code should download and load the dataset as normal without displaying progress bars\r\n\r\n## Actual results\r\n```ImportError                               Traceback (most recent call last)\r\n<ipython-input-4-aec65c0509c6> in <module>\r\n----> 1 datasets.load_dataset(\"patrickvonplaten/librispeech_asr_dummy\")\r\n\r\n~/venv/lib/python3.7/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, **config_kwargs)\r\n    713         dataset=True,\r\n    714         return_resolved_file_path=True,\r\n--> 715         use_auth_token=use_auth_token,\r\n    716     )\r\n    717     # Set the base path for downloads as the parent of the script location\r\n\r\n~/venv/lib/python3.7/site-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, dynamic_modules_path, return_resolved_file_path, **download_kwargs)\r\n    350                     file_path = hf_bucket_url(path, filename=name, dataset=False)\r\n    351                 try:\r\n--> 352                     local_path = cached_path(file_path, download_config=download_config)\r\n    353                 except FileNotFoundError:\r\n    354                     raise FileNotFoundError(\r\n\r\n~/venv/lib/python3.7/site-packages/datasets/utils/file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    289             use_etag=download_config.use_etag,\r\n    290             max_retries=download_config.max_retries,\r\n--> 291             use_auth_token=download_config.use_auth_token,\r\n    292         )\r\n    293     elif os.path.exists(url_or_filename):\r\n\r\n~/venv/lib/python3.7/site-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\r\n    668                     headers=headers,\r\n    669                     cookies=cookies,\r\n--> 670                     max_retries=max_retries,\r\n    671                 )\r\n    672 \r\n\r\n~/venv/lib/python3.7/site-packages/datasets/utils/file_utils.py in http_get(url, temp_file, proxies, resume_size, headers, cookies, timeout, max_retries)\r\n    493         initial=resume_size,\r\n    494         desc=\"Downloading\",\r\n--> 495         disable=not_verbose,\r\n    496     )\r\n    497     for chunk in response.iter_content(chunk_size=1024):\r\n\r\n~/venv/lib/python3.7/site-packages/tqdm/notebook.py in __init__(self, *args, **kwargs)\r\n    217         total = self.total * unit_scale if self.total else self.total\r\n    218         self.container = self.status_printer(\r\n--> 219             self.fp, total, self.desc, self.ncols)\r\n    220         self.sp = self.display\r\n    221 \r\n\r\n~/venv/lib/python3.7/site-packages/tqdm/notebook.py in status_printer(_, total, desc, ncols)\r\n     95         if IProgress is None:  # #187 #451 #558 #872\r\n     96             raise ImportError(\r\n---> 97                 \"IProgress not found. Please update jupyter and ipywidgets.\"\r\n     98                 \" See https://ipywidgets.readthedocs.io/en/stable\"\r\n     99                 \"/user_install.html\")\r\n\r\nImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-5.4.95-42.163.amzn2.x86_64-x86_64-with-debian-10.8\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\nI am running this code on Deepnote and which important to this issue **does not** support IPywidgets\r\n\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 926031525,
    "title": "Replace bad `n>1M` size tag",
    "dateCreated": "2021-06-21T09:42:35Z",
    "dateModified": "2021-06-21T09:42:35Z",
    "description": "Some datasets were still using the old `n>1M` tag which has been replaced with tags `1M<n<10M`, etc.\r\nThis resulted in unexpected results when searching for datasets bigger than 1M on the hub, since it was only showing the ones with the tag `n>1M`.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 925929228,
    "title": "Add COCO datasets",
    "dateCreated": "2021-06-21T07:48:32Z",
    "dateModified": "2021-06-21T07:48:32Z",
    "description": "## Adding a Dataset\r\n- **Name:** COCO\r\n- **Description:** COCO is a large-scale object detection, segmentation, and captioning dataset.\r\n- **Paper + website:** https://cocodataset.org/#home\r\n- **Data:** https://cocodataset.org/#download\r\n- **Motivation:** It would be great to have COCO available in HuggingFace datasets, as we are moving beyond just text. COCO includes multi-modalities (images + text), as well as a huge amount of images annotated with objects, segmentation masks, keypoints etc., on which models like DETR (which I recently added to HuggingFace Transformers) are trained. Currently, one needs to download everything from the website and place it in a local folder, but it would be much easier if we can directly access it through the datasets API.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 925896358,
    "title": "Use scikit-learn package rather than sklearn in setup.py",
    "dateCreated": "2021-06-21T07:04:25Z",
    "dateModified": "2021-06-21T07:04:25Z",
    "description": "The sklearn package is an historical thing and should probably not be used by anyone, see https://github.com/scikit-learn/scikit-learn/issues/8215#issuecomment-344679114 for some caveats.\r\n\r\nNote: this affects only TESTS_REQUIRE so I guess only developers not end users.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 925610934,
    "title": "Raise FileNotFoundError in WindowsFileLock",
    "dateCreated": "2021-06-20T14:25:11Z",
    "dateModified": "2021-06-20T14:25:11Z",
    "description": "Closes #2443 ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 925421008,
    "title": "Fr",
    "dateCreated": "2021-06-19T15:56:32Z",
    "dateModified": "2021-06-19T15:56:32Z",
    "description": "__Originally posted by @lewtun in https://github.com/huggingface/datasets/pull/2469__",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 925334379,
    "title": "Documentation Mistakes in Dataset: emotion",
    "dateCreated": "2021-06-19T07:08:57Z",
    "dateModified": "2021-06-19T07:08:57Z",
    "description": "As per documentation,\r\nDataset: emotion\r\nHomepage: https://github.com/dair-ai/emotion_dataset\r\n\r\nDataset: https://github.com/huggingface/datasets/blob/master/datasets/emotion/emotion.py\r\n\r\nPermalink: https://huggingface.co/datasets/viewer/?dataset=emotion\r\n\r\nEmotion is a dataset of English Twitter messages with eight basic emotions: anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. For more detailed information please refer to the paper.\r\n\r\nBut when we view the data, there are only 6 emotions, anger, fear, joy, sadness, surprise, and trust.",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 925030685,
    "title": "Insert text classification template for Emotion dataset",
    "dateCreated": "2021-06-18T15:56:19Z",
    "dateModified": "2021-06-18T15:56:19Z",
    "description": "This PR includes a template and updated `dataset_infos.json` for the `emotion` dataset.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 925015004,
    "title": "Datasets with tricky task templates",
    "dateCreated": "2021-06-18T15:33:57Z",
    "dateModified": "2021-06-18T15:33:57Z",
    "description": "I'm collecting a list of datasets here that don't follow the \"standard\" taxonomy and require further investigation to implement task templates for.\r\n\r\n## Text classification\r\n\r\n* [hatexplain](https://huggingface.co/datasets/hatexplain): ostensibly a form of text classification, but not in the standard `(text, target)` format and each sample appears to be tokenized.\r\n* [muchocine](https://huggingface.co/datasets/muchocine): contains two candidate text columns (long-form and summary) which in principle requires two `TextClassification` templates which is not currently supported ",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 924903240,
    "title": "Improve performance of pandas arrow extractor",
    "dateCreated": "2021-06-18T13:24:41Z",
    "dateModified": "2021-06-18T13:24:41Z",
    "description": "While reviewing PR #2505, I noticed that pandas arrow extractor could be refactored to be faster.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 924654100,
    "title": "Add task templates for tydiqa and xquad",
    "dateCreated": "2021-06-18T08:06:34Z",
    "dateModified": "2021-06-18T08:06:34Z",
    "description": "This PR adds question-answering templates to the remaining datasets that are linked to a model on the Hub.\r\n\r\nNotes: \r\n\r\n* I could not test the tydiqa implementation since I don't have enough disk space \ud83d\ude22 . But I am confident the template works :)\r\n* there exist other datasets like `fquad` and `mlqa` which are candidates for question-answering templates, but some work is needed to handle the ordering of nested column described in #2434 \r\n\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 924643345,
    "title": "Fix typo in MatthewsCorrelation class name",
    "dateCreated": "2021-06-18T07:53:06Z",
    "dateModified": "2021-06-18T07:53:06Z",
    "description": "Close #2513.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 924597470,
    "title": "datasets.map pickle issue resulting in invalid mapping function",
    "dateCreated": "2021-06-18T06:47:26Z",
    "dateModified": "2021-06-18T06:47:26Z",
    "description": "I trained my own tokenizer, and I needed to use a python custom class. Because of this I have to detach the custom step before saving and reattach after restore. I did this using the standard pickle `__get_state__` / `__set_state__` mechanism. I think it's correct but it fails when I use it inside a function which is mapped to a dataset, i.e. in the manner of run_mlm.py and other huggingface scripts.\r\n\r\nThe following reproduces the issue - most likely I'm missing something\r\n\r\nA simulated tokeniser which can be pickled\r\n\r\n```\r\nclass CustomTokenizer:\r\n    def __init__(self):\r\n        self.state = \"init\"\r\n\r\n    def __getstate__(self):\r\n        print(\"__getstate__ called\")\r\n        out = self.__dict__.copy()\r\n        self.state = \"pickled\"\r\n        return out\r\n    \r\n    def __setstate__(self, d):\r\n        print(\"__setstate__ called\")\r\n        self.__dict__ = d\r\n        self.state = \"restored\"\r\n\r\ntokenizer = CustomTokenizer()\r\n```\r\n\r\nTest that it actually works - prints \"__getstate__ called\" and \"__setstate__ called\"\r\n```\r\nimport pickle\r\nserialized = pickle.dumps(tokenizer)\r\nrestored = pickle.loads(serialized)\r\nassert restored.state == \"restored\"\r\n```\r\n\r\nSimulate a function that tokenises examples, when dataset.map is called, this function \r\n```\r\ndef tokenize_function(examples):\r\n    assert tokenizer.state == \"restored\" # this shouldn't fail but it does\r\n    output = tokenizer(examples)           # this will fail as tokenizer isn't really a tokenizer\r\n    return output\r\n```\r\n\r\nUse map to simulate tokenization\r\n```\r\nimport glob\r\nfrom datasets import load_dataset\r\n\r\nassert tokenizer.state == \"restored\"\r\ntrain_files = glob.glob('train*.csv')\r\nvalidation_files = glob.glob('validation*.csv')\r\ndatasets = load_dataset(\"csv\", data_files=dict(train=train_files, validation=validation_files))\r\n\r\ntokenized_datasets = datasets.map(\r\n    tokenize_function,\r\n    batched=True,\r\n)\r\n```\r\n\r\nWhat's happening is I can see that __getstate__ is called but not __setstate__, so the state of `tokenize_function` is invalid at the point that it's actually executed. This doesn't matter as far as I can see for the standard tokenizers as they don't use __getstate__ / __setstate__. I'm not sure if there's another hook I'm supposed to implement as well?\r\n\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-22-a2aef4f74aaa> in <module>\r\n      8 tokenized_datasets = datasets.map(\r\n      9     tokenize_function,\r\n---> 10     batched=True,\r\n     11 )\r\n\r\n~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/dataset_dict.py in map(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\r\n    487                     desc=desc,\r\n    488                 )\r\n--> 489                 for k, dataset in self.items()\r\n    490             }\r\n    491         )\r\n\r\n~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/dataset_dict.py in <dictcomp>(.0)\r\n    487                     desc=desc,\r\n    488                 )\r\n--> 489                 for k, dataset in self.items()\r\n    490             }\r\n    491         )\r\n\r\n~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   1633                 fn_kwargs=fn_kwargs,\r\n   1634                 new_fingerprint=new_fingerprint,\r\n-> 1635                 desc=desc,\r\n   1636             )\r\n   1637         else:\r\n\r\n~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    184         }\r\n    185         # apply actual function\r\n--> 186         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    187         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    188         # re-apply format to the output\r\n\r\n~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    395             # Call actual function\r\n    396 \r\n--> 397             out = func(self, *args, **kwargs)\r\n    398 \r\n    399             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/arrow_dataset.py in _map_single(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, desc)\r\n   1961                                 indices,\r\n   1962                                 check_same_num_examples=len(input_dataset.list_indexes()) > 0,\r\n-> 1963                                 offset=offset,\r\n   1964                             )\r\n   1965                         except NumExamplesMismatch:\r\n\r\n~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/arrow_dataset.py in apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples, offset)\r\n   1853                 effective_indices = [i + offset for i in indices] if isinstance(indices, list) else indices + offset\r\n   1854             processed_inputs = (\r\n-> 1855                 function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n   1856             )\r\n   1857             if update_data is None:\r\n\r\n<ipython-input-21-8ee4a8ba5b1b> in tokenize_function(examples)\r\n      1 def tokenize_function(examples):\r\n----> 2     assert tokenizer.state == \"restored\"\r\n      3     tokenizer(examples)\r\n      4     return examples\r\n\r\n\r\n",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 924435447,
    "title": "CRD3 dataset card",
    "dateCreated": "2021-06-18T00:24:07Z",
    "dateModified": "2021-06-18T00:24:07Z",
    "description": "This PR adds additional information to the CRD3 dataset card. ",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 924417172,
    "title": "Can datasets remove duplicated rows?",
    "dateCreated": "2021-06-17T23:35:38Z",
    "dateModified": "2021-06-17T23:35:38Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\ni find myself more and more relying on datasets just to do all the preprocessing. One thing however, for removing duplicated rows, I couldn't find out how and am always converting datasets to pandas to do that..\r\n\r\n\r\n**Describe the solution you'd like**\r\nhave a functionality of \" remove duplicated rows\"\r\n\r\n**Describe alternatives you've considered**\r\nconvert dataset to pandas, remove duplicate, and convert back...\r\n\r\n\r\n**Additional context**\r\nno",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 924174413,
    "title": "Corelation should be Correlation",
    "dateCreated": "2021-06-17T17:28:48Z",
    "dateModified": "2021-06-17T17:28:48Z",
    "description": "https://github.com/huggingface/datasets/blob/0e87e1d053220e8ecddfa679bcd89a4c7bc5af62/metrics/matthews_correlation/matthews_correlation.py#L66",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 924069353,
    "title": "seqeval metric does not work with a recent version of sklearn: classification_report() got an unexpected keyword argument 'output_dict'",
    "dateCreated": "2021-06-17T15:36:02Z",
    "dateModified": "2021-06-17T15:36:02Z",
    "description": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\nseqeval = load_metric(\"seqeval\")\r\nseqeval.compute(predictions=[['A']], references=[['A']])\r\n```\r\n\r\n## Expected results\r\nThe function computes a dict with metrics\r\n\r\n## Actual results\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-39-69a57f5cf06f> in <module>\r\n      1 from datasets import load_dataset, load_metric\r\n      2 seqeval = load_metric(\"seqeval\")\r\n----> 3 seqeval.compute(predictions=[['A']], references=[['A']])\r\n\r\n~/p3/lib/python3.7/site-packages/datasets/metric.py in compute(self, *args, **kwargs)\r\n    396             references = self.data[\"references\"]\r\n    397             with temp_seed(self.seed):\r\n--> 398                 output = self._compute(predictions=predictions, references=references, **kwargs)\r\n    399 \r\n    400             if self.buf_writer is not None:\r\n\r\n~/.cache/huggingface/modules/datasets_modules/metrics/seqeval/81eda1ff004361d4fa48754a446ec69bb7aa9cf4d14c7215f407d1475941c5ff/seqeval.py in _compute(self, predictions, references, suffix)\r\n     95 \r\n     96     def _compute(self, predictions, references, suffix=False):\r\n---> 97         report = classification_report(y_true=references, y_pred=predictions, suffix=suffix, output_dict=True)\r\n     98         report.pop(\"macro avg\")\r\n     99         report.pop(\"weighted avg\")\r\n\r\nTypeError: classification_report() got an unexpected keyword argument 'output_dict'\r\n```\r\n\r\n## Environment info\r\nsklearn=0.24\r\ndatasets=1.1.3\r\n\r\n",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 923762133,
    "title": "Add C4",
    "dateCreated": "2021-06-17T10:31:04Z",
    "dateModified": "2021-06-17T10:31:04Z",
    "description": "## Adding a Dataset\r\n- **Name:** *C4*\r\n- **Description:** *https://github.com/allenai/allennlp/discussions/5056*\r\n- **Paper:** *https://arxiv.org/abs/1910.10683*\r\n- **Data:** *https://huggingface.co/datasets/allenai/c4*\r\n- **Motivation:** *Used a lot for pretraining*\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n\r\nShould fix https://github.com/huggingface/datasets/issues/1710",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 923735485,
    "title": "Add align_labels_with_mapping to DatasetDict",
    "dateCreated": "2021-06-17T10:03:35Z",
    "dateModified": "2021-06-17T10:03:35Z",
    "description": "https://github.com/huggingface/datasets/pull/2457 added the `Dataset.align_labels_with_mapping` method.\r\nIn this PR I also added `DatasetDict.align_labels_with_mapping`",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 922846035,
    "title": "Fix fingerprint when moving cache dir",
    "dateCreated": "2021-06-16T16:45:09Z",
    "dateModified": "2021-06-16T16:45:09Z",
    "description": "The fingerprint of a dataset changes if the cache directory is moved.\r\nI fixed that by setting the fingerprint to be the hash of:\r\n- the relative cache dir (dataset_name/version/config_id)\r\n- the requested split\r\n\r\nClose #2496 \r\n\r\nI had to fix an issue with the filelock filename that was too long (>255). It prevented the tests to run on my machine. I just added `hash_filename_if_too_long` in case this happens, to not get filenames longer than 255.\r\nWe usually have long filenames for filelocks because they are named after the path that is being locked. In case the path is a cache directory that has long directory names, then the filelock filename could en up being very long.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 921863173,
    "title": "Load Image Classification Dataset from Local ",
    "dateCreated": "2021-06-15T22:43:33Z",
    "dateModified": "2021-06-15T22:43:33Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nYes - we would like to load an image classification dataset with datasets without having to write a custom data loader.\r\n\r\n**Describe the solution you'd like**\r\n\r\nGiven a folder structure with images of each class in each folder, the ability to load these folders into a HuggingFace dataset like \"cifar10\".\r\n\r\n**Describe alternatives you've considered**\r\n\r\nImplement ViT training outside of the HuggingFace Trainer and without datasets (we did this but prefer to stay on the main path)\r\n\r\nWrite custom data loader logic\r\n\r\n**Additional context**\r\n\r\nWe're training ViT on custom dataset\r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 921441962,
    "title": "Rearrange JSON field names to match passed features schema field names",
    "dateCreated": "2021-06-15T14:10:02Z",
    "dateModified": "2021-06-15T14:10:02Z",
    "description": "This PR depends on PR #2453 (which must be merged first).\r\n\r\nClose #2366.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 921435598,
    "title": "Add course banner",
    "dateCreated": "2021-06-15T14:03:54Z",
    "dateModified": "2021-06-15T14:03:54Z",
    "description": "This PR adds a course banner similar to the one you can now see in the [Transformers repo](https://github.com/huggingface/transformers) that links to the course. Let me know if placement seems right to you or not, I can move it just below the badges too.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 921234797,
    "title": "Make numpy arrow extractor faster",
    "dateCreated": "2021-06-15T10:11:32Z",
    "dateModified": "2021-06-15T10:11:32Z",
    "description": "I changed the NumpyArrowExtractor to call directly to_numpy and see if it can lead to speed-ups as discussed in https://github.com/huggingface/datasets/issues/2498\r\n\r\nThis could make the numpy/torch/tf/jax formatting faster",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 920636186,
    "title": "SubjQA wrong boolean values in entries",
    "dateCreated": "2021-06-14T17:42:46Z",
    "dateModified": "2021-06-14T17:42:46Z",
    "description": "## Describe the bug\r\nSubjQA seems to have a boolean that's consistently wrong.\r\n\r\nIt defines:\r\n- question_subj_level: The subjectiviy level of the question (on a 1 to 5 scale with 1 being the most subjective).\r\n- is_ques_subjective: A boolean subjectivity label derived from question_subj_level (i.e., scores below 4 are considered as subjective)\r\n\r\nHowever, `is_ques_subjective` seems to have wrong values in the entire dataset.\r\n\r\nFor instance, in the example in the dataset card, we have:\r\n- \"question_subj_level\": 2\r\n- \"is_ques_subjective\": false\r\n\r\nHowever, according to the description, the question should be subjective since the `question_subj_level` is below 4\r\n",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 920623572,
    "title": "JAX integration",
    "dateCreated": "2021-06-14T17:24:23Z",
    "dateModified": "2021-06-14T17:24:23Z",
    "description": "Hi !\r\n\r\nI just added the \"jax\" formatting, as we already have for pytorch, tensorflow, numpy (and also pandas and arrow).\r\nIt does pretty much the same thing as the pytorch formatter except it creates jax.numpy.ndarray objects.\r\n\r\n```python\r\nfrom datasets import Dataset\r\n\r\nd = Dataset.from_dict({\"foo\": [[0., 1., 2.]]})\r\nd = d.with_format(\"jax\")\r\nd[0]\r\n# {'foo': DeviceArray([0., 1., 2.], dtype=float32)}\r\n```\r\n\r\nA few details:\r\n- The default integer precision for jax depends on the jax configuration `jax_enable_x64` (see [here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#double-64bit-precision)), I took that into account. Unless `jax_enable_x64` is specified, it is int32 by default\r\n- AFAIK it's not possible to do a full conversion from arrow data to jax data. We are doing arrow -> numpy -> jax but the numpy -> jax part doesn't do zero copy unfortutanely (see [here](https://github.com/google/jax/issues/4486))\r\n- the env var for disabling JAX is `USE_JAX`. However I noticed that in `transformers` it is `USE_FLAX`. This is not an issue though IMO\r\n\r\nI also updated `convert_to_python_objects` to allow users to pass jax.numpy.ndarray objects to build a dataset.\r\n\r\nSince the `convert_to_python_objects` method became slow because it's the time when pytorch, tf (and now jax) are imported, I fixed it by checking the `sys.modules` to avoid unecessary import of pytorch, tf or jax.\r\n\r\nClose #2495",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 920579634,
    "title": "Add Zenodo metadata file with license",
    "dateCreated": "2021-06-14T16:28:12Z",
    "dateModified": "2021-06-14T16:28:12Z",
    "description": "This Zenodo metadata file fixes the name of the `Datasets` license appearing in the DOI as `\"Apache-2.0\"`, which otherwise by default is `\"other-open\"`.\r\n\r\nClose #2472. ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 920471411,
    "title": "Add load_dataset_builder",
    "dateCreated": "2021-06-14T14:27:45Z",
    "dateModified": "2021-06-14T14:27:45Z",
    "description": "Adds the `load_dataset_builder` function. The good thing is that we can reuse this function to load the dataset info without downloading the dataset itself.\r\n\r\nTODOs:\r\n- [x] Add docstring and entry in the docs\r\n- [x] Add tests\r\n\r\nCloses #2484 \r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 920413021,
    "title": " Python Programming Puzzles",
    "dateCreated": "2021-06-14T13:27:18Z",
    "dateModified": "2021-06-14T13:27:18Z",
    "description": "## Adding a Dataset\r\n- **Name:** Python Programming Puzzles\r\n- **Description:** Programming challenge called programming puzzles, as an objective and comprehensive evaluation of program synthesis\r\n- **Paper:** https://arxiv.org/pdf/2106.05784.pdf\r\n- **Data:** https://github.com/microsoft/PythonProgrammingPuzzles ([Scrolling through the data](https://github.com/microsoft/PythonProgrammingPuzzles/blob/main/problems/README.md))\r\n- **Motivation:** Spans a large range of difficulty, problems, and domains. A useful resource for evaluation as we don't have a clear understanding of the abilities and skills of extremely large LMs.\r\n\r\nNote: it's a growing dataset (contributions are welcome), so we'll need careful versioning for this dataset.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 920411285,
    "title": "Improve torch formatting performance",
    "dateCreated": "2021-06-14T13:25:24Z",
    "dateModified": "2021-06-14T13:25:24Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nIt would be great, if possible, to further improve read performance of raw encoded datasets and their subsequent conversion to torch tensors. \r\n\r\nA bit more background.  I am working on LM pre-training using HF ecosystem. We use encoded HF Wikipedia and BookCorpus datasets. The training machines are similar to DGX-1 workstations. We use HF trainer torch.distributed training approach on a single machine with 8 GPUs.\r\n\r\nThe current performance is about 30% slower than NVidia optimized BERT [examples](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling) baseline. Quite a bit of customized code and training loop tricks were used to achieve the baseline performance. It would be great to achieve the same performance while using nothing more than off the shelf HF ecosystem. Perhaps, in the future, with @stas00 work on deepspeed integration, it could even be exceeded. \r\n\r\n**Describe the solution you'd like**\r\nUsing profiling tools we've observed that appx. 25% of cumulative run time is spent on data loader next call.\r\n![dataloader_next](https://user-images.githubusercontent.com/458335/121895543-59742a00-ccee-11eb-85fb-f07715e3f1f6.png)\r\n\r\nAs you can observe most of the data loader next call is spent in HF datasets torch_formatter.py format_batch call. \r\n\r\nDigging a bit deeper into format_batch we can see the following profiler data:\r\n![torch_formatter](https://user-images.githubusercontent.com/458335/121895944-c7b8ec80-ccee-11eb-95d5-5875c5716c30.png)\r\n\r\nOnce again, a lot of time is spent in pyarrow table conversion to pandas which seems like an intermediary step. Offline @lhoestq told me that this approach was, for some unknown reason, faster than direct to numpy conversion. \r\n\r\n**Describe alternatives you've considered**\r\nI am not familiar with pyarrow and have not yet considered the alternatives to the current approach. \r\n\r\nMost of the online advice around data loader performance improvements revolve around increasing number of workers, using pin memory for copying tensors from host device to gpus but we've already tried these avenues without much performance improvement.  Weights & Biases dashboard for the pre-training task reports CPU utilization of ~ 10%, GPUs are completely saturated (GPU utilization is above 95% on all GPUs), while disk utilization is above 90%. \r\n\r\n",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 920250382,
    "title": "Use default cast for sliced list arrays if pyarrow >= 4",
    "dateCreated": "2021-06-14T10:02:47Z",
    "dateModified": "2021-06-14T10:02:47Z",
    "description": "From pyarrow version 4, it is supported to cast sliced lists.\r\n\r\nThis PR uses default pyarrow cast in Datasets to cast sliced list arrays if pyarrow version is >= 4.\r\n\r\nIn relation with PR #2461 and #2490.\r\n\r\ncc: @lhoestq, @abhi1thakur, @SBrandeis",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 920216314,
    "title": "Dataset fingerprint changes after moving the cache directory, which prevent cache reload when using `map`",
    "dateCreated": "2021-06-14T09:20:26Z",
    "dateModified": "2021-06-14T09:20:26Z",
    "description": "`Dataset.map` uses the dataset fingerprint (a hash) for caching.\r\nHowever the fingerprint seems to change when someone moves the cache directory of the dataset.\r\n\r\nThis is because it uses the default fingerprint generation:\r\n1. the dataset path is used to get the fingerprint\r\n2. the modification times of the arrow file is also used to get the fingerprint\r\n\r\nTo fix that we could set the fingerprint of the dataset to be a hash of (<dataset_name>, <config_name>, <version>, <script_hash>), i.e. a hash of the the cache path relative to the cache directory.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 920170030,
    "title": "JAX formatting",
    "dateCreated": "2021-06-14T08:32:07Z",
    "dateModified": "2021-06-14T08:32:07Z",
    "description": "We already support pytorch, tensorflow, numpy, pandas and arrow dataset formatting. Let's add jax as well",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 920149183,
    "title": "Improve docs on Enhancing performance",
    "dateCreated": "2021-06-14T08:11:48Z",
    "dateModified": "2021-06-14T08:11:48Z",
    "description": "In the [\"Enhancing performance\"](https://huggingface.co/docs/datasets/loading_datasets.html#enhancing-performance) section of docs, add specific use cases:\r\n- How to make datasets the fastest\r\n- How to make datasets take the less RAM\r\n- How to make datasets take the less hard drive mem\r\n\r\ncc: @thomwolf \r\n",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 919833281,
    "title": "add tensorflow-macos support",
    "dateCreated": "2021-06-13T16:20:08Z",
    "dateModified": "2021-06-13T16:20:08Z",
    "description": "ref - https://github.com/huggingface/datasets/issues/2068",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 919718102,
    "title": "Eduge",
    "dateCreated": "2021-06-13T05:10:59Z",
    "dateModified": "2021-06-13T05:10:59Z",
    "description": "Hi, awesome folks behind the huggingface! \r\n\r\nHere is my PR for the text classification dataset in Mongolian.\r\n\r\nPlease do let me know in case you have anything to clarify. \r\n\r\nThanks & Regards,\r\nEnod",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 919714506,
    "title": "add eduge classification dataset",
    "dateCreated": "2021-06-13T04:37:01Z",
    "dateModified": "2021-06-13T04:37:01Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 919571385,
    "title": "Allow latest pyarrow version",
    "dateCreated": "2021-06-12T14:17:34Z",
    "dateModified": "2021-06-12T14:17:34Z",
    "description": "Allow latest pyarrow version, once that version 4.0.1 fixes the segfault bug introduced in version 4.0.0.\r\n\r\nClose #2489.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 919569749,
    "title": "Allow latest pyarrow version once segfault bug is fixed",
    "dateCreated": "2021-06-12T14:09:52Z",
    "dateModified": "2021-06-12T14:09:52Z",
    "description": "As pointed out by @symeneses (see https://github.com/huggingface/datasets/pull/2268#issuecomment-860048613), pyarrow has fixed the segfault bug present in version 4.0.0 (see https://issues.apache.org/jira/browse/ARROW-12568):\r\n- it was fixed on 3 May 2021\r\n- version 4.0.1 was released on 19 May 2021 with the bug fix",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 919500756,
    "title": "Set configurable downloaded datasets path",
    "dateCreated": "2021-06-12T09:09:03Z",
    "dateModified": "2021-06-12T09:09:03Z",
    "description": "Part of #2480.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 919452407,
    "title": "Set configurable extracted datasets path",
    "dateCreated": "2021-06-12T05:47:29Z",
    "dateModified": "2021-06-12T05:47:29Z",
    "description": "Part of #2480.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 919174898,
    "title": "Add Rico Dataset",
    "dateCreated": "2021-06-11T20:17:41Z",
    "dateModified": "2021-06-11T20:17:41Z",
    "description": "Hi there!\r\n\r\nI'm wanting to add the Rico datasets for software engineering type data to y'alls awesome library. However, as I have started coding, I've ran into a few hiccups so I thought it best to open the PR early to get a bit of discussion on how the Rico datasets should be added to the `datasets` lib.\r\n\r\n1) There are 7 different datasets under Rico and so I was wondering, should I make a folder for each or should I put them as different configurations of a single dataset?\r\nYou can see the datasets available for Rico here: http://interactionmining.org/rico\r\n\r\n2) As of right now, I have a semi working version of the first dataset which has pairs of screenshots and hierarchies from android applications. However, these screenshots are very large (1440, 2560, 3) and there are 66,000 of them so I am not able to perform the processing that  the `datasets` lib does after downloading and extracting the dataset since I run out of memory very fast. Is there a way to have `datasets` lib not put everything into memory while it is processing the dataset?\r\n\r\n2.1) If there is not a way, would it be better to just return the path to the screenshots instead of the actual image?\r\n\r\n3) The hierarchies are JSON objects and looking through the documentation of `datasets`, I didn't see any feature that I could use for this type of data. So, for now I just have it being read in as a string, is this okay or should I be doing it differently?\r\n\r\n4) One of the Rico datasets is a bunch of animations (GIFs), is there a `datasets` feature that I can put this type of data into or should I just return the path as a string?\r\n\r\nI appreciate any and all help I can get for this PR, I think the Rico datasets will be an awesome addition to the library :nerd_face: !",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 919099218,
    "title": "Implement layered building",
    "dateCreated": "2021-06-11T18:54:25Z",
    "dateModified": "2021-06-11T18:54:25Z",
    "description": "As discussed with @stas00 and @lhoestq (see also here https://github.com/huggingface/datasets/issues/2481#issuecomment-859712190):\r\n\r\n> My suggestion for this would be to have this enabled by default.\r\n> \r\n> Plus I don't know if there should be a dedicated issue to that is another functionality. But I propose layered building rather than all at once. That is:\r\n>\r\n> 1. uncompress a handful of files via a generator enough to generate one arrow file\r\n> 2. process arrow file 1\r\n> 3. delete all the files that went in and aren't needed anymore.\r\n>\r\n> rinse and repeat.\r\n> \r\n> 1. This way much less disc space will be required - e.g. on JZ we won't be running into inode limitation, also it'd help with the collaborative hub training project\r\n> 2. The user doesn't need to go and manually clean up all the huge files that were left after pre-processing\r\n> 3. It would already include deleting temp files this issue is talking about\r\n> \r\n> I wonder if the new streaming API would be of help, except here the streaming would be into arrow files as the destination, rather than dataloaders.",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 919092635,
    "title": "Implement loading a dataset builder",
    "dateCreated": "2021-06-11T18:47:22Z",
    "dateModified": "2021-06-11T18:47:22Z",
    "description": "As discussed with @stas00 and @lhoestq, this would allow things like:\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\ndataset_name = \"openwebtext\"\r\nbuilder = load_dataset_builder(dataset_name)\r\nprint(builder.cache_dir)\r\n```",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 918871712,
    "title": "Use gc.collect only when needed to avoid slow downs",
    "dateCreated": "2021-06-11T15:09:30Z",
    "dateModified": "2021-06-11T15:09:30Z",
    "description": "In https://github.com/huggingface/datasets/commit/42320a110d9d072703814e1f630a0d90d626a1e6 we added a call to gc.collect to resolve some issues on windows (see https://github.com/huggingface/datasets/pull/2482)\r\n\r\nHowever calling gc.collect too often causes significant slow downs (the CI run time doubled).\r\nSo I just moved the gc.collect call to the exact place where it's actually needed: when post-processing a dataset",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 918846027,
    "title": "Allow to use tqdm>=4.50.0",
    "dateCreated": "2021-06-11T14:49:21Z",
    "dateModified": "2021-06-11T14:49:21Z",
    "description": "We used to have permission errors on windows whith the latest versions of tqdm (see [here](https://app.circleci.com/pipelines/github/huggingface/datasets/6365/workflows/24f7c960-3176-43a5-9652-7830a23a981e/jobs/39232))\r\n\r\nThey were due to open arrow files not properly closed by pyarrow.\r\nSince https://github.com/huggingface/datasets/commit/42320a110d9d072703814e1f630a0d90d626a1e6 gc.collect is called each time we don't need an arrow file to make sure that the files are closed.\r\n\r\nclose https://github.com/huggingface/datasets/issues/2471\r\n\r\ncc @lewtun ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 918680168,
    "title": "Delete extracted files to save disk space",
    "dateCreated": "2021-06-11T12:21:52Z",
    "dateModified": "2021-06-11T12:21:52Z",
    "description": "As discussed with @stas00 and @lhoestq, allowing the deletion of extracted files would save a great amount of disk space to typical user.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 918678578,
    "title": "Set download/extracted paths configurable",
    "dateCreated": "2021-06-11T12:20:24Z",
    "dateModified": "2021-06-11T12:20:24Z",
    "description": "As discussed with @stas00 and @lhoestq, setting these paths configurable may allow to overcome disk space limitation on different partitions/drives.\r\n\r\nTODO:\r\n- [x] Set configurable extracted datasets path: #2487\r\n- [x] Set configurable downloaded datasets path: #2488\r\n- [ ] Set configurable \"incomplete\" datasets path?",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 918672431,
    "title": "\u274c load_datasets \u274c",
    "dateCreated": "2021-06-11T12:14:36Z",
    "dateModified": "2021-06-11T12:14:36Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 918507510,
    "title": "Create release script",
    "dateCreated": "2021-06-11T09:38:02Z",
    "dateModified": "2021-06-11T09:38:02Z",
    "description": "Create a script so that releases can be done automatically (as done in `transformers`).",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 918334431,
    "title": "Fix docs custom stable version",
    "dateCreated": "2021-06-11T07:26:03Z",
    "dateModified": "2021-06-11T07:26:03Z",
    "description": "Currently docs default version is 1.5.0. This PR fixes this and sets the latest version instead.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 917686662,
    "title": "Add TimeDial",
    "dateCreated": "2021-06-10T18:33:07Z",
    "dateModified": "2021-06-10T18:33:07Z",
    "description": "Dataset: https://github.com/google-research-datasets/TimeDial\r\n\r\nTo-Do: Update README.md and add YAML tags",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 917650882,
    "title": "Issue in timit_asr database",
    "dateCreated": "2021-06-10T18:05:29Z",
    "dateModified": "2021-06-10T18:05:29Z",
    "description": "## Describe the bug\r\nI am trying to load the timit_asr dataset however only the first record is shown (duplicated over all the rows).\r\nI am using the next code line\r\ndataset = load_dataset(\u201ctimit_asr\u201d, split=\u201ctest\u201d).shuffle().select(range(10))\r\n\r\nThe above code result with the same sentence duplicated ten times.\r\nIt also happens when I use the dataset viewer at Streamlit .\r\n\r\n## Steps to reproduce the bug\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\u201ctimit_asr\u201d, split=\u201ctest\u201d).shuffle().select(range(10))\r\ndata = dataset.to_pandas()\r\n\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\ntable with different row information\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n\r\n- `datasets` version: 1.4.1 (also occur in the latest version)\r\n- Platform: Linux-4.15.0-143-generic-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyTorch version (GPU?): 1.8.1+cu102 (False)\r\n- Tensorflow version (GPU?): 1.15.3 (False)\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n- `datasets` version:\r\n- Platform:\r\n- Python version:\r\n- PyArrow version:\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 917622055,
    "title": "cache_dir parameter for load_from_disk ?",
    "dateCreated": "2021-06-10T17:39:36Z",
    "dateModified": "2021-06-10T17:39:36Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nWhen using Google Colab big datasets can be an issue, as they won't fit on the VM's disk. Therefore mounting google drive could be a possible solution. Unfortunatly when loading my own dataset by using the _load_from_disk_ function, the data gets cached to the VM's disk:\r\n\r\n`\r\nfrom datasets import load_from_disk\r\n\r\nmyPreprocessedData = load_from_disk(\"/content/gdrive/MyDrive/ASR_data/myPreprocessedData\")\r\n\r\n`\r\nI know that chaching on google drive could slow down learning. But at least it would run.\r\n\r\n**Describe the solution you'd like**\r\nAdd cache_Dir parameter to the load_from_disk function.\r\n\r\n**Describe alternatives you've considered**\r\nIt looks like you could write a custom loading script for the load_dataset function. But this seems to be much too complex for my use case. Is there perhaps a template here that uses the load_from_disk function?\r\n",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 917538629,
    "title": "Add Disfl-QA",
    "dateCreated": "2021-06-10T16:18:00Z",
    "dateModified": "2021-06-10T16:18:00Z",
    "description": "Dataset: https://github.com/google-research-datasets/disfl-qa\r\n\r\nTo-Do: Update README.md and add YAML tags",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 917463821,
    "title": "Fix automatic generation of Zenodo DOI",
    "dateCreated": "2021-06-10T15:15:46Z",
    "dateModified": "2021-06-10T15:15:46Z",
    "description": "After the last release of Datasets (1.8.0), the automatic generation of the Zenodo DOI failed: it appears in yellow as \"Received\", instead of in green as \"Published\".\r\n\r\nI have contacted Zenodo support to fix this issue.\r\n\r\nTODO:\r\n- [x] Check with Zenodo to fix the issue\r\n- [x] Check BibTeX entry is right",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 917067165,
    "title": "Fix PermissionError on Windows when using tqdm >=4.50.0",
    "dateCreated": "2021-06-10T08:31:49Z",
    "dateModified": "2021-06-10T08:31:49Z",
    "description": "See: https://app.circleci.com/pipelines/github/huggingface/datasets/235/workflows/cfb6a39f-68eb-4802-8b17-2cd5e8ea7369/jobs/1111\r\n\r\n```\r\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process\r\n```",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 916724260,
    "title": "Crash when `num_proc` > dataset length for `map()` on a `datasets.Dataset`.",
    "dateCreated": "2021-06-09T22:40:22Z",
    "dateModified": "2021-06-09T22:40:22Z",
    "description": "## Describe the bug\r\nCrash if when using `num_proc` > 1 (I used 16) for `map()` on a `datasets.Dataset`.\r\n\r\nI believe I've had cases where `num_proc` > 1 works before, but now it seems either inconsistent, or depends on my data. I'm not sure whether the issue is on my end, because it's difficult for me to debug! Any tips greatly appreciated, I'm happy to provide more info if it would helps us diagnose.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# this function will be applied with map()\r\ndef tokenize_function(examples):\r\n    return tokenizer(\r\n        examples[\"text\"],\r\n        padding=PaddingStrategy.DO_NOT_PAD,\r\n        truncation=True,\r\n    )\r\n\r\n# data_files is a Dict[str, str] mapping name -> path\r\ndatasets = load_dataset(\"text\", data_files={...})  \r\n\r\n# this is where the error happens if num_proc = 16,\r\n# but is fine if num_proc = 1\r\ntokenized_datasets = datasets.map(\r\n    tokenize_function,\r\n    batched=True,\r\n    num_proc=num_workers,\r\n)\r\n```\r\n\r\n## Expected results\r\nThe `map()` function succeeds with `num_proc` > 1.\r\n\r\n## Actual results\r\n![image](https://user-images.githubusercontent.com/1170062/121404271-a6cc5200-c910-11eb-8e27-5c893bd04042.png)\r\n![image](https://user-images.githubusercontent.com/1170062/121404362-be0b3f80-c910-11eb-9117-658943029aef.png)\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.6.2\r\n- Platform: Linux-5.4.0-73-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.5\r\n- PyTorch version (GPU?): 1.8.1+cu111 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: Yes, but I think N/A for this issue\r\n- Using distributed or parallel set-up in script?: Multi-GPU on one machine, but I think also N/A for this issue\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 916440418,
    "title": "Bump tqdm version",
    "dateCreated": "2021-06-09T17:24:40Z",
    "dateModified": "2021-06-09T17:24:40Z",
    "description": "",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 916427320,
    "title": "Implement ClassLabel encoding in JSON loader",
    "dateCreated": "2021-06-09T17:08:54Z",
    "dateModified": "2021-06-09T17:08:54Z",
    "description": "Close #2365.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 915914098,
    "title": "change udpos features structure",
    "dateCreated": "2021-06-09T08:03:31Z",
    "dateModified": "2021-06-09T08:03:31Z",
    "description": "The structure is change such that each example is a sentence\r\nThe change is done for issues:\r\n#2061 \r\n#2444 \r\n\r\nClose #2061 , close #2444.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 915525071,
    "title": "adding masahaner dataset",
    "dateCreated": "2021-06-08T21:20:25Z",
    "dateModified": "2021-06-08T21:20:25Z",
    "description": "Adding Masakhane dataset https://github.com/masakhane-io/masakhane-ner \r\n\r\n@lhoestq , can you please review",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 915485601,
    "title": "fix: adjusting indexing for the labels.",
    "dateCreated": "2021-06-08T20:47:25Z",
    "dateModified": "2021-06-08T20:47:25Z",
    "description": "The labels index were mismatching the actual ones used in the dataset. Specifically `0` is used  for `SUPPORTS` and `1` is used for `REFUTES`\r\nAfter this change, the `README.md` now reflects the content of `dataset_infos.json`.\r\n\r\nSigned-off-by: Matteo Manica <drugilsberg@gmail.com>",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 915454788,
    "title": "Fix proto_qa download link",
    "dateCreated": "2021-06-08T20:23:16Z",
    "dateModified": "2021-06-08T20:23:16Z",
    "description": "Fixes #2459 \r\n\r\nInstead of updating the path, this PR fixes a commit hash as suggested by @lhoestq.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 915384613,
    "title": "Merge DatasetDict and Dataset",
    "dateCreated": "2021-06-08T19:22:04Z",
    "dateModified": "2021-06-08T19:22:04Z",
    "description": "As discussed in #2424 and #2437 (please see there for detailed conversation):\r\n- It would be desirable to improve UX with respect the confusion between DatasetDict and Dataset.\r\n- The difference between Dataset and DatasetDict is an additional abstraction complexity that confuses \"typical\" end users. \r\n- A user expects a \"Dataset\" (whatever it contains multiple or a single split) and maybe it could be interesting to try to simplify the user-facing API as much as possible to hide this complexity from the end user.\r\n\r\nHere is a proposal for discussion and refined (and potential abandon if it's not good enough):\r\n- let's consider that a DatasetDict is also a Dataset with the various split concatenated one after the other\r\n- let's disallow the use of integers in split names (probably not a very big breaking change)\r\n- when you index with integers you access the examples progressively in split after the other is finished (in a deterministic order)\r\n- when you index with strings/split name you have the same behavior as now (full backward compat)\r\n- let's then also have all the methods of a Dataset on the DatasetDict\r\n\r\nThe end goal would be to merge both Dataset and DatasetDict object in a single object that would be (pretty much totally) backward compatible with both.\r\n\r\n\r\nThere are a few things that we could discuss if we want to merge Dataset and DatasetDict:\r\n\r\n1. what happens if you index by a string ? Does it return the column or the split ? We could disallow conflicts between column names and split names to avoid ambiguities. It can be surprising to be able to get a column or a split using the same indexing feature\r\n   ```\r\n   from datasets import load_dataset\r\n\r\n   dataset = load_dataset(...)\r\n   dataset[\"train\"]\r\n   dataset[\"input_ids\"]\r\n   ```\r\n2. what happens when you iterate over the object ? I guess it should iterate over the examples as a Dataset object, but a DatasetDict used to iterate over the splits as they are the dictionary keys. This is a breaking change that we can discuss.\r\n\r\nMoreover regarding your points:\r\n\r\n- integers are not allowed as split names already\r\n- it's definitely doable to have all the methods. Maybe some of them like train_test_split that is currently only available for Dataset can be tweaked to work for a split dataset\r\n\r\n\r\ncc: @thomwolf @lhoestq ",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 915286150,
    "title": "Support sliced list arrays in cast",
    "dateCreated": "2021-06-08T17:38:47Z",
    "dateModified": "2021-06-08T17:38:47Z",
    "description": "There is this issue in pyarrow:\r\n```python\r\nimport pyarrow as pa\r\n\r\narr = pa.array([[i * 10] for i in range(4)])\r\narr.cast(pa.list_(pa.int32()))  # works\r\n\r\narr = arr.slice(1)\r\narr.cast(pa.list_(pa.int32()))  # fails\r\n# ArrowNotImplementedError(\"Casting sliced lists (non-zero offset) not yet implemented\")\r\n```\r\n\r\nHowever in `Dataset.cast` we slice tables to cast their types (it's memory intensive), so we have the same issue.\r\nBecause of this it is currently not possible to cast a Dataset with a Sequence feature type (unless the table is small enough to not be sliced).\r\n\r\nIn this PR I fixed this by resetting the offset of `pyarrow.ListArray` arrays to zero in the table before casting.\r\nI used `pyarrow.compute.subtract` function to update the offsets of the ListArray.\r\n\r\ncc @abhi1thakur @SBrandeis ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 915268536,
    "title": "Revert default in-memory for small datasets",
    "dateCreated": "2021-06-08T17:14:23Z",
    "dateModified": "2021-06-08T17:14:23Z",
    "description": "Close #2458",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 915222015,
    "title": "`Proto_qa` hosting seems to be broken",
    "dateCreated": "2021-06-08T16:16:32Z",
    "dateModified": "2021-06-08T16:16:32Z",
    "description": "## Describe the bug\r\nThe hosting (on Github) of the `proto_qa` dataset seems broken. I haven't investigated more yet, just flagging it for now. \r\n\r\n@zaidalyafeai if you want to dive into it, I think it's just a matter of changing the links in `proto_qa.py`\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"proto_qa\")\r\n```\r\n\r\n## Actual results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/load.py\", line 751, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py\", line 575, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py\", line 630, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/hf/.cache/huggingface/modules/datasets_modules/datasets/proto_qa/445346efaad5c5f200ecda4aa7f0fb50ff1b55edde3003be424a2112c3e8102e/proto_qa.py\", line 131, in _split_generators\r\n    train_fpath = dl_manager.download(_URLs[self.config.name][\"train\"])\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 199, in download\r\n    num_proc=download_config.num_proc,\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 195, in map_nested\r\n    return function(data_struct)\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 218, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 291, in cached_path\r\n    use_auth_token=download_config.use_auth_token,\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 621, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/iesl/protoqa-data/master/data/train/protoqa_train.jsonl\r\n```",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 915199693,
    "title": "Revert default in-memory for small datasets",
    "dateCreated": "2021-06-08T15:51:41Z",
    "dateModified": "2021-06-08T15:51:41Z",
    "description": "Users are reporting issues and confusion about setting default in-memory to True for small datasets.\r\n\r\nWe see 2 clear use cases of Datasets:\r\n- the \"canonical\" way, where you can work with very large datasets, as they are memory-mapped and cached (after every transformation)\r\n- some edge cases (speed benchmarks, interactive/exploratory analysis,...), where default in-memory can explicitly be enabled, and no caching will be done\r\n\r\nAfter discussing with @lhoestq we have agreed to:\r\n- revert this feature (implemented in #2182)\r\n- explain in the docs how to optimize speed/performance by setting default in-memory\r\n\r\ncc: @stas00 https://github.com/huggingface/datasets/pull/2409#issuecomment-856210552",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 915079441,
    "title": "Add align_labels_with_mapping function",
    "dateCreated": "2021-06-08T13:54:00Z",
    "dateModified": "2021-06-08T13:54:00Z",
    "description": "This PR adds a helper function to align the `label2id` mapping between a `datasets.Dataset` and a classifier (e.g. a transformer with a `PretrainedConfig.label2id` dict), with the alignment performed on the dataset itself.\r\n\r\nThis will help us with the Hub evaluation, where we won't know in advance whether a model that is fine-tuned on say MNLI has the same mappings as the MNLI dataset we load from `datasets`.\r\n\r\nAn example where this is needed is if we naively try to evaluate `microsoft/deberta-base-mnli` on `mnli` because the model config has the following mappings:\r\n\r\n```python\r\n  \"id2label\": {\r\n    \"0\": \"CONTRADICTION\",\r\n    \"1\": \"NEUTRAL\",\r\n    \"2\": \"ENTAILMENT\"\r\n  },\r\n  \"label2id\": {\r\n    \"CONTRADICTION\": 0,\r\n    \"ENTAILMENT\": 2,\r\n    \"NEUTRAL\": 1\r\n  }\r\n```\r\n\r\nwhile the `mnli` dataset has the `contradiction` and `neutral` labels swapped:\r\n\r\n```python\r\nid2label = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\r\nlabel2id = {'contradiction': 2, 'entailment': 0, 'neutral': 1}\r\n```\r\n\r\nAs a result, we get a much lower accuracy during evaluation:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nfrom transformers.trainer_utils import EvalPrediction\r\nfrom transformers import AutoModelForSequenceClassification, Trainer\r\n\r\n# load dataset for evaluation\r\nmnli = load_dataset(\"glue\", \"mnli\", split=\"test\")\r\n# load model\r\nmodel_ckpt = \"microsoft/deberta-base-mnli\"\r\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\r\n# preprocess, create trainer ...\r\nmnli_enc = ...\r\ntrainer = Trainer(model, args=args, tokenizer=tokenizer)\r\n# generate preds\r\npreds = trainer.predict(mnli_enc)\r\n# preds.label_ids misalinged with model.config => returns wrong accuracy (too low)!\r\ncompute_metrics(EvalPrediction(preds.predictions, preds.label_ids))\r\n```\r\n\r\nThe fix is to use the helper function before running the evaluation to make sure the label IDs are aligned:\r\n\r\n```python\r\nmnli_enc_aligned = mnli_enc.align_labels_with_mapping(label2id=config.label2id, label_column=\"label\")\r\n# preds now aligned and everyone is happy :)\r\npreds = trainer.predict(mnli_enc_aligned)\r\n```\r\n\r\ncc @thomwolf @lhoestq ",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 914709293,
    "title": "Fix cross-reference typos in documentation",
    "dateCreated": "2021-06-08T09:45:14Z",
    "dateModified": "2021-06-08T09:45:14Z",
    "description": "Fix some minor typos in docs that avoid the creation of cross-reference links.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 914177468,
    "title": "Update version in xor_tydi_qa.py",
    "dateCreated": "2021-06-08T02:23:45Z",
    "dateModified": "2021-06-08T02:23:45Z",
    "description": "Fix #2449\r\n\r\n@lhoestq Should I revert to the old `dummy/1.0.0` or  delete it and keep only `dummy/1.1.0`?",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 913883631,
    "title": "Rename config and environment variable for in memory max size",
    "dateCreated": "2021-06-07T19:21:08Z",
    "dateModified": "2021-06-07T19:21:08Z",
    "description": "As discussed in #2409, both config and environment variable have been renamed.\r\n\r\ncc: @stas00, huggingface/transformers#12056",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 913729258,
    "title": "Keep original features order",
    "dateCreated": "2021-06-07T16:26:38Z",
    "dateModified": "2021-06-07T16:26:38Z",
    "description": "When loading a Dataset from a JSON file whose column names are not sorted alphabetically, we should get the same column name order, whether we pass features (in the same order as in the file) or not.\r\n\r\nI found this issue while working on #2366.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 913603877,
    "title": "MRPC test set differences between torch and tensorflow datasets",
    "dateCreated": "2021-06-07T14:20:26Z",
    "dateModified": "2021-06-07T14:20:26Z",
    "description": "## Describe the bug\r\nWhen using `load_dataset(\"glue\", \"mrpc\")` to load the MRPC dataset, the test set includes the labels. When using `tensorflow_datasets.load('glue/{}'.format('mrpc'))` to load the dataset the test set does not contain the labels. There should be consistency between torch and tensorflow ways of importing the GLUE datasets.\r\n\r\n## Steps to reproduce the bug\r\n\r\nMinimal working code  \r\n```python\r\nfrom datasets import load_dataset\r\nimport tensorflow as tf\r\nimport tensorflow_datasets\r\n\r\n# torch\r\ndataset = load_dataset(\"glue\", \"mrpc\")\r\n# tf\r\ndata = tensorflow_datasets.load('glue/{}'.format('mrpc'))\r\ndata = list(data['test'].as_numpy_iterator())\r\nfor i in range(40,50):\r\n  tf_sentence1 = data[i]['sentence1'].decode(\"utf-8\") \r\n  tf_sentence2 = data[i]['sentence2'].decode(\"utf-8\") \r\n\r\n  tf_label = data[i]['label']\r\n  \r\n  index = data[i]['idx']\r\n  print('Index {}'.format(index))\r\n  torch_sentence1 = dataset['test']['sentence1'][index]\r\n  torch_sentence2 = dataset['test']['sentence2'][index]\r\n\r\n  torch_label = dataset['test']['label'][index]\r\n  print('Tensorflow: \\n\\tSentence1 {}\\n\\tSentence2 {}\\n\\tLabel {}'.format(tf_sentence1, tf_sentence2, tf_label))\r\n  print('Torch: \\n\\tSentence1 {}\\n\\tSentence2 {}\\n\\tLabel {}'.format(torch_sentence1, torch_sentence2, torch_label))\r\n```\r\n\r\nSample output  \r\n```\r\nIndex 954\r\nTensorflow: \r\n\tSentence1 Sabri Yakou , an Iraqi native who is a legal U.S. resident , appeared before a federal magistrate yesterday on charges of violating U.S. arms-control laws .\r\n\tSentence2 The elder Yakou , an Iraqi native who is a legal U.S. resident , appeared before a federal magistrate Wednesday on charges of violating U.S. arms control laws .\r\n\tLabel -1\r\nTorch: \r\n\tSentence1 Sabri Yakou , an Iraqi native who is a legal U.S. resident , appeared before a federal magistrate yesterday on charges of violating U.S. arms-control laws .\r\n\tSentence2 The elder Yakou , an Iraqi native who is a legal U.S. resident , appeared before a federal magistrate Wednesday on charges of violating U.S. arms control laws .\r\n\tLabel 1\r\nIndex 711\r\nTensorflow: \r\n\tSentence1 Others keep records sealed for as little as five years or as much as 30 .\r\n\tSentence2 Some states make them available immediately ; others keep them sealed for as much as 30 years .\r\n\tLabel -1\r\nTorch: \r\n\tSentence1 Others keep records sealed for as little as five years or as much as 30 .\r\n\tSentence2 Some states make them available immediately ; others keep them sealed for as much as 30 years .\r\n\tLabel 0\r\n```\r\n\r\n## Expected results\r\nI would expect the datasets to be independent of whether I am working with torch or tensorflow.\r\n\r\n## Actual results\r\nTest set labels are provided in the `datasets.load_datasets()` for MRPC. However MRPC is the only task where the test set labels are not -1.\r\n\r\n## Environment info\r\n- `datasets` version: 1.7.0\r\n- Platform: Linux-5.4.109+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 913263340,
    "title": "Mention that there are no answers in adversarial_qa test set",
    "dateCreated": "2021-06-07T08:13:57Z",
    "dateModified": "2021-06-07T08:13:57Z",
    "description": "As mention in issue https://github.com/huggingface/datasets/issues/2447, there are no answers in the test set",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 912890291,
    "title": "BLUE file not found",
    "dateCreated": "2021-06-06T17:01:54Z",
    "dateModified": "2021-06-06T17:01:54Z",
    "description": "Hi, I'm having the following issue when I try to load the `blue` metric.\r\n\r\n```shell\r\nimport datasets\r\nmetric = datasets.load_metric('blue')\r\nTraceback (most recent call last):\r\n  File \"/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/load.py\", line 320, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 291, in cached_path\r\n    use_auth_token=download_config.use_auth_token,\r\n  File \"/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 621, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.7.0/metrics/blue/blue.py\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/load.py\", line 332, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 291, in cached_path\r\n    use_auth_token=download_config.use_auth_token,\r\n  File \"/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 621, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/master/metrics/blue/blue.py\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/load.py\", line 605, in load_metric\r\n    dataset=False,\r\n  File \"/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/load.py\", line 343, in prepare_module\r\n    combined_path, github_file_path\r\nFileNotFoundError: Couldn't find file locally at blue/blue.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.7.0/metrics/blue/blue.py.\r\nThe file is also not present on the master branch on github.\r\n```\r\nHere is dataset installed version info\r\n```shell\r\npip freeze | grep datasets\r\ndatasets==1.7.0\r\n```\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 912751752,
    "title": "Update `xor_tydi_qa` url to v1.1",
    "dateCreated": "2021-06-06T09:44:58Z",
    "dateModified": "2021-06-06T09:44:58Z",
    "description": "The dataset is updated and the old url no longer works. So I updated it.\r\n\r\nI faced a bug while trying to fix this. Documenting the solution here. Maybe we can add it to the doc (`CONTRIBUTING.md` and `ADD_NEW_DATASET.md`).\r\n> And to make the command work without the ExpectedMoreDownloadedFiles error, you just need to use the --ignore_verifications flag.\r\nhttps://github.com/huggingface/datasets/issues/2076#issuecomment-803904366",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 912360109,
    "title": "Fix flores download link",
    "dateCreated": "2021-06-05T17:30:24Z",
    "dateModified": "2021-06-05T17:30:24Z",
    "description": "",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 912299527,
    "title": "dataset adversarial_qa has no answers in the \"test\" set",
    "dateCreated": "2021-06-05T14:57:38Z",
    "dateModified": "2021-06-05T14:57:38Z",
    "description": "## Describe the bug\r\nWhen loading the adversarial_qa dataset the 'test' portion has no answers.  Only the 'train' and 'validation' portions do.  This occurs with all four of the configs ('adversarialQA', 'dbidaf', 'dbert', 'droberta')\r\n\r\n## Steps to reproduce the bug\r\n```\r\nfrom   datasets import load_dataset\r\nexamples = load_dataset('adversarial_qa', 'adversarialQA', script_version=\"master\")['test']\r\nprint('Loaded {:,} examples'.format(len(examples)))\r\nhas_answers = 0\r\nfor e in examples:\r\n    if e['answers']['text']:\r\n        has_answers += 1\r\nprint('{:,} have answers'.format(has_answers))\r\n>>> Loaded 3,000 examples\r\n>>> 0 have answers\r\n\r\nexamples = load_dataset('adversarial_qa', 'adversarialQA', script_version=\"master\")['validation']\r\n<...code above...>\r\n>>> Loaded 3,000 examples\r\n>>> 3,000 have answers\r\n```\r\n\r\n## Expected results\r\nIf 'test' is a valid dataset, it should have answers. Also note that all of the 'train' and 'validation' sets have answers, there are no \"no answer\" questions with this set (not sure if this is correct or not).\r\n\r\n## Environment info\r\n- `datasets` version: 1.7.0\r\n- Platform: Linux-5.8.0-53-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.5\r\n- PyArrow version: 1.0.0\r\n\r\n",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 911635399,
    "title": "`yelp_polarity` is broken",
    "dateCreated": "2021-06-04T15:44:29Z",
    "dateModified": "2021-06-04T15:44:29Z",
    "description": "![image](https://user-images.githubusercontent.com/22514219/120828150-c4a35b00-c58e-11eb-8083-a537cee4dbb3.png)\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 911577578,
    "title": "Fix broken URLs for bn_hate_speech and covid_tweets_japanese",
    "dateCreated": "2021-06-04T14:53:35Z",
    "dateModified": "2021-06-04T14:53:35Z",
    "description": "Closes #2388 ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 911297139,
    "title": "Sentence Boundaries missing in Dataset: xtreme / udpos",
    "dateCreated": "2021-06-04T09:10:26Z",
    "dateModified": "2021-06-04T09:10:26Z",
    "description": "I was browsing through annotation guidelines, as suggested by the datasets introduction.\r\n\r\nThe guidlines saids \"There must be exactly one blank line after every sentence, including the last sentence in the file. Empty sentences are not allowed.\" in the [Sentence Boundaries and Comments section](https://universaldependencies.org/format.html#sentence-boundaries-and-comments)\r\n\r\nBut the sentence boundaries seems not to be represented by huggingface datasets features well. I found out that multiple sentence are concatenated together as a 1D array, without any delimiter.\r\n\r\nPAN-x, which is another token classification subset from xtreme do represent the sentence boundary using a 2D array.\r\n\r\nYou may compare in PAN-x.en and udpos.English in the explorer:\r\n https://huggingface.co/datasets/viewer/?dataset=xtreme",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 909983574,
    "title": "Some tests hang on Windows",
    "dateCreated": "2021-06-03T00:27:30Z",
    "dateModified": "2021-06-03T00:27:30Z",
    "description": "Currently, several tests hang on Windows if the max path limit of 260 characters is not disabled. This happens due to the changes introduced by #2223 that cause an infinite loop in `WindowsFileLock` described in #2220.  This can be very tricky to debug, so I think now is a good time to address these issues/PRs. IMO throwing an error is too harsh, but maybe we can emit a warning in the top-level `__init__.py ` on startup if long paths are not enabled.\r\n",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 909677029,
    "title": "add english language tags for ~100 datasets",
    "dateCreated": "2021-06-02T16:24:56Z",
    "dateModified": "2021-06-02T16:24:56Z",
    "description": "As discussed on Slack, I have manually checked for ~100 datasets that they have at least one subset in English. This information was missing so adding into the READMEs.\r\n\r\nNote that I didn't check all the subsets so it's possible that some of the datasets have subsets in other languages than English...",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 908554713,
    "title": "DuplicatedKeysError on personal dataset",
    "dateCreated": "2021-06-01T17:59:41Z",
    "dateModified": "2021-06-01T17:59:41Z",
    "description": "## Describe the bug\r\nEver since today, I have been getting a DuplicatedKeysError while trying to load my dataset from my own script.\r\nError returned when running this line: `dataset = load_dataset('/content/drive/MyDrive/Thesis/Datasets/book_preprocessing/goodreads_maharjan_trimmed_and_nered/goodreadsnered.py')`\r\nNote that my script was working fine with earlier versions of the Datasets library. Cannot say with 100% certainty if I have been doing something wrong with my dataset script this whole time or if this is simply a bug with the new version of datasets.\r\n\r\n## Steps to reproduce the bug\r\nI cannot provide code to reproduce the error as I am working with my own dataset. I can however provide my script if requested.\r\n\r\n## Expected results\r\nFor my data to be loaded.\r\n\r\n## Actual results\r\n**DuplicatedKeysError** exception is raised\r\n```\r\nDownloading and preparing dataset good_reads_practice_dataset/main_domain (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/good_reads_practice_dataset/main_domain/1.1.0/64ff7c3fee2693afdddea75002eb6887d4fedc3d812ae3622128c8504ab21655...\r\n\r\n---------------------------------------------------------------------------\r\n\r\nDuplicatedKeysError                       Traceback (most recent call last)\r\n\r\n<ipython-input-6-c342ea0dae9d> in <module>()\r\n----> 1 dataset = load_dataset('/content/drive/MyDrive/Thesis/Datasets/book_preprocessing/goodreads_maharjan_trimmed_and_nered/goodreadsnered.py')\r\n\r\n5 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, **config_kwargs)\r\n    749         try_from_hf_gcs=try_from_hf_gcs,\r\n    750         base_path=base_path,\r\n--> 751         use_auth_token=use_auth_token,\r\n    752     )\r\n    753 \r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    573                     if not downloaded_from_gcs:\r\n    574                         self._download_and_prepare(\r\n--> 575                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    576                         )\r\n    577                     # Sync info\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    650             try:\r\n    651                 # Prepare split will record examples associated to the split\r\n--> 652                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    653             except OSError as e:\r\n    654                 raise OSError(\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _prepare_split(self, split_generator)\r\n    990                     writer.write(example, key)\r\n    991             finally:\r\n--> 992                 num_examples, num_bytes = writer.finalize()\r\n    993 \r\n    994         split_generator.split_info.num_examples = num_examples\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in finalize(self, close_stream)\r\n    407         # In case current_examples < writer_batch_size, but user uses finalize()\r\n    408         if self._check_duplicates:\r\n--> 409             self.check_duplicate_keys()\r\n    410             # Re-intializing to empty list for next batch\r\n    411             self.hkey_record = []\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in check_duplicate_keys(self)\r\n    347         for hash, key in self.hkey_record:\r\n    348             if hash in tmp_record:\r\n--> 349                 raise DuplicatedKeysError(key)\r\n    350             else:\r\n    351                 tmp_record.add(hash)\r\n\r\nDuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 0\r\nKeys should be unique and deterministic in nature\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.7.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.7.9\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 908521954,
    "title": "Remove `extended` field from dataset tagger",
    "dateCreated": "2021-06-01T17:18:42Z",
    "dateModified": "2021-06-01T17:18:42Z",
    "description": "## Describe the bug\r\nWhile working on #2435 I used the [dataset tagger](https://huggingface.co/datasets/tagging/) to generate the missing tags for the YAML metadata of each README.md file. However, it seems that our CI raises an error when the `extended` field is included:\r\n\r\n```\r\ndataset_name = 'arcd'\r\n\r\n    @pytest.mark.parametrize(\"dataset_name\", get_changed_datasets(repo_path))\r\n    def test_changed_dataset_card(dataset_name):\r\n        card_path = repo_path / \"datasets\" / dataset_name / \"README.md\"\r\n        assert card_path.exists()\r\n        error_messages = []\r\n        try:\r\n            ReadMe.from_readme(card_path)\r\n        except Exception as readme_error:\r\n            error_messages.append(f\"The following issues have been found in the dataset cards:\\nREADME:\\n{readme_error}\")\r\n        try:\r\n            DatasetMetadata.from_readme(card_path)\r\n        except Exception as metadata_error:\r\n            error_messages.append(\r\n                f\"The following issues have been found in the dataset cards:\\nYAML tags:\\n{metadata_error}\"\r\n            )\r\n    \r\n        if error_messages:\r\n>           raise ValueError(\"\\n\".join(error_messages))\r\nE           ValueError: The following issues have been found in the dataset cards:\r\nE           YAML tags:\r\nE           __init__() got an unexpected keyword argument 'extended'\r\n\r\ntests/test_dataset_cards.py:70: ValueError\r\n```\r\n\r\nConsider either removing this tag from the tagger or including it as part of the validation step in the CI.\r\n\r\ncc @yjernite ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 908511983,
    "title": "Better error message when trying to access elements of a DatasetDict without specifying the split",
    "dateCreated": "2021-06-01T17:04:32Z",
    "dateModified": "2021-06-01T17:04:32Z",
    "description": "As mentioned in #2437 it'd be nice to to have an indication to the users when they try to access an element of a DatasetDict without specifying the split name.\r\n\r\ncc @thomwolf ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 908461914,
    "title": "Fix NQ features loading: reorder fields of features to match nested fields order in arrow data",
    "dateCreated": "2021-06-01T16:09:30Z",
    "dateModified": "2021-06-01T16:09:30Z",
    "description": "As mentioned in #2401, there is an issue when loading the features of `natural_questions` since the order of the nested fields in the features don't match. The order is important since it matters for the underlying arrow schema.\r\n\r\nTo fix that I re-order the features based on the arrow schema:\r\n\r\n```python\r\ninferred_features = Features.from_arrow_schema(arrow_table.schema)\r\nself.info.features = self.info.features.reorder_fields_as(inferred_features)\r\nassert self.info.features.type == inferred_features.type\r\n```\r\n\r\nThe re-ordering is a recursive function. It takes into account that the `Sequence` feature type is a struct of list and not a list of struct.\r\n\r\nNow it's possible to load `natural_questions` again :)",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 908108882,
    "title": "Better error message when using the wrong load_from_disk",
    "dateCreated": "2021-06-01T09:43:22Z",
    "dateModified": "2021-06-01T09:43:22Z",
    "description": "As mentioned in #2424, the error message when one tries to use `Dataset.load_from_disk` to load a DatasetDict object (or _vice versa_) can be improved. I added a suggestion in the error message to let users know that they should use the other one.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 908100211,
    "title": "Update DatasetMetadata and ReadMe",
    "dateCreated": "2021-06-01T09:32:37Z",
    "dateModified": "2021-06-01T09:32:37Z",
    "description": "This PR contains the changes discussed in #2395.\r\n\r\n**Edit**:\r\nIn addition to those changes, I'll be updating the `ReadMe` as follows:\r\n\r\nCurrently, `Section` has separate parsing and validation error lists. In `.validate()`, we add these lists to the final lists and throw errors.\r\n\r\nOne way to make `ReadMe` consistent with `DatasetMetadata` and add a separate `.validate()` method is to throw separate parsing and validation errors. \r\n\r\nThis way, we don't have to throw validation errors, but only parsing errors in `__init__ ()`. We can have an option in `__init__()` to suppress parsing errors so that an object is created for validation. Doing this will allow the user to get all the errors in one go.\r\n\r\nIn `test_dataset_cards` , we are already catching error messages and appending to a list. This can be done for `ReadMe()`  for parsing errors, and `ReadMe(...,suppress_errors=True); readme.validate()`  for validation, separately.\r\n\r\n**Edit 2**:\r\nThe only parsing issue we have as of now is multiple headings at the same level with the same name. I assume this will happen very rarely, but it is still better to throw an error than silently pick one of them. It should be okay to separate it this way. \r\n\r\nWdyt @lhoestq ?\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 907505531,
    "title": "Insert Extractive QA templates for SQuAD-like datasets",
    "dateCreated": "2021-05-31T14:09:11Z",
    "dateModified": "2021-05-31T14:09:11Z",
    "description": "This PR adds task templates for 9 SQuAD-like templates with the following properties:\r\n\r\n* 1 config\r\n* A schema that matches the `squad` one (i.e. same column names, especially for the nested `answers` column because the current implementation does not support casting with mismatched columns. see #2434)\r\n* Less than 20GB (my laptop can't handle more right now)\r\n\r\nThe aim of this PR is to provide a few datasets to experiment with the task template integration in other libraries / services. \r\n\r\nPR #2429 should be merged before this one.\r\n\r\ncc @abhi1thakur ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 907503557,
    "title": "Extend QuestionAnsweringExtractive template to handle nested columns",
    "dateCreated": "2021-05-31T14:06:51Z",
    "dateModified": "2021-05-31T14:06:51Z",
    "description": "Currently the `QuestionAnsweringExtractive` task template and `preprare_for_task` only support \"flat\" features. We should extend the functionality to cover QA datasets like:\r\n\r\n* `iapp_wiki_qa_squad`\r\n* `parsinlu_reading_comprehension`\r\n\r\nwhere the nested features differ with those from `squad` and trigger an `ArrowNotImplementedError`:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nArrowNotImplementedError                  Traceback (most recent call last)\r\n<ipython-input-12-50e5b8f69c20> in <module>\r\n----> 1 ds.prepare_for_task(\"question-answering-extractive\")[0]\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in prepare_for_task(self, task)\r\n   1436         # We found a template so now flush `DatasetInfo` to skip the template update in `DatasetInfo.__post_init__`\r\n   1437         dataset.info.task_templates = None\r\n-> 1438         dataset = dataset.cast(features=template.features)\r\n   1439         return dataset\r\n   1440 \r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in cast(self, features, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, num_proc)\r\n    977         format = self.format\r\n    978         dataset = self.with_format(\"arrow\")\r\n--> 979         dataset = dataset.map(\r\n    980             lambda t: t.cast(schema),\r\n    981             batched=True,\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   1600 \r\n   1601         if num_proc is None or num_proc == 1:\r\n-> 1602             return self._map_single(\r\n   1603                 function=function,\r\n   1604                 with_indices=with_indices,\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    176         }\r\n    177         # apply actual function\r\n--> 178         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    179         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    180         # re-apply format to the output\r\n\r\n~/git/datasets/src/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    395             # Call actual function\r\n    396 \r\n--> 397             out = func(self, *args, **kwargs)\r\n    398 \r\n    399             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in _map_single(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, desc)\r\n   1940                         )  # Something simpler?\r\n   1941                         try:\r\n-> 1942                             batch = apply_function_on_filtered_inputs(\r\n   1943                                 batch,\r\n   1944                                 indices,\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples, offset)\r\n   1836                 effective_indices = [i + offset for i in indices] if isinstance(indices, list) else indices + offset\r\n   1837             processed_inputs = (\r\n-> 1838                 function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n   1839             )\r\n   1840             if update_data is None:\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in <lambda>(t)\r\n    978         dataset = self.with_format(\"arrow\")\r\n    979         dataset = dataset.map(\r\n--> 980             lambda t: t.cast(schema),\r\n    981             batched=True,\r\n    982             batch_size=batch_size,\r\n\r\n~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.cast()\r\n\r\n~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.ChunkedArray.cast()\r\n\r\n~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/compute.py in cast(arr, target_type, safe)\r\n    241     else:\r\n    242         options = CastOptions.unsafe(target_type)\r\n--> 243     return call_function(\"cast\", [arr], options)\r\n    244 \r\n    245 \r\n\r\n~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/_compute.pyx in pyarrow._compute.call_function()\r\n\r\n~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/_compute.pyx in pyarrow._compute.Function.call()\r\n\r\n~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowNotImplementedError: Unsupported cast from struct<answer_end: list<item: int32>, answer_start: list<item: int32>, text: list<item: string>> to struct using function cast_struct\r\n```",
    "status": "open",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 907488711,
    "title": "Fix DuplicatedKeysError in adversarial_qa",
    "dateCreated": "2021-05-31T13:48:47Z",
    "dateModified": "2021-05-31T13:48:47Z",
    "description": "Fixes #2431",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 907462881,
    "title": "Fix CI six installation on linux",
    "dateCreated": "2021-05-31T13:15:36Z",
    "dateModified": "2021-05-31T13:15:36Z",
    "description": "For some reason we end up with this error in the linux CI when running pip install .[tests]\r\n```\r\npip._vendor.resolvelib.resolvers.InconsistentCandidate: Provided candidate AlreadyInstalledCandidate(six 1.16.0 (/usr/local/lib/python3.6/site-packages)) does not satisfy SpecifierRequirement('six>1.9'), SpecifierRequirement('six>1.9'), SpecifierRequirement('six>=1.11'), SpecifierRequirement('six~=1.15'), SpecifierRequirement('six'), SpecifierRequirement('six>=1.5.2'), SpecifierRequirement('six>=1.9.0'), SpecifierRequirement('six>=1.11.0'), SpecifierRequirement('six'), SpecifierRequirement('six>=1.6.1'), SpecifierRequirement('six>=1.9'), SpecifierRequirement('six>=1.5'), SpecifierRequirement('six<2.0'), SpecifierRequirement('six<2.0'), SpecifierRequirement('six'), SpecifierRequirement('six'), SpecifierRequirement('six~=1.15.0'), SpecifierRequirement('six'), SpecifierRequirement('six<2.0,>=1.6.1'), SpecifierRequirement('six'), SpecifierRequirement('six>=1.5.2'), SpecifierRequirement('six>=1.9.0')\r\n```\r\nexample CI failure here:\r\nhttps://app.circleci.com/pipelines/github/huggingface/datasets/6200/workflows/b64fdec9-f9e6-431c-acd7-e9f2c440c568/jobs/38247\r\n\r\nThe main version requirement comes from tensorflow: `six~=1.15.0`\r\nSo I pinned the six version to this.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 907413691,
    "title": "DuplicatedKeysError when trying to load adversarial_qa",
    "dateCreated": "2021-05-31T12:11:19Z",
    "dateModified": "2021-05-31T12:11:19Z",
    "description": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset = load_dataset('adversarial_qa', 'adversarialQA')\r\n```\r\n\r\n## Expected results\r\nThe dataset should be loaded into memory\r\n\r\n## Actual results\r\n\r\n>DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\n>Found duplicate Key: 4d3cb5677211ee32895ca9c66dad04d7152254d4\r\n>Keys should be unique and deterministic in nature\r\n>\r\n>\r\n>During handling of the above exception, another exception occurred:\r\n>\r\n>DuplicatedKeysError                       Traceback (most recent call last)\r\n>\r\n>/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in check_duplicate_keys(self)\r\n>    347         for hash, key in self.hkey_record:\r\n>    348             if hash in tmp_record:\r\n>--> 349                 raise DuplicatedKeysError(key)\r\n>    350             else:\r\n>    351                 tmp_record.add(hash)\r\n>\r\n>DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\n>Found duplicate Key: 4d3cb5677211ee32895ca9c66dad04d7152254d4\r\n>Keys should be unique and deterministic in nature\r\n\r\n## Environment info\r\n- `datasets` version: 1.7.0\r\n- Platform: Linux-5.4.109+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 907322595,
    "title": "Add version-specific BibTeX",
    "dateCreated": "2021-05-31T10:05:42Z",
    "dateModified": "2021-05-31T10:05:42Z",
    "description": "As pointed out by @lhoestq in #2411, after the creation of the Zenodo DOI for Datasets, a new BibTeX entry is created with each release.\r\n\r\nThis PR adds a version-specific BibTeX entry, besides the existing one which is generic for the project.\r\n\r\nSee version-specific BibTeX entry here: https://zenodo.org/record/4817769/export/hx#.YLSyd6j7RPY",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 907321665,
    "title": "Rename QuestionAnswering template to QuestionAnsweringExtractive",
    "dateCreated": "2021-05-31T10:04:42Z",
    "dateModified": "2021-05-31T10:04:42Z",
    "description": "Following the discussion with @thomwolf  in #2255, this PR renames the QA template to distinguish extractive vs abstractive QA. The abstractive template will be added in a future PR.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 907169746,
    "title": "Add copyright info for wiki_lingua dataset",
    "dateCreated": "2021-05-31T07:22:52Z",
    "dateModified": "2021-05-31T07:22:52Z",
    "description": "",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 907162923,
    "title": "Add copyright info to MLSUM dataset",
    "dateCreated": "2021-05-31T07:15:57Z",
    "dateModified": "2021-05-31T07:15:57Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 906473546,
    "title": "Saving Graph/Structured Data in Datasets",
    "dateCreated": "2021-05-29T13:35:21Z",
    "dateModified": "2021-05-29T13:35:21Z",
    "description": "Thanks for this amazing library! And my question is I have structured data that is organized with a graph. For example, a dataset with users' friendship relations and user's articles. When I try to save a python dict in the dataset, an error occurred ``did not recognize Python value type when inferring an Arrow data type''.\r\nAlthough I also know that storing a python dict in pyarrow datasets is not the best practice, but I have no idea about how to save structured data in the Datasets. \r\n\r\nThank you very much for your help.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 906385457,
    "title": "Fix Docstring Mistake: dataset vs. metric",
    "dateCreated": "2021-05-29T06:09:53Z",
    "dateModified": "2021-05-29T06:09:53Z",
    "description": "PR to fix #2412",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 906193679,
    "title": "load_from_disk and save_to_disk are not compatible with each other",
    "dateCreated": "2021-05-28T23:07:10Z",
    "dateModified": "2021-05-28T23:07:10Z",
    "description": "## Describe the bug\r\nload_from_disk and save_to_disk are not compatible. When I use save_to_disk to save a dataset to disk it works perfectly but given the same directory load_from_disk throws an error that it can't find state.json. looks like the load_from_disk only works on one split\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"art\")\r\ndataset.save_to_disk(\"mydir\")\r\nd = Dataset.load_from_disk(\"mydir\")\r\n```\r\n\r\n## Expected results\r\nIt is expected that these two functions be the reverse of each other without more manipulation\r\n\r\n## Actual results\r\nFileNotFoundError: [Errno 2] No such file or directory: 'mydir/art/state.json'\r\n\r\n## Environment info\r\n- `datasets` version: 1.6.2\r\n- Platform: Linux-5.4.0-73-generic-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.10\r\n- PyTorch version (GPU?): 1.8.1+cu102 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n\r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 905935753,
    "title": "add `desc` in `map` for `DatasetDict` object",
    "dateCreated": "2021-05-28T19:28:44Z",
    "dateModified": "2021-05-28T19:28:44Z",
    "description": "`desc` in `map` currently only works with `Dataset` objects. This PR adds support for `DatasetDict` objects as well",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 905568548,
    "title": "Fix save_to_disk nested features order in dataset_info.json",
    "dateCreated": "2021-05-28T15:03:28Z",
    "dateModified": "2021-05-28T15:03:28Z",
    "description": "Fix issue https://github.com/huggingface/datasets/issues/2267\r\n\r\nThe order of the nested features matters (pyarrow limitation), but the save_to_disk method was saving the features types as JSON with `sort_keys=True`, which was breaking the order of the nested features.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 905549756,
    "title": "doc: fix typo HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
    "dateCreated": "2021-05-28T14:52:10Z",
    "dateModified": "2021-05-28T14:52:10Z",
    "description": "MAX_MEMORY_DATASET_SIZE_IN_BYTES should be HF_MAX_MEMORY_DATASET_SIZE_IN_BYTES",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 904821772,
    "title": "Updated Dataset Description",
    "dateCreated": "2021-05-28T07:10:51Z",
    "dateModified": "2021-05-28T07:10:51Z",
    "description": "Added Point of contact information and several other details about the dataset.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 904347339,
    "title": "adds license information for DailyDialog.",
    "dateCreated": "2021-05-27T23:03:42Z",
    "dateModified": "2021-05-27T23:03:42Z",
    "description": "",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 904051497,
    "title": "add utf-8 while reading README",
    "dateCreated": "2021-05-27T18:12:28Z",
    "dateModified": "2021-05-27T18:12:28Z",
    "description": "It was causing tests to fail in Windows (see #2416). In Windows, the default encoding is CP1252 which is unable to decode the character byte 0x9d ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 903956071,
    "title": "Make datasets PEP-561 compliant",
    "dateCreated": "2021-05-27T16:16:17Z",
    "dateModified": "2021-05-27T16:16:17Z",
    "description": "Allows to type-check datasets with `mypy` when imported as a third-party library\r\n\r\nPEP-561: https://www.python.org/dev/peps/pep-0561\r\nMyPy doc on the subject: https://mypy.readthedocs.io/en/stable/installed_packages.html\r\n",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 903932299,
    "title": "Add KLUE dataset",
    "dateCreated": "2021-05-27T15:49:51Z",
    "dateModified": "2021-05-27T15:49:51Z",
    "description": "Add `KLUE (Korean Language Understanding Evaluation)` dataset released recently from [paper](https://arxiv.org/abs/2105.09680), [github](https://github.com/KLUE-benchmark/KLUE) and [webpage](https://klue-benchmark.com/tasks).\r\nPlease let me know if there's anything missing in the code or README.\r\nThanks!\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 903923097,
    "title": "Cached dataset not loaded",
    "dateCreated": "2021-05-27T15:40:06Z",
    "dateModified": "2021-05-27T15:40:06Z",
    "description": "## Describe the bug\r\nI have a large dataset (common_voice, english) where I use several map and filter functions.\r\nSometimes my cached datasets after specific functions are not loaded.\r\nI always use the same arguments, same functions, no seed\u2026\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndef filter_by_duration(batch):\r\n    return (\r\n        batch[\"duration\"] <= 10\r\n        and batch[\"duration\"] >= 1\r\n        and len(batch[\"target_text\"]) > 5\r\n    )\r\n\r\ndef prepare_dataset(batch):\r\n    batch[\"input_values\"] = processor(\r\n        batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]\r\n    ).input_values\r\n    with processor.as_target_processor():\r\n        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\r\n    return batch\r\n\r\ntrain_dataset = train_dataset.filter(\r\n    filter_by_duration,\r\n    remove_columns=[\"duration\"],\r\n    num_proc=data_args.preprocessing_num_workers,\r\n)\r\n\r\n# PROBLEM HERE -> below function is reexecuted and cache is not loaded\r\ntrain_dataset = train_dataset.map(\r\n    prepare_dataset,\r\n    remove_columns=train_dataset.column_names,\r\n    batch_size=training_args.per_device_train_batch_size,\r\n    batched=True,\r\n    num_proc=data_args.preprocessing_num_workers,\r\n)\r\n\r\n# Later in script\r\nset_caching_enabled(False)\r\n# apply map on trained model to eval/test sets\r\n\r\n```\r\n\r\n## Expected results\r\nThe cached dataset should always be reloaded.\r\n\r\n## Actual results\r\nThe function is reexecuted.\r\n\r\nI have access to cached files `cache-xxxxx.arrow`.\r\nIs there a way I can somehow load manually 2 versions and see how the hash was created for debug purposes (to know if it's an issue with dataset or function)?\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.6.2\r\n- Platform: Linux-5.8.0-45-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.5\r\n- PyTorch version (GPU?): 1.8.1+cu102 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: Yes\r\n- Using distributed or parallel set-up in script?: No",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 903877096,
    "title": "Update README.md",
    "dateCreated": "2021-05-27T14:53:19Z",
    "dateModified": "2021-05-27T14:53:19Z",
    "description": "Provides description of data instances and dataset features\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 903777557,
    "title": "AttributeError: 'DatasetInfo' object has no attribute 'task_templates'",
    "dateCreated": "2021-05-27T13:44:28Z",
    "dateModified": "2021-05-27T13:44:28Z",
    "description": "## Describe the bug\r\nHello, \r\nI'm trying to add dataset and contribute, but test keep fail with below cli.\r\n` RUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_dataset_all_configs_<my_dataset>`\r\n\r\n## Steps to reproduce the bug\r\nIt seems like a bug when I see an error with the existing dataset, not the dataset I'm trying to add.\r\n\r\n` RUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_dataset_all_configs_<any_dataset>`\r\n\r\n\r\n## Expected results\r\nAll test passed\r\n\r\n## Actual results\r\n```\r\n                # check that dataset is not empty\r\n                self.parent.assertListEqual(sorted(dataset_builder.info.splits.keys()), sorted(dataset))\r\n                for split in dataset_builder.info.splits.keys():\r\n                    # check that loaded datset is not empty\r\n                    self.parent.assertTrue(len(dataset[split]) > 0)\r\n    \r\n                # check that we can cast features for each task template\r\n>               task_templates = dataset_builder.info.task_templates\r\nE               AttributeError: 'DatasetInfo' object has no attribute 'task_templates'\r\n\r\ntests/test_dataset_common.py:175: AttributeError\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.6.2\r\n- Platform: Darwin-20.4.0-x86_64-i386-64bit\r\n- Python version: 3.7.7\r\n- PyTorch version (GPU?): 1.7.0 (False)\r\n- Tensorflow version (GPU?): 2.3.0 (False)\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 903769151,
    "title": "Docstring mistake: dataset vs. metric",
    "dateCreated": "2021-05-27T13:39:11Z",
    "dateModified": "2021-05-27T13:39:11Z",
    "description": "This:\r\n\r\nhttps://github.com/huggingface/datasets/blob/d95b95f8cf3cb0cff5f77a675139b584dcfcf719/src/datasets/load.py#L582\r\n\r\nShould better be something like:\r\n\r\n`a metric identifier on HuggingFace AWS bucket (list all available metrics and ids with ``datasets.list_metrics()``)`\r\n\r\nI can provide a PR l8er...",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 903671778,
    "title": "Add DOI badge to README",
    "dateCreated": "2021-05-27T12:36:47Z",
    "dateModified": "2021-05-27T12:36:47Z",
    "description": "Once published the latest release, the DOI badge has been automatically generated by Zenodo.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 903613676,
    "title": "fix #2391 add original answers in kilt-TriviaQA",
    "dateCreated": "2021-05-27T11:54:29Z",
    "dateModified": "2021-05-27T11:54:29Z",
    "description": "cc @yjernite is it ok like this?",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 903441398,
    "title": "Add HF_ prefix to env var MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
    "dateCreated": "2021-05-27T09:07:00Z",
    "dateModified": "2021-05-27T09:07:00Z",
    "description": "As mentioned in https://github.com/huggingface/datasets/pull/2399 the env var should be prefixed by HF_",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 903422648,
    "title": "Fix head_qa keys",
    "dateCreated": "2021-05-27T08:50:19Z",
    "dateModified": "2021-05-27T08:50:19Z",
    "description": "There were duplicate in the keys, as mentioned in #2382 ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 903111755,
    "title": ".map() function got an unexpected keyword argument 'cache_file_name'",
    "dateCreated": "2021-05-27T01:54:26Z",
    "dateModified": "2021-05-27T01:54:26Z",
    "description": "## Describe the bug\r\n\r\nI'm trying to save the result of datasets.map() to a specific file, so that I can easily share it among multiple computers without reprocessing the dataset. However, when I try to pass an argument 'cache_file_name' to the .map() function, it throws an error that \".map() function got an unexpected keyword argument 'cache_file_name'\". \r\n\r\nI believe I'm using the latest dataset 1.6.2. Also seems like the document and the actual code indicates there is an argument 'cache_file_name' for the .map() function.\r\n\r\nHere is the code I use\r\n## Steps to reproduce the bug\r\n```datasets = load_from_disk(dataset_path=my_path)\r\n\r\n[...]\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[text_column_name])\r\n\r\nlogger.info(\"Mapping dataset to tokenized dataset.\")\r\ntokenized_datasets = datasets.map(\r\n    tokenize_function,\r\n    batched=True,\r\n    num_proc=preprocessing_num_workers,\r\n    remove_columns=column_names,\r\n    load_from_cache_file=True,\r\n   cache_file_name=\"my_tokenized_file\"\r\n)\r\n```\r\n\r\n## Actual results\r\n    tokenized_datasets = datasets.map(\r\nTypeError: map() got an unexpected keyword argument 'cache_file_name'\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:1.6.2\r\n- Platform:Linux-4.18.0-193.28.1.el8_2.x86_64-x86_64-with-glibc2.10\r\n- Python version:3.8.5\r\n- PyArrow version:3.0.0\r\n",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 902643844,
    "title": "Add guide on using task templates to documentation",
    "dateCreated": "2021-05-26T16:28:26Z",
    "dateModified": "2021-05-26T16:28:26Z",
    "description": "Once we have a stable API on the text classification and question answering task templates, add a guide on how to use them in the documentation.\r\n",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 901227658,
    "title": "Add dataset tags",
    "dateCreated": "2021-05-25T18:57:29Z",
    "dateModified": "2021-05-25T18:57:29Z",
    "description": "The dataset tags were provided by Peter Clark following the guide.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 901179832,
    "title": "Paperswithcode dataset mapping",
    "dateCreated": "2021-05-25T18:14:26Z",
    "dateModified": "2021-05-25T18:14:26Z",
    "description": "This is a continuation of https://github.com/huggingface/huggingface_hub/pull/43, encoded directly inside dataset cards.\r\n\r\nAs discussed:\r\n- `paperswithcode_id: null` when the dataset doesn't exist on paperswithcode's side.\r\n- I've added this new key at the end of the yaml instead of ordering all keys alphabetically as pyyaml's default. No strong opinion on that one though\r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 900059014,
    "title": "Free datasets with cache file in temp dir on exit",
    "dateCreated": "2021-05-24T22:15:11Z",
    "dateModified": "2021-05-24T22:15:11Z",
    "description": "This PR properly cleans up the memory-mapped tables that reference the cache files inside the temp dir.\r\nSince the built-in `_finalizer` of `TemporaryDirectory` can't be modified, this PR defines its own `TemporaryDirectory` class that accepts a custom clean-up function.\r\n\r\nFixes #2402",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 900025329,
    "title": "PermissionError on Windows when using temp dir for caching",
    "dateCreated": "2021-05-24T21:22:59Z",
    "dateModified": "2021-05-24T21:22:59Z",
    "description": "Currently, the following code raises a PermissionError on master if working on Windows:\r\n\r\n```python\r\n# run as a script or call exit() in REPL to initiate the temp dir cleanup\r\nfrom datasets import *\r\nd = load_dataset(\"sst\", split=\"train\", keep_in_memory=False)\r\nset_caching_enabled(False)\r\nd.map(lambda ex: ex)\r\n```\r\n\r\nError stack trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\weakref.py\", line 624, in _exitfunc\r\n    f()\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\weakref.py\", line 548, in __call__\r\n    return info.func(*info.args, **(info.kwargs or {}))\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\tempfile.py\", line 799, in _cleanup\r\n    _shutil.rmtree(name)\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\shutil.py\", line 500, in rmtree\r\n    return _rmtree_unsafe(path, onerror)\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\shutil.py\", line 395, in _rmtree_unsafe\r\n    onerror(os.unlink, fullname, sys.exc_info())\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\shutil.py\", line 393, in _rmtree_unsafe\r\n    os.unlink(fullname)\r\nPermissionError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Mario\\\\AppData\\\\Local\\\\Temp\\\\tmp20epyhmq\\\\cache-87a87ffb5a956e68.arrow'\r\n```",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 899910521,
    "title": "load_dataset('natural_questions') fails with \"ValueError: External features info don't match the dataset\"",
    "dateCreated": "2021-05-24T18:38:53Z",
    "dateModified": "2021-05-24T18:38:53Z",
    "description": "## Describe the bug\r\nload_dataset('natural_questions') throws ValueError\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndatasets = load_dataset('natural_questions', split='validation[:10]')\r\n```\r\n\r\n## Expected results\r\nCall to load_dataset returns data.\r\n\r\n## Actual results\r\n```\r\nUsing custom data configuration default\r\nReusing dataset natural_questions (/mnt/d/huggingface/datasets/natural_questions/default/0.0.2/19bc04755018a3ad02ee74f7045cde4ba9b4162cb64450a87030ab786b123b76)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-d55ab8a8cc1c> in <module>\r\n----> 1 datasets = load_dataset('natural_questions', split='validation[:10]', cache_dir='/mnt/d/huggingface/datasets')\r\n\r\n~/miniconda3/lib/python3.8/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)\r\n    756         keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n    757     )\r\n--> 758     ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications, in_memory=keep_in_memory)\r\n    759     if save_infos:\r\n    760         builder_instance._save_infos()\r\n\r\n~/miniconda3/lib/python3.8/site-packages/datasets/builder.py in as_dataset(self, split, run_post_process, ignore_verifications, in_memory)\r\n    735 \r\n    736         # Create a dataset for each of the given splits\r\n--> 737         datasets = utils.map_nested(\r\n    738             partial(\r\n    739                 self._build_single_dataset,\r\n\r\n~/miniconda3/lib/python3.8/site-packages/datasets/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types)\r\n    193     # Singleton\r\n    194     if not isinstance(data_struct, dict) and not isinstance(data_struct, types):\r\n--> 195         return function(data_struct)\r\n    196 \r\n    197     disable_tqdm = bool(logger.getEffectiveLevel() > INFO)\r\n\r\n~/miniconda3/lib/python3.8/site-packages/datasets/builder.py in _build_single_dataset(self, split, run_post_process, ignore_verifications, in_memory)\r\n    762 \r\n    763         # Build base dataset\r\n--> 764         ds = self._as_dataset(\r\n    765             split=split,\r\n    766             in_memory=in_memory,\r\n\r\n~/miniconda3/lib/python3.8/site-packages/datasets/builder.py in _as_dataset(self, split, in_memory)\r\n    838             in_memory=in_memory,\r\n    839         )\r\n--> 840         return Dataset(**dataset_kwargs)\r\n    841 \r\n    842     def _post_process(self, dataset: Dataset, resources_paths: Dict[str, str]) -> Optional[Dataset]:\r\n\r\n~/miniconda3/lib/python3.8/site-packages/datasets/arrow_dataset.py in __init__(self, arrow_table, info, split, indices_table, fingerprint)\r\n    271         assert self._fingerprint is not None, \"Fingerprint can't be None in a Dataset object\"\r\n    272         if self.info.features.type != inferred_features.type:\r\n--> 273             raise ValueError(\r\n    274                 \"External features info don't match the dataset:\\nGot\\n{}\\nwith type\\n{}\\n\\nbut expected something like\\n{}\\nwith type\\n{}\".format(\r\n    275                     self.info.features, self.info.features.type, inferred_features, inferred_features.type\r\n\r\nValueError: External features info don't match the dataset:\r\nGot\r\n{'id': Value(dtype='string', id=None), 'document': {'title': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'html': Value(dtype='string', id=None), 'tokens': Sequence(feature={'token': Value(dtype='string', id=None), 'is_html': Value(dtype='bool', id=None)}, length=-1, id=None)}, 'question': {'text': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'annotations': Sequence(feature={'id': Value(dtype='string', id=None), 'long_answer': {'start_token': Value(dtype='int64', id=None), 'end_token': Value(dtype='int64', id=None), 'start_byte': Value(dtype='int64', id=None), 'end_byte': Value(dtype='int64', id=None)}, 'short_answers': Sequence(feature={'start_token': Value(dtype='int64', id=None), 'end_token': Value(dtype='int64', id=None), 'start_byte': Value(dtype='int64', id=None), 'end_byte': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}, length=-1, id=None), 'yes_no_answer': ClassLabel(num_classes=2, names=['NO', 'YES'], names_file=None, id=None)}, length=-1, id=None)}\r\nwith type\r\nstruct<annotations: struct<id: list<item: string>, long_answer: list<item: struct<start_token: int64, end_token: int64, start_byte: int64, end_byte: int64>>, short_answers: list<item: struct<end_byte: list<item: int64>, end_token: list<item: int64>, start_byte: list<item: int64>, start_token: list<item: int64>, text: list<item: string>>>, yes_no_answer: list<item: int64>>, document: struct<title: string, url: string, html: string, tokens: struct<is_html: list<item: bool>, token: list<item: string>>>, id: string, question: struct<text: string, tokens: list<item: string>>>\r\n\r\nbut expected something like\r\n{'id': Value(dtype='string', id=None), 'document': {'html': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'tokens': {'is_html': Sequence(feature=Value(dtype='bool', id=None), length=-1, id=None), 'token': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'url': Value(dtype='string', id=None)}, 'question': {'text': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'annotations': {'id': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'long_answer': [{'end_byte': Value(dtype='int64', id=None), 'end_token': Value(dtype='int64', id=None), 'start_byte': Value(dtype='int64', id=None), 'start_token': Value(dtype='int64', id=None)}], 'short_answers': [{'end_byte': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'end_token': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'start_byte': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'start_token': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}], 'yes_no_answer': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}}\r\nwith type\r\nstruct<annotations: struct<id: list<item: string>, long_answer: list<item: struct<end_byte: int64, end_token: int64, start_byte: int64, start_token: int64>>, short_answers: list<item: struct<end_byte: list<item: int64>, end_token: list<item: int64>, start_byte: list<item: int64>, start_token: list<item: int64>, text: list<item: string>>>, yes_no_answer: list<item: int64>>, document: struct<html: string, title: string, tokens: struct<is_html: list<item: bool>, token: list<item: string>>, url: string>, id: string, question: struct<text: string, tokens: list<item: string>>>\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.6.2\r\n- Platform: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.10\r\n- Python version: 3.8.3\r\n- PyTorch version (GPU?): 1.6.0 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 899867212,
    "title": "Concatenate several datasets with removed columns is not working.",
    "dateCreated": "2021-05-24T17:40:15Z",
    "dateModified": "2021-05-24T17:40:15Z",
    "description": "## Describe the bug\r\n\r\nYou can't concatenate datasets when you removed columns before.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, concatenate_datasets\r\n\r\nwikiann= load_dataset(\"wikiann\",\"en\")\r\n\r\nwikiann[\"train\"] = wikiann[\"train\"].remove_columns([\"langs\",\"spans\"])\r\nwikiann[\"test\"] = wikiann[\"test\"].remove_columns([\"langs\",\"spans\"])\r\n\r\nassert wikiann[\"train\"].features.type == wikiann[\"test\"].features.type\r\n\r\nconcate = concatenate_datasets([wikiann[\"train\"],wikiann[\"test\"]])\r\n```\r\n\r\n## Expected results\r\nMerged dataset \r\n\r\n\r\n## Actual results\r\n```python\r\nValueError: External features info don't match the dataset:\r\nGot\r\n{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None), length=-1, id=None), 'langs': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'spans': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\r\nwith type\r\nstruct<langs: list<item: string>, ner_tags: list<item: int64>, spans: list<item: string>, tokens: list<item: string>>\r\n\r\nbut expected something like\r\n{'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\r\nwith type\r\nstruct<ner_tags: list<item: int64>, tokens: list<item: string>>\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: ~1.6.2~ 1.5.0\r\n- Platform: macos\r\n- Python version: 3.8.5\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 899853610,
    "title": "Add env variable for MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
    "dateCreated": "2021-05-24T17:19:15Z",
    "dateModified": "2021-05-24T17:19:15Z",
    "description": "Add env variable for `MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES`.\r\n\r\nThis will allow to turn off default behavior: loading in memory (and not caching) small datasets.\r\n\r\nFix #2387.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 899511837,
    "title": "News_commentary Dataset Translation Pairs are of Incorrect Language Specified Pairs",
    "dateCreated": "2021-05-24T10:03:34Z",
    "dateModified": "2021-05-24T10:03:34Z",
    "description": "I used load_dataset to load the news_commentary dataset for \"ar-en\" translation pairs but found translations from Arabic to Hindi.  \r\n\r\n```\r\ntrain_ds = load_dataset(\"news_commentary\", \"ar-en\", split='train[:98%]')\r\nval_ds = load_dataset(\"news_commentary\", \"ar-en\", split='train[98%:]')\r\n\r\n# filtering out examples that are not ar-en translations but ar-hi\r\nval_ds = val_ds.filter(lambda example, indice: indice not in chain(range(1312,1327) ,range(1384,1399), range(1030,1042)), with_indices=True)\r\n```\r\n\r\n* I'm fairly new to using datasets so I might be doing something wrong",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 899427378,
    "title": "Fix number of classes in indic_glue sna.bn dataset",
    "dateCreated": "2021-05-24T08:18:55Z",
    "dateModified": "2021-05-24T08:18:55Z",
    "description": "As read in the [paper](https://www.aclweb.org/anthology/2020.findings-emnlp.445.pdf), Table 11.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 899016308,
    "title": "strange datasets from OSCAR corpus",
    "dateCreated": "2021-05-23T13:06:02Z",
    "dateModified": "2021-05-23T13:06:02Z",
    "description": "![image](https://user-images.githubusercontent.com/50871412/119260850-4f876b80-bc07-11eb-8894-124302600643.png)\r\n![image](https://user-images.githubusercontent.com/50871412/119260875-675eef80-bc07-11eb-9da4-ee27567054ac.png)\r\nFrom the [official site ](https://oscar-corpus.com/), the Yue Chinese dataset should have 2.2KB data.\r\n7 training instances is obviously not a right number.\r\nAs I can read Yue Chinese, I call tell the last instance is definitely not something that would appear on Common Crawl.\r\nAnd even if you don't read Yue Chinese, you can tell the first six instance are problematic.\r\n(It is embarrassing, as the 7 training instances look exactly like something from a pornographic novel or flitting messages in a chat of a dating app)\r\nIt might not be the problem of the huggingface/datasets implementation, because when I tried to download the dataset from the official site, I found out that the zip file is corrupted.\r\nI will try to inform the host of OSCAR corpus later.\r\nAwy a remake about this dataset in huggingface/datasets is needed, perhaps after the host of the dataset fixes the issue.\r\n\r\n> Hi @jerryIsHere , sorry for the late response! Sadly this is normal, the problem comes form fasttext's classifier which we used to create the original corpus. In general the classifier is not really capable of properly recognizing Yue Chineese so the file ends un being just noise from Common Crawl. Some of these problems with OSCAR were already discussed [here](https://arxiv.org/pdf/2103.12028.pdf) but we are working on explicitly documenting the problems by language on our website. In fact, could please you open an issue on [our repo](https://github.com/oscar-corpus/oscar-website/issues) as well so that we can track it?\r\n\r\nThanks a lot, the new post is here:\r\nhttps://github.com/oscar-corpus/oscar-website/issues/11",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 898762730,
    "title": "`pretty_name` for dataset in YAML tags",
    "dateCreated": "2021-05-22T09:24:45Z",
    "dateModified": "2021-05-22T09:24:45Z",
    "description": "I'm updating `pretty_name` for datasets in YAML tags as discussed with @lhoestq. Here are the first 10, please let me know if they're looking good.\r\n\r\nIf dataset has 1 config, I've added `pretty_name` as `config_name: full_name_of_dataset` as config names were `plain_text`, `default`, `squad` etc (not so important in this case) whereas when dataset has >1 configs, I've added `config_name: full_name_of_dataset+config_name` so as to let user know about the `config` here. ",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 898156795,
    "title": "Update text classification template labels in DatasetInfo __post_init__",
    "dateCreated": "2021-05-21T15:29:41Z",
    "dateModified": "2021-05-21T15:29:41Z",
    "description": "This PR implements the idea discussed in #2389 to update the `labels` of the `TextClassification` template in the `DatasetInfo.__post_init__`. The main reason for doing so is so avoid duplicating the label definitions in both `DatasetInfo.features` and `DatasetInfo.task_templates`.\r\n\r\nTo avoid storing state in `DatasetInfo.__post_init__`, the current implementation flushes `DatasetInfo.task_templates` before the features are cast in `Dataset.prepare_for_task` (thanks to @mariosasko for this idea!).\r\n\r\nHere is an example of the current workflow:\r\n\r\n```python\r\nds1 = load_dataset(\"./datasets/emotion/\")\r\n# cast features and flush templates\r\nds2 = ds1.prepare_for_task(\"text-classification\")\r\nassert ds2.info.task_templates is None\r\n```\r\n\r\nNote that if users want to pass a `TextClassification` template to `prepare_for_task`, we require them to set `TextClassification.labels` to match the dataset's features corresponding to `label_column`:\r\n\r\n```python\r\nds1 = load_dataset(\"./datasets/emotion/\")\r\n# TextClassification.labels is None by default => invalid template\r\ntask = TextClassification(text_column=\"text\", label_column=\"label\")\r\n# Raises ValueError\r\nds1.prepare_for_task(task)\r\n# Specifying the labels => valid template\r\ntask = TextClassification(text_column=\"text\", label_column=\"label\", labels=['anger', 'fear', 'joy', 'love', 'sadness', 'surprise'])\r\nds1.prepare_for_task(task)\r\n```\r\n\r\nThis PR also adds:\r\n\r\n* New tests + fixed some old tests that weren't testing `assertRaises` properly\r\n* A decorator to share docstrings across common functions. This allows us to document `DatasetDict.prepare_for_task` and `Dataset.prepare_for_task` in one place.\r\n* Fixes to avoid side-effects from in-place replacements of `DatasetInfo.task_templates` in `DatasetInfo.__post_init__`. Thanks to @lhoestq for figuring this out!\r\n* Removal of `FeaturesWithLazyClassLabel` since we now create a new instance of `TextClassification` in `DatasetInfo.__post_init__` and avoid the side-effects first pointed out by @mariosasko \r\n\r\n### PR Description from original WIP \r\n\r\nHi @yjernite and @lhoestq, here's a first stab at the suggestion discussed in #2389 to update the `labels` of the `TextClassification` template in the `DatasetInfo.__post_init__`.\r\n\r\nOne problem I've spotted is that my current implementation introduces state into the `__post_init__`: \r\n\r\n* When we call `load_dataset`, `DatasetInfo.features` are the \"raw\" features without any casting so we can access the column names by the `label_column` specified in `TextClassification`\r\n* When we call `Dataset.prepare_for_task` we run into a problem because the `DatasetInfo.features` are first cast into the new schema which triggers a `KeyError` when we update the infos [here](https://github.com/huggingface/datasets/blob/8b2a78520828e0cc13c14a31f413a5395ef25110/src/datasets/arrow_dataset.py#L1959).\r\n\r\nHere's an explicit example of what I mean with the stack trace appended below:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n# this works \r\nds = load_dataset(\"emotion\")\r\n# we can verify the task template is correctly set\r\nds[\"train\"].info.task_templates # returns [TextClassification(labels=('sadness', 'joy', 'love', 'anger', 'fear', 'surprise'), text_column='text', label_column='label')]\r\n# but this fails because the _post_init__ is looking for the original column names\r\nds.prepare_for_task(\"text-classification\")\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-4-54a43019b319> in <module>\r\n----> 1 ds.prepare_for_task(\"text-classification\")\r\n\r\n~/git/datasets/src/datasets/dataset_dict.py in prepare_for_task(self, task)\r\n    807         \"\"\"\r\n    808         self._check_values_type()\r\n--> 809         return DatasetDict({k: dataset.prepare_for_task(task=task) for k, dataset in self.items()})\r\n\r\n~/git/datasets/src/datasets/dataset_dict.py in <dictcomp>(.0)\r\n    807         \"\"\"\r\n    808         self._check_values_type()\r\n--> 809         return DatasetDict({k: dataset.prepare_for_task(task=task) for k, dataset in self.items()})\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in prepare_for_task(self, task)\r\n   1421         dataset = self.remove_columns(columns_to_drop)\r\n   1422         dataset = dataset.rename_columns(column_mapping)\r\n-> 1423         dataset = dataset.cast(features=template.features)\r\n   1424         return dataset\r\n   1425 \r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in cast(self, features, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, num_proc)\r\n    970         format = self.format\r\n    971         dataset = self.with_format(\"arrow\")\r\n--> 972         dataset = dataset.map(\r\n    973             lambda t: t.cast(schema),\r\n    974             batched=True,\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\r\n   1583 \r\n   1584         if num_proc is None or num_proc == 1:\r\n-> 1585             return self._map_single(\r\n   1586                 function=function,\r\n   1587                 with_indices=with_indices,\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    173         }\r\n    174         # apply actual function\r\n--> 175         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    176         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    177         # re-apply format to the output\r\n\r\n~/git/datasets/src/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    338             # Call actual function\r\n    339 \r\n--> 340             out = func(self, *args, **kwargs)\r\n    341 \r\n    342             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in _map_single(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\r\n   1959         if update_data:\r\n   1960             # Create new Dataset from buffer or file\r\n-> 1961             info = self.info.copy()\r\n   1962             info.features = writer._features\r\n   1963             if buf_writer is None:\r\n\r\n~/git/datasets/src/datasets/info.py in copy(self)\r\n    274 \r\n    275     def copy(self) -> \"DatasetInfo\":\r\n--> 276         return self.__class__(**{k: copy.deepcopy(v) for k, v in self.__dict__.items()})\r\n    277 \r\n    278 \r\n\r\n~/git/datasets/src/datasets/info.py in __init__(self, description, citation, homepage, license, features, post_processed, supervised_keys, task_templates, builder_name, config_name, version, splits, download_checksums, download_size, post_processing_size, dataset_size, size_in_bytes)\r\n\r\n~/git/datasets/src/datasets/info.py in __post_init__(self)\r\n    174                     # The reason is that Dataset.prepare_for_task calls Dataset.cast which converts the\r\n    175                     # DatasetInfo.features to the new schema and thus template.label_column is no longer a valid key\r\n--> 176                     object.__setattr__(template, \"labels\", tuple(self.features[template.label_column].names))\r\n    177                     template.label_schema[\"labels\"] = ClassLabel(names=template.labels)\r\n    178                     self.task_templates[idx] = template\r\n\r\nKeyError: 'label'\r\n```\r\n\r\nWhat do you think? I did this a bit quickly, so maybe I'm overlooking something obvious :) One thing would be to only update the labels of the task template on load, but this seems a bit hacky IMO",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 898128099,
    "title": "Missing original answers in kilt-TriviaQA",
    "dateCreated": "2021-05-21T14:57:07Z",
    "dateModified": "2021-05-21T14:57:07Z",
    "description": "I previously opened an issue at https://github.com/facebookresearch/KILT/issues/42 but from the answer of @fabiopetroni it seems that the problem comes from HF-datasets\r\n\r\n## Describe the bug\r\nThe `answer` field in kilt-TriviaQA, e.g. `kilt_tasks['train_triviaqa'][0]['output']['answer']` contains a list of alternative answer which are accepted for the question.  \r\nHowever it'd be nice to know the original answer to the question (the only fields in `output` are `'answer', 'meta', 'provenance'`)\r\n\r\n## How to fix\r\nIt can be fixed by retrieving the original answer from the original TriviaQA (e.g. `trivia_qa['train'][0]['answer']['value']`), perhaps at the same place as here where one retrieves the questions https://github.com/huggingface/datasets/blob/master/datasets/kilt_tasks/README.md#loading-the-kilt-knowledge-source-and-task-data\r\n\r\ncc @yjernite who previously answered to an issue about KILT and TriviaQA :)\r\n",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 897903642,
    "title": "Add check for task templates on dataset load",
    "dateCreated": "2021-05-21T10:16:57Z",
    "dateModified": "2021-05-21T10:16:57Z",
    "description": "This PR adds a check that the features of a dataset match the schema of each compatible task template.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 897822270,
    "title": "Insert task templates for text classification",
    "dateCreated": "2021-05-21T08:36:26Z",
    "dateModified": "2021-05-21T08:36:26Z",
    "description": "This PR inserts text-classification templates for datasets with the following properties:\r\n\r\n* Only one config\r\n* At most two features of `(Value, ClassLabel)` type\r\n\r\nNote that this misses datasets like `sentiment140` which only has `Value` type features - these will be handled in a separate PR",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 897767470,
    "title": "Incorrect URLs for some datasets",
    "dateCreated": "2021-05-21T07:22:35Z",
    "dateModified": "2021-05-21T07:22:35Z",
    "description": "## Describe the bug\r\nIt seems that the URLs for the following datasets are invalid: \r\n\r\n- [ ]  `bn_hate_speech` has been renamed: https://github.com/rezacsedu/Bengali-Hate-Speech-Dataset/commit/c67ecfc4184911e12814f6b36901f9828df8a63a\r\n- [ ] `covid_tweets_japanese` has been renamed: http://www.db.info.gifu-u.ac.jp/covid-19-twitter-dataset/\r\n\r\nAs a result we can no longer load these datasets using `load_dataset`. The simple fix is to rename the URL in the dataset script - will do this asap.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n# pick one of the datasets from the list above\r\nds = load_dataset(\"bn_hate_speech\")\r\n```\r\n\r\n## Expected results\r\nDataset loads without error.\r\n\r\n## Actual results\r\n```\r\nDownloading: 3.36kB [00:00, 1.07MB/s]                                                                                                                                                                     \r\nDownloading: 2.03kB [00:00, 678kB/s]                                                                                                                                                                      \r\nUsing custom data configuration default\r\nDownloading and preparing dataset bn_hate_speech/default (download: 951.48 KiB, generated: 949.84 KiB, post-processed: Unknown size, total: 1.86 MiB) to /Users/lewtun/.cache/huggingface/datasets/bn_hate_speech/default/0.0.0/a2dc726e511a2177523301bcad196af05d4d8a2cff30d2769ba8aacc1f5fdb5c...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/load.py\", line 744, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/builder.py\", line 574, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/builder.py\", line 630, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/Users/lewtun/.cache/huggingface/modules/datasets_modules/datasets/bn_hate_speech/a2dc726e511a2177523301bcad196af05d4d8a2cff30d2769ba8aacc1f5fdb5c/bn_hate_speech.py\", line 76, in _split_generators\r\n    train_path = dl_manager.download_and_extract(_URL)\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 287, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 195, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 195, in map_nested\r\n    return function(data_struct)\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 218, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 281, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 621, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/rezacsedu/Bengali-Hate-Speech-Dataset/main/Bengali_%20Hate_Speech_Dataset_Subset.csv\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.6.2.dev0\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.8\r\n- PyArrow version: 3.0.0\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 897566666,
    "title": "datasets 1.6 ignores cache",
    "dateCreated": "2021-05-21T00:12:58Z",
    "dateModified": "2021-05-21T00:12:58Z",
    "description": "Moving from https://github.com/huggingface/transformers/issues/11801#issuecomment-845546612 \r\n\r\nQuoting @VictorSanh:\r\n\r\n> \r\n> I downgraded datasets to `1.5.0` and printed `tokenized_datasets.cache_files` (L335):\r\n> \r\n> > `{'train': [{'filename': '/home/victor/.cache/huggingface/datasets/openwebtext10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b/cache-c6aefe81ca4e5152.arrow'}], 'validation': [{'filename': '/home/victor/.cache/huggingface/datasets/openwebtext10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b/cache-97cf4c813e6469c6.arrow'}]}`\r\n> \r\n> while the same command with the latest version of datasets (actually starting at `1.6.0`) gives:\r\n> > `{'train': [], 'validation': []}`\r\n> \r\n\r\nI also confirm that downgrading to `datasets==1.5.0` makes things fast again - i.e. cache is used.\r\n\r\nto reproduce:\r\n```\r\nUSE_TF=0 python  examples/pytorch/language-modeling/run_clm.py \\\r\n    --model_name_or_path gpt2 \\\r\n    --dataset_name \"stas/openwebtext-10k\" \\\r\n    --output_dir output_dir \\\r\n    --overwrite_output_dir \\\r\n    --do_train \\\r\n    --do_eval \\\r\n    --max_train_samples 1000 \\\r\n    --max_eval_samples 200 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --num_train_epochs 1 \\\r\n    --warmup_steps 8 \\\r\n    --block_size 64 \\\r\n    --fp16 \\\r\n    --report_to none\r\n```\r\n\r\nthe first time the startup is slow and some 5 tqdm bars. It shouldn't do it on consequent runs. but with `datasets>1.5.0` it rebuilds on every run.\r\n\r\n@lhoestq \r\n",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 897560049,
    "title": "Accessing Arrow dataset cache_files",
    "dateCreated": "2021-05-20T23:57:43Z",
    "dateModified": "2021-05-20T23:57:43Z",
    "description": "## Describe the bug\r\nIn datasets 1.5.0 the following code snippet would have printed the cache_files:\r\n\r\n```\r\ntrain_data = load_dataset('conll2003', split='train', cache_dir='data')\r\nprint(train_data.cache_files[0]['filename'])\r\n\r\n```\r\n\r\nHowever, in the newest release (1.6.1), it prints an empty list.\r\n\r\nI also tried loading the dataset with `keep_in_memory=True` argument but still `cache_files` is empty.\r\n\r\nWas wondering if this is a bug or I need to pass additional arguments so I can access the cache_files.\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 897206823,
    "title": "update citations",
    "dateCreated": "2021-05-20T17:54:08Z",
    "dateModified": "2021-05-20T17:54:08Z",
    "description": "To update citations for [Offenseval_dravidiain](https://huggingface.co/datasets/offenseval_dravidian)\r\n ",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 896866461,
    "title": "Add args description to DatasetInfo",
    "dateCreated": "2021-05-20T13:53:10Z",
    "dateModified": "2021-05-20T13:53:10Z",
    "description": "Closes #2354 \r\n\r\nI am not sure what `post_processed` and `post_processing_size` correspond to, so have left them empty for now. I also took a guess at some of the other fields like `dataset_size` vs `size_in_bytes`, so might have misunderstood their meaning.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 895779723,
    "title": "Improve example in rounding docs",
    "dateCreated": "2021-05-19T18:59:23Z",
    "dateModified": "2021-05-19T18:59:23Z",
    "description": "Improves the example in the rounding subsection of the Split API docs. With this change, it should more clear what's the difference between the `closest` and the `pct1_dropremainder` rounding.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 895610216,
    "title": "DuplicatedKeysError: FAILURE TO GENERATE DATASET ! load_dataset('head_qa', 'en')",
    "dateCreated": "2021-05-19T15:49:48Z",
    "dateModified": "2021-05-19T15:49:48Z",
    "description": "Hello everyone,\r\n\r\nI try to use head_qa dataset in [https://huggingface.co/datasets/viewer/?dataset=head_qa&config=en](url)\r\n\r\n```\r\n!pip install datasets\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\r\n   'head_qa', 'en')\r\n```\r\nWhen I write above load_dataset(.), it throws the following:\r\n\r\n```\r\nDuplicatedKeysError                       Traceback (most recent call last)\r\n\r\n<ipython-input-6-ea87002d32f0> in <module>()\r\n      2 from datasets import load_dataset\r\n      3 dataset = load_dataset(\r\n----> 4    'head_qa', 'en')\r\n\r\n5 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in check_duplicate_keys(self)\r\n    347         for hash, key in self.hkey_record:\r\n    348             if hash in tmp_record:\r\n--> 349                 raise DuplicatedKeysError(key)\r\n    350             else:\r\n    351                 tmp_record.add(hash)\r\n\r\nDuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 1\r\nKeys should be unique and deterministic in nature\r\n```\r\nHow can I fix the error? Thanks\r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 895588844,
    "title": "add dataset card title",
    "dateCreated": "2021-05-19T15:30:03Z",
    "dateModified": "2021-05-19T15:30:03Z",
    "description": "few of them were missed by me earlier which I've added now",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 895367201,
    "title": "maintain YAML structure reading from README",
    "dateCreated": "2021-05-19T12:12:07Z",
    "dateModified": "2021-05-19T12:12:07Z",
    "description": "How YAML used be loaded earlier in the string (structure of YAML was affected because of this and YAML for datasets with multiple configs was not being loaded correctly):\r\n```\r\nannotations_creators:\r\nlabeled_final:\r\n- expert-generated\r\nlabeled_swap:\r\n- expert-generated\r\nunlabeled_final:\r\n- machine-generated\r\nlanguage_creators:\r\n- machine-generated\r\nlanguages:\r\n- en\r\nlicenses:\r\n- other\r\nmultilinguality:\r\n- monolingual\r\nsize_categories:\r\nlabeled_final:\r\n- 10K<n<100K\r\nlabeled_swap:\r\n- 10K<n<100K\r\nunlabeled_final:\r\n- 100K<n<1M\r\nsource_datasets:\r\n- original\r\ntask_categories:\r\n- text-classification\r\n- text-scoring\r\ntask_ids:\r\n- semantic-similarity-classification\r\n- semantic-similarity-scoring\r\n- text-scoring-other-paraphrase-identification\r\n```\r\nHow YAML is loaded in string now:\r\n```\r\nannotations_creators:\r\n  labeled_final:\r\n  - expert-generated\r\n  labeled_swap:\r\n  - expert-generated\r\n  unlabeled_final:\r\n  - machine-generated\r\nlanguage_creators:\r\n- machine-generated\r\nlanguages:\r\n- en\r\nlicenses:\r\n- other\r\nmultilinguality:\r\n- monolingual\r\nsize_categories:\r\n  labeled_final:\r\n  - 10K<n<100K\r\n  labeled_swap:\r\n  - 10K<n<100K\r\n  unlabeled_final:\r\n  - 100K<n<1M\r\nsource_datasets:\r\n- original\r\ntask_categories:\r\n- text-classification\r\n- text-scoring\r\ntask_ids:\r\n- semantic-similarity-classification\r\n- semantic-similarity-scoring\r\n- text-scoring-other-paraphrase-identification\r\n```",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 895252597,
    "title": "Disallow duplicate keys in yaml tags",
    "dateCreated": "2021-05-19T10:10:07Z",
    "dateModified": "2021-05-19T10:10:07Z",
    "description": "Make sure that there's no duplidate keys in yaml tags.\r\nI added the check in the yaml tree constructor's method, so that the verification is done at every level in the yaml structure.\r\n\r\ncc @julien-c ",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 895131774,
    "title": "Add missing dataset_infos.json files",
    "dateCreated": "2021-05-19T08:11:12Z",
    "dateModified": "2021-05-19T08:11:12Z",
    "description": "Some of the datasets in `datasets` are missing a `dataset_infos.json` file, e.g.\r\n\r\n```\r\n[PosixPath('datasets/chr_en/chr_en.py'), PosixPath('datasets/chr_en/README.md')]\r\n[PosixPath('datasets/telugu_books/README.md'), PosixPath('datasets/telugu_books/telugu_books.py')]\r\n[PosixPath('datasets/reclor/README.md'), PosixPath('datasets/reclor/reclor.py')]\r\n[PosixPath('datasets/json/README.md')]\r\n[PosixPath('datasets/csv/README.md')]\r\n[PosixPath('datasets/wikihow/wikihow.py'), PosixPath('datasets/wikihow/README.md')]\r\n[PosixPath('datasets/c4/c4.py'), PosixPath('datasets/c4/README.md')]\r\n[PosixPath('datasets/text/README.md')]\r\n[PosixPath('datasets/lm1b/README.md'), PosixPath('datasets/lm1b/lm1b.py')]\r\n[PosixPath('datasets/pandas/README.md')]\r\n```\r\n\r\nFor `json`, `text`, csv`, and `pandas` this is expected, but not for the others which should be fixed\r\n",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 894918927,
    "title": "ArrowDataset.save_to_disk produces files that cannot be read using pyarrow.feather",
    "dateCreated": "2021-05-19T02:04:37Z",
    "dateModified": "2021-05-19T02:04:37Z",
    "description": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nfrom pyarrow import feather\r\n\r\ndataset = load_dataset('imdb', split='train')\r\ndataset.save_to_disk('dataset_dir')\r\ntable = feather.read_table('dataset_dir/dataset.arrow')\r\n```\r\n\r\n## Expected results\r\nI expect that the saved dataset can be read by the official Apache Arrow methods.\r\n\r\n## Actual results\r\n```\r\n  File \"/usr/local/lib/python3.7/site-packages/pyarrow/feather.py\", line 236, in read_table\r\n    reader.open(source, use_memory_map=memory_map)\r\n  File \"pyarrow/feather.pxi\", line 67, in pyarrow.lib.FeatherReader.open\r\n  File \"pyarrow/error.pxi\", line 123, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Not a Feather V1 or Arrow IPC file\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: datasets-1.6.2\r\n- Platform: Linux\r\n- Python version: 3.7\r\n- PyArrow version: 0.17.1, also 2.0.0\r\n",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 894852264,
    "title": "Improve task api code quality",
    "dateCreated": "2021-05-18T23:13:40Z",
    "dateModified": "2021-05-18T23:13:40Z",
    "description": "Improves the code quality of the `TaskTemplate` dataclasses.\r\n\r\nChanges:\r\n* replaces `return NotImplemented` with raise `NotImplementedError` \r\n* replaces `sorted` with `len` in the uniqueness check \r\n* defines `label2id` and `id2label` in the `TextClassification` template as properties\r\n* replaces the `object.__setattr__(self, attr, value)` syntax with (IMO nicer) `self.__dict__[attr] = value`",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 894655157,
    "title": "Dataset Streaming",
    "dateCreated": "2021-05-18T18:20:00Z",
    "dateModified": "2021-05-18T18:20:00Z",
    "description": "# Dataset Streaming\r\n\r\n## API\r\n\r\nCurrent API is\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n# Load an IterableDataset without downloading data\r\nsnli = load_dataset(\"snli\", streaming=True)\r\n\r\n# Access examples by streaming data\r\nprint(next(iter(snli[\"train\"]))) \r\n# {'premise': 'A person on a horse jumps over a broken down airplane.',\r\n#  'hypothesis': 'A person is training his horse for a competition.',\r\n#  'label': 1}\r\n```\r\n\r\nI already implemented a few methods:\r\n- IterableDataset.map: apply transforms on-the-fly to the examples\r\n- IterableDataset.shuffle: shuffle the data _a la_ TFDS, i.e. with a shuffling buffer\r\n- IterableDataset.with_format: set the format to `\"torch\"` to get a `torch.utils.data.IterableDataset`\r\n- merge_datasets: merge two iterable datasets by alternating one or the other (you can specify the probabilities)\r\n\r\nI would love to have your opinion on the API design :)\r\n\r\n## Implementation details\r\n\r\n### Streaming\r\n\r\nData streaming is done using `fsspec` which has nice caching features.\r\n\r\nTo make dataset streaming work I extend the `open` function of dataset scripts to support opening remote files without downloading them entirely. It also works with remote compressed archives (currently only zip is supported):\r\n\r\n```python\r\n# Get a file-like object by streaming data from a remote file\r\nopen(\"https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/train.txt\")\r\n\r\n# Get a file-like object by streaming data from a remote compressed archive by using the hop separator \"::\"\r\nopen(\"zip://snli_1.0_train.txt::https://nlp.stanford.edu/projects/snli/snli_1.0.zip\")\r\n```\r\n\r\nI also extend the `os.path.join` function to support navigation in remote compressed archives, since it has to deal with the `\"::\"` separator. This separator is used by `fsspec`.\r\n\r\nFinally I also added a retry mechanism in case the connection fails during data streaming.\r\n\r\n### Transforms\r\n\r\nAn IterableDataset wraps an ExamplesIterable instance. There are different subclasses depending on the transforms we want to apply:\r\n- ExamplesIterable: the basic one\r\n- MappedExamplesIterable: an iterable with a `map` function applied on the fly\r\n- BufferShuffledExamplesIterable: an iterable with a shuffling buffer\r\n- CyclingMultiSourcesExamplesIterable: alternates between several ExamplesIterable\r\n- RandomlyCyclingMultiSourcesExamplesIterable: randomly alternates between several ExamplesIterable\r\n\r\n### DatasetBuilder\r\n\r\nI use the same builders as usual. I just added a new method `_get_examples_iterable_for_split` to get an ExamplesIterable for a given split. Currently only the GeneratorBasedBuilder and the ArrowBasedBuilder implement it.\r\n\r\nThe BeamBasedBuilder doesn't implement it yet.\r\nIt means that datasets like wikipedia and natural_questions can't be loaded as IterableDataset for now.\r\n\r\n## Other details\r\n\r\n<S>I may have to do some changes in many dataset script to use `download` instead of `download_and_extract` when extraction is not needed. This will avoid errors for streaming.</s>\r\n\r\nEDIT: Actually I just check for the extension of the file to do extraction only if needed.\r\n\r\nEDIT2: It's not possible to stream from .tar.gz files without downloading the file completely. For now I raise an error if one want to get a streaming dataset based on .tar.gz files.\r\n\r\n## TODO\r\n\r\nusual stuff:\r\n\r\n- [x] make streaming dependency \"aiohttp\" optional: `pip install datasets[streaming]`\r\n- [x] tests\r\n- [x] docs",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 894579364,
    "title": "add `desc` to `tqdm` in `Dataset.map()`",
    "dateCreated": "2021-05-18T16:44:29Z",
    "dateModified": "2021-05-18T16:44:29Z",
    "description": "Fixes #2330. Please let me know if anything is also required in this ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 894499909,
    "title": "Loading dataset from local path",
    "dateCreated": "2021-05-18T15:20:50Z",
    "dateModified": "2021-05-18T15:20:50Z",
    "description": "I'm trying to load a local dataset with the code below\r\n\r\n```\r\nds = datasets.load_dataset('my_script.py', \r\n                           data_files='corpus.txt', \r\n                           data_dir='/data/dir', \r\n                           cache_dir='.')\r\n```\r\nBut internally a BuilderConfig is created, which tries to use getmtime on the data_files string, without using data_dir. Is this a bug or am I not using the load_dataset correctly?\r\n\r\nhttps://github.com/huggingface/datasets/blob/bc61954083f74e6460688202e9f77dde2475319c/src/datasets/builder.py#L153",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 894496064,
    "title": "ConvQuestions benchmark added",
    "dateCreated": "2021-05-18T15:16:50Z",
    "dateModified": "2021-05-18T15:16:50Z",
    "description": "Hello,\r\nI would like to integrate our dataset on conversational QA. The answers are grounded in the KG.\r\nThe work was published in CIKM 2019 (https://dl.acm.org/doi/10.1145/3357384.3358016).\r\nWe hope for further research on how to deal with the challenges of factoid conversational QA.\r\nThanks! :)",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 894193403,
    "title": "Align question answering tasks with sub-domains",
    "dateCreated": "2021-05-18T09:47:59Z",
    "dateModified": "2021-05-18T09:47:59Z",
    "description": "As pointed out by @thomwolf in #2255 we should consider breaking with the pipeline taxonomy of `transformers` to account for the various types of question-answering domains:\r\n\r\n> `question-answering` exists in two forms: abstractive and extractive question answering.\r\n> \r\n> we can keep a generic `question-answering` but then it will probably mean diferrent schema of input/output for both (abstractive will have text for both while extractive can use spans indication as well as text).\r\n> \r\n> Or we can also propose to use `abstractive-question-answering` and `extractive-question-answering` for instance.\r\n> Maybe we could have `question-answering-abstractive` and `question-answering-extractive` if somehow we can use a for a completion or search in the future (detail).\r\n> Actually I see that people are more organizing in terms of general and sub-tasks, for instance on paperwithcode: https://paperswithcode.com/area/natural-language-processing and on nlpprogress: https://github.com/sebastianruder/NLP-progress/blob/master/english/question_answering.md#squad\r\n> \r\n> Probably the best is to align with one of these in terms of denomination, PaperWithCode is probably the most active and maintained and we work with them as well.\r\n> Maybe you want to check with a few QA datasets that this schema make sense. Typically NaturalQuestions, TriviaQA and can be good second datasets to compare to and be sure of the generality of the schema.\r\n> \r\n> A good recent list of QA datasets to compare the schemas among, is for instance in the UnitedQA paper: https://arxiv.org/abs/2101.00178\r\n\r\nInvestigate which grouping of QA is best suited for `datasets` and adapt / extend the QA task template accordingly.",
    "status": "open",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 893606432,
    "title": "Adding HendrycksTest dataset",
    "dateCreated": "2021-05-17T18:53:05Z",
    "dateModified": "2021-05-17T18:53:05Z",
    "description": "Adding Hendrycks test from https://arxiv.org/abs/2009.03300.\r\nI'm having a bit of trouble with dummy data creation because some lines in the csv files aren't being loaded properly (only the first entry loaded in a row of length 6). The dataset is loading just fine. Hope you can kindly help!\r\nThank you!",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 893554153,
    "title": "correct labels of conll2003",
    "dateCreated": "2021-05-17T17:37:54Z",
    "dateModified": "2021-05-17T17:37:54Z",
    "description": "# What does this PR\r\n\r\nIt fixes/extends the `ner_tags` for conll2003 to include all. \r\nPaper reference https://arxiv.org/pdf/cs/0306050v1.pdf\r\nModel reference https://huggingface.co/elastic/distilbert-base-cased-finetuned-conll03-english/blob/main/config.json \r\n",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 893411076,
    "title": "Allow \"other-X\" in licenses",
    "dateCreated": "2021-05-17T14:47:54Z",
    "dateModified": "2021-05-17T14:47:54Z",
    "description": "This PR allows \"other-X\" licenses during metadata validation.\r\n\r\n@lhoestq ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 893317427,
    "title": "Remove getchildren from hyperpartisan news detection",
    "dateCreated": "2021-05-17T13:10:37Z",
    "dateModified": "2021-05-17T13:10:37Z",
    "description": "`Element.getchildren()` is now deprecated in the ElementTree library (I think in python 3.9, so it still passes the automated tests which are using 3.6. But for those of us on bleeding-edge distros it now fails).\r\n\r\nhttps://bugs.python.org/issue29209",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 893185266,
    "title": "Json loader fails if user-specified features don't match the json data fields order",
    "dateCreated": "2021-05-17T10:26:08Z",
    "dateModified": "2021-05-17T10:26:08Z",
    "description": "If you do\r\n```python\r\ndataset = load_dataset(\"json\", data_files=data_files, features=features)\r\n```\r\nThen depending on the order of the features in the json data field it fails:\r\n```python\r\n[...]\r\n~/Desktop/hf/datasets/src/datasets/packaged_modules/json/json.py in _generate_tables(self, files)\r\n     94             if self.config.schema:\r\n     95                 # Cast allows str <-> int/float, while parse_option explicit_schema does NOT\r\n---> 96                 pa_table = pa_table.cast(self.config.schema)\r\n     97             yield i, pa_table\r\n[...]\r\nValueError: Target schema's field names are not matching the table's field names: ['tokens', 'ner_tags'], ['ner_tags', 'tokens']\r\n```\r\n\r\nThis is because one must first re-order the columns of the table to match the `self.config.schema` before calling cast.\r\n\r\nOne way to fix the `cast` would be to replace it with:\r\n```python\r\n# reorder the arrays if necessary + cast to schema\r\n# we can't simply use .cast here because we may need to change the order of the columns\r\npa_table = pa.Table.from_arrays([pa_table[name] for name in schema.names], schema=schema)\r\n```",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 893179697,
    "title": "Missing ClassLabel encoding in Json loader",
    "dateCreated": "2021-05-17T10:19:10Z",
    "dateModified": "2021-05-17T10:19:10Z",
    "description": "Currently if you want to load a json dataset this way\r\n```python\r\ndataset = load_dataset(\"json\", data_files=data_files, features=features)\r\n```\r\nThen if your features has ClassLabel types and if your json data needs class label encoding (i.e. if the labels in the json files are strings and not integers), then it would fail:\r\n```python\r\n[...]\r\n~/Desktop/hf/datasets/src/datasets/packaged_modules/json/json.py in _generate_tables(self, files)\r\n     94             if self.config.schema:\r\n     95                 # Cast allows str <-> int/float, while parse_option explicit_schema does NOT\r\n---> 96                 pa_table = pa_table.cast(self.config.schema)\r\n     97             yield i, pa_table\r\n[...]\r\nArrowInvalid: Failed to parse string: 'O' as a scalar of type int64\r\n```\r\n\r\nThis is because it just tries to cast the string data to integers, without applying the mapping str->int first\r\n\r\nThe current workaround is to do instead\r\n```python\r\ndataset = load_dataset(\"json\", data_files=data_files)\r\ndataset = dataset.map(features.encode_example, features=features)\r\n```",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 892420500,
    "title": "README updated for SNLI, MNLI",
    "dateCreated": "2021-05-15T11:37:59Z",
    "dateModified": "2021-05-15T11:37:59Z",
    "description": "Closes #2275. Mentioned about -1 labels in MNLI, SNLI and how they should be removed before training. @lhoestq `check_code_quality` test might fail for MNLI as the license name `other-Open Portion of the American National Corpus`  is not a registered tag for 'licenses'",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 892391232,
    "title": "Trying to use metric.compute but get OSError",
    "dateCreated": "2021-05-15T08:39:06Z",
    "dateModified": "2021-05-15T08:39:06Z",
    "description": "I want to use metric.compute from load_metric('accuracy') to get training accuracy, but receive OSError. I am wondering what is the mechanism behind the metric calculation, why would it report an OSError?\r\n\r\n```python\r\n195     for epoch in range(num_train_epochs):\r\n196         model.train()\r\n197         for step, batch in enumerate(train_loader):\r\n198             # print(batch['input_ids'].shape)\r\n199             outputs = model(**batch)\r\n200\r\n201             loss = outputs.loss\r\n202             loss /= gradient_accumulation_steps\r\n203             accelerator.backward(loss)\r\n204\r\n205             predictions = outputs.logits.argmax(dim=-1)\r\n206             metric.add_batch(\r\n207                 predictions=accelerator.gather(predictions),\r\n208                 references=accelerator.gather(batch['labels'])\r\n209             )\r\n210             progress_bar.set_postfix({'loss': loss.item(), 'train batch acc.': train_metrics})\r\n211\r\n212             if (step + 1) % 50 == 0 or step == len(train_loader) - 1:\r\n213                 train_metrics = metric.compute()\r\n```\r\n\r\nthe error message is as below:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"run_multi.py\", line 273, in <module>\r\n    main()\r\n  File \"/home/yshuang/.local/lib/python3.8/site-packages/click/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/yshuang/.local/lib/python3.8/site-packages/click/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/yshuang/.local/lib/python3.8/site-packages/click/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/yshuang/.local/lib/python3.8/site-packages/click/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"run_multi.py\", line 213, in main\r\n    train_metrics = metric.compute()\r\n  File \"/home/yshuang/.local/lib/python3.8/site-packages/datasets/metric.py\", line 391, in compute\r\n    self._finalize()\r\n  File \"/home/yshuang/.local/lib/python3.8/site-packages/datasets/metric.py\", line 342, in _finalize\r\n    self.writer.finalize()\r\n  File \"/home/yshuang/.local/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 370, in finalize\r\n    self.stream.close()\r\n  File \"pyarrow/io.pxi\", line 132, in pyarrow.lib.NativeFile.close\r\n  File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\nOSError: error closing file\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.6.1\r\n- Platform: Linux NAME=\"Ubuntu\" VERSION=\"20.04.1 LTS (Focal Fossa)\"\r\n- Python version: python3.8.5\r\n- PyArrow version: 4.0.0\r\n",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 892100749,
    "title": "Fix web_nlg metadata",
    "dateCreated": "2021-05-14T17:15:07Z",
    "dateModified": "2021-05-14T17:15:07Z",
    "description": "Our metadata storage system does not support `.` inside keys. cc @Pierrci \r\n\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 891982808,
    "title": "Preserve dtype for numpy/torch/tf/jax arrays",
    "dateCreated": "2021-05-14T14:45:23Z",
    "dateModified": "2021-05-14T14:45:23Z",
    "description": "Fixes #625. This lets the user preserve the dtype of numpy array to pyarrow array which was getting lost due to conversion of numpy array -> list -> pyarrow array. ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 891965964,
    "title": "Automatically detect datasets with compatible task schemas",
    "dateCreated": "2021-05-14T14:23:40Z",
    "dateModified": "2021-05-14T14:23:40Z",
    "description": "See description of #2255 for details.\r\n",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 891946017,
    "title": "Allow model labels to be passed during task preparation",
    "dateCreated": "2021-05-14T13:58:28Z",
    "dateModified": "2021-05-14T13:58:28Z",
    "description": "Models have a config with label2id. And we have the same for datasets with the ClassLabel feature type. At one point either the model or the dataset must sync with the other. It would be great to do that on the dataset side.\r\n\r\nFor example for sentiment classification on amazon reviews with you could have these labels:\r\n- \"1 star\", \"2 stars\", \"3 stars\", \"4 stars\", \"5 stars\"\r\n- \"1\", \"2\", \"3\", \"4\", \"5\"\r\n\r\nSome models may use the first set, while other models use the second set.\r\n\r\nHere in the `TextClassification` class, the user can only specify one set of labels, while many models could actually be compatible but have different sets of labels. Should we allow users to pass a list of compatible labels sets ?\r\n\r\nThen in terms of API, users could use `dataset.prepare_for_task(\"text-classification\", labels=model.labels)` or something like that.\r\n\r\nThe label set could also be the same but not in the same order. For NLI for example, some models use `[\"neutral\", \"entailment\", \"contradiction\"]` and some others use `[\"neutral\", \"contradiction\", \"entailment\"]`, so we should take care of updating the order of the labels in the dataset to match the labels order of the model.\r\n\r\nLet me know what you think ! This can be done in a future PR\r\n\r\n_Originally posted by @lhoestq in https://github.com/huggingface/datasets/pull/2255#discussion_r632412792_",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 891269577,
    "title": "Roman Urdu Stopwords List",
    "dateCreated": "2021-05-13T18:29:27Z",
    "dateModified": "2021-05-13T18:29:27Z",
    "description": "A list of most frequently used Roman Urdu words with different spellings and usages.\r\nThis is a very basic effort to collect some basic stopwords for Roman Urdu to help efforts of analyzing text data in roman Urdu which makes up a huge part of daily internet interaction of Roman-Urdu users.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 890595693,
    "title": "Adding Microsoft CodeXGlue Datasets",
    "dateCreated": "2021-05-13T00:43:01Z",
    "dateModified": "2021-05-13T00:43:01Z",
    "description": "Hi there, this is a new pull request to get the CodeXGlue datasets into the awesome HF datasets lib. Most of the work has been done in this PR #997 by the awesome @madlag. However, that PR has been stale for a while now and so I spoke with @lhoestq about finishing up the final mile and so he told me to open a new PR with the final changes :smile:. \r\n\r\nI believe I've met all of the changes still left in the old PR to do, except for the change to the languages. I believe the READMEs should include the different programming languages used rather than just using the tag \"code\" as when searching for datasets, SE researchers may specifically be looking only for what type of programming language and so being able to quickly filter will be very valuable. Let me know what you think of that or if you still believe it should be the \"code\" tag @lhoestq.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 890511019,
    "title": "How to Add New Metrics Guide",
    "dateCreated": "2021-05-12T21:42:06Z",
    "dateModified": "2021-05-12T21:42:06Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nCurrently there is an absolutely fantastic guide for how to contribute a new dataset to the library. However, there isn't one for adding new metrics.\r\n\r\n**Describe the solution you'd like**\r\nI'd like for a guide in a similar style to the dataset guide for adding metrics. I believe many of the content in the dataset guide such as setup can be easily copied over with minimal changes. Also, from what I've seen with existing metrics, it shouldn't be as complicated, especially in documentation of the metric, mainly just citation and usage. The most complicated part I see would be in automated tests that run the new metrics, but y'all's test suite seem pretty comprehensive, so it might not be that hard.\r\n\r\n**Describe alternatives you've considered**\r\nOne alternative would be just not having the metrics be community generated and so would not need a step by step guide. New metrics would just be proposed as issues and the internal team would take care of them. However, I think it makes more sense to have a step by step guide for contributors to follow.\r\n\r\n**Additional context**\r\nI'd be happy to help with creating this guide as I am very interested in adding software engineering metrics to the library :nerd_face:, the part I would need guidance on would be testing.\r\n\r\nP.S. Love the library and community y'all have built! :hugs: \r\n",
    "status": "open",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 890484408,
    "title": "normalized TOCs and titles in data cards",
    "dateCreated": "2021-05-12T20:59:59Z",
    "dateModified": "2021-05-12T20:59:59Z",
    "description": "I started fixing some of the READMEs that were failing the tests introduced by @gchhablani but then realized that there were some consistent differences between earlier and newer versions of some of the titles (e.g. Data Splits vs Data Splits Sample Size, Supported Tasks vs Supported Tasks and Leaderboards). We also had different versions of the Table of Content\r\n\r\nThis PR normalizes all of them to the newer version",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 890439523,
    "title": "Document DatasetInfo attributes",
    "dateCreated": "2021-05-12T20:01:29Z",
    "dateModified": "2021-05-12T20:01:29Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nAs noted in PR #2255, the attributes of `DatasetInfo` are not documented in the [docs](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=datasetinfo#datasetinfo). It would be nice to do so :)\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 890296262,
    "title": "Update README vallidation rules",
    "dateCreated": "2021-05-12T16:57:26Z",
    "dateModified": "2021-05-12T16:57:26Z",
    "description": "This PR allows unexpected subsections under third-level headings. All except `Contributions`.\r\n\r\n@lhoestq ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 889810100,
    "title": "Set to_json default to JSON lines",
    "dateCreated": "2021-05-12T08:19:25Z",
    "dateModified": "2021-05-12T08:19:25Z",
    "description": "With this PR, the method `Dataset.to_json`:\r\n- is added to the docs\r\n- defaults to JSON lines",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 889584953,
    "title": "simpllify faiss index save",
    "dateCreated": "2021-05-12T03:54:10Z",
    "dateModified": "2021-05-12T03:54:10Z",
    "description": "Fixes #2350\r\n\r\nIn some cases, Faiss GPU index objects do not have neither \"device\" nor \"getDevice\". Possibly this happens when some part of the index is computed on CPU.\r\n\r\nIn particular, this would happen with the index `OPQ16_128,IVF512,PQ32` (issue #2350). I did check it, but it is likely that `OPQ` or `PQ` transforms cause it.\r\n\r\nI propose, instead of using the index object to get the device, to infer it form the `FaissIndex.device` field as it is done in `.add_vectors`. Here we assume that `.device` always corresponds to the index placement and it seems reasonable. ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 889580247,
    "title": "`FaissIndex.save` throws error on GPU",
    "dateCreated": "2021-05-12T03:41:56Z",
    "dateModified": "2021-05-12T03:41:56Z",
    "description": "## Describe the bug\r\n\r\nAfter training an index with a factory string `OPQ16_128,IVF512,PQ32` on GPU, `.save_faiss_index` throws this error.\r\n\r\n```\r\n  File \"index_wikipedia.py\", line 119, in <module>\r\n    data[\"train\"].save_faiss_index(\"text_emb\", index_save_path)\r\n  File \"/home/vlialin/miniconda3/envs/cat/lib/python3.8/site-packages/datasets/search.py\", line 470, in save_faiss_index\r\n    index.save(file)\r\n  File \"/home/vlialin/miniconda3/envs/cat/lib/python3.8/site-packages/datasets/search.py\", line 334, in save\r\n    faiss.write_index(index, str(file))\r\n  File \"/home/vlialin/miniconda3/envs/cat/lib/python3.8/site-packages/faiss/swigfaiss_avx2.py\", line 5654, in write_index\r\n    return _swigfaiss.write_index(*args)\r\nRuntimeError: Error in void faiss::write_index(const faiss::Index*, faiss::IOWriter*) at /root/miniconda3/conda-bld/faiss-pkg_1613235005464/work/faiss/impl/index_write.cpp:453: don't know how to serialize this type of index\r\n```\r\n\r\n## Steps to reproduce the bug\r\n\r\nAny dataset will do, I just selected a familiar one.\r\n\r\n```python\r\nimport numpy as np\r\nimport datasets\r\nINDEX_STR = \"OPQ16_128,IVF512,PQ32\"\r\nINDEX_SAVE_PATH = \"will_not_save.faiss\"\r\n\r\ndata = datasets.load_dataset(\"Fraser/news-category-dataset\", split=f\"train[:10000]\")\r\n\r\ndef encode(item):\r\n    return {\"text_emb\": np.random.randn(768).astype(np.float32)}\r\n\r\ndata = data.map(encode)\r\n\r\ndata.add_faiss_index(column=\"text_emb\", string_factory=INDEX_STR, train_size=10_000, device=0)\r\ndata.save_faiss_index(\"text_emb\", INDEX_SAVE_PATH)\r\n```\r\n\r\n## Expected results\r\nSaving the index\r\n\r\n## Actual results\r\nError in void faiss::write_index(const faiss::Index*, faiss::IOWriter*) ... don't know how to serialize this type of index\r\n\r\n## Environment info\r\n- `datasets` version: 1.6.2\r\n- Platform: Linux-4.15.0-142-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.8\r\n- PyTorch version (GPU?): 1.8.1+cu111 (True)\r\n- Tensorflow version (GPU?): 2.2.0 (False)\r\n- Using GPU in script?: Yes\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n\r\nI will be proposing a fix in a couple of minutes",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 888586018,
    "title": "Update task_ids for Ascent KB",
    "dateCreated": "2021-05-11T20:44:33Z",
    "dateModified": "2021-05-11T20:44:33Z",
    "description": "This \"other-other-knowledge-base\" task is better suited for the dataset.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 887927737,
    "title": "Add tests for dataset cards",
    "dateCreated": "2021-05-11T17:14:27Z",
    "dateModified": "2021-05-11T17:14:27Z",
    "description": "Adding tests for dataset cards\r\n\r\nThis PR will potentially remove the scripts being used for dataset tags and readme validation.\r\n\r\nAdditionally, this will allow testing dataset readmes by providing the name as follows:\r\n\r\n```bash\r\npytest tests/test_dataset_cards.py::test_dataset_tags[fashion_mnist]\r\n```\r\nand\r\n\r\n```bash\r\npytest tests/test_dataset_cards.py::test_readme_content[fashion_mnist]\r\n```\r\nor a combined test as:\r\n\r\n```bash\r\npytest tests/test_dataset_cards.py::test_dataset_card[fashion_mnist]\r\n```\r\n@lhoestq ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 887404868,
    "title": "Add an API to access the language and pretty name of a dataset",
    "dateCreated": "2021-05-11T14:10:08Z",
    "dateModified": "2021-05-11T14:10:08Z",
    "description": "It would be super nice to have an API to get some metadata of the dataset from the name and args passed to `load_dataset`. This way we could programmatically infer the language and the name of a dataset when creating model cards automatically in the Transformers examples scripts.",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 886632114,
    "title": "Add Qasper Dataset",
    "dateCreated": "2021-05-11T09:25:44Z",
    "dateModified": "2021-05-11T09:25:44Z",
    "description": "[Question Answering on Scientific Research Papers](https://allenai.org/project/qasper/home)\r\n\r\nDoing NLP on NLP papers to do NLP \u267b\ufe0f I had to add it~\r\n\r\n- [x] Add README (just gotta fill out some more )\r\n- [x] Dataloader code\r\n- [x] Make dummy dataset\r\n- [x] generate dataset infos\r\n- [x] Tests\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 886586872,
    "title": "[Question] How to move and reuse preprocessed dataset? ",
    "dateCreated": "2021-05-11T09:09:17Z",
    "dateModified": "2021-05-11T09:09:17Z",
    "description": "Hi, I am training a gpt-2 from scratch using run_clm.py.\r\n\r\nI want to move and reuse the preprocessed dataset (It take 2 hour to preprocess),\r\n\r\nI tried to :\r\n\r\ncopy path_to_cache_dir/datasets to new_cache_dir/datasets\r\nset export HF_DATASETS_CACHE=\"new_cache_dir/\"\r\nbut the program still re-preprocess the whole dataset without loading cache.\r\n\r\nI also tried to torch.save(lm_datasets, fw), but the saved file is only 14M.\r\n\r\nWhat is the proper way to do this?",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 885331505,
    "title": "Is there a way to join multiple datasets in one?",
    "dateCreated": "2021-05-10T23:16:10Z",
    "dateModified": "2021-05-10T23:16:10Z",
    "description": "**Is your feature request related to a problem? Please describe.**\nI need to join 2 datasets, one that is in the hub and another I've created from my files. Is there an easy way to join these 2? \n\n**Describe the solution you'd like**\nId like to join them with a merge or join method, just like pandas dataframes. \n\n**Additional context**\nIf you want to extend an existing dataset with more data, for example for training a language model, you need that functionality. I've not found it in the documentation.",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 883208539,
    "title": "Columns are removed before or after map function applied?",
    "dateCreated": "2021-05-10T02:36:20Z",
    "dateModified": "2021-05-10T02:36:20Z",
    "description": "## Describe the bug\r\nAccording to the documentation when applying map function the [remove_columns ](https://huggingface.co/docs/datasets/processing.html#removing-columns) will be removed after they are passed to the function, but in the [source code](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) it's documented that they are removed before applying function. I thinks the source code doc is more accurate, right?\r\n\r\n\r\n\r\n",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 882981420,
    "title": "Docs - CER above 1",
    "dateCreated": "2021-05-09T23:41:00Z",
    "dateModified": "2021-05-09T23:41:00Z",
    "description": "CER can actually be greater than 1.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 882370933,
    "title": "Added the Ascent KB",
    "dateCreated": "2021-05-09T14:17:39Z",
    "dateModified": "2021-05-09T14:17:39Z",
    "description": "Added the Ascent Commonsense KB of 8.9M assertions.\r\n\r\n- Paper: [Advanced Semantics for Commonsense Knowledge Extraction (WWW'21)](https://arxiv.org/abs/2011.00905)\r\n- Website: https://ascent.mpi-inf.mpg.de/\r\n\r\n(I am the author of the dataset)",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 882370824,
    "title": "More consistent copy logic",
    "dateCreated": "2021-05-09T14:17:33Z",
    "dateModified": "2021-05-09T14:17:33Z",
    "description": "Use `info.copy()` instead of `copy.deepcopy(info)`.\r\n`Features.copy` now creates a deep copy.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 882046077,
    "title": "fixed download link for web_science",
    "dateCreated": "2021-05-09T09:12:20Z",
    "dateModified": "2021-05-09T09:12:20Z",
    "description": "Fixes #2337. Should work with:\r\n`dataset = load_dataset(\"web_of_science\", \"WOS11967\", ignore_verifications=True)`",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 881610567,
    "title": "NonMatchingChecksumError for web_of_science dataset",
    "dateCreated": "2021-05-09T02:02:02Z",
    "dateModified": "2021-05-09T02:02:02Z",
    "description": "NonMatchingChecksumError when trying to download the web_of_science dataset. \r\n\r\n>NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://data.mendeley.com/datasets/9rw3vkcfy4/6/files/c9ea673d-5542-44c0-ab7b-f1311f7d61df/WebOfScience.zip?dl=1']\r\n\r\nSetting `ignore_verfications=True` results in OSError.\r\n\r\n>OSError: Cannot find data file. \r\nOriginal error:\r\n[Errno 20] Not a directory: '/root/.cache/huggingface/datasets/downloads/37ab2c42f50d553c1d0ea432baca3e9e11fedea4aeec63a81e6b7e25dd10d4e7/WOS5736/X.txt'\r\n\r\n```python\r\ndataset = load_dataset('web_of_science', 'WOS5736')\r\n```\r\nThere are 3 data instances and they all don't work. 'WOS5736', 'WOS11967', 'WOS46985'\r\n\r\ndatasets 1.6.2\r\npython 3.7.10\r\nUbuntu 18.04.5 LTS",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 881298783,
    "title": "Fix overflow issue in interpolation search",
    "dateCreated": "2021-05-08T20:51:36Z",
    "dateModified": "2021-05-08T20:51:36Z",
    "description": "Fixes #2335 \r\n\r\nMore info about this error can be found [here](https://stackoverflow.com/questions/53239890/why-do-i-keep-getting-this-error-runtimewarning-overflow-encountered-in-int-sc/53240100). ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 881291887,
    "title": "Index error in Dataset.map",
    "dateCreated": "2021-05-08T20:44:57Z",
    "dateModified": "2021-05-08T20:44:57Z",
    "description": "The following code, if executed on master, raises an IndexError (due to overflow):\r\n```python\r\n>>> from datasets import *\r\n>>> d = load_dataset(\"bookcorpus\", split=\"train\")\r\nReusing dataset bookcorpus (C:\\Users\\Mario\\.cache\\huggingface\\datasets\\bookcorpus\\plain_text\\1.0.0\\44662c4a114441c35200992bea923b170e6f13f2f0beb7c14e43759cec498700)\r\n2021-05-08 21:23:46.859818: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n>>> d.map(lambda ex: ex)\r\n  0%|\u258e                                                                    | 289430/74004228 [00:13<58:41, 20935.33ex/s]c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\table.py:84: RuntimeWarning: overflow encountered in int_scalars\r\n  k = i + ((j - i) * (x - arr[i]) // (arr[j] - arr[i]))\r\n  0%|\u258e                                                                    | 290162/74004228 [00:13<59:11, 20757.23ex/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\arrow_dataset.py\", line 1498, in map\r\n    new_fingerprint=new_fingerprint,\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\arrow_dataset.py\", line 174, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\fingerprint.py\", line 340, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\arrow_dataset.py\", line 1799, in _map_single\r\n    for i, example in enumerate(pbar):\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\site-packages\\tqdm\\std.py\", line 1133, in __iter__\r\n    for obj in iterable:\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\arrow_dataset.py\", line 1145, in __iter__\r\n    format_kwargs=format_kwargs,\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\arrow_dataset.py\", line 1337, in _getitem\r\n    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\formatting\\formatting.py\", line 368, in query_table\r\n    pa_subtable = _query_table(table, key)\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\formatting\\formatting.py\", line 79, in _query_table\r\n    return table.fast_slice(key % table.num_rows, 1)\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\table.py\", line 128, in fast_slice\r\n    i = _interpolation_search(self._offsets, offset)\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\table.py\", line 91, in _interpolation_search\r\n    raise IndexError(f\"Invalid query '{x}' for size {arr[-1] if len(arr) else 'none'}.\")\r\nIndexError: Invalid query '290162' for size 74004228.\r\n```\r\nTested on Windows, can run on Linux if needed.\r\n\r\nEDIT:\r\nIt seems like for this to happen, the default NumPy dtype has to be np.int32.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 879810107,
    "title": "Updating the DART file checksums in GEM",
    "dateCreated": "2021-05-07T21:53:44Z",
    "dateModified": "2021-05-07T21:53:44Z",
    "description": "The DART files were just updated on the source GitHub\r\n\r\nhttps://github.com/Yale-LILY/dart/commit/34b3c872da4811523e334f1631e54ca8105dffab",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 879214067,
    "title": "Fix duplicate keys",
    "dateCreated": "2021-05-07T15:28:08Z",
    "dateModified": "2021-05-07T15:28:08Z",
    "description": "As noticed in https://github.com/huggingface/datasets/pull/2245, many datasets yield duplicate keys.\r\nMost of the time it was because the counter used for ids were reset at each new data file.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 879041608,
    "title": "Add note about indices mapping in save_to_disk docstring",
    "dateCreated": "2021-05-07T13:49:42Z",
    "dateModified": "2021-05-07T13:49:42Z",
    "description": "",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 879031427,
    "title": "Add Topical-Chat",
    "dateCreated": "2021-05-07T13:43:59Z",
    "dateModified": "2021-05-07T13:43:59Z",
    "description": "## Adding a Dataset\r\n- **Name:** Topical-Chat\r\n- **Description:** a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don\u2019t have explicitly defined roles\r\n- **Paper:** https://www.isca-speech.org/archive/Interspeech_2019/pdfs/3079.pdf\r\n- **Data:** https://github.com/alexa/Topical-Chat\r\n- **Motivation:** Good quality, knowledge-grounded dataset that spans a broad range of topics\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 878490927,
    "title": "Allow passing `desc` to `tqdm` in `Dataset.map()`",
    "dateCreated": "2021-05-07T05:52:54Z",
    "dateModified": "2021-05-07T05:52:54Z",
    "description": "It's normal to have many `map()` calls, and some of them can take a few minutes,\r\nit would be nice to have a description on the progress bar.\r\n\r\nAlternative solution:\r\nPrint the description before/after the `map()` call.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 877924198,
    "title": "Add cache dir for in-memory datasets",
    "dateCreated": "2021-05-06T19:35:32Z",
    "dateModified": "2021-05-06T19:35:32Z",
    "description": "Adds the cache dir attribute to DatasetInfo as suggested by @lhoestq.\r\n\r\nShould fix #2322 ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 877673896,
    "title": "Add Matthews/Pearson/Spearman correlation metrics",
    "dateCreated": "2021-05-06T16:09:27Z",
    "dateModified": "2021-05-06T16:09:27Z",
    "description": "Added three metrics:\r\n- The Matthews correlation coefficient (from sklearn)\r\n- The Pearson correlation coefficient (from scipy)\r\n- The Spearman correlation coefficient (from scipy)\r\n\r\ncc @sgugger ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 877565831,
    "title": "A syntax error in example",
    "dateCreated": "2021-05-06T14:34:44Z",
    "dateModified": "2021-05-06T14:34:44Z",
    "description": "![image](https://user-images.githubusercontent.com/6883957/117315905-b47a5c00-aeba-11eb-91eb-b2a4a0212a56.png)\r\n\r\nSorry to report with an image, I can't find the template source code of this snippet.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 876829254,
    "title": "Enable auto-download for PAN-X / Wikiann domain in XTREME",
    "dateCreated": "2021-05-05T20:58:38Z",
    "dateModified": "2021-05-05T20:58:38Z",
    "description": "This PR replaces the manual download of the `PAN-X.lang` domains with an auto-download from a Dropbox link provided by the Wikiann author. We also add the relevant dummy data for these domains.\r\n\r\nWhile re-generating `dataset_infos.json` I ran into a `KeyError` in the `udpos.Arabic` domain so have included a fix for this as well.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 876653121,
    "title": "Added the HLGD dataset",
    "dateCreated": "2021-05-05T16:53:29Z",
    "dateModified": "2021-05-05T16:53:29Z",
    "description": "Added the Headline Grouping Dataset (HLGD), from the NAACL2021 paper: News Headline Grouping as a Challenging NLU Task\r\nDataset Link: https://github.com/tingofurro/headline_grouping\r\nPaper link: https://people.eecs.berkeley.edu/~phillab/pdfs/NAACL2021_HLG.pdf",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 876602064,
    "title": "Create Audio feature",
    "dateCreated": "2021-05-05T15:55:22Z",
    "dateModified": "2021-05-05T15:55:22Z",
    "description": "Create `Audio` feature to handle raw audio files.\r\n\r\nSome decisions to be further discussed:\r\n- I have chosen `soundfile` as the audio library; another interesting library is `librosa`, but this requires `soundfile` (see [here](https://github.com/librosa/librosa/blob/main/setup.cfg#L53)). If we require some more advanced functionalities, we could eventually switch the library.\r\n- I have implemented the audio feature as an extra: `pip install datasets[audio]`. For the moment, the typical datasets user uses only text datasets, and there is no need for them for additional package requirements for audio/image if they do not need them.\r\n- For tests, I require audio dependencies (so that all audio functionalities are checked with our CI test suite); I exclude Linux platforms, which require an additional library to be installed with the distribution package manager\r\n  - I also require `pytest-datadir`, which allow to have (audio) data files for tests\r\n- The audio data contain: array and sample_rate.\r\n- The array is reshaped as 1D array (expected input for `Wav2Vec2`).\r\n\r\nNote that to install `soundfile` on Linux, you need to install `libsndfile` using your distribution\u2019s package manager, for example `sudo apt-get install libsndfile1`.",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 876438507,
    "title": "load_dataset(\"timit_asr\") gives back duplicates of just one sample text",
    "dateCreated": "2021-05-05T13:14:48Z",
    "dateModified": "2021-05-05T13:14:48Z",
    "description": "## Describe the bug\r\nWhen you look up on key [\"train\"] and then ['text'], you get back a list  with just one sentence duplicated 4620 times. Namely, the sentence \"Would such an act of refusal be useful?\". Similarly when you look up ['test'] and then ['text'], the list is one sentence repeated \"The bungalow was pleasantly situated near the shore.\" 1680 times. \r\n\r\nI tried to work around the issue by downgrading to datasets version 1.3.0, inspired by [this post](https://www.gitmemory.com/issue/huggingface/datasets/2052/798904836) and removing the entire huggingface directory from ~/.cache, but I still get the same issue.  \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ntimit = load_dataset(\"timit_asr\")\r\nprint(timit['train']['text'])\r\nprint(timit['test']['text'])\r\n```\r\n\r\n## Expected Result\r\nRows of diverse text, like how it is shown in the [wav2vec2.0 tutorial](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tuning_Wav2Vec2_for_English_ASR.ipynb)\r\n<img width=\"485\" alt=\"Screen Shot 2021-05-05 at 9 09 57 AM\" src=\"https://user-images.githubusercontent.com/33647474/117146094-d9b77f00-ad81-11eb-8306-f281850c127a.png\">\r\n\r\n\r\n## Actual results\r\nRows of repeated text.\r\n<img width=\"319\" alt=\"Screen Shot 2021-05-05 at 9 11 53 AM\" src=\"https://user-images.githubusercontent.com/33647474/117146231-f8b61100-ad81-11eb-834a-fc10410b0c9c.png\">\r\n\r\n\r\n## Versions\r\n- Datasets: 1.3.0\r\n- Python: 3.9.1\r\n- Platform: macOS-11.2.1-x86_64-i386-64bit}\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 876383853,
    "title": "Calls to map are not cached.",
    "dateCreated": "2021-05-05T12:11:27Z",
    "dateModified": "2021-05-05T12:11:27Z",
    "description": "## Describe the bug\r\nSomehow caching does not work for me anymore. Am I doing something wrong, or is there anything that I missed?\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n\r\nimport datasets\r\ndatasets.set_caching_enabled(True)\r\nsst = datasets.load_dataset(\"sst\")\r\n\r\ndef foo(samples, i):\r\n    print(\"executed\", i[:10])\r\n    return samples\r\n\r\n# first call\r\nx = sst.map(foo, batched=True, with_indices=True,  num_proc=2)\r\n\r\nprint('\\n'*3, \"#\" * 30, '\\n'*3)\r\n\r\n# second call\r\ny = sst.map(foo, batched=True, with_indices=True, num_proc=2)\r\n\r\n# print version\r\nimport sys\r\nimport platform\r\nprint(f\"\"\"\r\n- Datasets: {datasets.__version__}\r\n- Python: {sys.version}\r\n- Platform: {platform.platform()}\r\n\"\"\")\r\n```\r\n\r\n## Actual results\r\nThis code prints the following output for me:\r\n```bash\r\nNo config specified, defaulting to: sst/default\r\nReusing dataset sst (/home/johannes/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\r\n#0:   0%|          | 0/5 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/5 [00:00<?, ?ba/s]\r\nexecuted [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\nexecuted [4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281]\r\nexecuted [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]\r\nexecuted [5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281]\r\nexecuted [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009]\r\nexecuted [6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281]\r\nexecuted [3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009]\r\nexecuted [7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281]\r\nexecuted [4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009]\r\n#0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 59.85ba/s]\r\nexecuted [8272, 8273, 8274, 8275, 8276, 8277, 8278, 8279, 8280, 8281]\r\n#1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 60.85ba/s]\r\n#0:   0%|          | 0/1 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/1 [00:00<?, ?ba/s]executed [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n#0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 69.32ba/s]\r\nexecuted [551, 552, 553, 554, 555, 556, 557, 558, 559, 560]\r\n#1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 70.93ba/s]\r\n#0:   0%|          | 0/2 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/2 [00:00<?, ?ba/s]executed [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\nexecuted [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]\r\n#0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 63.25ba/s]\r\nexecuted [1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114]\r\nexecuted [2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114]\r\n#1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 57.69ba/s]\r\n\r\n\r\n\r\n ############################## \r\n\r\n\r\n\r\n#0:   0%|          | 0/5 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/5 [00:00<?, ?ba/s]\r\nexecuted [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\nexecuted [4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281]\r\nexecuted [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]\r\nexecuted [5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281]\r\nexecuted [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009]\r\nexecuted [6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281]\r\nexecuted [3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009]\r\nexecuted [4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009]\r\n#0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 58.10ba/s]\r\nexecuted [7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281]\r\nexecuted [8272, 8273, 8274, 8275, 8276, 8277, 8278, 8279, 8280, 8281]\r\n#1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 57.19ba/s]\r\n#0:   0%|          | 0/1 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/1 [00:00<?, ?ba/s]\r\nexecuted [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n#0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 60.10ba/s]\r\nexecuted [551, 552, 553, 554, 555, 556, 557, 558, 559, 560]\r\n#1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 53.82ba/s]\r\n#0:   0%|          | 0/2 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/2 [00:00<?, ?ba/s]\r\nexecuted [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\nexecuted [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]\r\nexecuted [1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114]\r\n#0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 72.76ba/s]\r\nexecuted [2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114]\r\n#1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 71.55ba/s]\r\n\r\n- Datasets: 1.6.1\r\n- Python: 3.8.3 (default, May 19 2020, 18:47:26) \r\n[GCC 7.3.0]\r\n- Platform: Linux-5.4.0-72-generic-x86_64-with-glibc2.10\r\n```\r\n\r\n## Expected results\r\nCaching should work.\r\n\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 876304364,
    "title": "Set encoding in OSCAR dataset",
    "dateCreated": "2021-05-05T10:27:03Z",
    "dateModified": "2021-05-05T10:27:03Z",
    "description": "Set explicit `utf-8` encoding in OSCAR dataset, to avoid using the system default `cp1252` on Windows platforms.\r\n\r\nFix #2319.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 876257026,
    "title": "Set default name in init_dynamic_modules",
    "dateCreated": "2021-05-05T09:30:03Z",
    "dateModified": "2021-05-05T09:30:03Z",
    "description": "Set default value for the name of dynamic modules.\r\n\r\nClose #2318. ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 876251376,
    "title": "UnicodeDecodeError for OSCAR (Afrikaans)",
    "dateCreated": "2021-05-05T09:22:52Z",
    "dateModified": "2021-05-05T09:22:52Z",
    "description": "## Describe the bug\r\nWhen loading the [OSCAR dataset](https://huggingface.co/datasets/oscar) (specifically `unshuffled_deduplicated_af`), I encounter a `UnicodeDecodeError`.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_af\")\r\n```\r\n\r\n## Expected results\r\nAnything but an error, really.\r\n\r\n## Actual results\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_af\")\r\nDownloading: 14.7kB [00:00, 4.91MB/s]\r\nDownloading: 3.07MB [00:00, 32.6MB/s]\r\nDownloading and preparing dataset oscar/unshuffled_deduplicated_af (download: 62.93 MiB, generated: 163.38 MiB, post-processed: Unknown size, total: 226.32 MiB) to C:\\Users\\sgraaf\\.cache\\huggingface\\datasets\\oscar\\unshuffled_deduplicated_af\\1.0.0\\bd4f96df5b4512007ef9fd17bbc1ecde459fa53d2fc0049cf99392ba2efcc464...\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81.0/81.0 [00:00<00:00, 40.5kB/s]\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 66.0M/66.0M [00:18<00:00, 3.50MB/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\sgraaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\load.py\", line 745, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"C:\\Users\\sgraaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\builder.py\", line 574, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"C:\\Users\\sgraaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\builder.py\", line 652, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"C:\\Users\\sgraaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\builder.py\", line 979, in _prepare_split\r\n    for key, record in utils.tqdm(\r\n  File \"C:\\Users\\sgraaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\std.py\", line 1133, in __iter__\r\n    for obj in iterable:\r\n  File \"C:\\Users\\sgraaf\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\oscar\\bd4f96df5b4512007ef9fd17bbc1ecde459fa53d2fc0049cf99392ba2efcc464\\oscar.py\", line 359, in _generate_examples\r\n    for line in f:\r\n  File \"C:\\Users\\sgraaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\encodings\\cp1252.py\", line 23, in decode\r\n    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\r\nUnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 7454: character maps to <undefined>\r\n```\r\n\r\n## Versions\r\nPaste the output of the following code:\r\n```python\r\nimport datasets\r\nimport sys\r\nimport platform\r\n\r\nprint(f\"\"\"\r\n- Datasets: {datasets.__version__}\r\n- Python: {sys.version}\r\n- Platform: {platform.platform()}\r\n\"\"\")\r\n```\r\n- Datasets: 1.6.2\r\n- Python: 3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]\r\n- Platform: Windows-10-10.0.19041-SP0",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 876212460,
    "title": "[api request] API to obtain \"dataset_module\" dynamic path?",
    "dateCreated": "2021-05-05T08:40:48Z",
    "dateModified": "2021-05-05T08:40:48Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is.\r\n\r\nThis is an awesome library. \r\n\r\nIt seems like the dynamic module path in this library has broken some of hyperparameter tuning functionality: https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/34\r\n\r\nThis is because Ray will spawn new processes, and each process will load modules by path. However, we need to explicitly inform Ray to load the right modules, or else it will error upon import. \r\n\r\nI'd like an API to obtain the dynamic paths. This will allow us to support this functionality in this awesome library while being future proof.\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\n`datasets.get_dynamic_paths -> List[str]` will be sufficient for my use case.\r\n\r\nBy offering this API, we will be able to address the following issues (by patching the ray integration sufficiently):\r\n\r\nhttps://github.com/huggingface/blog/issues/106\r\nhttps://github.com/huggingface/transformers/issues/11565\r\nhttps://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/34\r\nhttps://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/35\r\n\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 875767318,
    "title": "Fix incorrect version specification for the pyarrow package",
    "dateCreated": "2021-05-04T19:30:20Z",
    "dateModified": "2021-05-04T19:30:20Z",
    "description": "This PR addresses the bug in the pyarrow version specification, which is detailed in #2316 .\r\nSimply, I put a comma between the version bounds.\r\n\r\nFix #2316.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 875756353,
    "title": "Incorrect version specification for pyarrow",
    "dateCreated": "2021-05-04T19:15:11Z",
    "dateModified": "2021-05-04T19:15:11Z",
    "description": "## Describe the bug\r\nThe pyarrow dependency is incorrectly specified in setup.py file, in [this line](https://github.com/huggingface/datasets/blob/3a3e5a4da20bfcd75f8b6a6869b240af8feccc12/setup.py#L77).\r\nAlso as a snippet:\r\n```python\r\n \"pyarrow>=1.0.0<4.0.0\",\r\n```\r\n\r\n## Steps to reproduce the bug\r\n```bash\r\n pip install \"pyarrow>=1.0.0<4.0.0\"\r\n```\r\n\r\n## Expected results\r\nIt is expected to get a pyarrow version between 1.0.0 (inclusive) and 4.0.0 (exclusive).\r\n\r\n## Actual results\r\npip ignores the specified versions since there is a missing comma between the lower and upper limits. Therefore, pip installs the latest pyarrow version from PYPI, which is 4.0.0.\r\nThis is especially problematic since \"conda env export\" fails due to incorrect version specification. Here is the conda error as well:\r\n```bash\r\nconda env export\r\nInvalidVersionSpec: Invalid version '1.0.0<4.0.0': invalid character(s)\r\n```\r\n\r\n\r\n## Fix suggestion\r\nPut a comma between the version limits which means replacing the line in setup.py file with the following:\r\n```python\r\n \"pyarrow>=1.0.0,<4.0.0\",\r\n```\r\n\r\n## Versions\r\nPaste the output of the following code:\r\n```python\r\n- Datasets: 1.6.2\r\n- Python: 3.7.10 (default, Feb 26 2021, 18:47:35) \r\n[GCC 7.3.0]\r\n- Platform: Linux-5.4.0-42-generic-x86_64-with-debian-buster-sid\r\n```\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 875742200,
    "title": "Datasets cli improvements",
    "dateCreated": "2021-05-04T18:55:11Z",
    "dateModified": "2021-05-04T18:55:11Z",
    "description": "This PR:\r\n* replaces the code from the `bug_report.md` that was used to get relevant system info with a dedicated command (a more elegant approach than copy-pasting the code IMO)\r\n* removes the `download` command (copied from the transformers repo?)\r\n* adds missing help messages to the cli commands\r\n\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 875729271,
    "title": "Minor refactor prepare_module",
    "dateCreated": "2021-05-04T18:37:26Z",
    "dateModified": "2021-05-04T18:37:26Z",
    "description": "Start to refactor `prepare_module` to try to decouple functionality.\r\n\r\nThis PR does:\r\n- extract function `_initialize_dynamic_modules_namespace_package`\r\n- extract function `_find_module_in_github_or_s3`\r\n- some renaming of variables\r\n- use of f-strings",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 875475367,
    "title": "Remove unused head_hf_s3 function",
    "dateCreated": "2021-05-04T13:42:06Z",
    "dateModified": "2021-05-04T13:42:06Z",
    "description": "Currently, the function `head_hf_s3` is not used:\r\n- neither its returned result is used\r\n- nor it raises any exception, as exceptions are catched and returned (not raised)\r\n\r\nThis PR removes it.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 875435726,
    "title": "Add rename_columnS method",
    "dateCreated": "2021-05-04T12:57:53Z",
    "dateModified": "2021-05-04T12:57:53Z",
    "description": "Cherry-picked from #2255 ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 875262208,
    "title": "Add SLR52, SLR53 and SLR54 to OpenSLR",
    "dateCreated": "2021-05-04T09:08:03Z",
    "dateModified": "2021-05-04T09:08:03Z",
    "description": "Add large speech datasets for Sinhala, Bengali and Nepali.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 875096051,
    "title": "Update README.md",
    "dateCreated": "2021-05-04T04:38:01Z",
    "dateModified": "2021-05-04T04:38:01Z",
    "description": "Provides description of data instances and dataset features",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 874644990,
    "title": "Fix conda release",
    "dateCreated": "2021-05-03T14:52:59Z",
    "dateModified": "2021-05-03T14:52:59Z",
    "description": "There were a few issues with conda releases (they've been failing for a while now).\r\nTo fix this I had to:\r\n- add the --single-version-externally-managed tag to the build stage (suggestion from [here](https://stackoverflow.com/a/64825075))\r\n- set the python version of the conda build stage to 3.8 since 3.9 isn't supported\r\n- sync the evrsion requirement of `huggingface_hub`\r\n\r\nWith these changes I'm working on uploading all missing versions until 1.6.2 to conda\r\n\r\nEDIT: I managed to build and upload all missing versions until 1.6.2 to conda :)",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 874559846,
    "title": "Add COCO evaluation metrics",
    "dateCreated": "2021-05-03T13:08:05Z",
    "dateModified": "2021-05-03T13:08:05Z",
    "description": "I'm currently working on adding Facebook AI's DETR model (end-to-end object detection with Transformers) to HuggingFace Transformers. The model is working fine, but regarding evaluation, I'm currently relying on external `CocoEvaluator` and `PanopticEvaluator` objects which are defined in the original repository ([here](https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/datasets/coco_eval.py#L22) and [here](https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/datasets/panoptic_eval.py#L13) respectively). \r\n\r\nRunning these in a notebook gives you nice summaries like this:\r\n![image](https://user-images.githubusercontent.com/48327001/116878842-326f0680-ac20-11eb-9061-d6da02193694.png)\r\n\r\nIt would be great if we could import these metrics from the Datasets library, something like this:\r\n\r\n```\r\nimport datasets\r\n\r\nmetric = datasets.load_metric('coco')\r\n\r\nfor model_input, gold_references in evaluation_dataset:\r\n    model_predictions = model(model_inputs)\r\n    metric.add_batch(predictions=model_predictions, references=gold_references)\r\n\r\nfinal_score = metric.compute()\r\n```\r\n\r\nI think this would be great for object detection and semantic/panoptic segmentation in general, not just for DETR. Reproducing results of object detection papers would be way easier.\r\n\r\nHowever, object detection and panoptic segmentation evaluation is a bit more complex than accuracy (it's more like a summary of metrics at different thresholds rather than a single one). I'm not sure how to proceed here, but happy to help making this possible.\r\n\r\n\r\n\r\n",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 873961435,
    "title": "Add SubjQA dataset",
    "dateCreated": "2021-05-02T14:51:20Z",
    "dateModified": "2021-05-02T14:51:20Z",
    "description": "Hello datasetters \ud83d\ude42!\r\n\r\nHere's an interesting dataset about extractive question-answering on _subjective_ product / restaurant reviews. It's quite challenging for models fine-tuned on SQuAD and provides a nice example of domain adaptation (i.e. fine-tuning a SQuAD model on this domain gives better performance).\r\n\r\nI found a bug in the start/end indices that I've proposed a fix for here: https://github.com/megagonlabs/SubjQA/pull/2\r\n\r\nUnfortunately, the dataset creators are unresponsive, so for now I am using my fork as the source. Will update the URL if/when the creators respond.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 873941266,
    "title": "Unable to setup dev env on Windows",
    "dateCreated": "2021-05-02T13:20:42Z",
    "dateModified": "2021-05-02T13:20:42Z",
    "description": "Hi\r\n\r\nI tried installing the `\".[dev]\"` version on Windows 10 after cloning.\r\n\r\nHere is the error I'm facing:\r\n\r\n```bat\r\n(env) C:\\testing\\datasets>pip install -e \".[dev]\"\r\nObtaining file:///C:/testing/datasets\r\nRequirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (1.19.5)\r\nCollecting pyarrow>=0.17.1\r\n  Using cached pyarrow-4.0.0-cp37-cp37m-win_amd64.whl (13.3 MB)\r\nRequirement already satisfied: dill in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (0.3.1.1)\r\nCollecting pandas\r\n  Using cached pandas-1.2.4-cp37-cp37m-win_amd64.whl (9.1 MB)\r\nRequirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (2.25.1)\r\nRequirement already satisfied: tqdm<4.50.0,>=4.27 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (4.49.0)\r\nRequirement already satisfied: xxhash in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (2.0.2)\r\nCollecting multiprocess\r\n  Using cached multiprocess-0.70.11.1-py37-none-any.whl (108 kB)\r\nRequirement already satisfied: fsspec in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (2021.4.0)\r\nCollecting huggingface_hub<0.1.0\r\n  Using cached huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\r\nRequirement already satisfied: importlib_metadata in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (4.0.1)\r\nRequirement already satisfied: absl-py in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (0.12.0)\r\nRequirement already satisfied: pytest in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (6.2.3)\r\nCollecting pytest-xdist\r\n  Using cached pytest_xdist-2.2.1-py3-none-any.whl (37 kB)\r\nCollecting apache-beam>=2.24.0\r\n  Using cached apache_beam-2.29.0-cp37-cp37m-win_amd64.whl (3.7 MB)\r\nCollecting elasticsearch\r\n  Using cached elasticsearch-7.12.1-py2.py3-none-any.whl (339 kB)\r\nRequirement already satisfied: boto3==1.16.43 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (1.16.43)\r\nRequirement already satisfied: botocore==1.19.43 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (1.19.43)\r\nCollecting moto[s3]==1.3.16\r\n  Using cached moto-1.3.16-py2.py3-none-any.whl (879 kB)\r\nCollecting rarfile>=4.0\r\n  Using cached rarfile-4.0-py3-none-any.whl (28 kB)\r\nCollecting tensorflow>=2.3\r\n  Using cached tensorflow-2.4.1-cp37-cp37m-win_amd64.whl (370.7 MB)\r\nRequirement already satisfied: torch in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (1.8.1)\r\nRequirement already satisfied: transformers in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (4.5.1)\r\nCollecting bs4\r\n  Using cached bs4-0.0.1-py3-none-any.whl\r\nCollecting conllu\r\n  Using cached conllu-4.4-py2.py3-none-any.whl (15 kB)\r\nCollecting langdetect\r\n  Using cached langdetect-1.0.8-py3-none-any.whl\r\nCollecting lxml\r\n  Using cached lxml-4.6.3-cp37-cp37m-win_amd64.whl (3.5 MB)\r\nCollecting mwparserfromhell\r\n  Using cached mwparserfromhell-0.6-cp37-cp37m-win_amd64.whl (101 kB)\r\nCollecting nltk\r\n  Using cached nltk-3.6.2-py3-none-any.whl (1.5 MB)\r\nCollecting openpyxl\r\n  Using cached openpyxl-3.0.7-py2.py3-none-any.whl (243 kB)\r\nCollecting py7zr\r\n  Using cached py7zr-0.15.2-py3-none-any.whl (66 kB)\r\nCollecting tldextract\r\n  Using cached tldextract-3.1.0-py2.py3-none-any.whl (87 kB)\r\nCollecting zstandard\r\n  Using cached zstandard-0.15.2-cp37-cp37m-win_amd64.whl (582 kB)\r\nCollecting bert_score>=0.3.6\r\n  Using cached bert_score-0.3.9-py3-none-any.whl (59 kB)\r\nCollecting rouge_score\r\n  Using cached rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\r\nCollecting sacrebleu\r\n  Using cached sacrebleu-1.5.1-py3-none-any.whl (54 kB)\r\nRequirement already satisfied: scipy in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (1.6.3)\r\nCollecting seqeval\r\n  Using cached seqeval-1.2.2-py3-none-any.whl\r\nCollecting sklearn\r\n  Using cached sklearn-0.0-py2.py3-none-any.whl\r\nCollecting jiwer\r\n  Using cached jiwer-2.2.0-py3-none-any.whl (13 kB)\r\nRequirement already satisfied: toml>=0.10.1 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (0.10.2)\r\nRequirement already satisfied: requests_file>=1.5.1 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (1.5.1)\r\nRequirement already satisfied: texttable>=1.6.3 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (1.6.3)\r\nRequirement already satisfied: s3fs>=0.4.2 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (0.4.2)\r\nRequirement already satisfied: Werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from datasets==1.5.0.dev0) (1.0.1)\r\nCollecting black\r\n  Using cached black-21.4b2-py3-none-any.whl (130 kB)\r\nCollecting isort\r\n  Using cached isort-5.8.0-py3-none-any.whl (103 kB)\r\nCollecting flake8==3.7.9\r\n  Using cached flake8-3.7.9-py2.py3-none-any.whl (69 kB)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from boto3==1.16.43->datasets==1.5.0.dev0) (0.10.0)\r\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from boto3==1.16.43->datasets==1.5.0.dev0) (0.3.7)\r\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from botocore==1.19.43->datasets==1.5.0.dev0) (1.26.4)\r\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from botocore==1.19.43->datasets==1.5.0.dev0) (2.8.1)\r\nCollecting entrypoints<0.4.0,>=0.3.0\r\n  Using cached entrypoints-0.3-py2.py3-none-any.whl (11 kB)\r\nCollecting pyflakes<2.2.0,>=2.1.0\r\n  Using cached pyflakes-2.1.1-py2.py3-none-any.whl (59 kB)\r\nCollecting pycodestyle<2.6.0,>=2.5.0\r\n  Using cached pycodestyle-2.5.0-py2.py3-none-any.whl (51 kB)\r\nCollecting mccabe<0.7.0,>=0.6.0\r\n  Using cached mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\r\nRequirement already satisfied: jsondiff>=1.1.2 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.3.0)\r\nRequirement already satisfied: pytz in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (2021.1)\r\nRequirement already satisfied: mock in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (4.0.3)\r\nRequirement already satisfied: MarkupSafe<2.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.1.1)\r\nRequirement already satisfied: python-jose[cryptography]<4.0.0,>=3.1.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (3.2.0)\r\nRequirement already satisfied: aws-xray-sdk!=0.96,>=0.93 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.8.0)\r\nRequirement already satisfied: cryptography>=2.3.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (3.4.7)\r\nRequirement already satisfied: more-itertools in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (8.7.0)\r\nRequirement already satisfied: PyYAML>=5.1 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (5.4.1)\r\nRequirement already satisfied: boto>=2.36.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.49.0)\r\nRequirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.10)\r\nRequirement already satisfied: sshpubkeys>=3.1.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (3.3.1)\r\nRequirement already satisfied: responses>=0.9.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (0.13.3)\r\nRequirement already satisfied: xmltodict in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (0.12.0)\r\nRequirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (52.0.0.post20210125)\r\nRequirement already satisfied: Jinja2>=2.10.1 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.11.3)\r\nRequirement already satisfied: zipp in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (3.4.1)\r\nRequirement already satisfied: six>1.9 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.15.0)\r\nRequirement already satisfied: ecdsa<0.15 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (0.14.1)\r\nRequirement already satisfied: docker>=2.5.1 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (5.0.0)\r\nRequirement already satisfied: cfn-lint>=0.4.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (0.49.0)\r\nRequirement already satisfied: grpcio<2,>=1.29.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from apache-beam>=2.24.0->datasets==1.5.0.dev0) (1.32.0)\r\nCollecting hdfs<3.0.0,>=2.1.0\r\n  Using cached hdfs-2.6.0-py3-none-any.whl (33 kB)\r\nCollecting pyarrow>=0.17.1\r\n  Using cached pyarrow-3.0.0-cp37-cp37m-win_amd64.whl (12.6 MB)\r\nCollecting fastavro<2,>=0.21.4\r\n  Using cached fastavro-1.4.0-cp37-cp37m-win_amd64.whl (394 kB)\r\nRequirement already satisfied: httplib2<0.18.0,>=0.8 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from apache-beam>=2.24.0->datasets==1.5.0.dev0) (0.17.4)\r\nCollecting pymongo<4.0.0,>=3.8.0\r\n  Using cached pymongo-3.11.3-cp37-cp37m-win_amd64.whl (382 kB)\r\nCollecting crcmod<2.0,>=1.7\r\n  Using cached crcmod-1.7-py3-none-any.whl\r\nCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\r\n  Using cached avro_python3-1.9.2.1-py3-none-any.whl\r\nRequirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from apache-beam>=2.24.0->datasets==1.5.0.dev0) (3.7.4.3)\r\nRequirement already satisfied: future<1.0.0,>=0.18.2 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from apache-beam>=2.24.0->datasets==1.5.0.dev0) (0.18.2)\r\nCollecting oauth2client<5,>=2.0.1\r\n  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\r\nCollecting pydot<2,>=1.2.0\r\n  Using cached pydot-1.4.2-py2.py3-none-any.whl (21 kB)\r\nRequirement already satisfied: protobuf<4,>=3.12.2 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from apache-beam>=2.24.0->datasets==1.5.0.dev0) (3.15.8)\r\nRequirement already satisfied: wrapt in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from aws-xray-sdk!=0.96,>=0.93->moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.12.1)\r\nCollecting matplotlib\r\n  Using cached matplotlib-3.4.1-cp37-cp37m-win_amd64.whl (7.1 MB)\r\nRequirement already satisfied: junit-xml~=1.9 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.9)\r\nRequirement already satisfied: jsonpatch in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.32)\r\nRequirement already satisfied: jsonschema~=3.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (3.2.0)\r\nRequirement already satisfied: networkx~=2.4 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.5.1)\r\nRequirement already satisfied: aws-sam-translator>=1.35.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.35.0)\r\nRequirement already satisfied: cffi>=1.12 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from cryptography>=2.3.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.14.5)\r\nRequirement already satisfied: pycparser in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from cffi>=1.12->cryptography>=2.3.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.20)\r\nRequirement already satisfied: pywin32==227 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from docker>=2.5.1->moto[s3]==1.3.16->datasets==1.5.0.dev0) (227)\r\nRequirement already satisfied: websocket-client>=0.32.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from docker>=2.5.1->moto[s3]==1.3.16->datasets==1.5.0.dev0) (0.58.0)\r\nRequirement already satisfied: docopt in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam>=2.24.0->datasets==1.5.0.dev0) (0.6.2)\r\nRequirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from huggingface_hub<0.1.0->datasets==1.5.0.dev0) (3.0.12)\r\nRequirement already satisfied: pyrsistent>=0.14.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from jsonschema~=3.0->cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (0.17.3)\r\nRequirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from jsonschema~=3.0->cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (20.3.0)\r\nRequirement already satisfied: decorator<5,>=4.3 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from networkx~=2.4->cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (4.4.2)\r\nRequirement already satisfied: rsa>=3.1.4 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from oauth2client<5,>=2.0.1->apache-beam>=2.24.0->datasets==1.5.0.dev0) (4.7.2)\r\nRequirement already satisfied: pyasn1-modules>=0.0.5 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from oauth2client<5,>=2.0.1->apache-beam>=2.24.0->datasets==1.5.0.dev0) (0.2.8)\r\nRequirement already satisfied: pyasn1>=0.1.7 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from oauth2client<5,>=2.0.1->apache-beam>=2.24.0->datasets==1.5.0.dev0) (0.4.8)\r\nRequirement already satisfied: pyparsing>=2.1.4 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from pydot<2,>=1.2.0->apache-beam>=2.24.0->datasets==1.5.0.dev0) (2.4.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from requests>=2.19.0->datasets==1.5.0.dev0) (2020.12.5)\r\nRequirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from requests>=2.19.0->datasets==1.5.0.dev0) (4.0.0)\r\nCollecting keras-preprocessing~=1.1.2\r\n  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\nRequirement already satisfied: termcolor~=1.1.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from tensorflow>=2.3->datasets==1.5.0.dev0) (1.1.0)\r\nRequirement already satisfied: tensorboard~=2.4 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from tensorflow>=2.3->datasets==1.5.0.dev0) (2.5.0)\r\nRequirement already satisfied: wheel~=0.35 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from tensorflow>=2.3->datasets==1.5.0.dev0) (0.36.2)\r\nCollecting opt-einsum~=3.3.0\r\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\nCollecting gast==0.3.3\r\n  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\r\nCollecting google-pasta~=0.2\r\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\nRequirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from tensorflow>=2.3->datasets==1.5.0.dev0) (2.4.0)\r\nCollecting astunparse~=1.6.3\r\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\nCollecting flatbuffers~=1.12.0\r\n  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\r\nCollecting h5py~=2.10.0\r\n  Using cached h5py-2.10.0-cp37-cp37m-win_amd64.whl (2.5 MB)\r\nRequirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (3.3.4)\r\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (1.8.0)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (0.4.4)\r\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (0.6.0)\r\nRequirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (1.30.0)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (4.2.2)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (1.3.0)\r\nRequirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (3.1.0)\r\nRequirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from transformers->datasets==1.5.0.dev0) (2021.4.4)\r\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from transformers->datasets==1.5.0.dev0) (0.10.2)\r\nRequirement already satisfied: sacremoses in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from transformers->datasets==1.5.0.dev0) (0.0.45)\r\nRequirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from transformers->datasets==1.5.0.dev0) (20.9)\r\nCollecting pathspec<1,>=0.8.1\r\n  Using cached pathspec-0.8.1-py2.py3-none-any.whl (28 kB)\r\nRequirement already satisfied: click>=7.1.2 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from black->datasets==1.5.0.dev0) (7.1.2)\r\nCollecting appdirs\r\n  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\nCollecting mypy-extensions>=0.4.3\r\n  Using cached mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\r\nRequirement already satisfied: typed-ast>=1.4.2 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from black->datasets==1.5.0.dev0) (1.4.3)\r\nCollecting beautifulsoup4\r\n  Using cached beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\r\nRequirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from beautifulsoup4->bs4->datasets==1.5.0.dev0) (2.2.1)\r\nCollecting python-Levenshtein\r\n  Using cached python-Levenshtein-0.12.2.tar.gz (50 kB)\r\nRequirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from jsonpatch->cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.1)\r\nRequirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from matplotlib->bert_score>=0.3.6->datasets==1.5.0.dev0) (8.2.0)\r\nRequirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from matplotlib->bert_score>=0.3.6->datasets==1.5.0.dev0) (0.10.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from matplotlib->bert_score>=0.3.6->datasets==1.5.0.dev0) (1.3.1)\r\nCollecting multiprocess\r\n  Using cached multiprocess-0.70.11-py3-none-any.whl (98 kB)\r\n  Using cached multiprocess-0.70.10.zip (2.4 MB)\r\n  Using cached multiprocess-0.70.9-py3-none-any.whl\r\nRequirement already satisfied: joblib in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from nltk->datasets==1.5.0.dev0) (1.0.1)\r\nCollecting et-xmlfile\r\n  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\r\nRequirement already satisfied: pyzstd<0.15.0,>=0.14.4 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from py7zr->datasets==1.5.0.dev0) (0.14.4)\r\nCollecting pyppmd<0.13.0,>=0.12.1\r\n  Using cached pyppmd-0.12.1-cp37-cp37m-win_amd64.whl (32 kB)\r\nCollecting pycryptodome>=3.6.6\r\n  Using cached pycryptodome-3.10.1-cp35-abi3-win_amd64.whl (1.6 MB)\r\nCollecting bcj-cffi<0.6.0,>=0.5.1\r\n  Using cached bcj_cffi-0.5.1-cp37-cp37m-win_amd64.whl (21 kB)\r\nCollecting multivolumefile<0.3.0,>=0.2.0\r\n  Using cached multivolumefile-0.2.3-py3-none-any.whl (17 kB)\r\nRequirement already satisfied: iniconfig in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from pytest->datasets==1.5.0.dev0) (1.1.1)\r\nRequirement already satisfied: py>=1.8.2 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from pytest->datasets==1.5.0.dev0) (1.10.0)\r\nRequirement already satisfied: pluggy<1.0.0a1,>=0.12 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from pytest->datasets==1.5.0.dev0) (0.13.1)\r\nRequirement already satisfied: atomicwrites>=1.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from pytest->datasets==1.5.0.dev0) (1.4.0)\r\nRequirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from pytest->datasets==1.5.0.dev0) (0.4.4)\r\nCollecting pytest-forked\r\n  Using cached pytest_forked-1.3.0-py2.py3-none-any.whl (4.7 kB)\r\nCollecting execnet>=1.1\r\n  Using cached execnet-1.8.0-py2.py3-none-any.whl (39 kB)\r\nRequirement already satisfied: apipkg>=1.4 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from execnet>=1.1->pytest-xdist->datasets==1.5.0.dev0) (1.5)\r\nCollecting portalocker==2.0.0\r\n  Using cached portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\r\nRequirement already satisfied: scikit-learn>=0.21.3 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from seqeval->datasets==1.5.0.dev0) (0.24.2)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\envs\\env\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->datasets==1.5.0.dev0) (2.1.0)\r\nBuilding wheels for collected packages: python-Levenshtein\r\n  Building wheel for python-Levenshtein (setup.py) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: 'C:\\ProgramData\\Anaconda3\\envs\\env\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\VKC~1\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ynt_dbm4\\\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\VKC~1\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ynt_dbm4\\\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\VKC~1\\AppData\\Local\\Temp\\pip-wheel-8jh7fm18'\r\n       cwd: C:\\Users\\VKC~1\\AppData\\Local\\Temp\\pip-install-ynt_dbm4\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\\r\n  Complete output (27 lines):\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build\\lib.win-amd64-3.7\r\n  creating build\\lib.win-amd64-3.7\\Levenshtein\r\n  copying Levenshtein\\StringMatcher.py -> build\\lib.win-amd64-3.7\\Levenshtein\r\n  copying Levenshtein\\__init__.py -> build\\lib.win-amd64-3.7\\Levenshtein\r\n  running egg_info\r\n  writing python_Levenshtein.egg-info\\PKG-INFO\r\n  writing dependency_links to python_Levenshtein.egg-info\\dependency_links.txt\r\n  writing entry points to python_Levenshtein.egg-info\\entry_points.txt\r\n  writing namespace_packages to python_Levenshtein.egg-info\\namespace_packages.txt\r\n  writing requirements to python_Levenshtein.egg-info\\requires.txt\r\n  writing top-level names to python_Levenshtein.egg-info\\top_level.txt\r\n  reading manifest file 'python_Levenshtein.egg-info\\SOURCES.txt'\r\n  reading manifest template 'MANIFEST.in'\r\n  warning: no previously-included files matching '*pyc' found anywhere in distribution\r\n  warning: no previously-included files matching '*so' found anywhere in distribution\r\n  warning: no previously-included files matching '.project' found anywhere in distribution\r\n  warning: no previously-included files matching '.pydevproject' found anywhere in distribution\r\n  writing manifest file 'python_Levenshtein.egg-info\\SOURCES.txt'\r\n  copying Levenshtein\\_levenshtein.c -> build\\lib.win-amd64-3.7\\Levenshtein\r\n  copying Levenshtein\\_levenshtein.h -> build\\lib.win-amd64-3.7\\Levenshtein\r\n  running build_ext\r\n  building 'Levenshtein._levenshtein' extension\r\n  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for python-Levenshtein\r\n  Running setup.py clean for python-Levenshtein\r\nFailed to build python-Levenshtein\r\nInstalling collected packages: python-Levenshtein, pytest-forked, pyppmd, pymongo, pyflakes, pydot, pycryptodome, pycodestyle, pyarrow, portalocker, pathspec, pandas, opt-einsum, oauth2client, nltk, mypy-extensions, multivolumefile, multiprocess, moto, mccabe, matplotlib, keras-preprocessing, huggingface-hub, hdfs, h5py, google-pasta, gast, flatbuffers, fastavro, execnet, et-xmlfile, entrypoints, crcmod, beautifulsoup4, bcj-cffi, avro-python3, astunparse, appdirs, zstandard, tldextract, tensorflow, sklearn, seqeval, sacrebleu, rouge-score, rarfile, pytest-xdist, py7zr, openpyxl, mwparserfromhell, lxml, langdetect, jiwer, isort, flake8, elasticsearch, datasets, conllu, bs4, black, bert-score, apache-beam\r\n    Running setup.py install for python-Levenshtein ... error\r\n    ERROR: Command errored out with exit status 1:\r\n     command: 'C:\\ProgramData\\Anaconda3\\envs\\env\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\VKC~1\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ynt_dbm4\\\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\VKC~1\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ynt_dbm4\\\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\VKC~1\\AppData\\Local\\Temp\\pip-record-v7l7zitb\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\ProgramData\\Anaconda3\\envs\\env\\Include\\python-Levenshtein'\r\n         cwd: C:\\Users\\VKC~1\\AppData\\Local\\Temp\\pip-install-ynt_dbm4\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\\r\n    Complete output (27 lines):\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build\\lib.win-amd64-3.7\r\n    creating build\\lib.win-amd64-3.7\\Levenshtein\r\n    copying Levenshtein\\StringMatcher.py -> build\\lib.win-amd64-3.7\\Levenshtein\r\n    copying Levenshtein\\__init__.py -> build\\lib.win-amd64-3.7\\Levenshtein\r\n    running egg_info\r\n    writing python_Levenshtein.egg-info\\PKG-INFO\r\n    writing dependency_links to python_Levenshtein.egg-info\\dependency_links.txt\r\n    writing entry points to python_Levenshtein.egg-info\\entry_points.txt\r\n    writing namespace_packages to python_Levenshtein.egg-info\\namespace_packages.txt\r\n    writing requirements to python_Levenshtein.egg-info\\requires.txt\r\n    writing top-level names to python_Levenshtein.egg-info\\top_level.txt\r\n    reading manifest file 'python_Levenshtein.egg-info\\SOURCES.txt'\r\n    reading manifest template 'MANIFEST.in'\r\n    warning: no previously-included files matching '*pyc' found anywhere in distribution\r\n    warning: no previously-included files matching '*so' found anywhere in distribution\r\n    warning: no previously-included files matching '.project' found anywhere in distribution\r\n    warning: no previously-included files matching '.pydevproject' found anywhere in distribution\r\n    writing manifest file 'python_Levenshtein.egg-info\\SOURCES.txt'\r\n    copying Levenshtein\\_levenshtein.c -> build\\lib.win-amd64-3.7\\Levenshtein\r\n    copying Levenshtein\\_levenshtein.h -> build\\lib.win-amd64-3.7\\Levenshtein\r\n    running build_ext\r\n    building 'Levenshtein._levenshtein' extension\r\n    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: 'C:\\ProgramData\\Anaconda3\\envs\\env\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\VKC~1\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ynt_dbm4\\\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\VKC~1\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ynt_dbm4\\\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\VKC~1\\AppData\\Local\\Temp\\pip-record-v7l7zitb\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\ProgramData\\Anaconda3\\envs\\env\\Include\\python-Levenshtein' Check the logs for full command output.\r\n```\r\n\r\nHere are conda and python versions:\r\n\r\n```bat\r\n(env) C:\\testing\\datasets>conda --version\r\nconda 4.9.2\r\n\r\n(env) C:\\testing\\datasets>python --version\r\nPython 3.7.10\r\n```\r\n\r\nPlease help me out. Thanks.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 873928169,
    "title": "Add VoxPopuli",
    "dateCreated": "2021-05-02T12:17:40Z",
    "dateModified": "2021-05-02T12:17:40Z",
    "description": "## Adding a Dataset\r\n- **Name:** Voxpopuli\r\n- **Description:** VoxPopuli is raw data is collected from 2009-2020 European Parliament event recordings\r\n- **Paper:** https://arxiv.org/abs/2101.00390\r\n- **Data:** https://github.com/facebookresearch/voxpopuli\r\n- **Motivation:** biggest unlabeled speech dataset\r\n\r\n**Note**: Since the dataset is so huge, we should only add the config `10k` in the beginning.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 873914717,
    "title": "My iPhone",
    "dateCreated": "2021-05-02T11:11:11Z",
    "dateModified": "2021-05-02T11:11:11Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 873771942,
    "title": "Mapping in the distributed setting",
    "dateCreated": "2021-05-01T21:23:05Z",
    "dateModified": "2021-05-01T21:23:05Z",
    "description": "The barrier trick for distributed mapping as discussed on Thursday with @lhoestq",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 872974907,
    "title": "1",
    "dateCreated": "2021-04-30T17:53:49Z",
    "dateModified": "2021-04-30T17:53:49Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 872902867,
    "title": "Create ExtractManager",
    "dateCreated": "2021-04-30T17:13:34Z",
    "dateModified": "2021-04-30T17:13:34Z",
    "description": "Perform refactoring to decouple extract functionality.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 872136075,
    "title": "Slow #0 when using map to tokenize.",
    "dateCreated": "2021-04-30T08:00:33Z",
    "dateModified": "2021-04-30T08:00:33Z",
    "description": "Hi, _datasets_ is really amazing! I am following [run_mlm_no_trainer.py](url) to pre-train BERT, and it uses `tokenized_datasets = raw_datasets.map(\r\n            tokenize_function,\r\n            batched=True,\r\n            num_proc=args.preprocessing_num_workers,\r\n            remove_columns=column_names,\r\n            load_from_cache_file=not args.overwrite_cache,\r\n        )` to tokenize by multiprocessing. However, I have found that when `num_proc`>1\uff0cthe process _#0_ is much slower than others.\r\nIt looks like this:\r\n![image](https://user-images.githubusercontent.com/31714566/116665555-81246280-a9cc-11eb-8a37-6e608ab310d0.png)\r\nIt takes more than 12 hours for #0, while others just about half an hour. Could anyone tell me it is normal or not, and is there any methods to speed up it?\r\n",
    "status": "open",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 872079385,
    "title": "imdb dataset from Don't Stop Pretraining Paper",
    "dateCreated": "2021-04-30T06:40:48Z",
    "dateModified": "2021-04-30T06:40:48Z",
    "description": "",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 871230183,
    "title": "Fixed typo seperate->separate",
    "dateCreated": "2021-04-29T16:40:53Z",
    "dateModified": "2021-04-29T16:40:53Z",
    "description": "",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 871216757,
    "title": "Don't copy recordbatches in memory during a table deepcopy",
    "dateCreated": "2021-04-29T16:26:05Z",
    "dateModified": "2021-04-29T16:26:05Z",
    "description": "Fix issue #2276 and hopefully #2134\r\n\r\nThe recordbatches of the `IndexedTableMixin` used to speed up queries to the table were copied in memory during a table deepcopy.\r\nThis resulted in `concatenate_datasets`, `load_from_disk` and other methods to always bring the data in memory.\r\n\r\nI fixed the copy similarly to #2287 and updated the test to make sure it doesn't happen again (added a test for deepcopy + make sure that the immutable arrow objects are passed to the copied table without being copied).\r\n\r\nThe issue was not caught by our tests because the total allocated bytes value in PyArrow isn't updated when deepcopying recordbatches: the copy in memory wasn't detected. This behavior looks like a bug in PyArrow, I'll open a ticket on JIRA.\r\n\r\nThanks @samsontmr , @TaskManager91 and @mariosasko for the help\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 871145817,
    "title": "Bbaw egyptian",
    "dateCreated": "2021-04-29T15:27:58Z",
    "dateModified": "2021-04-29T15:27:58Z",
    "description": "This is the \"hieroglyph corpus\" that I could unfortunately not contribute during the marathon. I re-extracted it again now, so that it is in the state as used in my paper (seee documentation). I hope it satiesfies your requirements and wish every scientist out their loads of fun deciphering a 5.000 years old language :-)",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 871118573,
    "title": "Allow collaborators to self-assign issues",
    "dateCreated": "2021-04-29T15:07:06Z",
    "dateModified": "2021-04-29T15:07:06Z",
    "description": "Allow collaborators (without write access to the repository) to self-assign issues.\r\n\r\nIn order to self-assign an issue, they have to comment it with the word: `#take` or `#self-assign`.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 871111235,
    "title": "Load_dataset for local CSV files",
    "dateCreated": "2021-04-29T15:01:10Z",
    "dateModified": "2021-04-29T15:01:10Z",
    "description": "The method load_dataset fails to correctly load a dataset from csv. \r\n\r\nMoreover, I am working on a token-classification task ( POS tagging) , where each row in my CSV contains two columns each of them having a list of strings.\r\nrow example:\r\n```tokens  |  labels\r\n['I' , 'am', 'John']  |  ['PRON', 'AUX', 'PROPN' ] \r\n```\r\nThe method, loads each list as a string:  (i.g \"['I' , 'am', 'John']\").\r\nTo solve this issue, I copied the Datasets.Features, created Sequence types ( instead of Value)  and tried to cast the features type\r\n```\r\nnew_features['tokens'] = Sequence(feature=Value(dtype='string', id=None))\r\nnew_features['labels'] = Sequence(feature=ClassLabel(num_classes=len(tag2idx), names=list(unique_tags)))\r\ndataset = dataset.cast(new_features)\r\n```\r\nbut I got the following error \r\n```\r\nArrowNotImplementedError: Unsupported cast from string to list using function cast_list\r\n```\r\nMoreover, I tried to set feature parameter in load_dataset method, to my new_features, but this fails as well.\r\nHow can this be solved ?",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 871063374,
    "title": "Avoid copying table's record batches",
    "dateCreated": "2021-04-29T14:15:01Z",
    "dateModified": "2021-04-29T14:15:01Z",
    "description": "Fixes #2276",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 871032393,
    "title": "Fix metadata validation with config names",
    "dateCreated": "2021-04-29T13:44:32Z",
    "dateModified": "2021-04-29T13:44:32Z",
    "description": "I noticed in https://github.com/huggingface/datasets/pull/2280 that the metadata validator doesn't parse the tags in the readme properly when then contain the tags per config.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 871005236,
    "title": "Help understanding how to build a dataset for language modeling as with the old TextDataset",
    "dateCreated": "2021-04-29T13:16:45Z",
    "dateModified": "2021-04-29T13:16:45Z",
    "description": "Hello,\r\n\r\nI am trying to load a custom dataset that I will then use for language modeling. The dataset consists of a text file that has a whole document in each line, meaning that each line overpasses the normal 512 tokens limit of most tokenizers.\r\n\r\nI would like to understand what is the process to build a text dataset that tokenizes each line, having previously split the documents in the dataset into lines of a \"tokenizable\" size, as the old TextDataset class would do, where you only had to do the following, and a tokenized dataset without text loss would be available to pass to a DataCollator:\r\n\r\n```\r\nmodel_checkpoint = 'distilbert-base-uncased'\r\n\r\nfrom transformers import AutoTokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\r\n\r\nfrom transformers import TextDataset\r\n\r\ndataset = TextDataset(\r\n    tokenizer=tokenizer,\r\n    file_path=\"path/to/text_file.txt\",\r\n    block_size=512,\r\n)\r\n```\r\n\r\nFor now, what I have is the following, which, of course, throws an error because each line is longer than the maximum block size in the tokenizer:\r\n\r\n```\r\nimport datasets\r\ndataset = datasets.load_dataset('path/to/text_file.txt')\r\n\r\nmodel_checkpoint = 'distilbert-base-uncased'\r\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"])\r\n\r\ntokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\r\n\r\ntokenized_datasets\r\n```\r\n\r\nSo what would be the \"standard\" way of creating a dataset in the way it was done before?\r\n\r\nThank you very much for the help :))",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 870932710,
    "title": "Initialize Imdb dataset as used in Don't Stop Pretraining Paper",
    "dateCreated": "2021-04-29T11:52:38Z",
    "dateModified": "2021-04-29T11:52:38Z",
    "description": "",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 870926475,
    "title": "Initialize imdb dataset from don't stop pretraining paper",
    "dateCreated": "2021-04-29T11:44:54Z",
    "dateModified": "2021-04-29T11:44:54Z",
    "description": "",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 870900332,
    "title": "Initialize imdb dataset from don't stop pretraining paper",
    "dateCreated": "2021-04-29T11:17:56Z",
    "dateModified": "2021-04-29T11:17:56Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 870792784,
    "title": "Update multi_woz_v22 checksum",
    "dateCreated": "2021-04-29T09:09:11Z",
    "dateModified": "2021-04-29T09:09:11Z",
    "description": "Fix issue https://github.com/huggingface/datasets/issues/1876\r\nThe files were changed in https://github.com/budzianowski/multiwoz/pull/72",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 870780431,
    "title": "Fixed typo  seperate->separate",
    "dateCreated": "2021-04-29T08:55:46Z",
    "dateModified": "2021-04-29T08:55:46Z",
    "description": "",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 870431662,
    "title": "Compatibility with Ubuntu 18 and GLIBC 2.27?",
    "dateCreated": "2021-04-28T22:08:07Z",
    "dateModified": "2021-04-28T22:08:07Z",
    "description": "## Describe the bug\r\nFor use on Ubuntu systems, it seems that datasets requires GLIBC 2.29. However, Ubuntu 18 runs with GLIBC 2.27 and it seems [non-trivial to upgrade GLIBC to 2.29 for Ubuntu 18 users](https://www.digitalocean.com/community/questions/how-install-glibc-2-29-or-higher-in-ubuntu-18-04). \r\n\r\nI'm not sure if there is anything that can be done about this, but I'd like to confirm that using huggingface/datasets requires either an upgrade to Ubuntu 19/20 or a hand-rolled install of a higher version of GLIBC.\r\n\r\n## Steps to reproduce the bug\r\n1. clone the transformers repo\r\n2. move to examples/pytorch/language-modeling\r\n3. run example command:\r\n```python run_clm.py     --model_name_or_path gpt2     --dataset_name wikitext     --dataset_config_name wikitext-2-raw-v1     --do_train     --do_eval     --output_dir /tmp/test-clm```\r\n\r\n\r\n## Expected results\r\nAs described in the transformers repo.\r\n\r\n## Actual results\r\n```Traceback (most recent call last):\r\n  File \"run_clm.py\", line 34, in <module>\r\n    from transformers import (\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/__init__.py\", line 2487, in __getattr__\r\n    return super().__getattr__(name)\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/file_utils.py\", line 1699, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/__init__.py\", line 2481, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/models/__init__.py\", line 19, in <module>\r\n    from . import (\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/models/layoutlm/__init__.py\", line 23, in <module>\r\n    from .tokenization_layoutlm import LayoutLMTokenizer\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/models/layoutlm/tokenization_layoutlm.py\", line 19, in <module>\r\n    from ..bert.tokenization_bert import BertTokenizer\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\", line 23, in <module>\r\n    from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/tokenization_utils.py\", line 26, in <module>\r\n    from .tokenization_utils_base import (\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 68, in <module>\r\n    from tokenizers import AddedToken\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/tokenizers/__init__.py\", line 79, in <module>\r\n    from .tokenizers import (\r\nImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/tokenizers/tokenizers.cpython-37m-x86_64-linux-gnu.so)\r\n```\r\n\r\n## Versions\r\nPaste the output of the following code:\r\n```\r\n- Datasets: 1.6.1\r\n- Python: 3.7.10 (default, Feb 26 2021, 18:47:35) \r\n[GCC 7.3.0]\r\n- Platform: Linux-4.15.0-128-generic-x86_64-with-debian-buster-sid\r\n\r\n```\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 870088059,
    "title": "Loss result inGptNeoForCasual",
    "dateCreated": "2021-04-28T15:39:52Z",
    "dateModified": "2021-04-28T15:39:52Z",
    "description": "Is there any way you give the \" loss\" and \"logits\" results in the gpt neo api? ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 870071994,
    "title": "Create CacheManager",
    "dateCreated": "2021-04-28T15:23:42Z",
    "dateModified": "2021-04-28T15:23:42Z",
    "description": "Perform refactoring to decouple cache functionality (method `as_dataset`).",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 870010511,
    "title": "concatenate_datasets loads all the data into memory",
    "dateCreated": "2021-04-28T14:27:21Z",
    "dateModified": "2021-04-28T14:27:21Z",
    "description": "## Describe the bug\r\nWhen I try to concatenate 2 datasets (10GB each) , the entire data is loaded into memory instead of being written directly to disk.\r\n\r\nInterestingly, this happens when trying to save the new dataset to disk or concatenating it again.\r\n\r\n![image](https://user-images.githubusercontent.com/7063207/116420321-2b21b480-a83e-11eb-9006-8f6ca729fb6f.png)\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import concatenate_datasets, load_from_disk\r\n\r\ntest_sampled_pro = load_from_disk(\"test_sampled_pro\")\r\nval_sampled_pro = load_from_disk(\"val_sampled_pro\")\r\n\r\nbig_set = concatenate_datasets([test_sampled_pro, val_sampled_pro])\r\n\r\n# Loaded to memory\r\nbig_set.save_to_disk(\"big_set\")\r\n\r\n# Loaded to memory\r\nbig_set = concatenate_datasets([big_set, val_sampled_pro])\r\n```\r\n\r\n## Expected results\r\nThe data should be loaded into memory in batches and then saved directly to disk.\r\n\r\n## Actual results\r\nThe entire data set is loaded into the memory and then saved to the hard disk.\r\n\r\n## Versions\r\nPaste the output of the following code:\r\n```python\r\n- Datasets: 1.6.1\r\n- Python: 3.8.8 (default, Apr 13 2021, 19:58:26) \r\n[GCC 7.3.0]\r\n- Platform: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.10\r\n```\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 869378311,
    "title": "SNLI dataset has labels of -1 ",
    "dateCreated": "2021-04-28T00:32:25Z",
    "dateModified": "2021-04-28T00:32:25Z",
    "description": "There are a number of rows with a label of -1 in the SNLI dataset. The dataset descriptions [here](https://nlp.stanford.edu/projects/snli/) and [here](https://github.com/huggingface/datasets/tree/master/datasets/snli) don't list  -1 as a label possibility, and neither does the dataset viewer. As examples, see index 107 or 124 of the test set.\r\n\r\nIt isn't clear what these labels mean. I found a [line of code](https://github.com/huggingface/datasets/blob/80e59ef178d3bb2090d091bc32315c655eb0633d/datasets/snli/snli.py#L94) that seems to put them in but it seems still unclear why they are there. The current workaround is to just drop the rows from any model being trained. \r\n\r\nPerhaps the documentation should be updated.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 869186276,
    "title": "Always update metadata in arrow schema",
    "dateCreated": "2021-04-27T19:21:57Z",
    "dateModified": "2021-04-27T19:21:57Z",
    "description": "We store a redundant copy of the features in the metadata of the schema of the arrow table. This is used to recover the features when doing `Dataset.from_file`. These metadata are updated after each transfor, that changes the feature types.\r\n\r\nFor each function that transforms the feature types of the dataset, I added a step in the tests to make sure the metadata in the arrow schema are up to date.\r\n\r\nI also added a line to update the metadata directly in the Dataset.__init__ method.\r\nThis way even a dataset instantiated with __init__ will have a table with the right metadata.\r\n\r\ncc @mariosasko ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 869046290,
    "title": "Added CUAD metrics",
    "dateCreated": "2021-04-27T16:49:12Z",
    "dateModified": "2021-04-27T16:49:12Z",
    "description": "`EM`, `F1`, `AUPR`, `Precision@80%Recall`, and `Precision@90%Recall` metrics supported for CUAD",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 869017977,
    "title": "Bug in Dataset.class_encode_column",
    "dateCreated": "2021-04-27T16:13:18Z",
    "dateModified": "2021-04-27T16:13:18Z",
    "description": "## Describe the bug\r\n\r\nAll the rest of the columns except the one passed to `Dataset.class_encode_column` are discarded.\r\n\r\n## Expected results\r\n\r\nAll the original columns should be kept.\r\n\r\nThis needs regression tests.\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 869002141,
    "title": "Synchronize table metadata with features",
    "dateCreated": "2021-04-27T15:55:13Z",
    "dateModified": "2021-04-27T15:55:13Z",
    "description": "**Is your feature request related to a problem? Please describe.**\r\n\r\nAs pointed out in this [comment](https://github.com/huggingface/datasets/pull/2145#discussion_r621326767):\r\n> Metadata stored in the schema is just a redundant information regarding the feature types.\r\nIt is used when calling Dataset.from_file to know which feature types to use.\r\nThese metadata are stored in the schema of the pyarrow table by using `update_metadata_with_features`.\r\nHowever this something that's almost never tested properly.\r\n\r\n**Describe the solution you'd like**\r\n\r\nWe should find a way to always make sure that the metadata (in `self.data.schema.metadata`) are synced with the actual feature types (in `self.info.features`).",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 868913660,
    "title": "Fix iterable interface expected by numpy",
    "dateCreated": "2021-04-27T14:35:56Z",
    "dateModified": "2021-04-27T14:35:56Z",
    "description": "Numpy expects the old iterable interface with `__getitem__` instead of `__iter__`.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 868878468,
    "title": "Fix query table with iterable",
    "dateCreated": "2021-04-27T13:59:38Z",
    "dateModified": "2021-04-27T13:59:38Z",
    "description": "The benchmark runs are failing on master because it tries to use an iterable to query the dataset.\r\nHowever there's currently an issue caused by the use of `np.array` instead of `np.fromiter` on the iterable.\r\nThis PR fixes it",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 868773380,
    "title": "Don't use pyarrow 4.0.0 since it segfaults when casting a sliced ListArray of integers",
    "dateCreated": "2021-04-27T11:58:28Z",
    "dateModified": "2021-04-27T11:58:28Z",
    "description": "This test `tests/test_table.py::test_concatenation_table_cast` segfaults with the latest update of pyarrow 4.0.0.\r\nSetting `pyarrow<4.0.0` for now. I'll open an issue on JIRA once I know more about the origin of the issue",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 868291129,
    "title": "DatasetDict save load Failing test in 1.6 not in 1.5",
    "dateCreated": "2021-04-27T00:03:25Z",
    "dateModified": "2021-04-27T00:03:25Z",
    "description": "## Describe the bug\r\n\r\nWe have a test that saves a DatasetDict to disk and then loads it from disk. In 1.6 there is an incompatibility in the schema.\r\n\r\n\r\n\r\n\r\nDowngrading to `>1.6` -- fixes the problem.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n\r\n### Load a dataset dict from jsonl \r\n\r\npath = '/test/foo'\r\n\r\nds_dict.save_to_disk(path)\r\n\r\nds_from_disk = DatasetDict.load_from_disk(path).  ## <-- this is where I see the error on 1.6\r\n```\r\n\r\n## Expected results\r\n\r\nUpgrading to 1.6 shouldn't break that test. We should be able to serialize to and from disk.\r\n\r\n## Actual results\r\n```\r\n        # Infer features if None\r\n        inferred_features = Features.from_arrow_schema(arrow_table.schema)\r\n        if self.info.features is None:\r\n            self.info.features = inferred_features\r\n    \r\n        # Infer fingerprint if None\r\n    \r\n        if self._fingerprint is None:\r\n            self._fingerprint = generate_fingerprint(self)\r\n    \r\n        # Sanity checks\r\n    \r\n        assert self.features is not None, \"Features can't be None in a Dataset object\"\r\n        assert self._fingerprint is not None, \"Fingerprint can't be None in a Dataset object\"\r\n        if self.info.features.type != inferred_features.type:\r\n>           raise ValueError(\r\n                \"External features info don't match the dataset:\\nGot\\n{}\\nwith type\\n{}\\n\\nbut expected something like\\n{}\\nwith type\\n{}\".format(\r\n                    self.info.features, self.info.features.type, inferred_features, inferred_features.type\r\n                )\r\n            )\r\nE           ValueError: External features info don't match the dataset:\r\nE           Got\r\nE           {'_input_hash': Value(dtype='int64', id=None), '_task_hash': Value(dtype='int64', id=None), '_view_id': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None), 'encoding__ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'encoding__offsets': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None), 'encoding__overflowing': Sequence(feature=Value(dtype='null', id=None), length=-1, id=None), 'encoding__tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'encoding__words': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'ner_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'ner_labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'relations': [{'child': Value(dtype='int64', id=None), 'child_span': {'end': Value(dtype='int64', id=None), 'label': Value(dtype='string', id=None), 'start': Value(dtype='int64', id=None), 'token_end': Value(dtype='int64', id=None), 'token_start': Value(dtype='int64', id=None)}, 'color': Value(dtype='string', id=None), 'head': Value(dtype='int64', id=None), 'head_span': {'end': Value(dtype='int64', id=None), 'label': Value(dtype='string', id=None), 'start': Value(dtype='int64', id=None), 'token_end': Value(dtype='int64', id=None), 'token_start': Value(dtype='int64', id=None)}, 'label': Value(dtype='string', id=None)}], 'spans': [{'end': Value(dtype='int64', id=None), 'label': Value(dtype='string', id=None), 'start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None), 'token_end': Value(dtype='int64', id=None), 'token_start': Value(dtype='int64', id=None), 'type': Value(dtype='string', id=None)}], 'text': Value(dtype='string', id=None), 'tokens': [{'disabled': Value(dtype='bool', id=None), 'end': Value(dtype='int64', id=None), 'id': Value(dtype='int64', id=None), 'start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None), 'ws': Value(dtype='bool', id=None)}]}\r\nE           with type\r\nE           struct<_input_hash: int64, _task_hash: int64, _view_id: string, answer: string, encoding__ids: list<item: int64>, encoding__offsets: list<item: list<item: int64>>, encoding__overflowing: list<item: null>, encoding__tokens: list<item: string>, encoding__words: list<item: int64>, ner_ids: list<item: int64>, ner_labels: list<item: string>, relations: list<item: struct<child: int64, child_span: struct<end: int64, label: string, start: int64, token_end: int64, token_start: int64>, color: string, head: int64, head_span: struct<end: int64, label: string, start: int64, token_end: int64, token_start: int64>, label: string>>, spans: list<item: struct<end: int64, label: string, start: int64, text: string, token_end: int64, token_start: int64, type: string>>, text: string, tokens: list<item: struct<disabled: bool, end: int64, id: int64, start: int64, text: string, ws: bool>>>\r\nE           \r\nE           but expected something like\r\nE           {'_input_hash': Value(dtype='int64', id=None), '_task_hash': Value(dtype='int64', id=None), '_view_id': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None), 'encoding__ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'encoding__offsets': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None), 'encoding__overflowing': Sequence(feature=Value(dtype='null', id=None), length=-1, id=None), 'encoding__tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'encoding__words': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'ner_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'ner_labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'relations': [{'head': Value(dtype='int64', id=None), 'child': Value(dtype='int64', id=None), 'head_span': {'start': Value(dtype='int64', id=None), 'end': Value(dtype='int64', id=None), 'token_start': Value(dtype='int64', id=None), 'token_end': Value(dtype='int64', id=None), 'label': Value(dtype='string', id=None)}, 'child_span': {'start': Value(dtype='int64', id=None), 'end': Value(dtype='int64', id=None), 'token_start': Value(dtype='int64', id=None), 'token_end': Value(dtype='int64', id=None), 'label': Value(dtype='string', id=None)}, 'color': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None)}], 'spans': [{'text': Value(dtype='string', id=None), 'start': Value(dtype='int64', id=None), 'token_start': Value(dtype='int64', id=None), 'token_end': Value(dtype='int64', id=None), 'end': Value(dtype='int64', id=None), 'type': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None)}], 'text': Value(dtype='string', id=None), 'tokens': [{'text': Value(dtype='string', id=None), 'start': Value(dtype='int64', id=None), 'end': Value(dtype='int64', id=None), 'id': Value(dtype='int64', id=None), 'ws': Value(dtype='bool', id=None), 'disabled': Value(dtype='bool', id=None)}]}\r\nE           with type\r\nE           struct<_input_hash: int64, _task_hash: int64, _view_id: string, answer: string, encoding__ids: list<item: int64>, encoding__offsets: list<item: list<item: int64>>, encoding__overflowing: list<item: null>, encoding__tokens: list<item: string>, encoding__words: list<item: int64>, ner_ids: list<item: int64>, ner_labels: list<item: string>, relations: list<item: struct<head: int64, child: int64, head_span: struct<start: int64, end: int64, token_start: int64, token_end: int64, label: string>, child_span: struct<start: int64, end: int64, token_start: int64, token_end: int64, label: string>, color: string, label: string>>, spans: list<item: struct<text: string, start: int64, token_start: int64, token_end: int64, end: int64, type: string, label: string>>, text: string, tokens: list<item: struct<text: string, start: int64, end: int64, id: int64, ws: bool, disabled: bool>>>\r\n\r\n../../../../../.virtualenvs/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:274: ValueError\r\n```\r\n## Versions\r\n- Datasets: 1.6.1\r\n- Python: 3.8.5 (default, Jan 26 2021, 10:01:04) \r\n[Clang 12.0.0 (clang-1200.0.32.2)]\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n\r\n```\r\n",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 867864353,
    "title": "Make tests run faster",
    "dateCreated": "2021-04-26T15:55:40Z",
    "dateModified": "2021-04-26T15:55:40Z",
    "description": "From 7min to 2min to run pytest.\r\nIdeally we should keep the whole CI run time below 10min.\r\n\r\nIn this PR I removed the remote tests that were never used.\r\nI also replaced nested parametrized tests with unit tests.\r\nThis makes me think that we could still add more high level tests to check for a few combinations of parameters (but not all of them since there are too many of them).\r\nLet me know what you think\r\n\r\nFinally in another PR we can also separate in two circleci jobs:\r\n- the tests of the code code of the lib\r\n- the tests of the all the dataset/metric scripts.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 867490646,
    "title": "Update black",
    "dateCreated": "2021-04-26T09:35:09Z",
    "dateModified": "2021-04-26T09:35:09Z",
    "description": "Latest black version 21.4b0 requires to reformat most dataset scripts and also the core code of the lib.\r\nThis makes the CI currently fail on master",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 867476228,
    "title": "Fix memory issue in multiprocessing: Don't pickle table index",
    "dateCreated": "2021-04-26T09:21:35Z",
    "dateModified": "2021-04-26T09:21:35Z",
    "description": "The table index is currently being pickled when doing multiprocessing, which brings all the record batches of the dataset in memory.\r\n\r\nI fixed that by not pickling the index attributes. Therefore each process has to rebuild the index when unpickling the table.\r\n\r\nFix issue #2256\r\n\r\nWe'll do a patch release asap !",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 867420912,
    "title": "test data added, dataset_infos updated",
    "dateCreated": "2021-04-26T08:27:18Z",
    "dateModified": "2021-04-26T08:27:18Z",
    "description": "Fixes #2262. Thanks for pointing out issue with dataset @jinmang2!",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 867325351,
    "title": "NewsPH NLI dataset script fails to access test data.",
    "dateCreated": "2021-04-26T06:44:41Z",
    "dateModified": "2021-04-26T06:44:41Z",
    "description": "In Newsph-NLI Dataset (#1192), it fails to access test data.\r\n\r\nAccording to the script below, the download manager will download the train data when trying to download the test data. \r\n\r\nhttps://github.com/huggingface/datasets/blob/2a2dd6316af2cc7fdf24e4779312e8ee0c7ed98b/datasets/newsph_nli/newsph_nli.py#L71\r\n\r\nIf you download it according to the script above, you can see that train and test receive the same data as shown below.\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> newsph_nli = load_dataset(path=\"./datasets/newsph_nli.py\")\r\n>>> newsph_nli\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['premise', 'hypothesis', 'label'],\r\n        num_rows: 420000\r\n    })\r\n    test: Dataset({\r\n        features: ['premise', 'hypothesis', 'label'],\r\n        num_rows: 420000\r\n    })\r\n    validation: Dataset({\r\n        features: ['premise', 'hypothesis', 'label'],\r\n        num_rows: 90000\r\n    })\r\n})\r\n>>> newsph_nli[\"train\"][0]\r\n{'hypothesis': 'Ito ang dineklara ni Atty. Romulo Macalintal, abogado ni Robredo, kaugnay ng pagsisimula ng preliminary conference ngayong hapon sa Presidential Electoral Tribunal (PET).',\r\n 'label': 1,\r\n 'premise': '\"Hindi ko ugali ang mamulitika; mas gusto kong tahimik na magtrabaho. Pero sasabihin ko ito ngayon: ang tapang, lakas, at diskarte, hindi nadadaan sa mapanirang salita. Ang kailangan ng taumbayan ay tapang sa gawa,\" ayon kay Robredo sa inilabas nitong statement.'}\r\n>>> newsph_nli[\"test\"][0]\r\n{'hypothesis': 'Ito ang dineklara ni Atty. Romulo Macalintal, abogado ni Robredo, kaugnay ng pagsisimula ng preliminary conference ngayong hapon sa Presidential Electoral Tribunal (PET).',\r\n 'label': 1,\r\n 'premise': '\"Hindi ko ugali ang mamulitika; mas gusto kong tahimik na magtrabaho. Pero sasabihin ko ito ngayon: ang tapang, lakas, at diskarte, hindi nadadaan sa mapanirang salita. Ang kailangan ng taumbayan ay tapang sa gawa,\" ayon kay Robredo sa inilabas nitong statement.'}\r\n```\r\n\r\nIn local, I modified the code of the source as below and got the correct result.\r\n```python\r\n71    test_path = os.path.join(download_path, \"test.csv\") \r\n```\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> newsph_nli = load_dataset(path=\"./datasets/newsph_nli.py\")\r\n>>> newsph_nli\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['premise', 'hypothesis', 'label'],\r\n        num_rows: 420000\r\n    })\r\n    test: Dataset({\r\n        features: ['premise', 'hypothesis', 'label'],\r\n        num_rows: 9000\r\n    })\r\n    validation: Dataset({\r\n        features: ['premise', 'hypothesis', 'label'],\r\n        num_rows: 90000\r\n    })\r\n})\r\n>>> newsph_nli[\"train\"][0]\r\n{'hypothesis': 'Ito ang dineklara ni Atty. Romulo Macalintal, abogado ni Robredo, kaugnay ng pagsisimula ng preliminary conference ngayong hapon sa Presidential Electoral Tribunal (PET).',\r\n 'label': 1,\r\n 'premise': '\"Hindi ko ugali ang mamulitika; mas gusto kong tahimik na magtrabaho. Pero sasabihin ko ito ngayon: ang tapang, lakas, at diskarte, hindi nadadaan sa mapanirang salita. Ang kailangan ng taumbayan ay tapang sa gawa,\" ayon kay Robredo sa inilabas nitong statement.'}\r\n>>> newsph_nli[\"test\"][0]\r\n{'hypothesis': '-- JAI (@JaiPaller) September 13, 2019',\r\n 'label': 1,\r\n 'premise': 'Pinag-iingat ng Konsulado ng Pilipinas sa Dubai ang publiko, partikular ang mga donor, laban sa mga scam na gumagamit ng mga charitable organization.'}\r\n```\r\n\r\nI don't have experience with open source pull requests, so I suggest that you reflect them in the source.\r\n\r\nThank you for reading :)",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 867088818,
    "title": "Improve ReadInstruction logic and update docs",
    "dateCreated": "2021-04-25T19:07:26Z",
    "dateModified": "2021-04-25T19:07:26Z",
    "description": "Improve ReadInstruction logic and docs.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 866961697,
    "title": "GooAQ dataset added",
    "dateCreated": "2021-04-25T09:26:48Z",
    "dateModified": "2021-04-25T09:26:48Z",
    "description": "@lhoestq here the dataset is stored with Git LFS. Should I add option for manual downloading of dataset using `git lfs pull` post repo cloning or can we accommodate this in the current `download_and_extract`?",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 866880092,
    "title": "Add support for Split.ALL",
    "dateCreated": "2021-04-25T01:45:42Z",
    "dateModified": "2021-04-25T01:45:42Z",
    "description": "The title says it all.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 866870588,
    "title": "Fix incorrect update_metadata_with_features calls in ArrowDataset",
    "dateCreated": "2021-04-25T00:48:38Z",
    "dateModified": "2021-04-25T00:48:38Z",
    "description": "Fixes bugs in the `unpdate_metadata_with_features` calls (caused by changes in #2151)",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 866755203,
    "title": "added metrics for CUAD",
    "dateCreated": "2021-04-24T14:09:54Z",
    "dateModified": "2021-04-24T14:09:54Z",
    "description": "For now I've added F1, AUPR, Precision at 80% recall, and Precision at 90%. Last 3 metrics were reported in the [paper](https://arxiv.org/pdf/2103.06268.pdf). Please let me know if we require `exact_match` metric too here",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 866708609,
    "title": "Running `datase.map` with `num_proc > 1` uses a lot of memory",
    "dateCreated": "2021-04-24T09:56:20Z",
    "dateModified": "2021-04-24T09:56:20Z",
    "description": "## Describe the bug\r\nRunning `datase.map` with `num_proc > 1`  leads to a tremendous memory usage that requires swapping on disk and it becomes very slow.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndstc8_datset = load_dataset(\"roskoN/dstc8-reddit-corpus\", keep_in_memory=False)\r\n\r\n\r\ndef _prepare_sample(batch):\r\n    return {\"input_ids\": list(), \"attention_mask\": list()}\r\n\r\n\r\nfor split_name, dataset_split in list(dstc8_datset.items()):\r\n    print(f\"Processing {split_name}\")\r\n    encoded_dataset_split = dataset_split.map(\r\n        function=_prepare_sample,\r\n        batched=True,\r\n        num_proc=4,\r\n        remove_columns=dataset_split.column_names,\r\n        batch_size=10,\r\n        writer_batch_size=10,\r\n        keep_in_memory=False,\r\n    )\r\n    print(encoded_dataset_split)\r\n\r\n    path = f\"./data/encoded_{split_name}\"\r\n\r\n    encoded_dataset_split.save_to_disk(path)\r\n```\r\n\r\n## Expected results\r\nMemory usage should stay within reasonable boundaries.\r\n\r\n\r\n## Actual results\r\nThis is htop-output from running the provided script.\r\n\r\n![image](https://user-images.githubusercontent.com/8143425/115954836-66954980-a4f3-11eb-8340-0153bdc3a475.png)\r\n\r\n## Versions\r\n```\r\n- Datasets: 1.6.0\r\n- Python: 3.8.8 (default, Apr 13 2021, 19:58:26)\r\n[GCC 7.3.0]\r\n- Platform: Linux-4.19.128-microsoft-standard-x86_64-with-glibc2.10\r\n```\r\nRunning on WSL2\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 866242892,
    "title": "Task casting for text classification & question answering",
    "dateCreated": "2021-04-23T16:00:41Z",
    "dateModified": "2021-04-23T16:00:41Z",
    "description": "This PR implements task preparation for a given task, in the continuation of #2143 \r\n\r\nTask taxonomy follows \ud83e\udd17 Transformers's pipelines taxonomy: https://github.com/huggingface/transformers/tree/master/src/transformers/pipelines\r\n\r\nEdit by @lewtun:\r\n\r\nThis PR implements support for the following tasks:\r\n\r\n* `text-classification`\r\n* `question-answering`\r\n\r\nThe intended usage is as follows:\r\n\r\n```python\r\n# Load a dataset with default column names / features\r\nds = load_dataset(\"dataset_name\")\r\n# Cast column names / features to schema. Casting is defined in the dataset's `DatasetInfo`\r\nds = ds.prepare_for_task(task=\"text-classification\")\r\n# Casting can also be realised during load\r\nds = load_dataset(\"dataset_name\", task=\"text-classification\")\r\n# We can also combine shared tasks across dataset concatenation\r\nds1 = load_dataset(\"dataset_name_1\", task=\"text-classification\")\r\nds2 = load_dataset(\"dataset_name_2\", task=\"text-classification\")\r\n# If the tasks have the same schema, so will `ds_concat`\r\nds_concat = concatenate_datasets([ds1, ds2])\r\n```\r\n\r\nNote that the current implementation assumes that `DatasetInfo.task_templates` has been pre-defined by the user / contributor when overriding the `MyDataset(GeneratorBasedBuilder)._info` function.\r\n\r\nAs pointed out by @SBrandeis, for evaluation we'll need a way to detect which datasets are already have a compatible schema so we don't have to edit hundreds of dataset scripts. One possibility is to check if the schema features are a subset of the dataset ones, e.g.\r\n\r\n```python\r\nsquad = load_dataset(\"./datasets/squad\", split=\"train\")\r\nqa = QuestionAnswering()\r\nschema = Features({**qa.input_schema, **qa.label_schema})\r\nassert all(item in squad.features.items() for item in schema.items())\r\n```",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 866169312,
    "title": "Update format, fingerprint and indices after add_item",
    "dateCreated": "2021-04-23T14:31:49Z",
    "dateModified": "2021-04-23T14:31:49Z",
    "description": "Added fingerprint and format update wrappers + update the indices by adding the index of the newly added item in the table.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 866034321,
    "title": "Perform minor refactoring: use config",
    "dateCreated": "2021-04-23T11:45:47Z",
    "dateModified": "2021-04-23T11:45:47Z",
    "description": "Perform minor refactoring related to `config`.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 865870710,
    "title": "Slow dataloading with big datasets issue persists",
    "dateCreated": "2021-04-23T08:18:20Z",
    "dateModified": "2021-04-23T08:18:20Z",
    "description": "Hi,\r\n\r\nI reported too slow data fetching when data is large(#2210) a couple of weeks ago, and @lhoestq referred me to the fix (#2122).\r\nHowever, the problem seems to persist. Here is the profiled results:\r\n\r\n\r\n1) Running with 60GB\r\n```\r\nAction                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\r\n------------------------------------------------------------------------------------------------------------------------------------\r\nTotal                              \t|  -              \t|_              \t|  517.96         \t|  100 %          \t|\r\n------------------------------------------------------------------------------------------------------------------------------------\r\nmodel_backward                     \t|  0.26144        \t|100            \t|  26.144         \t|  5.0475         \t|\r\nmodel_forward                      \t|  0.11123        \t|100            \t|  11.123         \t|  2.1474         \t|\r\nget_train_batch                    \t|  0.097121       \t|100            \t|  9.7121         \t|  1.8751         \t|\r\n```\r\n\r\n\r\n3) Running with 600GB, datasets==1.6.0\r\n```\r\nAction                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\r\n------------------------------------------------------------------------------------------------------------------------------------\r\nTotal                              \t|  -              \t|_              \t|  4563.2         \t|  100 %          \t|\r\n------------------------------------------------------------------------------------------------------------------------------------\r\nget_train_batch                    \t|  5.1279         \t|100            \t|  512.79         \t|  11.237         \t|\r\nmodel_backward                     \t|  4.8394         \t|100            \t|  483.94         \t|  10.605         \t|\r\nmodel_forward                      \t|  0.12162        \t|100            \t|  12.162         \t|  0.26652        \t|\r\n```\r\n\r\nI see that `get_train_batch` lags when data is large. Could this be related to different issues?\r\nI would be happy to provide necessary information to investigate.",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 865848705,
    "title": "while running run_qa.py, ran into a value error",
    "dateCreated": "2021-04-23T07:51:03Z",
    "dateModified": "2021-04-23T07:51:03Z",
    "description": "command:\r\n\r\npython3 run_qa.py   --model_name_or_path hyunwoongko/kobart    --dataset_name squad_kor_v2   --do_train   --do_eval   --per_device_train_batch_size 8   --learning_rate 3e-5   --num_train_epochs 3   --max_seq_length 512   --doc_stride 128   --output_dir /tmp/debug_squad/\r\n\r\nerror: \r\n\r\nValueError: External features info don't match the dataset:\r\nGot\r\n{'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answer': {'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None), 'html_answer_start': Value(dtype='int32', id=None)}, 'url': Value(dtype='string', id=None), 'raw_html': Value(dtype='string', id=None)}\r\nwith type\r\nstruct<answer: struct<text: string, answer_start: int32, html_answer_start: int32>, context: string, id: string, question: string, raw_html: string, title: string, url: string>\r\n\r\nbut expected something like\r\n{'answer': {'answer_start': Value(dtype='int32', id=None), 'html_answer_start': Value(dtype='int32', id=None), 'text': Value(dtype='string', id=None)}, 'context': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'raw_html': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None)}\r\nwith type\r\nstruct<answer: struct<answer_start: int32, html_answer_start: int32, text: string>, context: string, id: string, question: string, raw_html: string, title: string, url: string>\r\n\r\nI didn't encounter this error 4 hours ago. any solutions for this kind of issue?\r\nlooks like gained dataset format refers to 'Data Fields', while expected refers to 'Data Instances'.",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 865402449,
    "title": "some issue in loading local txt file as Dataset for run_mlm.py",
    "dateCreated": "2021-04-22T19:39:13Z",
    "dateModified": "2021-04-22T19:39:13Z",
    "description": "![image](https://user-images.githubusercontent.com/14968123/115773877-18cef300-a3c6-11eb-8e58-a9cbfd1001ec.png)\r\n\r\nfirst of all, I tried to load 3 .txt files as a dataset (sure that the directory and permission is OK.), I face with the below error.\r\n\r\n> FileNotFoundError: [Errno 2] No such file or directory: 'c'\r\n\r\nby removing one of the training .txt files It's fixed and although if I put all file as training it's ok\r\n![image](https://user-images.githubusercontent.com/14968123/115774207-867b1f00-a3c6-11eb-953b-905cfb112d25.png)\r\n![image](https://user-images.githubusercontent.com/14968123/115774264-9b57b280-a3c6-11eb-9f36-7b109f0e5a31.png)\r\n\r\n\r\nafter this, my question is how could I use this defined Dataset for run_mlm.py for from scratch pretraining.\r\nby using  --train_file path_to_train_file just can use one .txt , .csv or, .json file. I tried to set my defined Dataset as --dataset_name but the below issue occurs.\r\n\r\n\r\n> Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 336, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py\", line 291, in cached_path\r\n    use_auth_token=download_config.use_auth_token,\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py\", line 621, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/master/datasets/dataset/dataset.py\r\n\r\n> During handling of the above exception, another exception occurred:\r\n\r\n> Traceback (most recent call last):\r\n  File \"run_mlm.py\", line 486, in <module>\r\n    main()\r\n  File \"run_mlm.py\", line 242, in main\r\n    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 719, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 347, in prepare_module\r\n    combined_path, github_file_path\r\nFileNotFoundError: Couldn't find file locally at dataset/dataset.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.6.0/datasets/dataset/dataset.py.\r\nThe file is also not present on the master branch on github.\r\n",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 865257826,
    "title": "Allow downloading/processing/caching only specific splits",
    "dateCreated": "2021-04-22T17:51:44Z",
    "dateModified": "2021-04-22T17:51:44Z",
    "description": "Allow downloading/processing/caching only specific splits without downloading/processing/caching the other splits.\r\n\r\nThis PR implements two steps to handle only specific splits:\r\n- it allows processing/caching only specific splits into Arrow files\r\n- for some simple cases, it allows downloading only specific splits (which is more intricate as it depends on the user-defined method `_split_generators`)\r\n\r\nThis PR makes several assumptions:\r\n- `DownloadConfig` contains the configuration settings for downloading\r\n- the parameter `split` passed to `load_dataset` is just a parameter for loading (from cache), not for downloading",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 864853447,
    "title": "Implement Dataset to JSON",
    "dateCreated": "2021-04-22T11:46:51Z",
    "dateModified": "2021-04-22T11:46:51Z",
    "description": "Implement `Dataset.to_json`.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 864817520,
    "title": "Implement Dataset from Parquet",
    "dateCreated": "2021-04-22T11:01:38Z",
    "dateModified": "2021-04-22T11:01:38Z",
    "description": "Implement instantiation of Dataset from Parquet file.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 864220031,
    "title": "Faster map w/ input_columns & faster slicing w/ Iterable keys",
    "dateCreated": "2021-04-21T19:49:07Z",
    "dateModified": "2021-04-21T19:49:07Z",
    "description": "@lhoestq Fixes #2193 \r\n\r\n- `map` now uses `with_format` to only load needed columns in memory when `input_columns` is set\r\n- Slicing datasets with Iterables of indices now uses a new `Table.fast_gather` method, implemented with `np.searchsorted`, to find the appropriate batch indices all at once. `pa.concat_tables` is no longer used for this; we just call `pa.Table.from_batches` with a list of all the batch slices.\r\n\r\nTogether these changes have sped up batched `map()` calls over subsets of columns quite considerably in my initial testing.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 863191655,
    "title": "Add `key` type and duplicates verification with hashing",
    "dateCreated": "2021-04-20T20:03:19Z",
    "dateModified": "2021-04-20T20:03:19Z",
    "description": "Closes #2230 \r\nThere is currently no verification for the data type and the uniqueness of the keys yielded by the `dataset_builder`.\r\nThis PR is currently a work in progress with the following goals:\r\n\r\n-  [x] Adding `hash_salt` to `ArrowWriter` so that the keys belonging to different splits have different hash\r\n-  [x] Add `key` arrtibute to `ArrowWriter.write()` for hashing\r\n-  [x] Add a hashing class which takes an input key of certain type (`str`/`int`/anything convertible to string) and produces a 128-bit hash using `hashlib.md5`\r\n-  [x] Creating a function giving a custom error message when non-unique keys are found \r\n   **[This will take care of type-checking for keys]**\r\n-  [x] Checking for duplicate keys in `writer.write()` for each batch\r\n\r\n[**NOTE**: This PR is currently concerned with `GeneratorBasedBuilder` only, for simplification. A subsequent PR will be made in future for `ArrowBasedBuilder`]\r\n\r\n@lhoestq Thank you for the feedback. It would be great to have your guidance on this!",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 863029946,
    "title": "Set specific cache directories per test function call",
    "dateCreated": "2021-04-20T17:06:22Z",
    "dateModified": "2021-04-20T17:06:22Z",
    "description": "Implement specific cache directories (datasets, metrics and modules) per test function call.\r\n\r\nCurrently, the cache directories are set within the temporary test directory, but they are shared across all test function calls.\r\n\r\nThis PR implements specific cache directories for each test function call, so that tests are atomic and there are no side effects.\r\n\r\n",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 862909389,
    "title": "Map is slow and processes batches one after another",
    "dateCreated": "2021-04-20T14:58:20Z",
    "dateModified": "2021-04-20T14:58:20Z",
    "description": "## Describe the bug\r\n\r\nI have a somewhat unclear bug to me, where I can't figure out what the problem is. The code works as expected on a small subset of my dataset (2000 samples) on my local machine, but when I execute the same code with a larger dataset (1.4 million samples) this problem occurs. Thats why I can't give exact steps to reproduce, I'm sorry. \r\n\r\nI process a large dataset in a two step process. I first call map on a dataset I load from disk and create a new dataset from it. This works like expected and `map` uses all workers I started it with. Then I process the dataset created by the first step, again with `map`, which is really slow and starting only one or two process at a time. Number of processes is the same for both steps.\r\n\r\npseudo code:\r\n```python\r\nds = datasets.load_from_disk(\"path\")\r\nnew_dataset = ds.map(work, batched=True, ...)  # fast uses all processes\r\nfinal_dataset = new_dataset.map(work2, batched=True, ...)  # slow starts one process after another\r\n```\r\n\r\n## Expected results\r\nSecond stage should be as fast as the first stage.\r\n\r\n## Versions\r\nPaste the output of the following code:\r\n- Datasets: 1.5.0\r\n- Python: 3.8.8 (default, Feb 24 2021, 21:46:12)\r\n- Platform: Linux-5.4.0-60-generic-x86_64-with-glibc2.10    \r\n\r\nDo you guys have any idea? Thanks a lot!",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 862870205,
    "title": "Link to datasets viwer on Quick Tour page returns \"502 Bad Gateway\"",
    "dateCreated": "2021-04-20T14:19:51Z",
    "dateModified": "2021-04-20T14:19:51Z",
    "description": "Link to datasets viwer (https://huggingface.co/datasets/viewer/)  on Quick Tour page  (https://huggingface.co/docs/datasets/quicktour.html) returns \"502 Bad Gateway\"\r\n\r\nThe same error with https://huggingface.co/datasets/viewer/?dataset=glue&config=mrpc ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 862696460,
    "title": "Add SLR32 to OpenSLR",
    "dateCreated": "2021-04-20T11:02:45Z",
    "dateModified": "2021-04-20T11:02:45Z",
    "description": "I would like to add SLR32 to OpenSLR. It contains four South African languages: Afrikaans, Sesotho, Setswana and isiXhosa",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 862537856,
    "title": "Clarify how to load wikihow",
    "dateCreated": "2021-04-20T08:02:58Z",
    "dateModified": "2021-04-20T08:02:58Z",
    "description": "Explain clearer how to load the dataset in the manual download instructions.\r\n\r\nEn relation with #2239.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 861904306,
    "title": "Error loading wikihow dataset",
    "dateCreated": "2021-04-19T21:02:31Z",
    "dateModified": "2021-04-19T21:02:31Z",
    "description": "## Describe the bug\r\n\r\nWhen attempting to load wikihow into a dataset with\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('wikihow', data_dir='./wikihow')\r\n```\r\nI get the message:\r\n```\r\nAttributeError: 'BuilderConfig' object has no attribute 'filename'\r\n```\r\nat the end of a [full stack trace](https://gist.github.com/odellus/602c3b2de52f541d353b1022f320ffc2).\r\n\r\n## Steps to reproduce the bug\r\n\r\nI have followed the instructions for creating a wikihow dataset. The [wikihow dataset site](https://huggingface.co/datasets/wikihow) says to use \r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('wikihow')\r\n```\r\nto load the dataset. I do so and I get the message\r\n```\r\nAssertionError: The dataset wikihow with config all requires manual data.\r\n Please follow the manual download instructions:   You need to manually download two wikihow files. An overview of which files to download can be seen at https://github.com/mahnazkoupaee/WikiHow-Dataset.\r\n  You need to download the following two files manually:\r\n    1) https://ucsb.app.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358 and save the file under <path/to/folder>/wikihowAll.csv\r\n    2) https://ucsb.app.box.com/s/7yq601ijl1lzvlfu4rjdbbxforzd2oag and save the file under <path/to/folder>/wikihowSep.csv\r\n\r\n  The <path/to/folder> can e.g. be \"~/manual_wikihow_data\".\r\n\r\n  Wikihow can then be loaded using the following command `datasets.load_dataset(\"wikihow\", data_dir=\"<path/to/folder>\")`.\r\n  .\r\n Manual data can be loaded with `datasets.load_dataset(wikihow, data_dir='<path/to/manual/data>')\r\n```\r\n\r\nSo I create a directory `./wikihow` and download `wikihowAll.csv` and `wikihowSep.csv` into the new directory.\r\n\r\nThen I run \r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('wikihow', data_dir='./wikihow')\r\n```\r\n\r\nthat's when I get the [stack trace](https://gist.github.com/odellus/602c3b2de52f541d353b1022f320ffc2)\r\n\r\n## Expected results\r\nI expected it to load the downloaded files into a dataset.\r\n\r\n## Actual results\r\n```python\r\nUsing custom data configuration default-data_dir=.%2Fwikihow\r\nDownloading and preparing dataset wikihow/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/azureuser/.cache/huggingface/datasets/wikihow/default-data_dir=.%2Fwikihow/0.0.0/58f42f8f0e4d459811a0f69aaab35870093830ccd58006769e7e1eb3e0e686c2...                                                    ---------------------------------------------------------------------------\r\nAttributeError\r\nTraceback (most recent call last)\r\n<ipython-input-9-5e4d40142f30> in <module>\r\n----> 1 dataset = load_dataset('wikihow',data_dir='./wikihow')\r\n~/.local/lib/python3.6/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)\r\n745         try_from_hf_gcs=try_from_hf_gcs,\r\n746         base_path=base_path,--> \r\n747         use_auth_token=use_auth_token,\r\n748     )\r\n749 \r\n~/.local/lib/python3.6/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n577                     if not downloaded_from_gcs:\r\n578                         self._download_and_prepare(                                                             -->\r\n579                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs        \r\n580                         )                                                                                          \r\n581                     # Sync info\r\n~/.local/lib/python3.6/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n632         split_dict = SplitDict(dataset_name=self.name)\r\n633         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)                      -->\r\n634         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)                            \r\n635                                                                                                                     \r\n636         # Checksums verification\r\n~/.cache/huggingface/modules/datasets_modules/datasets/wikihow/58f42f8f0e4d459811a0f69aaab35870093830ccd58006769e7e1eb3e0e686c2/wikihow.py in _split_generators(self, dl_manager)\r\n132\r\n133         path_to_manual_file = os.path.join(\r\n--> 134             os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), self.config.filename                        \r\n135         )                                                                                                           \r\n136\r\nAttributeError: 'BuilderConfig' object has no attribute 'filename'\r\n```\r\n## Versions\r\nPaste the output of the following code:\r\n```python\r\nimport datasets\r\nimport sys\r\nimport platform\r\n\r\nprint(f\"\"\"\r\n- Datasets: {datasets.__version__}\r\n- Python: {sys.version}\r\n- Platform: {platform.platform()}\r\n\"\"\")\r\n```\r\n```\r\n- Datasets: 1.5.0\r\n- Python: 3.6.9 (default, Jan 26 2021, 15:33:00)                                      [GCC 8.4.0]\r\n- Platform: Linux-5.4.0-1046-azure-x86_64-with-Ubuntu-18.04-bionic\r\n```",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 861518291,
    "title": "NLU evaluation data",
    "dateCreated": "2021-04-19T16:47:20Z",
    "dateModified": "2021-04-19T16:47:20Z",
    "description": "New intent classification dataset from https://github.com/xliuhw/NLU-Evaluation-Data",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 861427439,
    "title": "Update Dataset.dataset_size after transformed with map",
    "dateCreated": "2021-04-19T15:19:38Z",
    "dateModified": "2021-04-19T15:19:38Z",
    "description": "After loading a dataset, if we transform it by using `.map` its `dataset_size` attirbute is not updated.",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 861388145,
    "title": "Request to add StrategyQA dataset",
    "dateCreated": "2021-04-19T14:46:26Z",
    "dateModified": "2021-04-19T14:46:26Z",
    "description": "## Request to add StrategyQA dataset\r\n- **Name:** StrategyQA\r\n- **Description:** open-domain QA [(project page)](https://allenai.org/data/strategyqa)\r\n- **Paper:** [url](https://arxiv.org/pdf/2101.02235.pdf)\r\n- **Data:** [here](https://allenai.org/data/strategyqa)\r\n- **Motivation:** uniquely-formulated dataset that also includes a question-decomposition breakdown and associated Wikipedia annotations for each step. Good for multi-hop reasoning modeling.\r\n",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 861040716,
    "title": "Update README.md",
    "dateCreated": "2021-04-19T08:21:02Z",
    "dateModified": "2021-04-19T08:21:02Z",
    "description": "Adding relevant citations (paper accepted at AAAI 2020 & EMNLP 2020) to the benchmark",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 860442246,
    "title": "Fix bash snippet formatting in ADD_NEW_DATASET.md",
    "dateCreated": "2021-04-17T16:01:08Z",
    "dateModified": "2021-04-17T16:01:08Z",
    "description": "This PR indents the paragraphs around the bash snippets in ADD_NEW_DATASET.md to fix formatting.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 860097084,
    "title": "Fix `xnli` dataset tuple key",
    "dateCreated": "2021-04-16T19:12:42Z",
    "dateModified": "2021-04-16T19:12:42Z",
    "description": "Closes #2229 \r\nThe `xnli` dataset yields a tuple key in case of `ar` which is inconsistant with the acceptable key types (str/int).\r\nThe key was thus ported to `str` keeping the original information intact.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 860075931,
    "title": "Start filling GLUE dataset card",
    "dateCreated": "2021-04-16T18:37:37Z",
    "dateModified": "2021-04-16T18:37:37Z",
    "description": "The dataset card was pretty much empty.\r\n\r\nI added the descriptions (mainly from TFDS since the script is the same), and I also added the tasks tags as well as examples for a subset of the tasks.\r\n\r\ncc @sgugger ",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 859850488,
    "title": "Fix map when removing columns on a formatted dataset",
    "dateCreated": "2021-04-16T14:08:55Z",
    "dateModified": "2021-04-16T14:08:55Z",
    "description": "This should fix issue #2226\r\n\r\nThe `remove_columns` argument was ignored on formatted datasets",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 859817159,
    "title": "Keys yielded while generating dataset are not being checked",
    "dateCreated": "2021-04-16T13:29:47Z",
    "dateModified": "2021-04-16T13:29:47Z",
    "description": "The keys used in the dataset generation script to ensure the same order is generated on every user's end should be checked for their types (i.e either `str` or `int`) as well as whether they are unique or not.\r\nCurrently, the keys are not being checked for any of these, as evident from `xnli' dataset generation:\r\nhttps://github.com/huggingface/datasets/blob/56346791aed417306d054d89bd693d6b7eab17f7/datasets/xnli/xnli.py#L196\r\nEven after having a tuple as key, the dataset is generated without any warning.\r\n\r\nAlso, as tested in the case of `anli` dataset (I tweeked the dataset script to use `1` as a key for every example):\r\n```\r\n>>> import datasets\r\n>>> nik = datasets.load_dataset('anli')\r\nDownloading and preparing dataset anli/plain_text (download: 17.76 MiB, generated: 73.55 MiB, post-processed: Unknown size, total: 91.31 MiB) to C:\\Users\\nikhil\\.cache\\huggingface\\datasets\\anli\\plain_text\\0.1.0\\43fa2c99c10bf8478f1fa0860f7b122c6b277c4c41306255b7641257cf4e3299...\r\n0 examples [00:00, ? examples/s]1        {'uid': '0fd0abfb-659e-4453-b196-c3a64d2d8267', 'premise': 'The Parma trolleybus system (Italian: \"Rete filoviaria di Parma\" ) forms part of the public transport network of the city and \"comune\" of Parma, in the region of Emilia-Romagna, northern Italy. In operation since 1953, the system presently comprises four urban routes.', 'hypothesis': 'The trolleybus system has over 2 urban routes', 'label': 'entailment', 'reason': ''}\r\n2021-04-16 12:38:14.483968: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n1 examples [00:01,  1.87s/ examples]1    {'uid': '7ed72ff4-40b7-4f8a-b1b9-6c612aa62c84', 'premise': 'Alexandra Lendon Bastedo (9 March 1946 \u2013 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series \"The Champions\". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate.', 'hypothesis': \"Sharron Macready was a popular character through the 1980's.\", 'label': 'neutral', 'reason': ''}\r\n1        {'uid': '5d2930a3-62ac-485d-94d7-4e36cbbcd7b5', 'premise': 'Alexandra Lendon Bastedo (9 March 1946 \u2013 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series \"The Champions\". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate.', 'hypothesis': \"Bastedo didn't keep any pets because of her views on animal rights.\", 'label': 'neutral', 'reason': ''}\r\n1        {'uid': '324db753-ddc9-4a85-a825-f09e2e5aebdd', 'premise': 'Alexandra Lendon Bastedo (9 March 1946 \u2013 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series \"The Champions\". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate.', 'hypothesis': 'Alexandra Bastedo was named by her mother.', 'label': 'neutral', 'reason': ''}\r\n1        {'uid': '4874f429-da0e-406a-90c7-22240ff3ddf8', 'premise': 'Alexandra Lendon Bastedo (9 March 1946 \u2013 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series \"The Champions\". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate.', 'hypothesis': 'Bastedo cared for all the animals that inhabit the earth.', 'label': 'neutral', 'reason': ''}\r\n```\r\nHere also, the dataset was generated successfuly even hough it had same keys without any warning.\r\n\r\nThe reason appears to stem from here:\r\nhttps://github.com/huggingface/datasets/blob/56346791aed417306d054d89bd693d6b7eab17f7/src/datasets/builder.py#L988\r\nHere, although it has access to every key, but it is not being checked and the example is written directly:\r\nhttps://github.com/huggingface/datasets/blob/56346791aed417306d054d89bd693d6b7eab17f7/src/datasets/builder.py#L992\r\n\r\nI would like to take this issue if you allow me. Thank You!",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 859810602,
    "title": "`xnli` dataset creating a tuple key while yielding instead of `str` or `int`",
    "dateCreated": "2021-04-16T13:21:53Z",
    "dateModified": "2021-04-16T13:21:53Z",
    "description": "When using  `ds = datasets.load_dataset('xnli', 'ar')`, the dataset generation script uses the following section of code in the egging, which yields a tuple key instead of the specified `str` or `int` key:\r\nhttps://github.com/huggingface/datasets/blob/56346791aed417306d054d89bd693d6b7eab17f7/datasets/xnli/xnli.py#L196\r\n\r\nSince, community datasets in Tensorflow Datasets also use HF datasets, this causes a Tuple key error while loading HF's `xnli` dataset. \r\nI'm up for sending a fix for this, I think we can simply use `file_idx + \"_\" + row_idx` as a unique key instead of a tuple.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 859795563,
    "title": "[WIP] Add ArrayXD support for fixed size list.",
    "dateCreated": "2021-04-16T13:04:08Z",
    "dateModified": "2021-04-16T13:04:08Z",
    "description": "Add support for fixed size list for ArrayXD when shape is known . See https://github.com/huggingface/datasets/issues/2146\r\nSince offset are not stored anymore, the file size is now roughly equal to the actual data size. ",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 859771526,
    "title": "Use update_metadata_with_features decorator in class_encode_column method",
    "dateCreated": "2021-04-16T12:31:41Z",
    "dateModified": "2021-04-16T12:31:41Z",
    "description": "Following @mariosasko 's comment",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 859720302,
    "title": "Batched map fails when removing all columns",
    "dateCreated": "2021-04-16T11:17:01Z",
    "dateModified": "2021-04-16T11:17:01Z",
    "description": "Hi @lhoestq ,\r\n\r\nI'm hijacking this issue, because I'm currently trying to do the approach you recommend:\r\n\r\n> Currently the optimal setup for single-column computations is probably to do something like\r\n> \r\n> ```python\r\n> result = dataset.map(f, input_columns=\"my_col\", remove_columns=dataset.column_names)\r\n> ```\r\n\r\nHere is my code: (see edit, in which I added a simplified version\r\n\r\n```\r\nThis is the error:\r\n```bash\r\npyarrow.lib.ArrowInvalid: Column 1 named tokens expected length 8964 but got length 1000\r\n```\r\nI wonder why this error occurs, when I delete every column? Can you give me a hint?\r\n\r\n### Edit:\r\nI preprocessed my dataset before (using map with the features argument) and saved it to disk. May this be part of the error?  I can iterate over the\r\ncomplete dataset and print every sample before calling map. There seems to be no other problem with the dataset.\r\n\r\nI tried to simplify the code that crashes:\r\n\r\n```python\r\n# works\r\nlog.debug(dataset.column_names)\r\nlog.debug(dataset)\r\nfor i, sample in enumerate(dataset):\r\n    log.debug(i, sample)\r\n\r\n# crashes\r\ncounted_dataset = dataset.map(\r\n    lambda x: {\"a\": list(range(20))},\r\n    input_columns=column,\r\n    remove_columns=dataset.column_names,\r\n    load_from_cache_file=False,\r\n    num_proc=num_workers,\r\n    batched=True,\r\n)\r\n```\r\n\r\n```\r\npyarrow.lib.ArrowInvalid: Column 1 named tokens expected length 20 but got length 1000\r\n```\r\n\r\nEdit2: \r\n\r\nMay this be a problem with a schema I set when preprocessing the dataset before? I tried to add the `features` argument to the function and then I get a new error:\r\n\r\n```python\r\n# crashes\r\ncounted_dataset = dataset.map(\r\n    lambda x: {\"a\": list(range(20))},\r\n    input_columns=column,\r\n    remove_columns=dataset.column_names,\r\n    load_from_cache_file=False,\r\n    num_proc=num_workers,\r\n    batched=True,\r\n    features=datasets.Features(\r\n        {\r\n              \"a\": datasets.Sequence(datasets.Value(\"int32\"))\r\n         }\r\n    )\r\n)\r\n```\r\n\r\n```\r\n File \"env/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1704, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"env/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 312, in write_batch\r\n    col_type = schema.field(col).type if schema is not None else None\r\n  File \"pyarrow/types.pxi\", line 1341, in pyarrow.lib.Schema.field\r\nKeyError: 'Column tokens does not exist in schema'\r\n```\r\n\r\n_Originally posted by @villmow in https://github.com/huggingface/datasets/issues/2193#issuecomment-820230874_",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 858469561,
    "title": "fixed one instance of 'train' to 'test'",
    "dateCreated": "2021-04-15T04:26:40Z",
    "dateModified": "2021-04-15T04:26:40Z",
    "description": "I believe this should be 'test' instead of 'train'",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 857983361,
    "title": "Raise error if Windows max path length is not disabled",
    "dateCreated": "2021-04-14T14:57:20Z",
    "dateModified": "2021-04-14T14:57:20Z",
    "description": "On startup, raise an error if Windows max path length is not disabled; ask the user to disable it.\r\n\r\nLinked to discussion in #2220.",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 857870800,
    "title": "Set test cache config",
    "dateCreated": "2021-04-14T12:55:24Z",
    "dateModified": "2021-04-14T12:55:24Z",
    "description": "Currently, running the tests populates the default cache directory `\"~/.cache\"`.\r\n\r\nThis PR monkey-patches the config to set the cache directory within the temporary test directory, avoiding side effects.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 857847231,
    "title": "Fix too long WindowsFileLock name",
    "dateCreated": "2021-04-14T12:26:52Z",
    "dateModified": "2021-04-14T12:26:52Z",
    "description": "Fix WindowsFileLock name longer than allowed MAX_PATH by shortening the basename.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 857833770,
    "title": "Add SLR70 - SLR80 and SLR86 to OpenSLR dataset",
    "dateCreated": "2021-04-14T12:09:18Z",
    "dateModified": "2021-04-14T12:09:18Z",
    "description": "I would like to add SLR70, SLR71, SLR72, SLR73, SLR74, SLR75, SLR76, SLR77, SLR78, SLR79, SLR80 and SLR86 to OpenSLR dataset. The languages are:\r\nNigerian English, Chilean Spanish, Columbian Spanish, Peruvian Spanish, Puerto Rico Spanish, Venezuelan Spanish, Basque, Galician, Gujarati and Kannada.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 857774626,
    "title": "Fix infinite loop in WindowsFileLock",
    "dateCreated": "2021-04-14T10:49:58Z",
    "dateModified": "2021-04-14T10:49:58Z",
    "description": "Raise exception to avoid infinite loop.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 857321242,
    "title": "Added CUAD dataset",
    "dateCreated": "2021-04-13T21:05:03Z",
    "dateModified": "2021-04-13T21:05:03Z",
    "description": "Dataset link : https://github.com/TheAtticusProject/cuad/\r\n\r\nWorking on README.md currently.\r\n\r\nCloses #2084 and [#1](https://github.com/TheAtticusProject/cuad/issues/1). ",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 857238435,
    "title": "Duplicates in the LAMA dataset",
    "dateCreated": "2021-04-13T18:59:49Z",
    "dateModified": "2021-04-13T18:59:49Z",
    "description": "I observed duplicates in the LAMA probing dataset, see a minimal code below. \r\n\r\n```\r\n>>> import datasets\r\n>>> dataset = datasets.load_dataset('lama')\r\nNo config specified, defaulting to: lama/trex\r\nReusing dataset lama (/home/anam/.cache/huggingface/datasets/lama/trex/1.1.0/97deffae13eca0a18e77dfb3960bb31741e973586f5c1fe1ec0d6b5eece7bddc)\r\n>>> train_dataset = dataset['train']\r\n>>> train_dataset[0]\r\n{'description': 'language or languages a person has learned from early childhood', 'label': 'native language', 'masked_sentence': 'Louis Jules Trochu ([lwi \u0292yl t\u0281\u0254\u0283y]; 12 March 1815 \u2013 7 October 1896) was a [MASK] military leader and politician.', 'obj_label': 'French', 'obj_surface': 'French', 'obj_uri': 'Q150', 'predicate_id': 'P103', 'sub_label': 'Louis Jules Trochu', 'sub_surface': 'Louis Jules Trochu', 'sub_uri': 'Q441235', 'template': 'The native language of [X] is [Y] .', 'template_negated': '[X] is not owned by [Y] .', 'type': 'N-1', 'uuid': '40b2ed1c-0961-482e-844e-32596b6117c8'}\r\n>>> train_dataset[1]\r\n{'description': 'language or languages a person has learned from early childhood', 'label': 'native language', 'masked_sentence': 'Louis Jules Trochu ([lwi \u0292yl t\u0281\u0254\u0283y]; 12 March 1815 \u2013 7 October 1896) was a [MASK] military leader and politician.', 'obj_label': 'French', 'obj_surface': 'French', 'obj_uri': 'Q150', 'predicate_id': 'P103', 'sub_label': 'Louis Jules Trochu', 'sub_surface': 'Louis Jules Trochu', 'sub_uri': 'Q441235', 'template': 'The native language of [X] is [Y] .', 'template_negated': '[X] is not owned by [Y] .', 'type': 'N-1', 'uuid': '40b2ed1c-0961-482e-844e-32596b6117c8'}\r\n```\r\n\r\nI checked the original data available at https://dl.fbaipublicfiles.com/LAMA/data.zip. This particular duplicated comes from:\r\n```\r\n{\"uuid\": \"40b2ed1c-0961-482e-844e-32596b6117c8\", \"obj_uri\": \"Q150\", \"obj_label\": \"French\", \"sub_uri\": \"Q441235\", \"sub_label\": \"Louis Jules Trochu\", \"predicate_id\": \"P103\", \"evidences\": [{\"sub_surface\": \"Louis Jules Trochu\", \"obj_surface\": \"French\", \"masked_sentence\": \"Louis Jules Trochu ([lwi \\u0292yl t\\u0281\\u0254\\u0283y]; 12 March 1815 \\u2013 7 October 1896) was a [MASK] military leader and politician.\"}, {\"sub_surface\": \"Louis Jules Trochu\", \"obj_surface\": \"French\", \"masked_sentence\": \"Louis Jules Trochu ([lwi \\u0292yl t\\u0281\\u0254\\u0283y]; 12 March 1815 \\u2013 7 October 1896) was a [MASK] military leader and politician.\"}]}\r\n``` \r\n\r\nWhat is the best way to deal with these duplicates if I want to use `datasets` to probe with LAMA?  ",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 857011314,
    "title": "Revert breaking change in cache_files property",
    "dateCreated": "2021-04-13T14:20:04Z",
    "dateModified": "2021-04-13T14:20:04Z",
    "description": "#2025 changed the format of `Dataset.cache_files`.\r\nBefore it was formatted like\r\n```python\r\n[{\"filename\": \"path/to/file.arrow\", \"start\": 0, \"end\": 1337}]\r\n```\r\nand it was changed to\r\n```python\r\n[\"path/to/file.arrow\"]\r\n```\r\nsince there's no start/end offsets available anymore.\r\n\r\nTo make this less breaking, I'm setting the format back to a list of dicts:\r\n```python\r\n[{\"filename\": \"path/to/file.arrow\"}]\r\n```",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 856955534,
    "title": "added real label for glue/mrpc to test set",
    "dateCreated": "2021-04-13T13:20:20Z",
    "dateModified": "2021-04-13T13:20:20Z",
    "description": "Added real label to `glue.py` `mrpc` task for test split.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 856716791,
    "title": "Add datasets SLR35 and SLR36 to OpenSLR ",
    "dateCreated": "2021-04-13T08:24:07Z",
    "dateModified": "2021-04-13T08:24:07Z",
    "description": "I would like to add [SLR35](https://openslr.org/35/) (18GB) and [SLR36](https://openslr.org/36/)  (22GB) which are Large Javanese and Sundanese ASR training data set collected by Google in collaboration with Reykjavik University and Universitas Gadjah Mada in Indonesia.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 856333657,
    "title": "load_metric error: module 'datasets.utils.file_utils' has no attribute 'add_start_docstrings'",
    "dateCreated": "2021-04-12T20:26:01Z",
    "dateModified": "2021-04-12T20:26:01Z",
    "description": "I'm having the same problem as [Notebooks issue 10](https://github.com/huggingface/notebooks/issues/10) on datasets 1.2.1, and it seems to be an issue with the datasets package.\r\n\r\n```python\r\n>>> from datasets import load_metric\r\n>>> metric = load_metric(\"glue\", \"sst2\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/ext3/miniconda3/lib/python3.8/site-packages/datasets-1.2.1-py3.8.egg/datasets/load.py\", line 502, in load_metric\r\n  File \"/ext3/miniconda3/lib/python3.8/site-packages/datasets-1.2.1-py3.8.egg/datasets/load.py\", line 66, in import_main_class\r\n  File \"/ext3/miniconda3/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/ns4008/.cache/huggingface/modules/datasets_modules/metrics/glue/e4606ab9804a36bcd5a9cebb2cb65bb14b6ac78ee9e6d5981fa679a495dd55de/glue.py\", line 105, in <module>\r\n    @datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\r\nAttributeError: module 'datasets.utils.file_utils' has no attribute 'add_start_docstrings'\r\n```",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 856025320,
    "title": "Fix lc_quad download checksum",
    "dateCreated": "2021-04-12T14:16:59Z",
    "dateModified": "2021-04-12T14:16:59Z",
    "description": "Fixes #2211 ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 855999133,
    "title": "Can't reach \"https://storage.googleapis.com/illuin/fquad/train.json.zip\" when trying to load fquad dataset",
    "dateCreated": "2021-04-12T13:49:56Z",
    "dateModified": "2021-04-12T13:49:56Z",
    "description": "I'm trying to load the [fquad dataset](https://huggingface.co/datasets/fquad) by running: \r\n\r\n```Python\r\nfquad = load_dataset(\"fquad\")\r\n```\r\n\r\nwhich produces the following error:\r\n\r\n```\r\nUsing custom data configuration default\r\n\r\nDownloading and preparing dataset fquad/default (download: 3.14 MiB, generated: 6.62 MiB, post-processed: Unknown size, total: 9.76 MiB) to /root/.cache/huggingface/datasets/fquad/default/0.1.0/778dc2c85813d05ddd0c17087294d5f8f24820752340958070876b677af9f061...\r\n\r\n---------------------------------------------------------------------------\r\n\r\nConnectionError                           Traceback (most recent call last)\r\n\r\n<ipython-input-48-a2721797e23b> in <module>()\r\n----> 1 fquad = load_dataset(\"fquad\")\r\n\r\n11 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\r\n    614             raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\n    615         _raise_if_offline_mode_is_enabled(f\"Tried to reach {url}\")\r\n--> 616         raise ConnectionError(\"Couldn't reach {}\".format(url))\r\n    617 \r\n    618     # Try a second time\r\n\r\nConnectionError: Couldn't reach https://storage.googleapis.com/illuin/fquad/train.json.zip\r\n```\r\n\r\nDoes anyone know why that is and how to fix it? ",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 855988410,
    "title": "Getting checksum error when trying to load lc_quad dataset",
    "dateCreated": "2021-04-12T13:38:58Z",
    "dateModified": "2021-04-12T13:38:58Z",
    "description": "I'm having issues loading the [lc_quad](https://huggingface.co/datasets/fquad) dataset by running:\r\n\r\n```Python\r\nlc_quad = load_dataset(\"lc_quad\")\r\n```\r\n\r\nwhich is giving me the following error:\r\n\r\n``` \r\nUsing custom data configuration default\r\n\r\nDownloading and preparing dataset lc_quad/default (download: 3.69 MiB, generated: 19.77 MiB, post-processed: Unknown size, total: 23.46 MiB) to /root/.cache/huggingface/datasets/lc_quad/default/2.0.0/5a98fe174603f5dec6df07edf1c2b4d2317210d2ad61f5a393839bca4d64e5a7...\r\n\r\n---------------------------------------------------------------------------\r\n\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n\r\n<ipython-input-42-404ace83f73c> in <module>()\r\n----> 1 lc_quad = load_dataset(\"lc_quad\")\r\n\r\n3 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     37     if len(bad_urls) > 0:\r\n     38         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 39         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     40     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     41 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://github.com/AskNowQA/LC-QuAD2.0/archive/master.zip']\r\n```\r\n\r\nDoes anyone know why this could be and how I fix it? ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 855709400,
    "title": "dataloading slow when using HUGE dataset",
    "dateCreated": "2021-04-12T08:33:02Z",
    "dateModified": "2021-04-12T08:33:02Z",
    "description": "Hi,\r\n\r\nWhen I use datasets with 600GB data, the dataloading speed increases significantly. \r\nI am experimenting with two datasets, and one is about 60GB and the other 600GB.\r\nSimply speaking, my code uses `datasets.set_format(\"torch\")` function and let pytorch-lightning handle ddp training.\r\nWhen looking at the pytorch-lightning supported profile of two different runs, I see that fetching a batch(`get_train_batch`) consumes an unreasonable amount of time when data is large. What could be the cause?\r\n\r\n* 60GB data\r\n```\r\nAction                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\r\n------------------------------------------------------------------------------------------------------------------------------------\r\nTotal                              \t|  -              \t|_              \t|  200.33         \t|  100 %          \t|\r\n------------------------------------------------------------------------------------------------------------------------------------\r\nrun_training_epoch                 \t|  71.994         \t|1              \t|  71.994         \t|  35.937         \t|\r\nrun_training_batch                 \t|  0.64373        \t|100            \t|  64.373         \t|  32.133         \t|\r\noptimizer_step_and_closure_0       \t|  0.64322        \t|100            \t|  64.322         \t|  32.108         \t|\r\ntraining_step_and_backward         \t|  0.61004        \t|100            \t|  61.004         \t|  30.452         \t|\r\nmodel_backward                     \t|  0.37552        \t|100            \t|  37.552         \t|  18.745         \t|\r\nmodel_forward                      \t|  0.22813        \t|100            \t|  22.813         \t|  11.387         \t|\r\ntraining_step                      \t|  0.22759        \t|100            \t|  22.759         \t|  11.361         \t|\r\nget_train_batch                    \t|  0.066385       \t|100            \t|  6.6385         \t|  3.3138         \t|\r\n```\r\n\r\n* 600GB data\r\n```\r\nAction                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\r\n------------------------------------------------------------------------------------------------------------------------------------\r\nTotal                              \t|  -              \t|_              \t|  3285.6         \t|  100 %          \t|\r\n------------------------------------------------------------------------------------------------------------------------------------\r\nrun_training_epoch                 \t|  1397.9         \t|1              \t|  1397.9         \t|  42.546         \t|\r\nrun_training_batch                 \t|  7.2596         \t|100            \t|  725.96         \t|  22.095         \t|\r\noptimizer_step_and_closure_0       \t|  7.2589         \t|100            \t|  725.89         \t|  22.093         \t|\r\ntraining_step_and_backward         \t|  7.223          \t|100            \t|  722.3          \t|  21.984         \t|\r\nmodel_backward                     \t|  6.9662         \t|100            \t|  696.62         \t|  21.202         \t|\r\nget_train_batch                    \t|  6.322          \t|100            \t|  632.2          \t|  19.241         \t|\r\nmodel_forward                      \t|  0.24902        \t|100            \t|  24.902         \t|  0.75789        \t|\r\ntraining_step                      \t|  0.2485         \t|100            \t|  24.85          \t|  0.75633        \t|\r\n```\r\n",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 855638232,
    "title": "Add code of conduct to the project",
    "dateCreated": "2021-04-12T07:16:14Z",
    "dateModified": "2021-04-12T07:16:14Z",
    "description": "Add code of conduct to the project and link it from README and CONTRIBUTING.\r\n\r\nThis was already done in `transformers`.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 855343835,
    "title": "Remove Python2 leftovers",
    "dateCreated": "2021-04-11T16:08:03Z",
    "dateModified": "2021-04-11T16:08:03Z",
    "description": "This PR removes Python2 leftovers since this project aims for Python3.6+ (and as of 2020 Python2 is no longer officially supported)",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 855267383,
    "title": "making labels consistent across the datasets",
    "dateCreated": "2021-04-11T10:03:56Z",
    "dateModified": "2021-04-11T10:03:56Z",
    "description": "Hi\r\nFor accessing the labels one can type \r\n```\r\n>>> a.features['label']\r\nClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], names_file=None, id=None)\r\n```\r\nThe labels however are not consistent with the actual labels sometimes, for instance in case of XNLI, the actual labels are 0,1,2, but if one try to access as above they are entailment, neutral,contradiction,\r\nit would be great to have the labels consistent.\r\n\r\nthanks \r\n",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 855252415,
    "title": "Got pyarrow error when loading a dataset while adding special tokens into the tokenizer",
    "dateCreated": "2021-04-11T08:40:09Z",
    "dateModified": "2021-04-11T08:40:09Z",
    "description": "I added five more special tokens into the GPT2 tokenizer. But after that, when I try to pre-process the data using my previous code, I got an error shown below:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1687, in _map_single\r\n    writer.write(example)\r\n  File \"/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/datasets/arrow_writer.py\", line 296, in write\r\n    self.write_on_file()\r\n  File \"/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/datasets/arrow_writer.py\", line 270, in write_on_file\r\n    pa_array = pa.array(typed_sequence)\r\n  File \"pyarrow/array.pxi\", line 222, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/datasets/arrow_writer.py\", line 108, in __arrow_array__\r\n    out = out.cast(pa.list_(self.optimized_int_type))\r\n  File \"pyarrow/array.pxi\", line 810, in pyarrow.lib.Array.cast\r\n  File \"/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/pyarrow/compute.py\", line 281, in cast\r\n    return call_function(\"cast\", [arr], options)\r\n  File \"pyarrow/_compute.pyx\", line 465, in pyarrow._compute.call_function\r\n  File \"pyarrow/_compute.pyx\", line 294, in pyarrow._compute.Function.call\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Integer value 50259 not in range: -128 to 127\r\n\r\nDo you have any idea about it?",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 855207605,
    "title": "Updating citation information on LinCE readme",
    "dateCreated": "2021-04-11T03:18:05Z",
    "dateModified": "2021-04-11T03:18:05Z",
    "description": "Hi!\r\n\r\nI just updated the citation information in this PR. It had an additional bibtex from one of the datasets used in LinCE and then the LinCE bibtex. I removed the former and added a link that shows the full list of citations for each dataset. \r\n\r\nThanks!",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 855144431,
    "title": "Add configurable options to `seqeval` metric",
    "dateCreated": "2021-04-10T19:58:19Z",
    "dateModified": "2021-04-10T19:58:19Z",
    "description": "Fixes #2148\r\n\r\nAdds options to use strict mode, different schemes of evaluation, sample weight and adjust zero_division behavior, if encountered.\r\n\r\n`seqeval` provides schemes as objects, hence dynamic import from string, to avoid making the user do the import (thanks to @albertvillanova for the `importlib` idea).",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 855053595,
    "title": "updated banking77 train and test data",
    "dateCreated": "2021-04-10T12:10:10Z",
    "dateModified": "2021-04-10T12:10:10Z",
    "description": "",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 854501109,
    "title": "Add classes GenerateMode, DownloadConfig and Version to the documentation",
    "dateCreated": "2021-04-09T12:58:19Z",
    "dateModified": "2021-04-09T12:58:19Z",
    "description": "Add documentation for classes `GenerateMode`, `DownloadConfig` and `Version`.\r\n\r\nUpdate the docstring of `load_dataset` to create cross-reference links to the classes.\r\n\r\nRelated to #2187.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 854499563,
    "title": "Fix ArrowWriter overwriting features in ArrowBasedBuilder",
    "dateCreated": "2021-04-09T12:56:19Z",
    "dateModified": "2021-04-09T12:56:19Z",
    "description": "This should fix the issues with CSV loading experienced in #2153 and #2200.\r\n\r\nThe CSV builder is an ArrowBasedBuilder that had an issue with its ArrowWriter used to write the arrow file from the csv data.\r\nThe writer wasn't initialized with the features passed by the user. Therefore the writer was inferring the features from the arrow data, discarding the features passed by the user.\r\n\r\nI fixed that and I updated the tests",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 854449656,
    "title": "_prepare_split will overwrite DatasetBuilder.info.features",
    "dateCreated": "2021-04-09T11:47:13Z",
    "dateModified": "2021-04-09T11:47:13Z",
    "description": "Hi, here is my issue:\r\nI initialized a Csv datasetbuilder with specific features:\r\n```\r\ndef get_dataset_features(data_args):\r\n    features = {}\r\n    if data_args.text_features:\r\n        features.update({text_feature: hf_features.Value(\"string\") for text_feature in data_args.text_features.strip().split(\",\")})\r\n    if data_args.num_features:\r\n        features.update({text_feature: hf_features.Value(\"float32\") for text_feature in data_args.num_features.strip().split(\",\")})\r\n    if data_args.label_classes:\r\n        features[\"label\"] = hf_features.ClassLabel(names=data_args.label_classes.strip().split(\",\"))\r\n    else:\r\n        features[\"label\"] = hf_features.Value(\"float32\")\r\n    return hf_features.Features(features)\r\n\r\ndatasets = load_dataset(extension,\r\n                                data_files=data_files,\r\n                                sep=data_args.delimiter,\r\n                                header=data_args.header,\r\n                                column_names=data_args.column_names.split(\",\") if data_args.column_names else None,\r\n                                features=get_dataset_features(data_args=data_args))\r\n```\r\nThe `features` is printout as below before `builder_instance.as_dataset` is called:\r\n```\r\n{'label': ClassLabel(num_classes=2, names=['unacceptable', 'acceptable'], names_file=None, id=None), 'notated': Value(dtype='string', id=None), 'sentence': Value(dtype='string', id=None), 'src_code': Value(dtype='string', id=None)}\r\n````\r\n\r\nBut after the `builder_instance.as_dataset` is called for Csv dataset builder, the `features` is changed to:\r\n```\r\n{'label': Value(dtype='int64', id=None), 'notated': Value(dtype='string', id=None), 'sentence': Value(dtype='string', id=None), 'src_code': Value(dtype='string', id=None)}\r\n```\r\n\r\nAfter digged into the code, I releazed that in `ArrowBasedBuilder._prepare_split`, the DatasetBuilder's info's features will be overwrited by `ArrowWriter`'s `_features`. \r\nBut `ArrowWriter` is initailized without passing `features`.\r\nSo my concern is:\r\nIt's this overwrite must be done, or, should it be an option to pass features in `_prepare_split` function?",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 854417318,
    "title": "Fix backward compatibility in Dataset.load_from_disk",
    "dateCreated": "2021-04-09T11:01:10Z",
    "dateModified": "2021-04-09T11:01:10Z",
    "description": "Fix backward compatibility when loading from disk an old dataset saved to disk with indices using key \"_indices_data_files\".\r\n\r\nRelated to #2195.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 854357481,
    "title": "added file_permission in load_dataset",
    "dateCreated": "2021-04-09T09:39:06Z",
    "dateModified": "2021-04-09T09:39:06Z",
    "description": "As discussed in #2065 I've added `file_permission` argument in `load_dataset`. \r\n\r\nAdded mainly 2 things here:\r\n1) Permission of downloaded datasets when converted to .arrow files  can be changed with argument `file_permission` argument in `load_dataset` (default is 0o644 only)\r\n2) Incase the user uses `map` later on to generate another cache file of dataset, it ensures the permissions of newly generated file are similar to that of` *-train.arrow` file inside cache_dir for that dataset.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 854356559,
    "title": "fix missing indices_files in load_form_disk",
    "dateCreated": "2021-04-09T09:37:57Z",
    "dateModified": "2021-04-09T09:37:57Z",
    "description": "This should fix #2195\r\n\r\n`load_from_disk` was failing if there was no \"_indices_files\" field in state.json. This can happen if the dataset has no indices mapping",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 854126114,
    "title": "`load_dataset` caches two arrow files?",
    "dateCreated": "2021-04-09T03:49:19Z",
    "dateModified": "2021-04-09T03:49:19Z",
    "description": "Hi,\r\n\r\nI am using datasets to load large json file of 587G.\r\nI checked the cached folder and found that there are two arrow files created:\r\n* `cache-ed205e500a7dc44c.arrow` - 355G\r\n*  `json-train.arrow` - 582G\r\n\r\nWhy is the first file created?\r\nIf I delete it, would I still be able to `load_from_disk`?",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 854070194,
    "title": "KeyError: '_indices_files' in `arrow_dataset.py`",
    "dateCreated": "2021-04-09T01:37:12Z",
    "dateModified": "2021-04-09T01:37:12Z",
    "description": "After pulling the latest master, I'm getting a crash when `load_from_disk` tries to load my local dataset.\r\n\r\nTrace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"load_data.py\", line 11, in <module>\r\n    dataset = load_from_disk(SRC)\r\n  File \"/opt/conda/envs/py38/lib/python3.8/site-packages/datasets/load.py\", line 784, in load_from_disk\r\n    return DatasetDict.load_from_disk(dataset_path, fs, keep_in_memory=keep_in_memory)\r\n  File \"/opt/conda/envs/py38/lib/python3.8/site-packages/datasets/dataset_dict.py\", line 692, in load_from_disk\r\n    dataset_dict[k] = Dataset.load_from_disk(dataset_dict_split_path, fs, keep_in_memory=keep_in_memory)\r\n  File \"/opt/conda/envs/py38/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 634, in load_from_disk\r\n    if state[\"_indices_files\"]:\r\nKeyError: '_indices_files'\r\n```\r\n\r\nI believe this is the line causing the error since there may not be a `_indices_files` key in the older versions:\r\nhttps://github.com/huggingface/datasets/blob/b70141e3c5149430951773aaa0155555c5fb3e76/src/datasets/arrow_dataset.py#L634\r\n\r\nMay I suggest using `state.get()` instead of directly indexing the dictionary?\r\n\r\n@lhoestq ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 853909452,
    "title": "py3.7: TypeError: can't pickle _LazyModule objects",
    "dateCreated": "2021-04-08T21:02:48Z",
    "dateModified": "2021-04-08T21:02:48Z",
    "description": "While this works fine with py3.8, under py3.7, with a totally new conda env and transformers install:\r\n\r\n```\r\ngit clone https://github.com/huggingface/transformers\r\ncd transformers\r\npip install -e .[testing]\r\n\r\nexport BS=1; rm -rf /tmp/test-clm; PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 python \\\r\nexamples/language-modeling/run_clm.py --model_name_or_path distilgpt2 --dataset_name wikitext \\\r\n--dataset_config_name wikitext-2-raw-v1 --do_train --max_train_samples 1 \\\r\n--per_device_train_batch_size $BS --output_dir /tmp/test-clm --block_size 128 --logging_steps 1  \\\r\n--fp16\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"examples/language-modeling/run_clm.py\", line 453, in <module>\r\n    main()\r\n  File \"examples/language-modeling/run_clm.py\", line 336, in main\r\n    load_from_cache_file=not data_args.overwrite_cache,\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/datasets/dataset_dict.py\", line 303, in map\r\n    for k, dataset in self.items()\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/datasets/dataset_dict.py\", line 303, in <dictcomp>\r\n    for k, dataset in self.items()\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1259, in map\r\n    update_data=update_data,\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 157, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/datasets/fingerprint.py\", line 158, in wrapper\r\n    self._fingerprint, transform, kwargs_for_fingerprint\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/datasets/fingerprint.py\", line 105, in update_fingerprint\r\n    hasher.update(transform_args[key])\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/datasets/fingerprint.py\", line 57, in update\r\n    self.m.update(self.hash(value).encode(\"utf-8\"))\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/datasets/fingerprint.py\", line 53, in hash\r\n    return cls.hash_default(value)\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/datasets/fingerprint.py\", line 46, in hash_default\r\n    return cls.hash_bytes(dumps(value))\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 389, in dumps\r\n    dump(obj, file)\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 361, in dump\r\n    Pickler(file, recurse=True).dump(obj)\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/dill/_dill.py\", line 454, in dump\r\n    StockPickler.dump(self, obj)\r\n  File \"/home/stas/anaconda3/lib/python3.7/pickle.py\", line 437, in dump\r\n    self.save(obj)\r\n  File \"/home/stas/anaconda3/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 556, in save_function\r\n    obj=obj,\r\n  File \"/home/stas/anaconda3/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/home/stas/anaconda3/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/stas/anaconda3/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/home/stas/anaconda3/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/home/stas/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/home/stas/anaconda3/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/home/stas/anaconda3/lib/python3.7/pickle.py\", line 524, in save\r\n    rv = reduce(self.proto)\r\nTypeError: can't pickle _LazyModule objects\r\n```\r\n```\r\n$ python --version\r\nPython 3.7.4\r\n\r\n$ python -m torch.utils.collect_env\r\nCollecting environment information...\r\nPyTorch version: 1.8.0.dev20210110+cu110\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.0\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.2 LTS (x86_64)\r\nGCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\nClang version: 10.0.0-4ubuntu1 \r\nCMake version: version 3.16.3\r\n```\r\n\r\nThanks.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 853725707,
    "title": "Filtering/mapping on one column is very slow",
    "dateCreated": "2021-04-08T18:16:14Z",
    "dateModified": "2021-04-08T18:16:14Z",
    "description": "I'm currently using the `wikipedia` dataset\u2014 I'm tokenizing the articles with the `tokenizers` library using `map()` and also adding a new `num_tokens` column to the dataset as part of that map operation.\r\n\r\nI want to be able to _filter_ the dataset based on this `num_tokens` column, but even when I specify `input_columns=['num_tokens']`, it seems that the entirety of each row is loaded into memory, which makes the operation take much longer than it should. Indeed, `filter` currently just calls `map`, and I found that in `_map_single` on lines 1690-1704 of `arrow_dataset.py`, the method is just grabbing slices of _all the rows_ of the dataset and then passing only the specified columns to the map function. It seems that, when the user passes a value for `input_columns`, the `map` function should create a temporary pyarrow table by selecting just those columns, and then get slices from that table. Or something like that\u2014 I'm not very familiar with the pyarrow API.\r\n\r\nI know that in the meantime I can sort of get around this by simply only returning the rows that match my filter criterion from the tokenizing function I pass to `map()`, but I actually _also_ want to map on just the `num_tokens` column in order to compute batches with a roughly uniform number of tokens per batch. I would also ideally like to be able to change my minimum and maximum article lengths without having to re-tokenize the entire dataset.\r\n\r\nPS: This is definitely not a \"dataset request.\" I'm realizing that I don't actually know how to remove labels from my own issues on other people's repos, if that is even possible.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 853547910,
    "title": "Fix typo in huggingface hub",
    "dateCreated": "2021-04-08T14:42:24Z",
    "dateModified": "2021-04-08T14:42:24Z",
    "description": "pip knows how to resolve to `huggingface_hub`, but conda doesn't!\r\n\r\nThe `packaging` dependency is also required for the build to complete.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 853364204,
    "title": "Refactorize tests to use Dataset as context manager",
    "dateCreated": "2021-04-08T11:21:04Z",
    "dateModified": "2021-04-08T11:21:04Z",
    "description": "Refactorize Dataset tests to use Dataset as context manager.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 853181564,
    "title": "News_commentary Dataset Translation Pairs are of Incorrect Language Specified Pairs",
    "dateCreated": "2021-04-08T07:53:43Z",
    "dateModified": "2021-04-08T07:53:43Z",
    "description": "I used load_dataset to load the news_commentary dataset for \"ar-en\" translation pairs but found translations from Arabic to Hindi.  \r\n\r\n```\r\ntrain_ds = load_dataset(\"news_commentary\", \"ar-en\", split='train[:98%]')\r\nval_ds = load_dataset(\"news_commentary\", \"ar-en\", split='train[98%:]')\r\n\r\n# filtering out examples that are not ar-en translations but ar-hi\r\nval_ds = val_ds.filter(lambda example, indice: indice not in chain(range(1312,1327) ,range(1384,1399), range(1030,1042)), with_indices=True)\r\n```\r\n\r\n* I'm fairly new to using datasets so I might be doing something wrong",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 853052891,
    "title": "save_to_disk doesn't work when we use concatenate_datasets function before creating the final dataset_object.",
    "dateCreated": "2021-04-08T04:42:53Z",
    "dateModified": "2021-04-08T04:42:53Z",
    "description": "As you can see, it saves the entire dataset.\r\n\r\n@lhoestq \r\n\r\nYou can  check by going through the following example,\r\n\r\n```\r\nfrom datasets import load_from_disk,concatenate_datasets\r\n\r\nloaded_data=load_from_disk('/home/gsir059/HNSW-ori/my_knowledge_dataset')\r\nn=20\r\nkb_list=[loaded_data.shard(n, i, contiguous=True) for i in range(n)]\r\nfinal_dataset=concatenate_datasets([kb_list[1],kb_list[2]])\r\nfinal_dataset.save_to_disk('/home/gsir059/haha/k.arrow')\r\n```",
    "status": "open",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 853044166,
    "title": "Duplicate data in Timit dataset",
    "dateCreated": "2021-04-08T04:21:54Z",
    "dateModified": "2021-04-08T04:21:54Z",
    "description": "I ran a simple code to list all texts in Timit dataset and the texts were all the same.\r\nIs this dataset corrupted?\r\n**Code:**\r\ntimit = load_dataset(\"timit_asr\")\r\nprint(*timit['train']['text'], sep='\\n')\r\n**Result:**\r\nWould such an act of refusal be useful?\r\nWould such an act of refusal be useful?\r\nWould such an act of refusal be useful?\r\nWould such an act of refusal be useful?\r\n...\r\n...\r\nWould such an act of refusal be useful?",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 852939736,
    "title": "Question (potential issue?) related to datasets caching",
    "dateCreated": "2021-04-08T00:16:28Z",
    "dateModified": "2021-04-08T00:16:28Z",
    "description": "I thought I had disabled datasets caching in my code, as follows:\r\n```\r\nfrom datasets import set_caching_enabled\r\n...\r\ndef main():\r\n\r\n    # disable caching in datasets\r\n    set_caching_enabled(False)\r\n```\r\nHowever, in my log files I see messages like the following:\r\n\r\n```\r\n04/07/2021 18:34:42 - WARNING - datasets.builder -   Using custom data configuration default-888a87931cbc5877\r\n04/07/2021 18:34:42 - WARNING - datasets.builder -   Reusing dataset csv (xxxx/cache-transformers/datasets/csv/default-888a87931cbc5877/0.0.0/965b6429be0fc05f975b608ce64e1fa941cc8fb4f30629b523d2390f3c0e1a93\r\n```\r\nCan you please let me know what this reusing dataset csv means? I wouldn't expect any reusing with the datasets caching disabled. Thank you!",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 852840819,
    "title": "GEM: new challenge sets",
    "dateCreated": "2021-04-07T21:39:07Z",
    "dateModified": "2021-04-07T21:39:07Z",
    "description": "This PR updates the GEM dataset to:\r\n- remove extraneous fields in WikiAuto after https://github.com/huggingface/datasets/pull/2171 fixed the source\r\n- add context and services to Schema Guided Dialog\r\n- Add new or update challenge sets for MLSUM ES and DE, XSUM, and SGD",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 852684395,
    "title": ".map() and distributed training",
    "dateCreated": "2021-04-07T18:22:14Z",
    "dateModified": "2021-04-07T18:22:14Z",
    "description": "Hi,\r\nI have a question regarding distributed training and the `.map` call on a dataset.\r\n\r\nI have a local dataset \"my_custom_dataset\" that I am loading with `datasets = load_from_disk(dataset_path=my_path)`.\r\n`dataset` is then tokenized:\r\n```python\r\ndatasets = load_from_disk(dataset_path=my_path)\r\n\r\n[...]\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[text_column_name])\r\n\r\nlogger.info(\"Mapping dataset to tokenized dataset.\")\r\ntokenized_datasets = datasets.map(\r\n    tokenize_function,\r\n    batched=True,\r\n    num_proc=preprocessing_num_workers,\r\n    remove_columns=column_names,\r\n    load_from_cache_file=True,\r\n)\r\n```\r\nI am using 31 workers (`preprocessing_num_workers=31`) and thus it creates 31 `cache*.arrow` files in `my_path/train` (there is only a train split).\r\nWhen I relaunch the script, the map is tokenization is skipped in favor of loading the 31 previously cached files, and that's perfect.\r\n\r\nEverything so far was done by launching a **single process script**.\r\nI now launch the same training script in **distributed mode** (`pytorch -m torch.distributed.launch --nproc_per_node 2`). However, once it reaches the map call, it re-does the tokenization... instead of loading the 31 cached files. \r\n\r\nI tried adding the `cache_file_name` argument: `cache_file_name={\"train\": my_path/one_of_the_arrow_file}`, but I can't give the 31 cached files, so it probably isn't the right way to do it.\r\n\r\n**My question: what is the best way to load cached files if they were pre-processed and dumped in multiple arrow files?** It seems automatically handled for single processes but fails on distributed training.\r\n\r\n- I am following the same structure as the examples of transformers (more specifically [run_clm.py](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py) in my case)\r\n- I am using 1.5.0 version of datasets if that matters.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 852597258,
    "title": "Implementation of class_encode_column",
    "dateCreated": "2021-04-07T16:47:43Z",
    "dateModified": "2021-04-07T16:47:43Z",
    "description": "Addresses #2176 \r\n\r\nI'm happy to discuss the API and internals!",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 852518411,
    "title": "Fix s3fs tests for py36 and py37+",
    "dateCreated": "2021-04-07T15:17:11Z",
    "dateModified": "2021-04-07T15:17:11Z",
    "description": "Recently several changes happened:\r\n1. latest versions of `fsspec` require python>3.7 for async features\r\n2. `s3fs` added a dependency on `aiobotocore`, which is not compatible with the `moto` s3 mock context manager\r\n\r\nThis PR fixes both issues, by pinning `fsspec` and `s3fs` for python 3.6, and by using `moto` in server mode to support running the tests on python>=3.7 with the latest version of `fsspec` and `s3fs`.\r\n\r\ncc @philschmid ",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 852384872,
    "title": "Set default in-memory value depending on the dataset size",
    "dateCreated": "2021-04-07T13:00:18Z",
    "dateModified": "2021-04-07T13:00:18Z",
    "description": "Set a default value for `in_memory` depending on the size of the dataset to be loaded.\r\n\r\nClose #2179.\r\n\r\nTODO:\r\n- [x] Add a section in the docs about this.\r\n- ~Add a warning if someone tries to specify `cache_file_name=` in `map`, `filter` etc. on a dataset that is in memory, since the computation is not going to be cached in this case.~",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 852261607,
    "title": "Error when loading a HUGE json file (pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries)",
    "dateCreated": "2021-04-07T10:26:46Z",
    "dateModified": "2021-04-07T10:26:46Z",
    "description": "Hi, thanks for the great library. I have used the brilliant library for a couple of small projects, and now using it for a fairly big project.\r\nWhen loading a huge json file of 500GB, pyarrow complains as follows:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/datasets/builder.py\", line 531, in incomplete_dir\r\n    yield tmp_dir\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/datasets/builder.py\", line 573, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/datasets/builder.py\", line 650, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/datasets/builder.py\", line 1027, in _prepare_split\r\n    for key, table in utils.tqdm(generator, unit=\" tables\", leave=False, disable=not_verbose):\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/tqdm/std.py\", line 1133, in __iter__\r\n    for obj in iterable:\r\n  File \"/app/.cache/huggingface/modules/datasets_modules/datasets/json/9498524fd296a6cca99c66d6c5be507d1c0991f5a814e535b507f4a66096a641/json.py\", line 83, in _generate_tables\r\n    parse_options=self.config.pa_parse_options,\r\n  File \"pyarrow/_json.pyx\", line 247, in pyarrow._json.read_json\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries (try to increase block size?)\r\n```\r\nWhen using only a small portion of the sample file, say first 100 lines, it works perfectly well..\r\n\r\nI see that it is the error from pyarrow, but could you give me a hint or possible solutions?\r\n#369 describes the same error and #372 claims to have fixed the issue, but I have no clue why I am still getting this one. Thanks in advance!",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 852258635,
    "title": "Add tel to xtreme tatoeba",
    "dateCreated": "2021-04-07T10:23:15Z",
    "dateModified": "2021-04-07T10:23:15Z",
    "description": "This should fix issue #2149 ",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 852237957,
    "title": "Load small datasets in-memory instead of using memory map",
    "dateCreated": "2021-04-07T09:58:16Z",
    "dateModified": "2021-04-07T09:58:16Z",
    "description": "Currently all datasets are loaded using memory mapping by default in `load_dataset`.\r\nHowever this might not be necessary for small datasets. If a dataset is small enough, then it can be loaded in-memory and:\r\n- its memory footprint would be small so it's ok\r\n- in-memory computations/queries would be faster\r\n- the caching on-disk would be disabled, making computations even faster (no I/O bound because of the disk)\r\n- but running the same computation a second time would recompute everything since there would be no cached results on-disk. But this is probably fine since computations would be fast anyway + users should be able to provide a cache filename if needed.\r\n\r\nTherefore, maybe the default behavior of `load_dataset` should be to load small datasets in-memory and big datasets using memory mapping.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 852215058,
    "title": "Fix cast memory usage by using map on subtables",
    "dateCreated": "2021-04-07T09:30:50Z",
    "dateModified": "2021-04-07T09:30:50Z",
    "description": "The `cast` operation on a pyarrow Table may create new arrays in memory.\r\nThis is an issue since users expect memory mapped datasets to not fill up the RAM.\r\n\r\nTo fix that I used `map` to write a new arrow file on disk when cast is used.\r\nTo make things more convenient I introduced the `arrow` formatting of a dataset, to make it return pyarrow tables instead of python dicts. This way one can use pyarrow transforms directly when using `map`.\r\n\r\nedit: we'll use the same mechanism for `filter`",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 852065307,
    "title": "add social thumbnial",
    "dateCreated": "2021-04-07T06:40:06Z",
    "dateModified": "2021-04-07T06:40:06Z",
    "description": "# What does this PR do?\r\n\r\nI added OpenGraph/ Twitter Card support to the docs to create nice social thumbnails.\r\n\r\n![Bildschirmfoto 2021-04-07 um 08 36 50](https://user-images.githubusercontent.com/32632186/113821698-bac2ce80-977c-11eb-81aa-d8f16355857e.png)\r\n\r\nTo be able to add these I needed to install `sphinxext-opengraph`. I came across this [issue](https://github.com/readthedocs/readthedocs.org/issues/1758) on the readthedocs repo saying that since someone has built this plugin they are not integrating and providing documentation to it. That's why I added it for creating the documentation. The repository can be found [here](https://github.com/wpilibsuite/sphinxext-opengraph/tree/main).\r\n\r\nP.S. It seemed that `make style` never ran for `docs/` i hope the changes are okay otherwise I'll revert it. ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 851865795,
    "title": "Converting a Value to a ClassLabel",
    "dateCreated": "2021-04-06T22:54:16Z",
    "dateModified": "2021-04-06T22:54:16Z",
    "description": "Hi!\r\n\r\nIn the docs for `cast`, it's noted that `For non-trivial conversion, e.g. string <-> ClassLabel you should use map() to update the Dataset.`\r\n\r\nWould it be possible to have an example  that demonstrates such a string <-> ClassLabel conversion using `map`? Thanks!",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 851836096,
    "title": "dataset.search_batch() function outputs all -1 indices sometime.",
    "dateCreated": "2021-04-06T21:50:49Z",
    "dateModified": "2021-04-06T21:50:49Z",
    "description": "I am working with RAG and playing around with different faiss indexes. At the moment I use **index = faiss.index_factory(768, \"IVF65536_HNSW32,Flat\")**.\r\n\r\nDuring the retrieval phase exactly in [this line of retrieval_rag.py](https://github.com/huggingface/transformers/blob/master/src/transformers/models/rag/retrieval_rag.py#L231) an error issue when all retrieved indices are -1.  Please refer to the screenshot of a PID worker. \r\n\r\n![image](https://user-images.githubusercontent.com/16892570/113782387-37a67600-9786-11eb-9c29-acad661a9648.png)\r\n\r\n\r\nHere, my retrieve batch size is 2 and n_docs is 5. I can solve this by working around np. stack, but I want to ask, why we get an output index of -1. Do you have any idea :) ?\r\n\r\nIs this a problem of the index, where the faiss can't find any similar vector?\r\nIs there documentation on the output index being -1?\r\n\r\n@lhoestq \r\n ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 851383675,
    "title": "Pin docutils for better doc",
    "dateCreated": "2021-04-06T12:40:20Z",
    "dateModified": "2021-04-06T12:40:20Z",
    "description": "The latest release of docutils make the navbar in the documentation weird and the Markdown wrongly interpreted:\r\n\r\n![image](https://user-images.githubusercontent.com/35901082/113711773-5be55280-96b3-11eb-9b3b-9794f17709aa.png)\r\n\r\nWe had the same problem in Transformers and solved it by pinning docutils (a dep of sphinx).\r\n\r\nYou can see the version after the change [here](https://32769-250213286-gh.circle-artifacts.com/0/docs/_build/html/index.html).\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 851359284,
    "title": "Add OpenSLR dataset",
    "dateCreated": "2021-04-06T12:08:34Z",
    "dateModified": "2021-04-06T12:08:34Z",
    "description": "OpenSLR (https://openslr.org/) is a site devoted to hosting speech and language resources, such as training corpora for speech recognition, and software related to speech recognition. There are around 80 speech datasets listed in OpenSLR, currently this PR includes only 9 speech datasets SLR41, SLR42, SLR43, SLR44, SLR63, SLR64, SLR65, SLR66 and SLR69 (Javanese, Khmer, Nepali and Sundanese, Malayalam, Marathi, Tamil, Telugu and Catalan). I can add other speech datasets gradually next time.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 851229399,
    "title": "Pin fsspec lower than 0.9.0",
    "dateCreated": "2021-04-06T09:19:09Z",
    "dateModified": "2021-04-06T09:19:09Z",
    "description": "Today's release of `fsspec` 0.9.0 implied a new release of `s3fs` 0.6.0 but this version breaks the CI (see [here](https://app.circleci.com/pipelines/github/huggingface/datasets/5312/workflows/490f3240-cd1c-4dd1-bb60-b416771c5584/jobs/32734) for example)\r\n\r\nI'm pinning `fsspec` until this has been resolved",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 851090662,
    "title": "Fixed the link to wikiauto training data.",
    "dateCreated": "2021-04-06T07:13:11Z",
    "dateModified": "2021-04-06T07:13:11Z",
    "description": "",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 850913228,
    "title": "Wikipedia historic dumps are deleted but hf/datasets hardcodes dump date",
    "dateCreated": "2021-04-06T03:13:18Z",
    "dateModified": "2021-04-06T03:13:18Z",
    "description": "Wikimedia does not keep all historical dumps. For example, as of today https://dumps.wikimedia.org/kowiki/ only provides\r\n\r\n```\r\n20201220/                                          02-Feb-2021 01:36                   -\r\n20210101/                                          21-Feb-2021 01:26                   -\r\n20210120/                                          02-Mar-2021 01:25                   -\r\n20210201/                                          21-Mar-2021 01:26                   -\r\n20210220/                                          02-Apr-2021 01:26                   -\r\n20210301/                                          03-Mar-2021 08:10                   -\r\n20210320/                                          21-Mar-2021 18:13                   -\r\n20210401/                                          03-Apr-2021 10:08                   -\r\nlatest/                                            03-Apr-2021 10:08                   -\r\n```\r\n\r\nHowever, the wikipedia dataset provided in the library, only supports the following configs, none of which are applicable anymore when disregarding the cached datasets:\r\n\r\n```\r\nValueError: BuilderConfig 20210401.ko not found. Available: ['20200501.aa', '20200501.ab', '20200501.ace', '20200501.ady', '20200501.af', '20200501.ak', '20200501.als', '20200501.am', '20200501.an', '20200501.ang', '20200501.ar', '20200501.arc', '20200501.arz', '20200501.as', '20200501.ast', '20200501.atj', '20200501.av', '20200501.ay', '20200501.az', '20200501.azb', '20200501.ba', '20200501.bar', '20200501.bat-smg', '20200501.bcl', '20200501.be', '20200501.be-x-old', '20200501.bg', '20200501.bh', '20200501.bi', '20200501.bjn', '20200501.bm', '20200501.bn', '20200501.bo', '20200501.bpy', '20200501.br', '20200501.bs', '20200501.bug', '20200501.bxr', '20200501.ca', '20200501.cbk-zam', '20200501.cdo', '20200501.ce', '20200501.ceb', '20200501.ch', '20200501.cho', '20200501.chr', '20200501.chy', '20200501.ckb', '20200501.co', '20200501.cr', '20200501.crh', '20200501.cs', '20200501.csb', '20200501.cu', '20200501.cv', '20200501.cy', '20200501.da', '20200501.de', '20200501.din', '20200501.diq', '20200501.dsb', '20200501.dty', '20200501.dv', '20200501.dz', '20200501.ee', '20200501.el', '20200501.eml', '20200501.en', '20200501.eo', '20200501.es', '20200501.et', '20200501.eu', '20200501.ext', '20200501.fa', '20200501.ff', '20200501.fi', '20200501.fiu-vro', '20200501.fj', '20200501.fo', '20200501.fr', '20200501.frp', '20200501.frr', '20200501.fur', '20200501.fy', '20200501.ga', '20200501.gag', '20200501.gan', '20200501.gd', '20200501.gl', '20200501.glk', '20200501.gn', '20200501.gom', '20200501.gor', '20200501.got', '20200501.gu', '20200501.gv', '20200501.ha', '20200501.hak', '20200501.haw', '20200501.he', '20200501.hi', '20200501.hif', '20200501.ho', '20200501.hr', '20200501.hsb', '20200501.ht', '20200501.hu', '20200501.hy', '20200501.ia', '20200501.id', '20200501.ie', '20200501.ig', '20200501.ii', '20200501.ik', '20200501.ilo', '20200501.inh', '20200501.io', '20200501.is', '20200501.it', '20200501.iu', '20200501.ja', '20200501.jam', '20200501.jbo', '20200501.jv', '20200501.ka', '20200501.kaa', '20200501.kab', '20200501.kbd', '20200501.kbp', '20200501.kg', '20200501.ki', '20200501.kj', '20200501.kk', '20200501.kl', '20200501.km', '20200501.kn', '20200501.ko', '20200501.koi', '20200501.krc', '20200501.ks', '20200501.ksh', '20200501.ku', '20200501.kv', '20200501.kw', '20200501.ky', '20200501.la', '20200501.lad', '20200501.lb', '20200501.lbe', '20200501.lez', '20200501.lfn', '20200501.lg', '20200501.li', '20200501.lij', '20200501.lmo', '20200501.ln', '20200501.lo', '20200501.lrc', '20200501.lt', '20200501.ltg', '20200501.lv', '20200501.mai', '20200501.map-bms', '20200501.mdf', '20200501.mg', '20200501.mh', '20200501.mhr', '20200501.mi', '20200501.min', '20200501.mk', '20200501.ml', '20200501.mn', '20200501.mr', '20200501.mrj', '20200501.ms', '20200501.mt', '20200501.mus', '20200501.mwl', '20200501.my', '20200501.myv', '20200501.mzn', '20200501.na', '20200501.nah', '20200501.nap', '20200501.nds', '20200501.nds-nl', '20200501.ne', '20200501.new', '20200501.ng', '20200501.nl', '20200501.nn', '20200501.no', '20200501.nov', '20200501.nrm', '20200501.nso', '20200501.nv', '20200501.ny', '20200501.oc', '20200501.olo', '20200501.om', '20200501.or', '20200501.os', '20200501.pa', '20200501.pag', '20200501.pam', '20200501.pap', '20200501.pcd', '20200501.pdc', '20200501.pfl', '20200501.pi', '20200501.pih', '20200501.pl', '20200501.pms', '20200501.pnb', '20200501.pnt', '20200501.ps', '20200501.pt', '20200501.qu', '20200501.rm', '20200501.rmy', '20200501.rn', '20200501.ro', '20200501.roa-rup', '20200501.roa-tara', '20200501.ru', '20200501.rue', '20200501.rw', '20200501.sa', '20200501.sah', '20200501.sat', '20200501.sc', '20200501.scn', '20200501.sco', '20200501.sd', '20200501.se', '20200501.sg', '20200501.sh', '20200501.si', '20200501.simple', '20200501.sk', '20200501.sl', '20200501.sm', '20200501.sn', '20200501.so', '20200501.sq', '20200501.sr', '20200501.srn', '20200501.ss', '20200501.st', '20200501.stq', '20200501.su', '20200501.sv', '20200501.sw', '20200501.szl', '20200501.ta', '20200501.tcy', '20200501.te', '20200501.tet', '20200501.tg', '20200501.th', '20200501.ti', '20200501.tk', '20200501.tl', '20200501.tn', '20200501.to', '20200501.tpi', '20200501.tr', '20200501.ts', '20200501.tt', '20200501.tum', '20200501.tw', '20200501.ty', '20200501.tyv', '20200501.udm', '20200501.ug', '20200501.uk', '20200501.ur', '20200501.uz', '20200501.ve', '20200501.vec', '20200501.vep', '20200501.vi', '20200501.vls', '20200501.vo', '20200501.wa', '20200501.war', '20200501.wo', '20200501.wuu', '20200501.xal', '20200501.xh', '20200501.xmf', '20200501.yi', '20200501.yo', '20200501.za', '20200501.zea', '20200501.zh', '20200501.zh-classical', '20200501.zh-min-nan', '20200501.zh-yue', '20200501.zu']\r\n```\r\n\r\nThe cached datasets:\r\n\r\n```\r\n% aws s3 --no-sign-request --endpoint-url https://storage.googleapis.com ls s3://huggingface-nlp/cache/datasets/wikipedia/\r\n                           PRE 20200501.de/\r\n                           PRE 20200501.en/\r\n                           PRE 20200501.fr/\r\n                           PRE 20200501.frr/\r\n                           PRE 20200501.it/\r\n                           PRE 20200501.simple/\r\n```",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 850456180,
    "title": "Updated WER metric implementation to avoid memory issues",
    "dateCreated": "2021-04-05T15:43:20Z",
    "dateModified": "2021-04-05T15:43:20Z",
    "description": "This is in order to fix this issue:\r\n\r\nhttps://github.com/huggingface/datasets/issues/2078\r\n\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 849957941,
    "title": "Preserve split type when realoding dataset",
    "dateCreated": "2021-04-04T20:46:21Z",
    "dateModified": "2021-04-04T20:46:21Z",
    "description": "Fixes #2167 \r\n\r\nUsing `eval` is not ideal for security reasons (in web apps I assume), but without it the code would be much more complex IMO.\r\n\r\nIn terms of style, instead of explicitly importing a private member (`_RelativeInstruction`), we can add these imports at the top of the module:\r\n```python\r\nfrom . import arrow_reader  # gives us access to ReadInstruction and _RelativeInstruction\r\nfrom . import splits # gives us access to NamedSplit\r\n```\r\n\r\nand then define the `eval` globals as follows:\r\n```python\r\n{**arrow_reader.__dict__, **splits.__dict__}\r\n```\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 849944891,
    "title": " Split type not preserved when reloading the dataset",
    "dateCreated": "2021-04-04T19:29:54Z",
    "dateModified": "2021-04-04T19:29:54Z",
    "description": "A minimal reproducible example:\r\n```python\r\n>>> from datasets import load_dataset, Dataset\r\n>>> dset = load_dataset(\"sst\", split=\"train\")\r\n>>> dset.save_to_disk(\"sst\")\r\n>>> type(dset.split)\r\n<class 'datasets.splits.NamedSplit'>\r\n>>> dset = Dataset.load_from_disk(\"sst\")\r\n>>> type(dset.split)  # NamedSplit expected\r\n<class 'str'>\r\n```\r\n\r\nIt seems like this bug was introduced in #2025.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 849778545,
    "title": "Regarding Test Sets for the GEM datasets",
    "dateCreated": "2021-04-04T02:02:45Z",
    "dateModified": "2021-04-04T02:02:45Z",
    "description": "@yjernite Hi, are the test sets for the GEM datasets scheduled to be [added soon](https://gem-benchmark.com/shared_task)? \r\n\r\ne.g.\r\n\r\n```\r\nfrom datasets import load_dataset\r\nDATASET_NAME=\"common_gen\"\r\ndata = load_dataset(\"gem\", DATASET_NAME)\r\n```\r\n\r\nThe test set doesn't have the target or references.\r\n\r\n```\r\ndata['test'][0]\r\n{'concept_set_id': 0, 'concepts': ['drill', 'field', 'run', 'team'], 'gem_id': 'common_gen-test-0', 'gem_parent_id': 'common_gen-test-0', 'references': [], 'target': ''}\r\n```\r\n\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 849771665,
    "title": "How to convert datasets.arrow_dataset.Dataset to torch.utils.data.Dataset",
    "dateCreated": "2021-04-04T01:01:48Z",
    "dateModified": "2021-04-04T01:01:48Z",
    "description": "Hi,\r\n\r\nI'm trying to pretraine deep-speed model using HF arxiv dataset like:\r\n```\r\ntrain_ds = nlp.load_dataset('scientific_papers', 'arxiv')\r\ntrain_ds.set_format(\r\n        type=\"torch\",\r\n        columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\r\n    )\r\nengine, _, _, _ = deepspeed.initialize(\r\n    args=args,\r\n    model=model,\r\n    model_parameters=[p for p in model.parameters() if p.requires_grad],\r\n    training_data=train_ds)\r\n```\r\nbut deepspeed.initialize accepts torch.utils.data.Dataset only. How can I convert HF-style dataset to torch-style dataset?\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 849739759,
    "title": "Replace assertTrue(isinstance with assertIsInstance in tests",
    "dateCreated": "2021-04-03T21:07:02Z",
    "dateModified": "2021-04-03T21:07:02Z",
    "description": "Replaces all the occurrences of the `assertTrue(isinstance(` pattern with `assertIsInstance`.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 849669366,
    "title": "Concat only unique fields in DatasetInfo.from_merge",
    "dateCreated": "2021-04-03T14:31:30Z",
    "dateModified": "2021-04-03T14:31:30Z",
    "description": "I thought someone from the community with less experience would be interested in fixing this issue, but that wasn't the case.\r\n\r\nFixes #2103 ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 849129201,
    "title": "visualization for cc100 is broken ",
    "dateCreated": "2021-04-02T10:11:13Z",
    "dateModified": "2021-04-02T10:11:13Z",
    "description": "Hi\r\nvisualization through dataset viewer for cc100 is broken\r\nhttps://huggingface.co/datasets/viewer/\r\n\r\nthanks a lot\r\n",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 849127041,
    "title": "any possibility to download part of large datasets only?",
    "dateCreated": "2021-04-02T10:06:46Z",
    "dateModified": "2021-04-02T10:06:46Z",
    "description": "Hi\r\nSome of the datasets I need like cc100 are very large, and then I wonder if I can download first X samples of the shuffled/unshuffled data without going through first downloading the whole data then sampling? thanks",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 849052921,
    "title": "data_args.preprocessing_num_workers almost freezes ",
    "dateCreated": "2021-04-02T07:56:13Z",
    "dateModified": "2021-04-02T07:56:13Z",
    "description": "Hi @lhoestq \r\n\r\nI am running this code from huggingface transformers https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py \r\n\r\nto speed up tokenization, since I am running on multiple datasets, I am using data_args.preprocessing_num_workers = 4 with opus100 corpus but this moves on till a point and then this freezes almost for sometime during  tokenization steps and then this is back again, overall to me taking more time than normal case, I appreciate your advice on how I can use this option properly to speed up.\r\n\r\nthanks",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 848851962,
    "title": "adding ccnet dataset",
    "dateCreated": "2021-04-01T23:28:36Z",
    "dateModified": "2021-04-01T23:28:36Z",
    "description": "## Adding a Dataset\r\n- **Name:**  ccnet\r\n\r\n- **Description:** \r\nCommon Crawl\r\n\r\n- **Paper:** \r\nhttps://arxiv.org/abs/1911.00359\r\n\r\n- **Data:** \r\nhttps://github.com/facebookresearch/cc_net\r\n\r\n- **Motivation:**\r\nthis is one of the most comprehensive clean monolingual datasets across a variety of languages. Quite important for cross-lingual reseach\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n\r\nthanks",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 848506746,
    "title": "viewer \"fake_news_english\" error",
    "dateCreated": "2021-04-01T14:13:20Z",
    "dateModified": "2021-04-01T14:13:20Z",
    "description": "When I visit the [Huggingface - viewer](https://huggingface.co/datasets/viewer/) web site, under the dataset \"fake_news_english\" I've got this error:\r\n\r\n> ImportError: To be able to use this dataset, you need to install the following dependencies['openpyxl'] using 'pip install # noqa: requires this pandas optional dependency for reading xlsx files' for instance'\r\n\r\nas well as the error Traceback.\r\n\r\n",
    "status": "open",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 847205239,
    "title": "updated user permissions based on umask",
    "dateCreated": "2021-03-31T19:38:29Z",
    "dateModified": "2021-03-31T19:38:29Z",
    "description": "Updated user permissions based on running user's umask (#2065). Let me know if `0o666` is looking good or should I change it to `~umask` only (to give execute permissions as well) ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 847198295,
    "title": "User permissions",
    "dateCreated": "2021-03-31T19:33:48Z",
    "dateModified": "2021-03-31T19:33:48Z",
    "description": "Updated user permissions based on running user's umask. Let me know if `0o666` is looking good or should I change it to `~umask` only (to give execute permissions as well)",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 846786897,
    "title": "Add table classes to the documentation",
    "dateCreated": "2021-03-31T14:36:10Z",
    "dateModified": "2021-03-31T14:36:10Z",
    "description": "Following #2025 , I added the table classes to the documentation\r\n\r\ncc @albertvillanova ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 846763960,
    "title": "Adding the NorNE dataset for Norwegian POS and NER",
    "dateCreated": "2021-03-31T14:22:50Z",
    "dateModified": "2021-03-31T14:22:50Z",
    "description": "NorNE is a manually annotated corpus of named entities which extends the annotation of the existing Norwegian Dependency Treebank. Comprising both of the official standards of written Norwegian (Bokm\u00e5l and Nynorsk), the corpus contains around 600,000 tokens and annotates a rich set of entity types including persons, organizations, locations, geo-political entities, products, and events, in addition to a class corresponding to nominals derived from names.\r\n\r\nSee #1720.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 846181502,
    "title": "load_dataset ignoring features",
    "dateCreated": "2021-03-31T08:30:09Z",
    "dateModified": "2021-03-31T08:30:09Z",
    "description": "First of all, I'm sorry if it is a repeated issue or the changes are already in master, I searched and I didn't find anything. \r\n\r\nI'm using datasets 1.5.0\r\n\r\n![image](https://user-images.githubusercontent.com/37592763/113114369-8f376580-920b-11eb-900d-94365b59f04b.png)\r\n\r\nAs you can see, when I load the dataset, the ClassLabels are ignored, I have to cast the dataset in order to make it work.\r\n\r\nCode to reproduce:\r\n\r\n```python\r\nimport datasets\r\ndata_location = \"/data/prueba_multiclase\"\r\nfeatures = datasets.Features(\r\n        {\"texto\": datasets.Value(\"string\"), \"label\": datasets.features.ClassLabel(names=[\"false\", \"true\"])}\r\n    )\r\ndataset = datasets.load_dataset(\r\n        \"csv\", data_files=data_location, delimiter=\"\\t\", features=features\r\n    )\r\n```\r\n\r\nDataset I used:\r\n\r\n\r\n[prueba_multiclase.zip](https://github.com/huggingface/datasets/files/6235022/prueba_multiclase.zip) (it has to be unzipped)\r\n\r\n\r\nThank you! \u2764\ufe0f \r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 845751273,
    "title": "Update README.md",
    "dateCreated": "2021-03-31T03:21:19Z",
    "dateModified": "2021-03-31T03:21:19Z",
    "description": "Updated some descriptions of Wino_Bias dataset.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 844886081,
    "title": "Add support for axis in concatenate datasets",
    "dateCreated": "2021-03-30T16:58:44Z",
    "dateModified": "2021-03-30T16:58:44Z",
    "description": "Add support for `axis` (0 or 1) in `concatenate_datasets`.\r\n\r\nClose #853.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 844776448,
    "title": "Allow pickling of big in-memory tables",
    "dateCreated": "2021-03-30T15:51:56Z",
    "dateModified": "2021-03-30T15:51:56Z",
    "description": "This should fix issue #2134 \r\n\r\nPickling is limited to <4GiB objects, it's not possible to pickle a big arrow table (for multiprocessing for example).\r\nFor big tables, we have to write them on disk and only pickle the path to the table.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 844734076,
    "title": "Telugu subset missing for xtreme tatoeba dataset",
    "dateCreated": "2021-03-30T15:26:34Z",
    "dateModified": "2021-03-30T15:26:34Z",
    "description": "from nlp import load_dataset\r\ntrain_dataset = load_dataset('xtreme', 'tatoeba.tel')['validation']\r\nValueError: BuilderConfig tatoeba.tel not found.\r\n\r\nbut language tel is actually included in xtreme:\r\nhttps://github.com/google-research/xtreme/blob/master/utils_preprocess.py\r\ndef tatoeba_preprocess(args):\r\n  lang3_dict = {\r\n    'afr':'af', 'ara':'ar', 'bul':'bg', 'ben':'bn',\r\n    'deu':'de', 'ell':'el', 'spa':'es', 'est':'et',\r\n    'eus':'eu', 'pes':'fa', 'fin':'fi', 'fra':'fr',\r\n    'heb':'he', 'hin':'hi', 'hun':'hu', 'ind':'id',\r\n    'ita':'it', 'jpn':'ja', 'jav':'jv', 'kat':'ka',\r\n    'kaz':'kk', 'kor':'ko', 'mal':'ml', 'mar':'mr',\r\n    'nld':'nl', 'por':'pt', 'rus':'ru', 'swh':'sw',\r\n    'tam':'ta', **_'tel':'te'_**, 'tha':'th', 'tgl':'tl', <----here\r\n    'tur':'tr', 'urd':'ur', 'vie':'vi', 'cmn':'zh',\r\n    'eng':'en',\r\n  }",
    "status": "open",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 844700910,
    "title": "Add configurable options to `seqeval` metric",
    "dateCreated": "2021-03-30T15:04:06Z",
    "dateModified": "2021-03-30T15:04:06Z",
    "description": "Right now `load_metric(\"seqeval\")` only works in the default mode of evaluation (equivalent to conll evaluation).\r\n\r\nHowever, seqeval library [supports](https://github.com/chakki-works/seqeval#support-features) different evaluation schemes (IOB1, IOB2, etc.), which can be plugged in just by supporting additional kwargs in `Seqeval._compute`\r\nhttps://github.com/huggingface/datasets/blob/85cf7ff920c90ca2e12bedca12b36d2a043c3da2/metrics/seqeval/seqeval.py#L109\r\n\r\nThings that would be relevant are, for example, supporting `mode=\"strict\", scheme=IOB2` to count only full entity match as a true positive and omit partial matches.\r\n\r\nThe only problem I see is that the spirit of `metrics` seems to not require additional imports from user. `seqeval` only supports schemes as objects, without any string aliases. \r\n\r\nIt can be solved naively with mapping like `{\"IOB2\": seqeval.scheme.IOB2}`. Or just left as is and require user to explicitly import scheme from `seqeval` if he wants to configure it past the default implementation.\r\n\r\nIf that makes sense, I am happy to implement the change.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 844687831,
    "title": "Render docstring return type as inline",
    "dateCreated": "2021-03-30T14:55:43Z",
    "dateModified": "2021-03-30T14:55:43Z",
    "description": "This documentation setting will avoid having the return type in a separate line under `Return type`. \r\n\r\nSee e.g. current docs for `Dataset.to_csv`.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 844673244,
    "title": "Dataset file size on disk is very large with 3D Array",
    "dateCreated": "2021-03-30T14:46:09Z",
    "dateModified": "2021-03-30T14:46:09Z",
    "description": "Hi, \r\n\r\nI have created my own dataset using the provided dataset loading script. It is an image dataset where images are stored as 3D Array with dtype=uint8. \r\n\r\nThe actual size on disk is surprisingly large. It takes 520 MB. Here is some info from `dataset_info.json`. \r\n\r\n`{\r\n    \"description\": \"\",\r\n    \"citation\": \"\",\r\n    \"homepage\": \"\",\r\n    \"license\": \"\",\r\n    \"features\": {\r\n        \"image\": {\r\n            \"shape\": [224, 224, 3],\r\n            \"dtype\": \"uint8\",\r\n            \"id\": null,\r\n            \"_type\": \"Array3D\",\r\n        }\r\n    },\r\n    \"post_processed\": null,\r\n    \"supervised_keys\": null,\r\n    \"builder_name\": \"shot_type_image_dataset\",\r\n    \"config_name\": \"default\",\r\n    \"version\": {\r\n        \"version_str\": \"0.0.0\",\r\n        \"description\": null,\r\n        \"major\": 0,\r\n        \"minor\": 0,\r\n        \"patch\": 0,\r\n    },\r\n    \"splits\": {\r\n        \"train\": {\r\n            \"name\": \"train\",\r\n            \"num_bytes\": 520803408,\r\n            \"num_examples\": 1479,\r\n            \"dataset_name\": \"shot_type_image_dataset\",\r\n        }\r\n    },\r\n    \"download_checksums\": {\r\n        \"\": {\r\n            \"num_bytes\": 16940447118,\r\n            \"checksum\": \"5854035705efe08b0ed8f3cf3da7b4d29cba9055c2d2d702c79785350d72ee03\",\r\n        }\r\n    },\r\n    \"download_size\": 16940447118,\r\n    \"post_processing_size\": null,\r\n    \"dataset_size\": 520803408,\r\n    \"size_in_bytes\": 17461250526,\r\n}`\r\n\r\nI have created the same dataset with tensorflow_dataset and it takes only 125MB on disk.\r\n\r\nI am wondering, is it normal behavior ? I understand `Datasets` uses Arrow for serialization wheres tf uses TF Records.\r\n\r\nThis might be a problem for large dataset. \r\n\r\nThanks for your help. \r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 844603518,
    "title": "Implement Dataset add_column",
    "dateCreated": "2021-03-30T14:02:14Z",
    "dateModified": "2021-03-30T14:02:14Z",
    "description": "Implement `Dataset.add_column`.\r\n\r\nClose #1954.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 844352067,
    "title": "Loading wikipedia 20200501.en throws pyarrow related error",
    "dateCreated": "2021-03-30T10:38:31Z",
    "dateModified": "2021-03-30T10:38:31Z",
    "description": "**Problem description**\r\nI am getting the following error when trying to load wikipedia/20200501.en dataset.\r\n\r\n**Error log**\r\nDownloading and preparing dataset wikipedia/20200501.en (download: 16.99 GiB, generated: 17.07 GiB, post-processed: Unknown size, total: 34.06 GiB) to /usr/local/workspace/NAS_NLP/cache/wikipedia/20200501.en/1.0.0/50aa706aa417bb77d910ad61211cc672c0ef3e0f224225a5e0a18277ade8b931...\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.6k/14.6k [00:00<00:00, 5.41MB/s]\r\nDownloading:  59%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                              | 10.7G/18.3G [11:30<08:08, 15.5MB/s]\r\nDataset wikipedia downloaded and prepared to /usr/local/workspace/NAS_NLP/cache/wikipedia/20200501.en/1.0.0/50aa706aa417bb77d910ad61211cc672c0ef3e0f224225a5e0a18277ade8b931. Subsequent calls will reuse this data.\r\nTraceback (most recent call last):\r\n  File \"load_wiki.py\", line 2, in <module>\r\n    ds = load_dataset('wikipedia', '20200501.en', cache_dir='/usr/local/workspace/NAS_NLP/cache')\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/load.py\", line 751, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications, in_memory=keep_in_memory)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/builder.py\", line 746, in as_dataset\r\n    map_tuple=True,\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/py_utils.py\", line 204, in map_nested\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/py_utils.py\", line 204, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/py_utils.py\", line 142, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/builder.py\", line 763, in _build_single_dataset\r\n    in_memory=in_memory,\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/builder.py\", line 835, in _as_dataset\r\n    in_memory=in_memory,\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/arrow_reader.py\", line 215, in read\r\n    return self.read_files(files=files, original_instructions=instructions, in_memory=in_memory)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/arrow_reader.py\", line 236, in read_files\r\n    pa_table = self._read_files(files, in_memory=in_memory)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/arrow_reader.py\", line 171, in _read_files\r\n    pa_table: pa.Table = self._get_dataset_from_filename(f_dict, in_memory=in_memory)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/arrow_reader.py\", line 302, in _get_dataset_from_filename\r\n    pa_table = ArrowReader.read_table(filename, in_memory=in_memory)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/arrow_reader.py\", line 324, in read_table\r\n    pa_table = f.read_all()\r\n  File \"pyarrow/ipc.pxi\", line 544, in pyarrow.lib.RecordBatchReader.read_all\r\n  File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\nOSError: Expected to be able to read 9176784 bytes for message body, got 4918712\r\n\r\n**Detailed version info**\r\ndatasets==1.5.0\r\n  - dataclasses [required: Any, installed: 0.8]\r\n  - dill [required: Any, installed: 0.3.3]\r\n  - fsspec [required: Any, installed: 0.8.7]\r\n    - importlib-metadata [required: Any, installed: 1.7.0]\r\n      - zipp [required: >=0.5, installed: 3.1.0]\r\n  - huggingface-hub [required: <0.1.0, installed: 0.0.7]\r\n    - filelock [required: Any, installed: 3.0.12]\r\n    - importlib-metadata [required: Any, installed: 1.7.0]\r\n      - zipp [required: >=0.5, installed: 3.1.0]\r\n    - requests [required: Any, installed: 2.24.0]\r\n      - certifi [required: >=2017.4.17, installed: 2020.6.20]\r\n      - chardet [required: >=3.0.2,<4, installed: 3.0.4]\r\n      - idna [required: >=2.5,<3, installed: 2.6]\r\n      - urllib3 [required: >=1.21.1,<1.26,!=1.25.1,!=1.25.0, installed: 1.25.10]\r\n    - tqdm [required: Any, installed: 4.49.0]\r\n  - importlib-metadata [required: Any, installed: 1.7.0]\r\n    - zipp [required: >=0.5, installed: 3.1.0]\r\n  - multiprocess [required: Any, installed: 0.70.11.1]\r\n    - dill [required: >=0.3.3, installed: 0.3.3]\r\n  - numpy [required: >=1.17, installed: 1.17.0]\r\n  - pandas [required: Any, installed: 1.1.5]\r\n    - numpy [required: >=1.15.4, installed: 1.17.0]\r\n    - python-dateutil [required: >=2.7.3, installed: 2.8.0]\r\n      - six [required: >=1.5, installed: 1.15.0]\r\n    - pytz [required: >=2017.2, installed: 2020.1]\r\n  - pyarrow [required: >=0.17.1, installed: 3.0.0]\r\n    - numpy [required: >=1.16.6, installed: 1.17.0]\r\n  - requests [required: >=2.19.0, installed: 2.24.0]\r\n    - certifi [required: >=2017.4.17, installed: 2020.6.20]\r\n    - chardet [required: >=3.0.2,<4, installed: 3.0.4]\r\n    - idna [required: >=2.5,<3, installed: 2.6]\r\n    - urllib3 [required: >=1.21.1,<1.26,!=1.25.1,!=1.25.0, installed: 1.25.10]\r\n  - tqdm [required: >=4.27,<4.50.0, installed: 4.49.0]\r\n  - xxhash [required: Any, installed: 2.0.0]\r\n",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 844313228,
    "title": "task casting via load_dataset",
    "dateCreated": "2021-03-30T10:00:42Z",
    "dateModified": "2021-03-30T10:00:42Z",
    "description": "wip\r\nnot satisfied with the API, it means as a dataset implementer I need to write a function with boilerplate and write classes for each `<dataset><task>` \"facet\".",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 843919420,
    "title": "Gem V1.1",
    "dateCreated": "2021-03-29T23:47:02Z",
    "dateModified": "2021-03-29T23:47:02Z",
    "description": "This branch updates the GEM benchmark to its 1.1 version which includes:\r\n- challenge sets for most tasks\r\n- detokenized TurkCorpus to match the rest of the text simplification subtasks\r\n- fixed inputs for TurkCorpus and ASSET test sets\r\n- 18 languages in WikiLingua\r\n\r\ncc @sebastianGehrmann",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 843914790,
    "title": "added spans field for the wikiann datasets",
    "dateCreated": "2021-03-29T23:38:26Z",
    "dateModified": "2021-03-29T23:38:26Z",
    "description": "Hi @lhoestq \r\nI tried to add spans to the wikiann datasets.\r\nThanks a lot for kindly having a look.\r\nThis addresses https://github.com/huggingface/datasets/issues/2130. \r\nBest regards\r\nRabeeh ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 843830451,
    "title": "add banking77 dataset",
    "dateCreated": "2021-03-29T21:32:23Z",
    "dateModified": "2021-03-29T21:32:23Z",
    "description": "Intent classification/detection dataset from banking category with 77 unique intents.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 843662613,
    "title": "TypeError when using save_to_disk in a dataset loaded with ReadInstruction split",
    "dateCreated": "2021-03-29T18:23:54Z",
    "dateModified": "2021-03-29T18:23:54Z",
    "description": "Hi,\r\n\r\nLoading a dataset with `load_dataset` using a split defined via `ReadInstruction` and then saving it to disk results in the following error: `TypeError: Object of type ReadInstruction is not JSON serializable`.\r\n\r\nHere is the minimal reproducible example:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nfrom datasets import ReadInstruction\r\n\r\ndata_1 = load_dataset(\r\n    \"wikiann\",\r\n    \"en\",\r\n    split=\"validation\",\r\n)\r\n\r\ndata_1.save_to_disk(\"temporary_path_1\")\r\n\r\nprint(\"Save with regular split works.\")\r\n\r\ndata_2 = load_dataset(\r\n    \"wikiann\",\r\n    \"en\",\r\n    split=ReadInstruction(\"validation\", to=50, unit=\"%\"),\r\n)\r\n\r\ndata_2.save_to_disk(\"temporary_path_2\")\r\n```\r\n\r\nand the corresponding output:\r\n\r\n```\r\nReusing dataset wikiann (/xxxxx/.cache/huggingface/datasets/wikiann/en/1.1.0/0b11a6fb31eea02f38ca17610657bfba3206100685283014daceb8da291c3be9)\r\nSave with regular split works.\r\nReusing dataset wikiann (/xxxxx/.cache/huggingface/datasets/wikiann/en/1.1.0/0b11a6fb31eea02f38ca17610657bfba3206100685283014daceb8da291c3be9)\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 20, in <module>\r\n    data_2.save_to_disk(\"temporary_path_2\")\r\n  File \"/xxxxx/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 645, in save_to_disk\r\n    json.dump(state, state_file, indent=2, sort_keys=True)\r\n  File \"/usr/lib/python3.7/json/__init__.py\", line 179, in dump\r\n    for chunk in iterable:\r\n  File \"/usr/lib/python3.7/json/encoder.py\", line 431, in _iterencode\r\n    yield from _iterencode_dict(o, _current_indent_level)\r\n  File \"/usr/lib/python3.7/json/encoder.py\", line 405, in _iterencode_dict\r\n    yield from chunks\r\n  File \"/usr/lib/python3.7/json/encoder.py\", line 438, in _iterencode\r\n    o = _default(o)\r\n  File \"/usr/lib/python3.7/json/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\nTypeError: Object of type ReadInstruction is not JSON serializable\r\n```\r\n\r\nLet me know if there is some misuse from my end.\r\n\r\nThanks in advance.\r\n ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 843508402,
    "title": "Add CER metric",
    "dateCreated": "2021-03-29T15:52:27Z",
    "dateModified": "2021-03-29T15:52:27Z",
    "description": "Add Character Error Rate (CER) metric that is used in evaluation in ASR. I also have written unittests (hopefully thorough enough) but I'm not sure how to integrate them into the existed codebase.\r\n\r\n```python\r\nfrom cer import CER\r\n\r\ncer = CER()\r\n\r\nclass TestCER(unittest.TestCase):\r\n    def test_cer_case_senstive(self):\r\n        refs = ['White House']\r\n        preds = ['white house']\r\n        # S = 2, D = 0, I = 0, N = 11, CER = 2 / 11\r\n        char_error_rate = cer.compute(predictions=preds, references=refs)\r\n        self.assertTrue(abs(char_error_rate - 0.1818181818) < 1e-6)\r\n\r\n    def test_cer_whitespace(self):\r\n        refs = ['were wolf']\r\n        preds = ['werewolf']\r\n        # S = 0, D = 0, I = 1, N = 9, CER = 1 / 9\r\n        char_error_rate = cer.compute(predictions=preds, references=refs)\r\n        self.assertTrue(abs(char_error_rate - 0.1111111) < 1e-6)\r\n\r\n        refs = ['werewolf']\r\n        preds = ['weae     wolf']\r\n        # S = 1, D = 1, I = 0, N = 8, CER = 0.25\r\n        char_error_rate = cer.compute(predictions=preds, references=refs)\r\n        self.assertTrue(abs(char_error_rate - 0.25) < 1e-6)\r\n\r\n        # consecutive whitespaces case 1\r\n        refs = ['were wolf']\r\n        preds = ['were               wolf']\r\n        # S = 0, D = 0, I = 0, N = 9, CER = 0\r\n        char_error_rate = cer.compute(predictions=preds, references=refs)\r\n        self.assertTrue(abs(char_error_rate - 0.0) < 1e-6)\r\n\r\n        # consecutive whitespaces case 2\r\n        refs = ['were   wolf']\r\n        preds = ['were               wolf']\r\n        # S = 0, D = 0, I = 0, N = 9, CER = 0\r\n        char_error_rate = cer.compute(predictions=preds, references=refs)\r\n        self.assertTrue(abs(char_error_rate - 0.0) < 1e-6)\r\n\r\n    def test_cer_sub(self):\r\n        refs = ['werewolf']\r\n        preds = ['weaewolf']\r\n        # S = 1, D = 0, I = 0, N = 8, CER = 0.125\r\n        char_error_rate = cer.compute(predictions=preds, references=refs)\r\n        self.assertTrue(abs(char_error_rate - 0.125) < 1e-6)\r\n\r\n    def test_cer_del(self):\r\n        refs = ['werewolf']\r\n        preds = ['wereawolf']\r\n        # S = 0, D = 1, I = 0, N = 8, CER = 0.125\r\n        char_error_rate = cer.compute(predictions=preds, references=refs)\r\n        self.assertTrue(abs(char_error_rate - 0.125) < 1e-6)\r\n\r\n    def test_cer_insert(self):\r\n        refs = ['werewolf']\r\n        preds = ['wereolf']\r\n        # S = 0, D = 0, I = 1, N = 8, CER = 0.125\r\n        char_error_rate = cer.compute(predictions=preds, references=refs)\r\n        self.assertTrue(abs(char_error_rate - 0.125) < 1e-6)\r\n\r\n    def test_cer_equal(self):\r\n        refs = ['werewolf']\r\n        char_error_rate = cer.compute(predictions=refs, references=refs)\r\n        self.assertEqual(char_error_rate, 0.0)\r\n\r\n    def test_cer_list_of_seqs(self):\r\n        refs = ['werewolf', 'I am your father']\r\n        char_error_rate = cer.compute(predictions=refs, references=refs)\r\n        self.assertEqual(char_error_rate, 0.0)\r\n\r\n        refs = ['werewolf', 'I am your father', 'doge']\r\n        preds = ['werxwolf', 'I       am your father', 'doge']\r\n        # S = 1, D = 0, I = 0, N = 28, CER = 1 / 28\r\n        char_error_rate = cer.compute(predictions=preds, references=refs)\r\n        self.assertTrue(abs(char_error_rate - 0.03571428) < 1e-6)\r\n\r\n    def test_cer_unicode(self):\r\n        ref = [u'\u6211\u80fd\u541e\u4e0b\u73bb\u7483\u800c\u4e0d\u4f24\u8eab\u4f53']\r\n        pred = [u' \u80fd\u541e\u867e\u73bb\u7483\u800c \u4e0d\u971c\u8eab\u4f53\u5566']\r\n        # S = 3, D = 2, I = 0, N = 11\r\n        # CER = 5 / 11\r\n        char_error_rate = cer.compute(predictions=pred, references=ref)\r\n        self.assertTrue(abs(char_error_rate - 0.4545454545) < 1e-6)\r\n\r\n        ref = [u'\u6211\u80fd\u541e', u'\u4e0b\u73bb\u7483\u800c\u4e0d\u4f24\u8eab\u4f53']\r\n        pred = [u'\u6211    \u80fd \u541e \u4e0b \u73bb \u7483', u'\u800c\u4e0d\u4f24\u8eab\u4f53']\r\n        # S = 0, D = 5, I = 0, N = 11\r\n        # CER = 5 / 11\r\n        char_error_rate = cer.compute(predictions=pred, references=ref)\r\n        self.assertTrue(abs(char_error_rate - 0.454545454545) < 1e-6)\r\n\r\n        ref = [u'\u6211\u80fd\u541e\u4e0b\u73bb\u7483\u800c\u4e0d\u4f24\u8eab\u4f53']\r\n        char_error_rate = cer.compute(predictions=ref, references=ref)\r\n        self.assertFalse(char_error_rate, 0.0)\r\n\r\n    def test_cer_empty(self):\r\n        ref = ''\r\n        pred = 'Hypothesis'\r\n        with self.assertRaises(ValueError):\r\n            char_error_rate = cer.compute(predictions=pred, references=ref)\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n```",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 843502835,
    "title": "Fix missing infos from concurrent dataset loading",
    "dateCreated": "2021-03-29T15:46:12Z",
    "dateModified": "2021-03-29T15:46:12Z",
    "description": "This should fix issue #2131 \r\n\r\nWhen calling `load_dataset` at the same time from 2 workers, one of the worker could have missing split infos when reloading the dataset from the cache.\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 843492015,
    "title": "fix dialogue action slot name and value",
    "dateCreated": "2021-03-29T15:34:13Z",
    "dateModified": "2021-03-29T15:34:13Z",
    "description": "fix #2128",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 843246344,
    "title": "en language data from MLQA dataset is missing",
    "dateCreated": "2021-03-29T10:47:50Z",
    "dateModified": "2021-03-29T10:47:50Z",
    "description": "Hi\r\nI need mlqa-translate-train.en dataset, but it is missing from the MLQA dataset. could you have a look please? @lhoestq  thank you for your help to fix this issue. ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 843242849,
    "title": "Saving large in-memory datasets with save_to_disk crashes because of pickling",
    "dateCreated": "2021-03-29T10:43:15Z",
    "dateModified": "2021-03-29T10:43:15Z",
    "description": "Using Datasets 1.5.0 on Python 3.7.\r\nRecently I've been working on medium to large size datasets (pretokenized raw text sizes from few gigabytes to low tens of gigabytes), and have found out that several preprocessing steps are massively faster when done in memory, and I have the ability to requisition a lot of RAM, so I decided to do these steps completely out of the datasets library.\r\n\r\n So my workflow is to do several .map() on datasets object, then for the operation which is faster in memory to extract the necessary columns from the dataset and then drop it whole, do the transformation in memory, and then create a fresh Dataset object using .from_dict() or other method. \r\n\r\nWhen I then try to call save_to_disk(path) on the dataset, it crashes because of pickling, which appears to be because of using old pickle protocol which doesn't support large files (over 4 GiB).\r\n```\r\nTraceback (most recent call last):\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 80, in <module>\r\n    main()\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 75, in main\r\n    tokenize_and_chunkify(config)\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 60, in tokenize_and_chunkify\r\n    contexts_dataset.save_to_disk(chunked_path)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 457, in save_to_disk\r\n    self = pickle.loads(pickle.dumps(self))\r\nOverflowError: cannot serialize a bytes object larger than 4 GiB\r\n```\r\nFrom what I've seen this issue may be possibly fixed, as the line `self = pickle.loads(pickle.dumps(self))` does not appear to be present in the current state of the repository.\r\n\r\nTo save these datasets to disk, I've resorted to calling .map() over them with `function=None` and specifying the .arrow cache file, and then creating a new dataset using the .from_file() method, which I can then safely save to disk.\r\n\r\nAdditional issue when working with these large in-memory datasets is when using multiprocessing, is again to do with pickling. I've tried to speed up the mapping with function=None by specifying num_proc to the available cpu count, and I again get issues with transferring the dataset, with the following traceback. I am not sure if I should open a separate issue for that.\r\n```\r\nTraceback (most recent call last):\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 94, in <module>\r\n    main()\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 89, in main\r\n    tokenize_and_chunkify(config)\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 67, in tokenize_and_chunkify\r\n    contexts_dataset.map(function=None, cache_file_name=str(output_dir_path / \"tmp.arrow\"), writer_batch_size=50000, num_proc=config.threads)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1485, in map\r\n    transformed_shards = [r.get() for r in results]\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1485, in <listcomp>\r\n    transformed_shards = [r.get() for r in results]\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py\", line 657, in get\r\n    raise self._value\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py\", line 431, in _handle_tasks\r\n    put(task)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/connection.py\", line 209, in send\r\n    self._send_bytes(_ForkingPickler.dumps(obj))\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/reduction.py\", line 54, in dumps\r\n    cls(buf, protocol, *args, **kwds).dump(obj)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py\", line 454, in dump\r\n    StockPickler.dump(self, obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 437, in dump\r\n    self.save(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 843, in _batch_appends\r\n    save(x)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 843, in _batch_appends\r\n    save(x)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 732, in save_bytes\r\n    self._write_large_bytes(BINBYTES + pack(\"<I\", n), obj)\r\nstruct.error: 'I' format requires 0 <= number <= 4294967295Traceback (most recent call last):\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 94, in <module>\r\n    main()\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 89, in main\r\n    tokenize_and_chunkify(config)\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 67, in tokenize_and_chunkify\r\n    contexts_dataset.map(function=None, cache_file_name=str(output_dir_path / \"tmp.arrow\"), writer_batch_size=50000, num_proc=config.threads)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1485, in map\r\n    transformed_shards = [r.get() for r in results]\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1485, in <listcomp>\r\n    transformed_shards = [r.get() for r in results]\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py\", line 657, in get\r\n    raise self._value\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py\", line 431, in _handle_tasks\r\n    put(task)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/connection.py\", line 209, in send\r\n    self._send_bytes(_ForkingPickler.dumps(obj))\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/reduction.py\", line 54, in dumps\r\n    cls(buf, protocol, *args, **kwds).dump(obj)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py\", line 454, in dump\r\n    StockPickler.dump(self, obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 437, in dump\r\n    self.save(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 843, in _batch_appends\r\n    save(x)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 843, in _batch_appends\r\n    save(x)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 732, in save_bytes\r\n    self._write_large_bytes(BINBYTES + pack(\"<I\", n), obj)\r\nstruct.error: 'I' format requires 0 <= number <= 4294967295\r\n```",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 843149680,
    "title": "bug in mlqa dataset ",
    "dateCreated": "2021-03-29T09:03:09Z",
    "dateModified": "2021-03-29T09:03:09Z",
    "description": "Hi \r\nLooking into MLQA dataset for langauge \"ar\":\r\n\r\n```\r\n \"question\": [\r\n    \"\\u0645\\u062a\\u0649 \\u0628\\u062f\\u0627\\u062a \\u0627\\u0644\\u0645\\u062c\\u0644\\u0629 \\u0627\\u0644\\u0645\\u062f\\u0631\\u0633\\u064a\\u0629 \\u0641\\u064a \\u0646\\u0648\\u062a\\u0631\\u062f\\u0627\\u0645 \\u0628\\u0627\\u0644\\u0646\\u0634\\u0631?\",\r\n    \"\\u0643\\u0645 \\u0645\\u0631\\u0629 \\u064a\\u062a\\u0645 \\u0646\\u0634\\u0631\\u0647\\u0627 \\u0641\\u064a \\u0646\\u0648\\u062a\\u0631\\u062f\\u0627\\u0645?\",\r\n    \"\\u0645\\u0627 \\u0647\\u064a \\u0627\\u0644\\u0648\\u0631\\u0642\\u0629 \\u0627\\u0644\\u064a\\u0648\\u0645\\u064a\\u0629 \\u0644\\u0644\\u0637\\u0644\\u0627\\u0628 \\u0641\\u064a \\u0646\\u0648\\u062a\\u0631\\u062f\\u0627\\u0645?\",\r\n    \"\\u0643\\u0645 \\u0639\\u062f\\u062f \\u0627\\u0644\\u0627\\u0648\\u0631\\u0627\\u0642 \\u0627\\u0644\\u0627\\u062e\\u0628\\u0627\\u0631\\u064a\\u0629 \\u0644\\u0644\\u0637\\u0644\\u0627\\u0628 \\u0627\\u0644\\u062a\\u064a \\u0648\\u062c\\u062f\\u062a \\u0641\\u064a \\u0646\\u0648\\u062a\\u0631\\u062f\\u0627\\u0645?\",\r\n    \"\\u0641\\u064a \\u0627\\u064a \\u0633\\u0646\\u0629 \\u0628\\u062f\\u0627\\u062a \\u0648\\u0631\\u0642\\u0629 \\u0627\\u0644\\u0637\\u0627\\u0644\\u0628 \\u0627\\u0644\\u062d\\u0633 \\u0627\\u0644\\u0633\\u0644\\u064a\\u0645 \\u0628\\u0627\\u0644\\u0646\\u0634\\u0631 \\u0641\\u064a \\u0646\\u0648\\u062a\\u0631\\u062f\\u0627\\u0645?\"\r\n  ]\r\n```\r\n\r\nthe questions are in the wrong format, and not readable, could you please have a look? thanks @lhoestq \r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 843142822,
    "title": "TydiQA dataset is mixed and is not split per language ",
    "dateCreated": "2021-03-29T08:56:21Z",
    "dateModified": "2021-03-29T08:56:21Z",
    "description": "Hi @lhoestq \r\nCurrently TydiQA is mixed and user can only access the whole training set of all languages:\r\nhttps://www.tensorflow.org/datasets/catalog/tydi_qa\r\n\r\nfor using this dataset, one need to train/evaluate in each separate language, and having them mixed, makes it hard to use this dataset. This is much convenient for user to have  them split and I appreciate your help on this. \r\n\r\nMeanwhile, till hopefully this is split per language, I greatly appreciate telling me how I can preprocess and get data per language. thanks a lot ",
    "status": "open",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 843133112,
    "title": "When training with Multi-Node Multi-GPU the worker 2 has TypeError: 'NoneType' object",
    "dateCreated": "2021-03-29T08:45:58Z",
    "dateModified": "2021-03-29T08:45:58Z",
    "description": "\bversion: 1.5.0\r\nmet a very strange error, I am training large scale language model, and need train on 2 machines(workers).\r\nAnd sometimes I will get this error `TypeError: 'NoneType' object is not iterable`\r\nThis is traceback\r\n```\r\n\r\n71 | \u00a0 | Traceback (most recent call last):\r\n-- | -- | --\r\n72 | \u00a0 | File \"run_gpt.py\", line 316, in <module>\r\n73 | \u00a0 | main()\r\n74 | \u00a0 | File \"run_gpt.py\", line 222, in main\r\n75 | \u00a0 | delimiter=\"\\t\", column_names=[\"input_ids\", \"attention_mask\", \"chinese_ref\"])\r\n76 | \u00a0 | File \"/data/miniconda3/lib/python3.7/site-packages/datasets/load.py\", line 747, in load_dataset\r\n77 | \u00a0 | use_auth_token=use_auth_token,\r\n78 | \u00a0 | File \"/data/miniconda3/lib/python3.7/site-packages/datasets/builder.py\", line 513, in download_and_prepare\r\n79 | \u00a0 | self.download_post_processing_resources(dl_manager)\r\n80 | \u00a0 | File \"/data/miniconda3/lib/python3.7/site-packages/datasets/builder.py\", line 673, in download_post_processing_resources\r\n81 | \u00a0 | for split in self.info.splits:\r\n82 | \u00a0 | TypeError: 'NoneType' object is not iterable\r\n83 | \u00a0 | WARNING:datasets.builder:Reusing dataset csv (/usr/local/app/.cache/huggingface/datasets/csv/default-1c257ebd48e225e7/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2)\r\n84 | \u00a0 | Traceback (most recent call last):\r\n85 | \u00a0 | File \"/data/miniconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n86 | \u00a0 | \"__main__\", mod_spec)\r\n87 | \u00a0 | File \"/data/miniconda3/lib/python3.7/runpy.py\", line 85, in _run_code\r\n88 | \u00a0 | exec(code, run_globals)\r\n89 | \u00a0 | File \"/data/miniconda3/lib/python3.7/site-packages/torch/distributed/launch.py\", line 340, in <module>\r\n90 | \u00a0 | main()\r\n91 | \u00a0 | File \"/data/miniconda3/lib/python3.7/site-packages/torch/distributed/launch.py\", line 326, in main\r\n92 | \u00a0 | sigkill_handler(signal.SIGTERM, None)  # not coming back\r\n93 | \u00a0 | File \"/data/miniconda3/lib/python3.7/site-packages/torch/distributed/launch.py\", line 301, in sigkill_handler\r\n94 | \u00a0 | raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)\r\n\r\n```\r\nOn worker 1 it loads the dataset well, however on worker 2 will get this error. \r\nAnd I will meet this error from time to time, sometimes it just goes well.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 843111936,
    "title": "wikiann dataset is missing columns ",
    "dateCreated": "2021-03-29T08:23:00Z",
    "dateModified": "2021-03-29T08:23:00Z",
    "description": "Hi\r\nWikiann dataset needs to have \"spans\" columns, which is necessary to be able to use this dataset, but this column is missing from huggingface datasets, could you please have a look? thank you @lhoestq ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 843033656,
    "title": "How to train BERT model with next sentence prediction?",
    "dateCreated": "2021-03-29T06:48:03Z",
    "dateModified": "2021-03-29T06:48:03Z",
    "description": "Hello.\r\n\r\nI'm trying to pretrain the BERT model with next sentence prediction. Is there any function that supports next sentence prediction \r\nlike ` TextDatasetForNextSentencePrediction` of `huggingface/transformers` ?\r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 843023910,
    "title": "Dialogue action slot name and value are reversed in MultiWoZ 2.2",
    "dateCreated": "2021-03-29T06:34:02Z",
    "dateModified": "2021-03-29T06:34:02Z",
    "description": "Hi @yjernite, thank you for adding MultiWoZ 2.2 in the huggingface datasets platform. It is beneficial!\r\n\r\nI spot an error that the order of Dialogue action slot names and values are reversed.\r\n\r\nhttps://github.com/huggingface/datasets/blob/649b2c469779bc4221e1b6969aa2496d63eb5953/datasets/multi_woz_v22/multi_woz_v22.py#L251-L262",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 843017199,
    "title": "make documentation more clear to use different cloud storage",
    "dateCreated": "2021-03-29T06:24:06Z",
    "dateModified": "2021-03-29T06:24:06Z",
    "description": "This PR extends the cloud storage documentation. To show you can use a different `fsspec` implementation. ",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 842779966,
    "title": "Replace legacy torch.Tensor constructor with torch.tensor",
    "dateCreated": "2021-03-28T16:57:30Z",
    "dateModified": "2021-03-28T16:57:30Z",
    "description": "The title says it all (motivated by [this issue](https://github.com/pytorch/pytorch/issues/53146) in the pytorch repo).",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 842690570,
    "title": "Is dataset timit_asr broken?",
    "dateCreated": "2021-03-28T08:30:18Z",
    "dateModified": "2021-03-28T08:30:18Z",
    "description": "Using `timit_asr` dataset, I saw all records are the same.\r\n\r\n``` python\r\nfrom datasets import load_dataset, load_metric\r\n\r\ntimit = load_dataset(\"timit_asr\")\r\n\r\nfrom datasets import ClassLabel\r\nimport random\r\nimport pandas as pd\r\nfrom IPython.display import display, HTML\r\n\r\ndef show_random_elements(dataset, num_examples=10):\r\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\r\n    picks = []\r\n    for _ in range(num_examples):\r\n        pick = random.randint(0, len(dataset)-1)\r\n        while pick in picks:\r\n            pick = random.randint(0, len(dataset)-1)\r\n        picks.append(pick)\r\n\r\n    df = pd.DataFrame(dataset[picks])\r\n    display(HTML(df.to_html()))\r\n\r\n\r\nshow_random_elements(timit['train'].remove_columns([\"file\", \"phonetic_detail\", \"word_detail\", \"dialect_region\", \"id\", \r\n                                                    \"sentence_type\", \"speaker_id\"]), num_examples=20)\r\n\r\n```\r\n\r\n`output`\r\n\r\n<img width=\"312\" alt=\"Screen Shot 2021-03-28 at 17 29 04\" src=\"https://user-images.githubusercontent.com/42398050/112746646-21acee80-8feb-11eb-84f3-dbb5d4269724.png\">\r\n\r\n\r\nI double-checked it [here](https://huggingface.co/datasets/viewer/), and met the same problem.\r\n\r\n<img width=\"1374\" alt=\"Screen Shot 2021-03-28 at 17 32 07\" src=\"https://user-images.githubusercontent.com/42398050/112746698-9bdd7300-8feb-11eb-97ed-5babead385f4.png\">\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 842627729,
    "title": "Adding ScaNN library to do MIPS?",
    "dateCreated": "2021-03-28T00:07:00Z",
    "dateModified": "2021-03-28T00:07:00Z",
    "description": "@lhoestq Hi I am thinking of adding this new google library to do the MIPS similar to **add_faiss_idex**. As the paper suggests, it is really fast when it comes to retrieving the nearest neighbors. \r\n\r\nhttps://github.com/google-research/google-research/tree/master/scann\r\n\r\n![image](https://user-images.githubusercontent.com/16892570/112738294-78ec9800-8fc6-11eb-9a5f-3d7ee5818e76.png)\r\n",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 842577285,
    "title": "Problem downloading GEM wiki_auto_asset_turk dataset",
    "dateCreated": "2021-03-27T18:41:28Z",
    "dateModified": "2021-03-27T18:41:28Z",
    "description": "@yjernite \r\n\r\n### Summary\r\n\r\nI am currently working on the GEM datasets and do not manage to download the wiki_auto_asset_turk data, whereas all other datasets download well with the same code.\r\n\r\n### Steps to reproduce\r\nCode snippet:\r\n\r\nfrom datasets import load_dataset\r\n#dataset = load_dataset('gem', 'web_nlg_en')\r\ndataset = load_dataset('gem', 'wiki_auto_asset_turk')\r\n\r\n```\r\n\r\n**Expected behavior:**\r\n\r\nI expect the dataset to start downloading (download bar appears and progresses toward 100%)\r\n\r\n**Actual behavior:**\r\nInstead of seeing the download bar appearing, nothing happens; the following appears in the console as expected, but nothing more:\r\n\r\nDownloading: 36.6kB [00:00, 37.2MB/s]\r\nDownloading: 41.7kB [00:00, ?B/s]\r\nDownloading and preparing dataset gem/wiki_auto_asset_turk (download: 121.37 MiB, generated: 145.69 MiB, post-processed: Unknown size, total: 267.07 MiB) to C:\\Users\\sfmil\\.cache\\huggingface\\datasets\\gem\\wiki_auto_asset_turk\\1.0.0\\f252756d7f1b8f019aac71a1623b2950acfe10d25d956668ac4eae4e93c58b8d...\r\n\r\n### Is this a regression?\r\nNo, it was the first time I was trying to download this dataset (same for the other ones).\r\n\r\n### Debug info\r\n- Python version: Python 3.8.2\r\n- OS version: Windows 10 Family",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 842194588,
    "title": "Fast table queries with interpolation search",
    "dateCreated": "2021-03-26T18:09:20Z",
    "dateModified": "2021-03-26T18:09:20Z",
    "description": "## Intro\r\n\r\nThis should fix issue #1803 \r\n\r\nCurrently querying examples in a dataset is O(n) because of the underlying pyarrow ChunkedArrays implementation.\r\nTo fix this I implemented interpolation search that is pretty effective since datasets usually verifies the condition of evenly distributed chunks (the default chunk size is fixed).\r\n\r\n## Benchmark\r\n\r\nHere is a [benchmark](https://pastebin.com/utEXUqsR) I did on bookcorpus (74M rows):\r\n\r\nfor the current implementation\r\n```python\r\n>>> python speed.py\r\nLoaded dataset 'bookcorpus', len=74004228, nbytes=4835358766\r\n\r\n\r\n========================= Querying unshuffled bookcorpus =========================\r\n\r\nAvg access time key=1                                                 : 0.018ms\r\nAvg access time key=74004227                                          : 0.215ms\r\nAvg access time key=range(74003204, 74004228)                         : 1.416ms\r\nAvg access time key=RandIter(low=0, high=74004228, size=1024, seed=42): 92.532ms\r\n\r\n========================== Querying shuffled bookcorpus ==========================\r\n\r\nAvg access time key=1                                                 : 0.187ms\r\nAvg access time key=74004227                                          : 6.642ms\r\nAvg access time key=range(74003204, 74004228)                         : 90.941ms\r\nAvg access time key=RandIter(low=0, high=74004228, size=1024, seed=42): 3448.456ms\r\n```\r\n\r\nfor the new one using interpolation search:\r\n```python\r\n>>> python speed.py\r\nLoaded dataset 'bookcorpus', len=74004228, nbytes=4835358766\r\n\r\n\r\n========================= Querying unshuffled bookcorpus =========================\r\n\r\nAvg access time key=1                                                 : 0.076ms\r\nAvg access time key=74004227                                          : 0.056ms\r\nAvg access time key=range(74003204, 74004228)                         : 1.807ms\r\nAvg access time key=RandIter(low=0, high=74004228, size=1024, seed=42): 24.028ms\r\n\r\n========================== Querying shuffled bookcorpus ==========================\r\n\r\nAvg access time key=1                                                 : 0.061ms\r\nAvg access time key=74004227                                          : 0.058ms\r\nAvg access time key=range(74003204, 74004228)                         : 22.166ms\r\nAvg access time key=RandIter(low=0, high=74004228, size=1024, seed=42): 42.757ms\r\n```\r\n\r\nThe RandIter class is just an iterable of 1024 random indices from 0 to 74004228.\r\n\r\nHere is also a plot showing the speed improvement depending on the dataset size:\r\n![image](https://user-images.githubusercontent.com/42851186/112673587-32335c80-8e65-11eb-9a0c-58ad774abaec.png)\r\n\r\n## Implementation details:\r\n- `datasets.table.Table` objects implement interpolation search for the `slice` method\r\n- The interpolation search requires to store the offsets of all the chunks of a table. The offsets are stored when the `Table` is initialized.\r\n- `datasets.table.Table.slice` returns a `datasets.table.Table` using interpolation search\r\n- `datasets.table.Table.fast_slice` returns a `pyarrow.Table` object using interpolation search. This is useful to get a part of a dataset if we don't need the indexing structure for future computations. For example it's used when querying an example as a dictionary.\r\n- Now a `Dataset` object is always backed by a `datasets.table.Table` object. If one passes a `pyarrow.Table` to initialize a `Dataset`, then it's converted to a `datasets.table.Table`\r\n\r\n## Checklist:\r\n\r\n- [x] implement interpolation search\r\n- [x] use `datasets.table.Table` in `Dataset` objects\r\n- [x] update current tests\r\n- [x] add tests for interpolation search\r\n- [x] comments and docstring\r\n- [x] add the benchmark to the CI\r\n\r\nFix #1803.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 842148633,
    "title": "Add Validation For README",
    "dateCreated": "2021-03-26T17:02:17Z",
    "dateModified": "2021-03-26T17:02:17Z",
    "description": "Hi @lhoestq, @yjernite \r\n\r\nThis is a simple Readme parser. All classes specific to different sections can inherit `Section` class, and we can define more attributes in each.\r\n\r\nLet me know if this is going in the right direction :)\r\n\r\nCurrently the output looks like this, for `to_dict()` on `FashionMNIST` `README.md`:\r\n\r\n```json\r\n{\r\n    \"name\": \"./datasets/fashion_mnist/README.md\",\r\n    \"attributes\": \"\",\r\n    \"subsections\": [\r\n        {\r\n            \"name\": \"Dataset Card for FashionMNIST\",\r\n            \"attributes\": \"\",\r\n            \"subsections\": [\r\n                {\r\n                    \"name\": \"Table of Contents\",\r\n                    \"attributes\": \"- [Dataset Description](#dataset-description)\\n  - [Dataset Summary](#dataset-summary)\\n  - [Supported Tasks](#supported-tasks-and-leaderboards)\\n  - [Languages](#languages)\\n- [Dataset Structure](#dataset-structure)\\n  - [Data Instances](#data-instances)\\n  - [Data Fields](#data-instances)\\n  - [Data Splits](#data-instances)\\n- [Dataset Creation](#dataset-creation)\\n  - [Curation Rationale](#curation-rationale)\\n  - [Source Data](#source-data)\\n  - [Annotations](#annotations)\\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\\n- [Considerations for Using the Data](#considerations-for-using-the-data)\\n  - [Social Impact of Dataset](#social-impact-of-dataset)\\n  - [Discussion of Biases](#discussion-of-biases)\\n  - [Other Known Limitations](#other-known-limitations)\\n- [Additional Information](#additional-information)\\n  - [Dataset Curators](#dataset-curators)\\n  - [Licensing Information](#licensing-information)\\n  - [Citation Information](#citation-information)\\n  - [Contributions](#contributions)\",\r\n                    \"subsections\": []\r\n                },\r\n                {\r\n                    \"name\": \"Dataset Description\",\r\n                    \"attributes\": \"- **Homepage:** [GitHub](https://github.com/zalandoresearch/fashion-mnist)\\n- **Repository:** [GitHub](https://github.com/zalandoresearch/fashion-mnist)\\n- **Paper:** [arXiv](https://arxiv.org/pdf/1708.07747.pdf)\\n- **Leaderboard:**\\n- **Point of Contact:**\",\r\n                    \"subsections\": [\r\n                        {\r\n                            \"name\": \"Dataset Summary\",\r\n                            \"attributes\": \"Fashion-MNIST is a dataset of Zalando's article images\\u2014consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\",\r\n                            \"subsections\": []\r\n                        },\r\n                        {\r\n                            \"name\": \"Supported Tasks and Leaderboards\",\r\n                            \"attributes\": \"[More Information Needed]\",\r\n                            \"subsections\": []\r\n                        },\r\n                        {\r\n                            \"name\": \"Languages\",\r\n                            \"attributes\": \"[More Information Needed]\",\r\n                            \"subsections\": []\r\n                        }\r\n                    ]\r\n                },\r\n                {\r\n                    \"name\": \"Dataset Structure\",\r\n                    \"attributes\": \"\",\r\n                    \"subsections\": [\r\n                        {\r\n                            \"name\": \"Data Instances\",\r\n                            \"attributes\": \"A data point comprises an image and its label.\",\r\n                            \"subsections\": []\r\n                        },\r\n                        {\r\n                            \"name\": \"Data Fields\",\r\n                            \"attributes\": \"- `image`: a 2d array of integers representing the 28x28 image.\\n- `label`: an integer between 0 and 9 representing the classes with the following mapping:\\n  | Label | Description |\\n  | --- | --- |\\n  | 0 | T-shirt/top |\\n  | 1 | Trouser |\\n  | 2 | Pullover |\\n  | 3 | Dress |\\n  | 4 | Coat |\\n  | 5 | Sandal |\\n  | 6 | Shirt |\\n  | 7 | Sneaker |\\n  | 8 | Bag |\\n  | 9 | Ankle boot |\",\r\n                            \"subsections\": []\r\n                        },\r\n                        {\r\n                            \"name\": \"Data Splits\",\r\n                            \"attributes\": \"The data is split into training and test set. The training set contains 60,000 images and the test set 10,000 images.\",\r\n                            \"subsections\": []\r\n                        }\r\n                    ]\r\n                },\r\n                {\r\n                    \"name\": \"Dataset Creation\",\r\n                    \"attributes\": \"\",\r\n                    \"subsections\": [\r\n                        {\r\n                            \"name\": \"Curation Rationale\",\r\n                            \"attributes\": \"**From the arXiv paper:**\\nThe original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \\\"If it doesn't work on MNIST, it won't work at all\\\", they said. \\\"Well, if it does work on MNIST, it may still fail on others.\\\"\\nHere are some good reasons:\\n- MNIST is too easy. Convolutional nets can achieve 99.7% on MNIST. Classic machine learning algorithms can also achieve 97% easily. Check out our side-by-side benchmark for Fashion-MNIST vs. MNIST, and read \\\"Most pairs of MNIST digits can be distinguished pretty well by just one pixel.\\\"\\n- MNIST is overused. In this April 2017 Twitter thread, Google Brain research scientist and deep learning expert Ian Goodfellow calls for people to move away from MNIST.\\n- MNIST can not represent modern CV tasks, as noted in this April 2017 Twitter thread, deep learning expert/Keras author Fran\\u00e7ois Chollet.\",\r\n                            \"subsections\": []\r\n                        },\r\n                        {\r\n                            \"name\": \"Source Data\",\r\n                            \"attributes\": \"\",\r\n                            \"subsections\": [\r\n                                {\r\n                                    \"name\": \"Initial Data Collection and Normalization\",\r\n                                    \"attributes\": \"**From the arXiv paper:**\\nFashion-MNIST is based on the assortment on Zalando\\u2019s website. Every fashion product on Zalando has a set of pictures shot by professional photographers, demonstrating different aspects of the product, i.e. front and back looks, details, looks with model and in an outfit. The original picture has a light-gray background (hexadecimal color: #fdfdfd) and stored in 762 \\u00d7 1000 JPEG format. For efficiently serving different frontend components, the original picture is resampled with multiple resolutions, e.g. large, medium, small, thumbnail and tiny.\\nWe use the front look thumbnail images of 70,000 unique products to build Fashion-MNIST. Those products come from different gender groups: men, women, kids and neutral. In particular, whitecolor products are not included in the dataset as they have low contrast to the background. The thumbnails (51 \\u00d7 73) are then fed into the following conversion pipeline:\\n1. Converting the input to a PNG image.\\n2. Trimming any edges that are close to the color of the corner pixels. The \\u201ccloseness\\u201d is defined by the distance within 5% of the maximum possible intensity in RGB space.\\n3. Resizing the longest edge of the image to 28 by subsampling the pixels, i.e. some rows and columns are skipped over.\\n4. Sharpening pixels using a Gaussian operator of the radius and standard deviation of 1.0, with increasing effect near outlines.\\n5. Extending the shortest edge to 28 and put the image to the center of the canvas.\\n6. Negating the intensities of the image.\\n7. Converting the image to 8-bit grayscale pixels.\",\r\n                                    \"subsections\": []\r\n                                },\r\n                                {\r\n                                    \"name\": \"Who are the source image producers?\",\r\n                                    \"attributes\": \"**From the arXiv paper:**\\nEvery fashion product on Zalando has a set of pictures shot by professional photographers, demonstrating different aspects of the product, i.e. front and back looks, details, looks with model and in an outfit.\",\r\n                                    \"subsections\": []\r\n                                }\r\n                            ]\r\n                        },\r\n                        {\r\n                            \"name\": \"Annotations\",\r\n                            \"attributes\": \"\",\r\n                            \"subsections\": [\r\n                                {\r\n                                    \"name\": \"Annotation process\",\r\n                                    \"attributes\": \"**From the arXiv paper:**\\nFor the class labels, they use the silhouette code of the product. The silhouette code is manually labeled by the in-house fashion experts and reviewed by a separate team at Zalando. Each product Zalando is the Europe\\u2019s largest online fashion platform. Each product contains only one silhouette code.\",\r\n                                    \"subsections\": []\r\n                                },\r\n                                {\r\n                                    \"name\": \"Who are the annotators?\",\r\n                                    \"attributes\": \"**From the arXiv paper:**\\nThe silhouette code is manually labeled by the in-house fashion experts and reviewed by a separate team at Zalando.\",\r\n                                    \"subsections\": []\r\n                                }\r\n                            ]\r\n                        },\r\n                        {\r\n                            \"name\": \"Personal and Sensitive Information\",\r\n                            \"attributes\": \"[More Information Needed]\",\r\n                            \"subsections\": []\r\n                        }\r\n                    ]\r\n                },\r\n                {\r\n                    \"name\": \"Considerations for Using the Data\",\r\n                    \"attributes\": \"\",\r\n                    \"subsections\": [\r\n                        {\r\n                            \"name\": \"Social Impact of Dataset\",\r\n                            \"attributes\": \"[More Information Needed]\",\r\n                            \"subsections\": []\r\n                        },\r\n                        {\r\n                            \"name\": \"Discussion of Biases\",\r\n                            \"attributes\": \"[More Information Needed]\",\r\n                            \"subsections\": []\r\n                        },\r\n                        {\r\n                            \"name\": \"Other Known Limitations\",\r\n                            \"attributes\": \"[More Information Needed]\",\r\n                            \"subsections\": []\r\n                        }\r\n                    ]\r\n                },\r\n                {\r\n                    \"name\": \"Additional Information\",\r\n                    \"attributes\": \"\",\r\n                    \"subsections\": [\r\n                        {\r\n                            \"name\": \"Dataset Curators\",\r\n                            \"attributes\": \"Han Xiao and Kashif Rasul and Roland Vollgraf\",\r\n                            \"subsections\": []\r\n                        },\r\n                        {\r\n                            \"name\": \"Licensing Information\",\r\n                            \"attributes\": \"MIT Licence\",\r\n                            \"subsections\": []\r\n                        },\r\n                        {\r\n                            \"name\": \"Citation Information\",\r\n                            \"attributes\": \"@article{DBLP:journals/corr/abs-1708-07747,\\n  author    = {Han Xiao and\\n               Kashif Rasul and\\n               Roland Vollgraf},\\n  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\\n               Algorithms},\\n  journal   = {CoRR},\\n  volume    = {abs/1708.07747},\\n  year      = {2017},\\n  url       = {http://arxiv.org/abs/1708.07747},\\n  archivePrefix = {arXiv},\\n  eprint    = {1708.07747},\\n  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\",\r\n                            \"subsections\": []\r\n                        },\r\n                        {\r\n                            \"name\": \"Contributions\",\r\n                            \"attributes\": \"Thanks to [@gchhablani](https://github.com/gchablani) for adding this dataset.\",\r\n                            \"subsections\": []\r\n                        }\r\n                    ]\r\n                }\r\n            ]\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nThanks,\r\nGunjan",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 841954521,
    "title": "dataset viewer does not work anymore ",
    "dateCreated": "2021-03-26T13:22:13Z",
    "dateModified": "2021-03-26T13:22:13Z",
    "description": "Hi\r\nI normally use this link to see all datasets and how I can load them \r\n\r\n\r\nhttps://huggingface.co/datasets/viewer/\r\n\r\nNow I am getting \r\n\r\n502 Bad Gateway\r\nnginx/1.18.0 (Ubuntu)\r\n\r\ncould you bring this webpage back ? this was very helpful @lhoestq \r\nthanks for your help ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 841567199,
    "title": "copy.deepcopy os.environ instead of copy",
    "dateCreated": "2021-03-26T03:58:38Z",
    "dateModified": "2021-03-26T03:58:38Z",
    "description": "Fixes: https://github.com/huggingface/datasets/issues/2115\r\n\r\n- bug fix: using envrion.copy() returns a dict.\r\n- using deepcopy(environ) returns an `_environ` object\r\n- Changing the datatype of the _environ object can break code, if subsequent libraries perform operations using apis exclusive to the environ object, like `environ.getenv()` for example.\r\n\r\n\r\nTesting:\r\n\r\nTested the change on my terminal:\r\n\r\n```\r\n>>> import os\r\n>>> x = deepcopy(os.environ)\r\n>>> y = os.environ\r\n>>> x is y\r\nFalse\r\n>>> isinstance(x, type(os.environ))\r\nTrue\r\n>>> z = os.environ.copy()\r\n>>> isinstance(z, type(os.environ))\r\nFalse\r\n>>> isinstance(z, dict)\r\nTrue\r\n```",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 841563329,
    "title": "Remove os.environ.copy in Dataset.map",
    "dateCreated": "2021-03-26T03:48:17Z",
    "dateModified": "2021-03-26T03:48:17Z",
    "description": "Replace `os.environ.copy` with in-place modification\r\nFixes #2115 ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 841535283,
    "title": "load_metric from local \"glue.py\" meet error 'NoneType' object is not callable",
    "dateCreated": "2021-03-26T02:35:22Z",
    "dateModified": "2021-03-26T02:35:22Z",
    "description": "actual_task = \"mnli\" if task == \"mnli-mm\" else task\r\ndataset = load_dataset(path='/home/glue.py', name=actual_task)\r\nmetric = load_metric(path='/home/glue.py', name=actual_task)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-8-7ab77a465d81> in <module>\r\n      1 actual_task = \"mnli\" if task == \"mnli-mm\" else task\r\n      2 dataset = load_dataset(path='/home/jcli/glue.py', name=actual_task)\r\n----> 3 metric = load_metric(path='/home/jcli/glue.py', name=actual_task)\r\n\r\n~/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/load.py in load_metric(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, script_version, **metric_init_kwargs)\r\n    508         keep_in_memory=keep_in_memory,\r\n    509         experiment_id=experiment_id,\r\n--> 510         **metric_init_kwargs,\r\n    511     )\r\n    512 \r\n\r\nTypeError: 'NoneType' object is not callable\r\n\r\nPlease help",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 841481292,
    "title": "Creating custom dataset results in error while calling the map() function",
    "dateCreated": "2021-03-26T00:37:46Z",
    "dateModified": "2021-03-26T00:37:46Z",
    "description": "calling `map()` of `datasets` library results into an error while defining a Custom dataset.\r\nReproducible example:\r\n```\r\nimport datasets\r\nclass MyDataset(datasets.Dataset):\r\n\r\n    def __init__(self, sentences):\r\n        \"Initialization\"\r\n        self.samples = sentences\r\n\r\n    def __len__(self):\r\n        \"Denotes the total number of samples\"\r\n        return len(self.samples)\r\n\r\n    def __getitem__(self, index):\r\n        \"Generates one sample of data\"\r\n        # Select sample\r\n        # Load data and get label\r\n        samples = self.samples[index]\r\n\r\n        return samples\r\n\r\ndef preprocess_function_train(examples):\r\n        inputs = examples\r\n        labels = [example+tokenizer.eos_token for example in examples ]\r\n        inputs = tokenizer(inputs, max_length=30, padding=True, truncation=True)\r\n        labels = tokenizer(labels, max_length=30, padding=True, truncation=True)\r\n        model_inputs = inputs\r\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\r\n        print(\"about to return\")\r\n        return model_inputs\r\n\r\n\r\n##train[\"sentence\"] is dataframe column\r\ntrain_dataset = MyDataset(train['sentence'].values.tolist())\r\ntrain_dataset = train_dataset.map(\r\n            preprocess_function,\r\n            batched = True,\r\n            batch_size=32\r\n        )\r\n```\r\n\r\nStack trace of error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"dir/train_generate.py\", line 362, in <module>\r\n    main()\r\n  File \"dir/train_generate.py\", line 245, in main\r\n    train_dataset = train_dataset.map(\r\n  File \"anaconda_dir/anaconda3/envs/env1/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1244, in map\r\n    return self._map_single(\r\n  File \"anaconda_dir/anaconda3/envs/env1/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 149, in wrapper\r\n    unformatted_columns = set(self.column_names) - set(self._format_columns or [])\r\n  File \"anaconda_dir/anaconda3/envs/env1/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 526, in column_names\r\n    return self._data.column_names\r\nAttributeError: 'MyDataset' object has no attribute '_data'\r\n```",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 841283974,
    "title": "The datasets.map() implementation modifies the datatype of os.environ object",
    "dateCreated": "2021-03-25T20:29:19Z",
    "dateModified": "2021-03-25T20:29:19Z",
    "description": "In our testing, we noticed that the datasets.map() implementation is modifying the datatype of python os.environ object from '_Environ' to 'dict'.\r\n\r\nThis causes following function calls to fail as follows:\r\n\r\n`   \r\n     x = os.environ.get(\"TEST_ENV_VARIABLE_AFTER_dataset_map\", default=None)\r\n    TypeError: get() takes no keyword arguments\r\n`\r\nIt looks like the following line in datasets.map implementation introduced this functionality.\r\n\r\nhttps://github.com/huggingface/datasets/blob/0cb1ac06acb0df44a1cf4128d03a01865faa2504/src/datasets/arrow_dataset.py#L1421\r\n\r\nHere is the test script to reproduce this error. \r\n\r\n\r\n```\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer\r\nimport os\r\n\r\n\r\ndef test_train():\r\n    model_checkpoint = \"distilgpt2\"\r\n    datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\r\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\r\n    tokenizer.pad_token = tokenizer.eos_token\r\n\r\n\r\n    def tokenize_function(examples):\r\n        y = tokenizer(examples['text'], truncation=True, max_length=64)\r\n        return y\r\n\r\n    x = os.environ.get(\"TEST_ENV_VARIABLE_BEFORE_dataset_map\", default=None)\r\n    print(f\"Testing environment variable: TEST_ENV_VARIABLE_BEFORE_dataset_map {x}\")\r\n    print(f\"Data type of os.environ before datasets.map = {os.environ.__class__.__name__}\")\r\n    datasets.map(tokenize_function, batched=True, num_proc=2, remove_columns=[\"text\"])\r\n    print(f\"Data type of os.environ after datasets.map = {os.environ.__class__.__name__}\")\r\n    x = os.environ.get(\"TEST_ENV_VARIABLE_AFTER_dataset_map\", default=None)\r\n    print(f\"Testing environment variable: TEST_ENV_VARIABLE_AFTER_dataset_map {x}\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    test_train()\r\n\r\n\r\n```\r\n\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 841207878,
    "title": "Support for legal NLP datasets (EURLEX, ECtHR cases and EU-REG-IR)",
    "dateCreated": "2021-03-25T18:40:17Z",
    "dateModified": "2021-03-25T18:40:17Z",
    "description": "Add support for two legal NLP datasets:\r\n\r\n- EURLEX (https://www.aclweb.org/anthology/P19-1636/)\r\n- ECtHR cases (https://arxiv.org/abs/2103.13084)\r\n- EU-REG-IR (https://arxiv.org/abs/2101.10726)",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 841191303,
    "title": "Implement Dataset as context manager",
    "dateCreated": "2021-03-25T18:18:30Z",
    "dateModified": "2021-03-25T18:18:30Z",
    "description": "When used as context manager, it would be safely deleted if some exception is raised.\r\n\r\nThis will avoid \r\n> During handling of the above exception, another exception occurred:",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 841098008,
    "title": "Support for legal NLP datasets (EURLEX and ECtHR cases)",
    "dateCreated": "2021-03-25T16:24:17Z",
    "dateModified": "2021-03-25T16:24:17Z",
    "description": "Add support for two legal NLP datasets:\r\n- EURLEX (https://www.aclweb.org/anthology/P19-1636/)\r\n- ECtHR cases (https://arxiv.org/abs/2103.13084)",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 841082087,
    "title": "Compute WER metric iteratively",
    "dateCreated": "2021-03-25T16:06:48Z",
    "dateModified": "2021-03-25T16:06:48Z",
    "description": "Compute WER metric iteratively to avoid MemoryError.\r\n\r\nFix #2078.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 840794995,
    "title": "Fix incorrect assertion in builder.py",
    "dateCreated": "2021-03-25T10:39:20Z",
    "dateModified": "2021-03-25T10:39:20Z",
    "description": "Fix incorrect num_examples comparison assertion in builder.py",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 840746598,
    "title": "Add more issue templates and customize issue template chooser",
    "dateCreated": "2021-03-25T09:41:53Z",
    "dateModified": "2021-03-25T09:41:53Z",
    "description": "When opening an issue, it is not evident for the users how to choose a blank issue template. There is a link at the bottom of all the other issue templates (`Don\u2019t see your issue here? Open a blank issue.`), but this is not very visible for users. This is the reason why many users finally chose the `add-dataset` template instead (this is more visible) for issues that indeed are not requesting the addition of a new dataset.\r\n\r\n~~With this PR, the default blank issue template would be as visible as the other templates (as the `add-dataset` template), thus making easier for the users to choose it.~~\r\n\r\nWith this PR:\r\n- more issue templates, besides `add-dataset`, are added: `bug-report` and `feature-request`\r\n- the issue template chooser is customized, so that it now includes a link to `Discussions` for questions",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 840181055,
    "title": "Is there a way to use a GPU only when training an Index  in the process of add_faisis_index?",
    "dateCreated": "2021-03-24T21:32:16Z",
    "dateModified": "2021-03-24T21:32:16Z",
    "description": "Motivation - Some FAISS indexes like IVF consist of the training step that clusters the dataset into a given number of indexes. It would be nice if we can use a GPU to do the training step and covert the index back to CPU as mention in [this faiss example](https://gist.github.com/mdouze/46d6bbbaabca0b9778fca37ed2bcccf6).",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 839495825,
    "title": "Metadata validation",
    "dateCreated": "2021-03-24T08:52:41Z",
    "dateModified": "2021-03-24T08:52:41Z",
    "description": "- `pydantic` metadata schema with dedicated validators against our taxonomy\r\n- ci script to validate new changes against this schema and start a vertuous loop\r\n- soft validation on tasks ids since we expect the taxonomy to undergo some changes in the near future\r\n\r\nfor reference with the current validation we have ~365~ 378 datasets with invalid metadata! full error report [_here_.](https://gist.github.com/theo-m/61b3c0c47fc6121d08d3174bd4c2a26b)",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 839084264,
    "title": "WMT19 Dataset for Kazakh-English is not formatted correctly",
    "dateCreated": "2021-03-23T20:14:47Z",
    "dateModified": "2021-03-23T20:14:47Z",
    "description": "In addition to the bug of languages being switched from Issue @415, there are incorrect translations in the dataset because the English-Kazakh translations have a one off formatting error.\r\n\r\nThe News Commentary v14 parallel data set for kk-en from http://www.statmt.org/wmt19/translation-task.html has a bug here:\r\n\r\n> Line 94. The Swiss National Bank, for its part, has been battling with the deflationary effects of the franc\u2019s dramatic appreciation over the past few years.\t\u0428\u0432\u0435\u0439\u0446\u0430\u0440\u0438\u044f\u043d\u044b\u04a3 \u04b0\u043b\u0442\u0442\u044b\u049b \u0431\u0430\u043d\u043a\u0456 \u04e9\u0437 \u0442\u0430\u0440\u0430\u043f\u044b\u043d\u0430\u043d, \u0441\u043e\u04a3\u0493\u044b \u0431\u0456\u0440\u043d\u0435\u0448\u0435 \u0436\u044b\u043b \u0456\u0448\u0456\u043d\u0434\u0435 \u0444\u0440\u0430\u043d\u043a \u049b\u04b1\u043d\u044b\u043d\u044b\u04a3 \u049b\u0430\u0442\u0442\u044b \u04e9\u0441\u0443\u0456\u043d\u0456\u04a3 \u0434\u0435\u0444\u043b\u044f\u0446\u0438\u044f\u043b\u044b\u049b \u04d9\u0441\u0435\u0440\u0456\u043c\u0435\u043d \u043a\u04af\u0440\u0435\u0441\u0456\u043f \u043a\u0435\u043b\u0435\u0434\u0456.\r\n> \r\n> Line 95. \u0414\u0435\u0444\u043b\u044f\u0446\u0438\u044f\u043b\u044b\u049b \u043a\u04af\u0448\u0442\u0435\u0440 2008 \u0436\u044b\u043b\u044b \u0442\u0435\u0440\u0435\u04a3 \u0436\u04d9\u043d\u0435 \u04b1\u0437\u0430\u049b\u049b\u0430 \u0441\u043e\u0437\u044b\u043b\u0493\u0430\u043d \u0436\u0430\u04bb\u0430\u043d\u0434\u044b\u049b \u0434\u0430\u0493\u0434\u0430\u0440\u044b\u0441\u049b\u0430 \u0431\u0430\u0439\u043b\u0430\u043d\u044b\u0441\u0442\u044b \u043e\u0440\u044b\u043d \u0430\u043b\u0493\u0430\u043d \u0456\u0440\u0456 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430\u043b\u044b\u049b \u0436\u04d9\u043d\u0435 \u049b\u0430\u0440\u0436\u044b\u043b\u044b\u049b \u043e\u0440\u044b\u043d \u0430\u043b\u043c\u0430\u0441\u0443\u043b\u0430\u0440\u0434\u044b\u04a3 \u0430\u0440\u049b\u0430\u0441\u044b\u043d\u0434\u0430 \u0431\u043e\u0441\u0430\u0442\u044b\u043b\u0434\u044b.  \u0416\u0435\u043a\u0435 \u049b\u0430\u0440\u044b\u0437 \u049b\u0430\u0440\u0430\u0436\u0430\u0442\u044b \u04af\u043b\u0435\u0441\u0456\u043d\u0456\u04a3 \u049b\u044b\u0441\u049b\u0430\u0440\u0443\u044b \u043e\u0440\u0442\u0430\u043b\u044b\u049b \u0431\u0430\u043d\u043a\u0442\u0456\u04a3 \u0440\u0435\u0444\u043b\u044f\u0446\u0438\u044f\u0493\u0430 \u0436\u04b1\u043c\u0441\u0430\u043b\u0493\u0430\u043d \u043a\u04af\u0448-\u0436\u0456\u0433\u0435\u0440\u0456\u043d\u0435 \u0442\u04b1\u0440\u0430\u049b\u0442\u044b \u0441\u043e\u049b\u049b\u0430\u043d \u049b\u0430\u0440\u0441\u044b \u0436\u0435\u043b\u0434\u0435\u0439 \u0431\u043e\u043b\u0434\u044b.\r\n> \r\n> Line 96. The deflationary forces were unleashed by the major economic and financial dislocations associated with the deep and protracted global crisis that erupted in 2008. Private deleveraging became a steady headwind to central bank efforts to reflate.\t2009 \u0436\u044b\u043b\u044b, \u0430\u043b\u0434\u044b\u04a3\u0493\u044b \u049b\u0430\u0442\u0430\u0440\u043b\u044b \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430\u043b\u0430\u0440\u0434\u044b\u04a3 \u0448\u0430\u043c\u0430\u043c\u0435\u043d \u04af\u0448\u0442\u0435\u043d \u0431\u0456\u0440\u0456 \u0431\u0430\u0493\u0430\u043d\u044b\u04a3 \u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0443\u0456\u043d \u043a\u04e9\u0440\u0441\u0435\u0442\u0442\u0456, \u0431\u04b1\u043b \u0441\u043e\u0493\u044b\u0441\u0442\u0430\u043d \u043a\u0435\u0439\u0456\u043d\u0433\u0456 \u0436\u043e\u0493\u0430\u0440\u044b \u0434\u0435\u04a3\u0433\u0435\u0439 \u0431\u043e\u043b\u0434\u044b.\r\n\r\nAs you can see, line 95 has only the Kazakh translation which should be part of line 96. This causes all of the following English-Kazakh translation pairs to be one off rendering ALL of those translations incorrect. This issue was not fixed when the dataset was imported to Huggingface. By running this code \r\n\r\n```\r\nimport datasets\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('wmt19', 'kk-en')\r\nfor key in dataset['train']['translation']:\r\n    if 'The deflationary forces were unleashed by the major economic and financial dislocations associated with the deep and protracted global crisis that erupted in 2008.' in key['kk']:\r\n        print(key['en'])\r\n        print(key['kk'])\r\n        break\r\n```\r\nwe get: \r\n> 2009 \u0436\u044b\u043b\u044b, \u0430\u043b\u0434\u044b\u04a3\u0493\u044b \u049b\u0430\u0442\u0430\u0440\u043b\u044b \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430\u043b\u0430\u0440\u0434\u044b\u04a3 \u0448\u0430\u043c\u0430\u043c\u0435\u043d \u04af\u0448\u0442\u0435\u043d \u0431\u0456\u0440\u0456 \u0431\u0430\u0493\u0430\u043d\u044b\u04a3 \u0442\u04e9\u043c\u0435\u043d\u0434\u0435\u0443\u0456\u043d \u043a\u04e9\u0440\u0441\u0435\u0442\u0442\u0456, \u0431\u04b1\u043b \u0441\u043e\u0493\u044b\u0441\u0442\u0430\u043d \u043a\u0435\u0439\u0456\u043d\u0433\u0456 \u0436\u043e\u0493\u0430\u0440\u044b \u0434\u0435\u04a3\u0433\u0435\u0439 \u0431\u043e\u043b\u0434\u044b.\r\n> The deflationary forces were unleashed by the major economic and financial dislocations associated with the deep and protracted global crisis that erupted in 2008. Private deleveraging became a steady headwind to central bank efforts to reflate.\r\n\r\nwhich shows that the issue still persists in the Huggingface dataset. The Kazakh sentence matches up to the next English sentence in the dataset instead of the current one.\r\n\r\nPlease let me know if there's you have any ideas to fix this one-off error from the dataset or if this can be fixed by Huggingface.",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 839059226,
    "title": "Request to remove S2ORC dataset",
    "dateCreated": "2021-03-23T19:43:06Z",
    "dateModified": "2021-03-23T19:43:06Z",
    "description": "Hi!  I was wondering if it's possible to remove [S2ORC](https://huggingface.co/datasets/s2orc) from hosting on Huggingface's platform?  Unfortunately, there are some legal considerations about how we make this data available.  Happy to add back to Huggingface's platform once we work out those hurdles!  Thanks!",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 839027834,
    "title": "Trouble loading wiki_movies",
    "dateCreated": "2021-03-23T18:59:54Z",
    "dateModified": "2021-03-23T18:59:54Z",
    "description": "Hello,\r\nI am trying to load_dataset(\"wiki_movies\") and it gives me this error - \r\n\r\n`FileNotFoundError: Couldn't find file locally at wiki_movies/wiki_movies.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/wiki_movies/wiki_movies.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/wiki_movies/wiki_movies.py`\r\n\r\nTrying to do `python run_mlm.py \\\r\n    --model_name_or_path roberta-base \\\r\n    --dataset_name wiki_movies \\` also gives the same error. \r\n\r\nIs this something on my end? From what I can tell, this dataset was re-added by @lhoestq a few months ago. \r\nThank you!",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 838946916,
    "title": "citation, homepage, and license fields of `dataset_info.json` are duplicated many times",
    "dateCreated": "2021-03-23T17:18:09Z",
    "dateModified": "2021-03-23T17:18:09Z",
    "description": "This happens after a `map` operation when `num_proc` is set to `>1`. I tested this by cleaning up the json before running the `map` op on the dataset so it's unlikely it's coming from an earlier concatenation.\r\n\r\nExample result:\r\n```\r\n\"citation\": \"@ONLINE {wikidump,\\n    author = {Wikimedia Foundation},\\n    title  = {Wikimedia Downloads},\\n    url    = {https://dumps.wikimedia.org}\\n}\\n\\n@ONLINE {wikidump,\\n    author = {Wikimedia Foundation},\\n    title  = {Wikimedia Downloads},\\n    url    = {https://dumps.wikimedia.org}\\n}\\n\\n@ONLINE {wikidump,\\n    author = {Wikimedia Foundation},\\n    title  = {Wikimedia Downloads},\\n    url    = {https://dumps.wikimedia.org}\\n}\\n\\n@ONLINE {wikidump,\\n    author = {Wikimedia Foundation},\\n    title  = {Wikimedia Downloads},\\n    url    = {https://dumps.wikimedia.org}\\n}\\n\\n@ONLINE {wikidump,\\n    author = {Wikimedia Foundation},\\n    title  = {Wikimedia Downloads},\\n    url    = {https://dumps.wikimedia.org}\\n}\\n\\n@ONLINE {wikidump,\\n    author = {Wikimedia Foundation},\\n    title  = {Wikimedia Downloads},\\n    url    = {https://dumps.wikimedia.org}\\n}\\n\\n@ONLINE {wikidump,\\n    author = {Wikimedia Foundation},\\n    title  = {Wikimedia Downloads},\\n    url    = {https://dumps.wikimedia.org}\\n}\\n\\n@ONLINE {wikidump,\\n    author = {Wikimedia Foundation},\\n    title  = {Wikimedia Downloads},\\n    url    = {https://dumps.wikimedia.org}\\n}\\n\\n@ONLINE {wikidump,\\n    author = {Wikimedia Foundation},\\n    title  = {Wikimedia Downloads},\\n    url    = {https://dumps.wikimedia.org}\\n}\\n\\n@ONLINE {wikidump,\\n    author = {Wikimedia Foundation},\\n    title  = {Wikimedia Downloads},\\n    url    = {https://dumps.wikimedia.org}\\n}\\n\\n@ONLINE {wikidump,\\n    author = {Wikimedia Foundation},\\n    title  = {Wikimedia Downloads},\\n    url    = {https://dumps.wikimedia.org}\\n}\\n\\n@ONLINE {wikidump,\\n    author = {Wikimedia Foundation},\\n    title  = {Wikimedia Downloads},\\n\r\n```\r\n\r\n@lhoestq and I believe this is happening due to the fields being concatenated `num_proc` times.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 838794090,
    "title": "Move Dataset.to_csv to csv module",
    "dateCreated": "2021-03-23T14:35:46Z",
    "dateModified": "2021-03-23T14:35:46Z",
    "description": "Move the implementation of `Dataset.to_csv` to module `datasets.io.csv`.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 838586184,
    "title": "MIAM dataset - new citation details",
    "dateCreated": "2021-03-23T10:41:23Z",
    "dateModified": "2021-03-23T10:41:23Z",
    "description": "Hi @lhoestq, I have updated the citations to reference an OpenReview preprint.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 838574631,
    "title": "Fix deprecated warning message and docstring",
    "dateCreated": "2021-03-23T10:27:52Z",
    "dateModified": "2021-03-23T10:27:52Z",
    "description": "Fix deprecated warnings:\r\n- Use deprecated Sphinx directive in docstring\r\n- Fix format of deprecated message\r\n- Raise FutureWarning",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 838523819,
    "title": "load_from_disk takes a long time to load local dataset",
    "dateCreated": "2021-03-23T09:28:37Z",
    "dateModified": "2021-03-23T09:28:37Z",
    "description": "I have an extremely large tokenized dataset (24M examples) that loads in a few minutes. However, after adding a column similar to `input_ids` (basically a list of integers) and saving the dataset to disk, the load time goes to >1 hour. I've even tried using `np.uint8` after seeing #1985 but it doesn't seem to be helping (the total size seems to be smaller though).\r\n\r\nDoes anyone know what could be the issue? Or does the casting of that column to `int8` need to happen in the function that writes the arrow table instead of in the `map` where I create the list of integers?\r\n\r\nTagging @lhoestq since you seem to be working on these issues and PRs :)",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 838447959,
    "title": "SQuAD version ",
    "dateCreated": "2021-03-23T07:47:54Z",
    "dateModified": "2021-03-23T07:47:54Z",
    "description": "Hi~ \r\nI want train on squad dataset. What's the version of the squad? Is it 1.1 or 1.0? I'm new in QA, I don't find some descriptions about it. ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 838105289,
    "title": "fixes issue #1110 by descending further if `obj[\"_type\"]` is a dict",
    "dateCreated": "2021-03-22T21:00:55Z",
    "dateModified": "2021-03-22T21:00:55Z",
    "description": "Check metrics",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 838038379,
    "title": "CoNLL 2003 dataset not including German",
    "dateCreated": "2021-03-22T19:23:56Z",
    "dateModified": "2021-03-22T19:23:56Z",
    "description": "Hello, thanks for all the work on developing and maintaining this amazing platform, which I am enjoying working with!\r\n\r\nI was wondering if there is a reason why the German CoNLL 2003 dataset is not included in the [repository](https://github.com/huggingface/datasets/tree/master/datasets/conll2003), since a copy of it could be found in some places on the internet such as GitHub? I could help adding the German data to the hub, unless there are some copyright issues that I am unaware of...\r\n\r\nThis is considering that many work use the union of CoNLL 2002 and 2003 datasets for comparing cross-lingual NER transfer performance in `en`, `de`, `es`, and `nl`. E.g., [XLM-R](https://www.aclweb.org/anthology/2020.acl-main.747.pdf).\r\n\r\n## Adding a Dataset\r\n- **Name:** CoNLL 2003 German\r\n- **Paper:** https://www.aclweb.org/anthology/W03-0419/\r\n- **Data:** https://github.com/huggingface/datasets/tree/master/datasets/conll2003\r\n",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 837209211,
    "title": "Fix: Allows a feature to be named \"_type\"",
    "dateCreated": "2021-03-21T23:21:57Z",
    "dateModified": "2021-03-21T23:21:57Z",
    "description": "This PR tries to fix issue #1110. Sorry for taking so long to come back to this.\r\n\r\nIt's a simple fix, but i am not sure if it works for all possible types of `obj`. Let me know what you think @lhoestq ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 836984043,
    "title": "How to disable making arrow tables in load_dataset ?",
    "dateCreated": "2021-03-21T04:50:07Z",
    "dateModified": "2021-03-21T04:50:07Z",
    "description": "Is there a way to disable the construction of arrow tables, or to make them on the fly as the dataset is being used ?",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 836831403,
    "title": "Fix copy snippet in docs",
    "dateCreated": "2021-03-20T15:08:22Z",
    "dateModified": "2021-03-20T15:08:22Z",
    "description": "With this change the lines starting with `...` in the code blocks can be properly copied to clipboard.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 836807498,
    "title": "Add machine translated multilingual STS benchmark dataset",
    "dateCreated": "2021-03-20T13:28:07Z",
    "dateModified": "2021-03-20T13:28:07Z",
    "description": "also see here https://github.com/PhilipMay/stsb-multi-mt",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 836788019,
    "title": "Add documentaton for dataset README.md files",
    "dateCreated": "2021-03-20T11:44:38Z",
    "dateModified": "2021-03-20T11:44:38Z",
    "description": "Hi,\r\nthe dataset README files have special headers.\r\nSomehow a documenation of the allowed values and tags is missing.\r\nCould you add that?\r\n\r\nJust to give some concrete questions that should be answered imo:\r\n- which values can be passted to multilinguality?\r\n- what should be passed to language_creators?\r\n- which values should licenses have? What do I say when it is a custom license? Should I add a link?\r\n- how should I choose size_categories ? What are valid ranges?\r\n- what are valid task_categories?\r\n\r\nThanks\r\nPhilip",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 836763733,
    "title": "change bibtex template to author instead of authors",
    "dateCreated": "2021-03-20T09:23:44Z",
    "dateModified": "2021-03-20T09:23:44Z",
    "description": "Hi,\r\nIMO when using BibTex Author should be used instead of Authors.\r\nSee here: http://www.bibtex.org/Using/de/\r\n\r\nThanks\r\nPhilip",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 836587392,
    "title": "Update metadata if dataset features are modified",
    "dateCreated": "2021-03-20T02:05:23Z",
    "dateModified": "2021-03-20T02:05:23Z",
    "description": "This PR adds a decorator that updates the dataset metadata if a previously executed transform modifies its features. \r\nFixes #2083 \r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 836249587,
    "title": "change user permissions to -rw-r--r--",
    "dateCreated": "2021-03-19T18:14:56Z",
    "dateModified": "2021-03-19T18:14:56Z",
    "description": "Fix for #2065 ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 835870994,
    "title": "Fix max_wait_time in requests",
    "dateCreated": "2021-03-19T11:22:26Z",
    "dateModified": "2021-03-19T11:22:26Z",
    "description": "it was handled as a min time, not max cc @SBrandeis ",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 835750671,
    "title": "CUAD - Contract Understanding Atticus Dataset",
    "dateCreated": "2021-03-19T09:27:43Z",
    "dateModified": "2021-03-19T09:27:43Z",
    "description": "## Adding a Dataset\r\n- **Name:** CUAD - Contract Understanding Atticus Dataset\r\n- **Description:** As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.\r\n- **Paper:** https://arxiv.org/abs/2103.06268\r\n- **Data:** https://github.com/TheAtticusProject/cuad/\r\n- **Motivation:** good domain specific datasets are valuable\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 835695425,
    "title": "`concatenate_datasets` throws error when changing the order of datasets to concatenate",
    "dateCreated": "2021-03-19T08:29:48Z",
    "dateModified": "2021-03-19T08:29:48Z",
    "description": "Hey, \r\n\r\nI played around with the `concatenate_datasets(...)` function: https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=concatenate_datasets#datasets.concatenate_datasets\r\n\r\nand noticed that when the order in which the datasets are concatenated changes an error is thrown where it should not IMO.\r\n\r\nHere is a google colab to reproduce the error: https://colab.research.google.com/drive/17VTFU4KQ735-waWZJjeOHS6yDTfV5ekK?usp=sharing",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 835401555,
    "title": "Updated card using information from data statement and datasheet",
    "dateCreated": "2021-03-19T00:39:38Z",
    "dateModified": "2021-03-19T00:39:38Z",
    "description": "I updated and clarified the REFreSD [data card](https://github.com/mcmillanmajora/datasets/blob/refresd_card/datasets/refresd/README.md) with information from the Eleftheria's [website](https://elbria.github.io/post/refresd/). I added brief descriptions where the initial card referred to the paper, and I also recreated some of the tables in the paper to show relevant dataset statistics.\r\n\r\nI'll email Eleftheria to see if she has any comments on the card. ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 835112968,
    "title": "Fix docstrings issues",
    "dateCreated": "2021-03-18T18:11:01Z",
    "dateModified": "2021-03-18T18:11:01Z",
    "description": "Fix docstring issues.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 835023000,
    "title": "Multidimensional arrays in a Dataset",
    "dateCreated": "2021-03-18T16:29:14Z",
    "dateModified": "2021-03-18T16:29:14Z",
    "description": "Hi,\r\n\r\nI'm trying to put together a `datasets.Dataset` to be used with LayoutLM which is available in `transformers`. This model requires as input the bounding boxes of each of the token of a sequence. This is when I realized that `Dataset` does not support multi-dimensional arrays as a value for a column in a row.\r\n\r\nThe following code results in conversion error in pyarrow (`pyarrow.lib.ArrowInvalid: ('Can only convert 1-dimensional array values', 'Conversion failed for column bbox with type object')`)\r\n\r\n```\r\nfrom datasets import Dataset\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndataset = pd.DataFrame({\r\n    'bbox': [\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]])\r\n    ],\r\n    'input_ids': [1, 2, 3, 4]\r\n})\r\ndataset = Dataset.from_pandas(dataset)\r\n```\r\n\r\nSince I wanted to use pytorch for the downstream training task, I also tried a few ways to directly put in a column of 2-D pytorch tensor in a formatted dataset, but I can only have a list of 1-D tensors, or a list of arrays, or a list of lists.\r\n\r\n```\r\nimport torch\r\nfrom datasets import Dataset\r\nimport pandas as pd\r\n\r\ndataset = pd.DataFrame({\r\n    'bbox': [\r\n        [[1,2,3,4],[1,2,3,4],[1,2,3,4]],\r\n        [[1,2,3,4],[1,2,3,4],[1,2,3,4]],\r\n        [[1,2,3,4],[1,2,3,4],[1,2,3,4]],\r\n        [[1,2,3,4],[1,2,3,4],[1,2,3,4]]\r\n    ],\r\n    'input_ids': [1, 2, 3, 4]\r\n})\r\ndataset = Dataset.from_pandas(dataset)\r\n\r\ndef test(examples):\r\n    return {'bbbox': torch.Tensor(examples['bbox'])}\r\ndataset = dataset.map(test)\r\nprint(dataset[0]['bbox'])\r\nprint(dataset[0]['bbbox'])\r\n\r\ndataset.set_format(type='torch', columns=['input_ids', 'bbox'], output_all_columns=True)\r\nprint(dataset[0]['bbox'])\r\nprint(dataset[0]['bbbox'])\r\n\r\ndef test2(examples):\r\n    return {'bbbox': torch.stack(examples['bbox'])}\r\ndataset = dataset.map(test2)\r\n\r\nprint(dataset[0]['bbox'])\r\nprint(dataset[0]['bbbox'])\r\n```\r\n\r\nIs is possible to support n-D arrays/tensors in datasets? \r\nIt seems that it can also be useful for this [feature request](https://github.com/huggingface/datasets/issues/263).",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 834920493,
    "title": "Refactorize Metric.compute signature to force keyword arguments only",
    "dateCreated": "2021-03-18T15:05:50Z",
    "dateModified": "2021-03-18T15:05:50Z",
    "description": "Minor refactoring of Metric.compute signature to force the use of keyword arguments, by using the single star syntax.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 834694819,
    "title": "MemoryError when computing WER metric",
    "dateCreated": "2021-03-18T11:30:05Z",
    "dateModified": "2021-03-18T11:30:05Z",
    "description": "Hi, I'm trying to follow the ASR example to try Wav2Vec. This is the code that I use for WER calculation:\r\n\r\n```\r\nwer = load_metric(\"wer\")\r\nprint(wer.compute(predictions=result[\"predicted\"], references=result[\"target\"]))\r\n```\r\n\r\nHowever, I receive the following exception:\r\n\r\n`Traceback (most recent call last):\r\n  File \"/home/diego/IpGlobal/wav2vec/test_wav2vec.py\", line 51, in <module>\r\n    print(wer.compute(predictions=result[\"predicted\"], references=result[\"target\"]))\r\n  File \"/home/diego/miniconda3/envs/wav2vec3.6/lib/python3.6/site-packages/datasets/metric.py\", line 403, in compute\r\n    output = self._compute(predictions=predictions, references=references, **kwargs)\r\n  File \"/home/diego/.cache/huggingface/modules/datasets_modules/metrics/wer/73b2d32b723b7fb8f204d785c00980ae4d937f12a65466f8fdf78706e2951281/wer.py\", line 94, in _compute\r\n    return wer(references, predictions)\r\n  File \"/home/diego/miniconda3/envs/wav2vec3.6/lib/python3.6/site-packages/jiwer/measures.py\", line 81, in wer\r\n    truth, hypothesis, truth_transform, hypothesis_transform, **kwargs\r\n  File \"/home/diego/miniconda3/envs/wav2vec3.6/lib/python3.6/site-packages/jiwer/measures.py\", line 192, in compute_measures\r\n    H, S, D, I = _get_operation_counts(truth, hypothesis)\r\n  File \"/home/diego/miniconda3/envs/wav2vec3.6/lib/python3.6/site-packages/jiwer/measures.py\", line 273, in _get_operation_counts\r\n    editops = Levenshtein.editops(source_string, destination_string)\r\nMemoryError`\r\n\r\nMy system has more than 10GB of available RAM. Looking at the code, I think that it could be related to the way jiwer does the calculation, as it is pasting all the sentences in a single string before calling Levenshtein editops function.\r\n\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 834649536,
    "title": "Bump huggingface_hub version",
    "dateCreated": "2021-03-18T10:54:34Z",
    "dateModified": "2021-03-18T10:54:34Z",
    "description": "`0.0.2 => 0.0.6`",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 834445296,
    "title": "Issue: Dataset download error",
    "dateCreated": "2021-03-18T06:36:06Z",
    "dateModified": "2021-03-18T06:36:06Z",
    "description": "The download link in `iwslt2017.py` file does not seem to work anymore.\r\n\r\nFor example, `FileNotFoundError: Couldn't find file at https://wit3.fbk.eu/archive/2017-01-trnted/texts/zh/en/zh-en.tgz`\r\n\r\nWould be nice if we could modify it script and use the new downloadable link?",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 834301246,
    "title": "ConnectionError: Couldn't reach common_voice.py",
    "dateCreated": "2021-03-18T01:19:06Z",
    "dateModified": "2021-03-18T01:19:06Z",
    "description": "When I run: \r\nfrom datasets import load_dataset, load_metric\r\n\r\ncommon_voice_train = load_dataset(\"common_voice\", \"zh-CN\", split=\"train+validation\")\r\ncommon_voice_test = load_dataset(\"common_voice\", \"zh-CN\", split=\"test\")\r\n\r\nGot:\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/master/datasets/common_voice/common_voice.py\r\n\r\nVersion:\r\n1.4.1\r\n\r\nThanks!  @lhoestq @LysandreJik @thomwolf ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 834268463,
    "title": "Fix size categories in YAML Tags",
    "dateCreated": "2021-03-18T00:02:36Z",
    "dateModified": "2021-03-18T00:02:36Z",
    "description": "This PR fixes several `size_categories` in YAML tags and makes them consistent. Additionally, I have added a few more categories after `1M`, up to `1T`. I would like to add that to the streamlit app also.\r\n\r\nThis PR also adds a couple of infos that I found missing.\r\n\r\nThe code for generating this:\r\n```python\r\nfor dataset in sorted(os.listdir('./datasets/')):\r\n    if '.' not in dataset and dataset not in ['c4', 'csv', 'downloads', 'cc100', 'ccaligned_multilingual', 'celeb_a', 'chr_en', 'emea', 'glue']:\r\n        infos = {}\r\n        stats = {}\r\n        st = ''\r\n        with open(f'datasets/{dataset}/README.md') as f:\r\n            d = f.read()\r\n        start_dash = d.find('---') + 3\r\n        end_dash = d[start_dash:].find('---') + 3\r\n        rest_text = d[end_dash + 3:]\r\n        try:\r\n            full_yaml = OmegaConf.create(d[start_dash:end_dash])\r\n            readme = OmegaConf.to_container(full_yaml['size_categories'], resolve=True)\r\n        except Exception as e:\r\n            print(e)\r\n            continue \r\n        try:\r\n            with open(f'datasets/{dataset}/dataset_infos.json') as f:\r\n                data = json.load(f)\r\n        except Exception as e:\r\n            print(e)\r\n            continue  # Skip those without infos.\r\n        done_set = set([])\r\n        num_keys = len(data.keys())\r\n        for keys in data:\r\n            # dataset = load_dataset('opus100', f'{dirs}')\r\n            total = 0\r\n            for split in data[keys]['splits']:\r\n                total = total + data[keys]['splits'][split]['num_examples']\r\n            if total < 1000:\r\n                st += \"- n<1K\" + '\\n'\r\n                infos[keys] = [\"n<1K\"]\r\n            elif total >= 1000 and total < 10000:\r\n                infos[keys] = [\"1K<n<10K\"]\r\n            elif total >= 10000 and total < 100000:\r\n                infos[keys] = [\"10K<n<100K\"]\r\n            elif total >= 100000 and total < 1000000:\r\n                infos[keys] = [\"100K<n<1M\"]\r\n            elif total >= 1000000 and total < 10000000:\r\n                infos[keys] = [\"1M<n<10M\"]\r\n            elif total >= 10000000 and total < 100000000:\r\n                infos[keys] = [\"10M<n<100M\"]\r\n            elif total >= 100000000 and total < 1000000000:\r\n                infos[keys] = [\"100M<n<1B\"]\r\n            elif total >= 1000000000 and total < 10000000000:\r\n                infos[keys] = [\"1B<n<10B\"]\r\n            elif total >= 10000000000 and total < 100000000000:\r\n                infos[keys] = [\"10B<n<100B\"]\r\n            elif total >= 100000000000 and total < 1000000000000:\r\n                infos[keys] = [\"100B<n<1T\"]\r\n            else:\r\n                infos[keys] = [\"n>1T\"]\r\n            done_set = done_set.union(infos[keys])\r\n        if (isinstance(readme, list) and list(infos.values())[0] != readme) or (isinstance(readme, dict) and readme != infos):\r\n\r\n            print('-' * 30)\r\n            print(done_set)\r\n            print(f\"Changing Full YAML for {dataset}\")\r\n            print(OmegaConf.to_yaml(full_yaml))\r\n\r\n            if len(done_set) == 1:\r\n                full_yaml['size_categories'] = list(done_set)\r\n            else:\r\n                full_yaml['size_categories'] = dict([(k, v) for k, v in sorted(infos.items(), key=lambda x: x[0])])\r\n\r\n            full_yaml_string = OmegaConf.to_yaml(full_yaml)\r\n            print('-' * 30)\r\n            print(full_yaml_string)\r\n            inp = input('Do you wish to continue?(Y/N)')\r\n            if inp == 'Y':\r\n                with open(f'./datasets/{dataset}/README.md', 'w') as f:\r\n                    f.write('---\\n')\r\n                    f.write(full_yaml_string)\r\n                    f.write('---')\r\n                    f.write(rest_text)\r\n            else:\r\n                break\r\n```\r\n\r\nNote that the lower-bound is inclusive. I'm unsure if this is how it is done in the tagging app.\r\n\r\nEDIT:\r\nIt would be great if there was a way to make the task categories consistent too. For this, the streamlit app can look into all the datasets and check for existing categories and show them in the list. This may add some consistency.\r\n\r\nEDIT:\r\nI understand this will not work for cases where only the infos for some of the configs are present, for example: `ccaligned_multingual` has only 5 out of several configs present, and infos has only information about them.  Hence, I have skipped a few datasets in the code, if there are more such datasets, then I'll ignore them too.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 834192501,
    "title": "Fixes check of TF_AVAILABLE and TORCH_AVAILABLE",
    "dateCreated": "2021-03-17T21:28:53Z",
    "dateModified": "2021-03-17T21:28:53Z",
    "description": "# What is this PR doing\r\n\r\nThis PR implements the checks if `Tensorflow` and `Pytorch` are available the same way as `transformers` does it. I added the additional checks for the different `Tensorflow` and `torch` versions.  #2068 ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 834054837,
    "title": "Fix docstring issues",
    "dateCreated": "2021-03-17T18:13:44Z",
    "dateModified": "2021-03-17T18:13:44Z",
    "description": "Fix docstring issues.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 833950824,
    "title": "Multiprocessing is slower than single process",
    "dateCreated": "2021-03-17T16:08:58Z",
    "dateModified": "2021-03-17T16:08:58Z",
    "description": "```python\r\n# benchmark_filter.py\r\nimport logging\r\nimport sys\r\nimport time\r\n\r\nfrom datasets import load_dataset, set_caching_enabled\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    set_caching_enabled(False)\r\n    logging.basicConfig(level=logging.DEBUG)\r\n\r\n    bc = load_dataset(\"bookcorpus\")\r\n\r\n    now = time.time()\r\n    try:\r\n        bc[\"train\"].filter(lambda x: len(x[\"text\"]) < 64, num_proc=int(sys.argv[1]))\r\n    except Exception as e:\r\n        print(f\"cancelled: {e}\")\r\n    elapsed = time.time() - now\r\n\r\n    print(elapsed)\r\n```\r\n\r\nRunning `python benchmark_filter.py 1` (20min+) is faster than `python benchmark_filter.py 2` (2hrs+)",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 833799035,
    "title": "ArrowInvalid issue for squad v2 dataset",
    "dateCreated": "2021-03-17T13:51:49Z",
    "dateModified": "2021-03-17T13:51:49Z",
    "description": "Hello, I am using the huggingface official question answering example notebook (https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb). \r\n\r\nIn the prepare_validation_features function, I made some modifications to tokenize a new set of quesions with the original contexts and save them in three different list called candidate_input_dis, candidate_attetion_mask and candidate_token_type_ids. When I try to run the next cell for dataset.map, I got the following error:\r\n\r\n`ArrowInvalid: Column 1 named candidate_attention_mask expected length 1180 but got length 1178`\r\n\r\nMy code is as follows:\r\n\r\n```\r\ndef generate_candidate_questions(examples):\r\n  val_questions = examples[\"question\"]\r\n  candididate_questions = random.sample(datasets[\"train\"][\"question\"], len(val_questions))\r\n  candididate_questions = [x[:max_length] for x in candididate_questions]\r\n  return candididate_questions\r\n\r\ndef prepare_validation_features(examples, use_mixing=False):\r\n  pad_on_right = tokenizer.padding_side == \"right\"\r\n  tokenized_examples = tokenizer(\r\n      examples[\"question\" if pad_on_right else \"context\"],\r\n      examples[\"context\" if pad_on_right else \"question\"],\r\n      truncation=\"only_second\" if pad_on_right else \"only_first\",\r\n      max_length=max_length,\r\n      stride=doc_stride,\r\n      return_overflowing_tokens=True,\r\n      return_offsets_mapping=True,\r\n      padding=\"max_length\",\r\n  )\r\n  if use_mixing:\r\n    candidate_questions = generate_candidate_questions(examples)\r\n    tokenized_candidates = tokenizer(\r\n        candidate_questions if pad_on_right else examples[\"context\"],\r\n        examples[\"context\"] if pad_on_right else candidate_questions,\r\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\r\n        max_length=max_length,\r\n        stride=doc_stride,\r\n        return_overflowing_tokens=True,\r\n        return_offsets_mapping=True,\r\n        padding=\"max_length\",\r\n    )\r\n\r\n  sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\r\n\r\n  tokenized_examples[\"example_id\"] = []\r\n\r\n  if use_mixing:\r\n    tokenized_examples[\"candidate_input_ids\"] = tokenized_candidates[\"input_ids\"]\r\n    tokenized_examples[\"candidate_attention_mask\"] = tokenized_candidates[\"attention_mask\"]\r\n    tokenized_examples[\"candidate_token_type_ids\"] = tokenized_candidates[\"token_type_ids\"]\r\n\r\n  for i in range(len(tokenized_examples[\"input_ids\"])):\r\n      sequence_ids = tokenized_examples.sequence_ids(i)\r\n      context_index = 1 if pad_on_right else 0\r\n\r\n      sample_index = sample_mapping[i]\r\n      tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\r\n\r\n      tokenized_examples[\"offset_mapping\"][i] = [\r\n          (o if sequence_ids[k] == context_index else None)\r\n          for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\r\n      ]\r\n\r\n  return tokenized_examples\r\n\r\n\r\n\r\nvalidation_features = datasets[\"validation\"].map(\r\n    lambda xs: prepare_validation_features(xs, True),\r\n    batched=True,\r\n    remove_columns=datasets[\"validation\"].column_names\r\n)\r\n```\r\n\r\nI guess this might happen because of the batched=True. I see similar issues in this repo related to arrow table length mismatch error, but in their cases, the numbers vary a lot. In my case, this error always happens when the expected length and unexpected length are very close. Thanks for the help!",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 833768926,
    "title": "Add and fix docstring for NamedSplit",
    "dateCreated": "2021-03-17T13:19:28Z",
    "dateModified": "2021-03-17T13:19:28Z",
    "description": "Add and fix docstring for `NamedSplit`, which was missing.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 833602832,
    "title": "PyTorch not available error on SageMaker GPU docker though it is installed ",
    "dateCreated": "2021-03-17T10:04:27Z",
    "dateModified": "2021-03-17T10:04:27Z",
    "description": "I get en error when running data loading using SageMaker SDK\r\n\r\n```\r\n  File \"main.py\", line 34, in <module>\r\n    run_training()\r\n  File \"main.py\", line 25, in run_training\r\n    dm.setup('fit')\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/datamodule.py\", line 92, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/ml/code/data_module.py\", line 103, in setup\r\n    self.dataset[split].set_format(type=\"torch\", columns=self.columns)\r\n  File \"/opt/conda/lib/python3.6/site-packages/datasets/fingerprint.py\", line 337, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py\", line 995, in set_format\r\n    _ = get_formatter(type, **format_kwargs)\r\nFile \"/opt/conda/lib/python3.6/site-packages/datasets/formatting/__init__.py\", line 114, in get_formatter\r\n    raise _FORMAT_TYPES_ALIASES_UNAVAILABLE[format_type]\r\nValueError: PyTorch needs to be installed to be able to return PyTorch tensors.\r\n```\r\n\r\nwhen trying to execute dataset loading using this notebook  https://github.com/PyTorchLightning/pytorch-lightning/blob/master/notebooks/04-transformers-text-classification.ipynb, specifically lines \r\n\r\n```\r\nself.columns = [c for c in self.dataset[split].column_names if c in self.loader_columns]\r\nself.dataset[split].set_format(type=\"torch\", columns=self.columns)\r\n```\r\n\r\nThe SageMaker docker image used is 763104351884.dkr.ecr.eu-central-1.amazonaws.com/pytorch-training:1.4.0-gpu-py3 .\r\n\r\nBy running container interactively I have checked that torch loading completes successfully by executing `https://github.com/huggingface/datasets/blob/master/src/datasets/config.py#L39`. \r\n\r\nAlso as a first line in the data loading module I have \r\n\r\n```\r\nimport os\r\nos.environ[\"USE_TF\"] = \"0\" \r\nos.environ[\"USE_TORCH\"] = \"1\" \r\n````\r\n\r\nBut unfortunately the error stills persists. Any suggestions would be appreciated as I am stack.\r\nMany Thanks! \r\n\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 833559940,
    "title": "Multiprocessing windows error",
    "dateCreated": "2021-03-17T09:12:28Z",
    "dateModified": "2021-03-17T09:12:28Z",
    "description": "As described here https://huggingface.co/blog/fine-tune-xlsr-wav2vec2\r\n\r\nWhen using the num_proc argument on windows the whole Python environment crashes and hanging in loop.\r\nFor example at the map_to_array part.\r\nAn error occures because the cache file already exists and windows throws and error. After this the log crashes into an loop ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 833480551,
    "title": "Fix docstring rendering of Dataset/DatasetDict.from_csv args",
    "dateCreated": "2021-03-17T07:23:10Z",
    "dateModified": "2021-03-17T07:23:10Z",
    "description": "Fix the docstring rendering of Dataset/DatasetDict.from_csv args.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 833291432,
    "title": "Only user permission of saved cache files, not group",
    "dateCreated": "2021-03-17T00:20:22Z",
    "dateModified": "2021-03-17T00:20:22Z",
    "description": "Hello,\r\n\r\nIt seems when a cached file is saved from calling `dataset.map` for preprocessing, it gets the user permissions and none of the user's group permissions. As we share data files across members of our team, this is causing a bit of an issue as we have to continually reset the permission of the files. Do you know any ways around this or a way to correctly set the permissions?",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 833002360,
    "title": "Fix ted_talks_iwslt version error",
    "dateCreated": "2021-03-16T16:43:45Z",
    "dateModified": "2021-03-16T16:43:45Z",
    "description": "This PR fixes the bug where the version argument would be passed twice if the dataset configuration was created on the fly.\r\n\r\nFixes #2059 ",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 832993705,
    "title": "[Common Voice] Adapt dataset script so that no manual data download is actually needed",
    "dateCreated": "2021-03-16T16:33:44Z",
    "dateModified": "2021-03-16T16:33:44Z",
    "description": "This PR changes the dataset script so that no manual data dir is needed anymore. ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 832625483,
    "title": "docs: fix missing quotation",
    "dateCreated": "2021-03-16T10:07:54Z",
    "dateModified": "2021-03-16T10:07:54Z",
    "description": "The json code misses a quote",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 832596228,
    "title": "Cannot load udpos subsets from xtreme dataset using load_dataset()",
    "dateCreated": "2021-03-16T09:32:13Z",
    "dateModified": "2021-03-16T09:32:13Z",
    "description": "Hello, \r\n\r\nI am trying to load the udpos English subset from xtreme dataset, but this faces an error during loading. I am using datasets v1.4.1, pip install. I have tried with other udpos languages which also fail, though loading a different subset altogether (such as XNLI) has no issue. I have also tried on Colab and faced the same error. \r\n\r\nReprex is: \r\n\r\n`from datasets import load_dataset `\r\n`dataset = load_dataset('xtreme', 'udpos.English')`\r\n\r\nThe error is: \r\n`KeyError: '_'`\r\n\r\nThe full traceback is: \r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-5-7181359ea09d> in <module>\r\n      1 from datasets import load_dataset\r\n----> 2 dataset = load_dataset('xtreme', 'udpos.English')\r\n\r\n~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\datasets\\load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)\r\n    738 \r\n    739     # Download and prepare data\r\n--> 740     builder_instance.download_and_prepare(\r\n    741         download_config=download_config,\r\n    742         download_mode=download_mode,\r\n\r\n~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\datasets\\builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    576                             logger.warning(\"HF google storage unreachable. Downloading and preparing it from source\")\r\n    577                     if not downloaded_from_gcs:\r\n--> 578                         self._download_and_prepare(\r\n    579                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    580                         )\r\n\r\n~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\datasets\\builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    654             try:\r\n    655                 # Prepare split will record examples associated to the split\r\n--> 656                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    657             except OSError as e:\r\n    658                 raise OSError(\r\n\r\n~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\datasets\\builder.py in _prepare_split(self, split_generator)\r\n    977                 generator, unit=\" examples\", total=split_info.num_examples, leave=False, disable=not_verbose\r\n    978             ):\r\n--> 979                 example = self.info.features.encode_example(record)\r\n    980                 writer.write(example)\r\n    981         finally:\r\n\r\n~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\datasets\\features.py in encode_example(self, example)\r\n    946     def encode_example(self, example):\r\n    947         example = cast_to_python_objects(example)\r\n--> 948         return encode_nested_example(self, example)\r\n    949 \r\n    950     def encode_batch(self, batch):\r\n\r\n~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\datasets\\features.py in encode_nested_example(schema, obj)\r\n    840     # Nested structures: we allow dict, list/tuples, sequences\r\n    841     if isinstance(schema, dict):\r\n--> 842         return {\r\n    843             k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)\r\n    844         }\r\n\r\n~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\datasets\\features.py in <dictcomp>(.0)\r\n    841     if isinstance(schema, dict):\r\n    842         return {\r\n--> 843             k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)\r\n    844         }\r\n    845     elif isinstance(schema, (list, tuple)):\r\n\r\n~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\datasets\\features.py in encode_nested_example(schema, obj)\r\n    868     # ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\r\n    869     elif isinstance(schema, (ClassLabel, TranslationVariableLanguages, Value, _ArrayXD)):\r\n--> 870         return schema.encode_example(obj)\r\n    871     # Other object should be directly convertible to a native Arrow type (like Translation and Translation)\r\n    872     return obj\r\n\r\n~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\datasets\\features.py in encode_example(self, example_data)\r\n    647         # If a string is given, convert to associated integer\r\n    648         if isinstance(example_data, str):\r\n--> 649             example_data = self.str2int(example_data)\r\n    650 \r\n    651         # Allowing -1 to mean no label.\r\n\r\n~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\datasets\\features.py in str2int(self, values)\r\n    605                 if value not in self._str2int:\r\n    606                     value = value.strip()\r\n--> 607                 output.append(self._str2int[str(value)])\r\n    608             else:\r\n    609                 # No names provided, try to integerize\r\n\r\nKeyError: '_'\r\n\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 832588591,
    "title": "Filtering refactor",
    "dateCreated": "2021-03-16T09:23:30Z",
    "dateModified": "2021-03-16T09:23:30Z",
    "description": "fix https://github.com/huggingface/datasets/issues/2032\r\n\r\nbenchmarking is somewhat inconclusive, currently running on `book_corpus` with:\r\n\r\n```python\r\n    bc = load_dataset(\"bookcorpus\")\r\n    now = time.time()\r\n    bc.filter(lambda x: len(x[\"text\"]) < 64)\r\n    elapsed = time.time() - now\r\n    print(elapsed)\r\n```\r\n\r\nthis branch does it in 233 seconds, master in 1409 seconds.",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 832579156,
    "title": "Error while following docs to load the `ted_talks_iwslt` dataset",
    "dateCreated": "2021-03-16T09:12:19Z",
    "dateModified": "2021-03-16T09:12:19Z",
    "description": "I am currently trying to load the `ted_talks_iwslt` dataset into google colab.\r\n\r\nThe [docs](https://huggingface.co/datasets/ted_talks_iwslt) mention the following way of doing so.\r\n\r\n```python\r\ndataset = load_dataset(\"ted_talks_iwslt\", language_pair=(\"it\", \"pl\"), year=\"2014\")\r\n```\r\n\r\nExecuting it results in the error attached below.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-6-7dcc67154ef9> in <module>()\r\n----> 1 dataset = load_dataset(\"ted_talks_iwslt\", language_pair=(\"it\", \"pl\"), year=\"2014\")\r\n\r\n4 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)\r\n    730         hash=hash,\r\n    731         features=features,\r\n--> 732         **config_kwargs,\r\n    733     )\r\n    734 \r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in __init__(self, writer_batch_size, *args, **kwargs)\r\n    927 \r\n    928     def __init__(self, *args, writer_batch_size=None, **kwargs):\r\n--> 929         super(GeneratorBasedBuilder, self).__init__(*args, **kwargs)\r\n    930         # Batch size used by the ArrowWriter\r\n    931         # It defines the number of samples that are kept in memory before writing them\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in __init__(self, cache_dir, name, hash, features, **config_kwargs)\r\n    241             name,\r\n    242             custom_features=features,\r\n--> 243             **config_kwargs,\r\n    244         )\r\n    245 \r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _create_builder_config(self, name, custom_features, **config_kwargs)\r\n    337             if \"version\" not in config_kwargs and hasattr(self, \"VERSION\") and self.VERSION:\r\n    338                 config_kwargs[\"version\"] = self.VERSION\r\n--> 339             builder_config = self.BUILDER_CONFIG_CLASS(**config_kwargs)\r\n    340 \r\n    341         # otherwise use the config_kwargs to overwrite the attributes\r\n\r\n/root/.cache/huggingface/modules/datasets_modules/datasets/ted_talks_iwslt/024d06b1376b361e59245c5878ab8acf9a7576d765f2d0077f61751158e60914/ted_talks_iwslt.py in __init__(self, language_pair, year, **kwargs)\r\n    219             description=description,\r\n    220             version=datasets.Version(\"1.1.0\", \"\"),\r\n--> 221             **kwargs,\r\n    222         )\r\n    223 \r\n\r\nTypeError: __init__() got multiple values for keyword argument 'version'\r\n```\r\n\r\nHow to resolve this? \r\n\r\nPS: Thanks a lot @huggingface team for creating this great library!",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 832159844,
    "title": "Is it possible to convert a `tfds` to HuggingFace `dataset`?",
    "dateCreated": "2021-03-15T20:18:47Z",
    "dateModified": "2021-03-15T20:18:47Z",
    "description": "I was having some weird bugs with `C4`dataset version of HuggingFace, so I decided to try to download `C4`from `tfds`. I would like to know if it is possible to convert a tfds dataset to HuggingFace dataset format :)\r\n\r\nI can also open a new issue reporting the bug I'm receiving with `datasets.load_dataset('c4','en')` in the future if you think that it would be useful.\r\n\r\nThanks!\r\n",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 832120522,
    "title": "update link to ZEST dataset",
    "dateCreated": "2021-03-15T19:22:57Z",
    "dateModified": "2021-03-15T19:22:57Z",
    "description": "Updating the link as the original one is no longer working. ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 831718397,
    "title": "issue with opus100/en-fr dataset ",
    "dateCreated": "2021-03-15T11:32:42Z",
    "dateModified": "2021-03-15T11:32:42Z",
    "description": "Hi\r\nI am running run_mlm.py code of huggingface repo with opus100/fr-en pair, I am getting this error, note that this error occurs for only this pairs and not the other pairs. Any idea why this is occurring? and how I can solve this? \r\n\r\nThanks a lot  @lhoestq for your help in advance.\r\n\r\n`\r\nthread '<unnamed>' panicked at 'index out of bounds: the len is 617 but the index is 617', /__w/tokenizers/tokenizers/tokenizers/src/tokenizer/normalizer.rs:382:21\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\n 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                   | 626/1000 [00:27<00:16, 22.69ba/s]\r\n\r\nTraceback (most recent call last):\r\n  File \"run_mlm.py\", line 550, in <module>\r\n    main()\r\n  File \"run_mlm.py\", line 412, in main\r\n    in zip(data_args.dataset_name, data_args.dataset_config_name)]\r\n  File \"run_mlm.py\", line 411, in <listcomp>\r\n    logger) for dataset_name, dataset_config_name\\\r\n  File \"/user/dara/dev/codes/seq2seq/data/tokenize_datasets.py\", line 96, in get_tokenized_dataset\r\n    load_from_cache_file=not data_args.overwrite_cache,\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/dataset_dict.py\", line 448, in map\r\n    for k, dataset in self.items()\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/dataset_dict.py\", line 448, in <dictcomp>\r\n    for k, dataset in self.items()\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1309, in map\r\n    update_data=update_data,\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 204, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/fingerprint.py\", line 337, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1574, in _map_single\r\n    batch, indices, check_same_num_examples=len(self.list_indexes()) > 0, offset=offset\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1490, in apply_function_on_filtered_inputs\r\n    function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n  File \"/user/dara/dev/codes/seq2seq/data/tokenize_datasets.py\", line 89, in tokenize_function\r\n    return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 2347, in __call__\r\n    **kwargs,\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 2532, in batch_encode_plus\r\n    **kwargs,\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\", line 384, in _batch_encode_plus\r\n    is_pretokenized=is_split_into_words,\r\npyo3_runtime.PanicException: index out of bounds: the len is 617 but the index is 617\r\n\r\n`",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 831684312,
    "title": "is there a way to override a dataset object saved with save_to_disk?",
    "dateCreated": "2021-03-15T10:50:53Z",
    "dateModified": "2021-03-15T10:50:53Z",
    "description": "At the moment when I use save_to_disk, it uses the arbitrary name for the arrow file.  Is there a way to override such an object? ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 831597665,
    "title": "Could not find file for ZEST dataset",
    "dateCreated": "2021-03-15T09:11:58Z",
    "dateModified": "2021-03-15T09:11:58Z",
    "description": "I am trying to use zest dataset from Allen AI using below code in colab,\r\n```\r\n!pip install -q datasets\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"zest\")\r\n```\r\n\r\nI am getting the following error,\r\n```\r\nUsing custom data configuration default\r\n\r\nDownloading and preparing dataset zest/default (download: 5.53 MiB, generated: 19.96 MiB, post-processed: Unknown size, total: 25.48 MiB) to /root/.cache/huggingface/datasets/zest/default/0.0.0/1f7a230fbfc964d979bbca0f0130fbab3259fce547ee758ad8aa4f9c9bec6cca...\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-6-18dbbc1a4b8a> in <module>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 dataset = load_dataset(\"zest\")\r\n\r\n9 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\r\n    612             )\r\n    613         elif response is not None and response.status_code == 404:\r\n--> 614             raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\n    615         _raise_if_offline_mode_is_enabled(f\"Tried to reach {url}\")\r\n    616         raise ConnectionError(\"Couldn't reach {}\".format(url))\r\n\r\nFileNotFoundError: Couldn't find file at https://ai2-datasets.s3-us-west-2.amazonaws.com/zest/zest.zip\r\n```",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 831151728,
    "title": "Add bAbI QA tasks",
    "dateCreated": "2021-03-14T13:04:39Z",
    "dateModified": "2021-03-14T13:04:39Z",
    "description": "- **Name:** *The (20) QA bAbI tasks*\r\n- **Description:** *The (20) QA bAbI tasks are a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. The aim is to classify these tasks into skill sets,so that researchers can identify (and then rectify) the failings of their systems.*\r\n- **Paper:** [arXiv](https://arxiv.org/pdf/1502.05698.pdf)\r\n- **Data:** [Facebook Research Page](https://research.fb.com/downloads/babi/)\r\n- **Motivation:** This is a unique dataset with story-based Question Answering. It  is a part of the `bAbI` project by Facebook Research.\r\n\r\n**Note**: I have currently added all the 160 configs. If this seems impractical, I can keep only a few. While each `dummy_data.zip` weighs a few KBs, overall it is around 1.3MB for all configurations. This is problematic. Let me know what is to be done.\r\n\r\nThanks :)\r\n\r\n\r\n### Checkbox\r\n\r\n- [x] Create the dataset script `/datasets/my_dataset/my_dataset.py` using the template\r\n- [x] Fill the `_DESCRIPTION` and `_CITATION` variables\r\n- [x] Implement `_infos()`, `_split_generators()` and `_generate_examples()`\r\n- [x] Make sure that the `BUILDER_CONFIGS` class attribute is filled with the different configurations of the dataset and that the `BUILDER_CONFIG_CLASS` is specified if there is a custom config class.\r\n- [x] Generate the metadata file `dataset_infos.json` for all configurations\r\n- [x] Generate the dummy data `dummy_data.zip` files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card `README.md` using the template : fill the tags and the various paragraphs\r\n- [x] Both tests for the real data and the dummy data pass.\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 831135704,
    "title": "Timit_asr dataset repeats examples",
    "dateCreated": "2021-03-14T11:43:43Z",
    "dateModified": "2021-03-14T11:43:43Z",
    "description": "Summary\r\n\r\nWhen loading timit_asr dataset on datasets 1.4+, every row in the dataset is the same\r\nSteps to reproduce\r\n\r\nAs an example, on this code there is the text from the training part:\r\n\r\nCode snippet:\r\n```\r\nfrom datasets import load_dataset, load_metric\r\n\r\ntimit = load_dataset(\"timit_asr\")\r\ntimit['train']['text']\r\n#['Would such an act of refusal be useful?',\r\n# 'Would such an act of refusal be useful?',\r\n# 'Would such an act of refusal be useful?',\r\n# 'Would such an act of refusal be useful?',\r\n# 'Would such an act of refusal be useful?',\r\n# 'Would such an act of refusal be useful?',\r\n```\r\nThe same behavior happens for other columns\r\n\r\nExpected behavior:\r\n\r\nDifferent info on the actual timit_asr dataset\r\n\r\nActual behavior:\r\n\r\nWhen loading timit_asr dataset on datasets 1.4+, every row in the dataset is the same. I've checked datasets 1.3 and the rows are different\r\nDebug info\r\n\r\n    Streamlit version: (get it with $ streamlit version)\r\n    Python version: Python 3.6.12\r\n    Using Conda? PipEnv? PyEnv? Pex? Using pip\r\n    OS version: Centos-release-7-9.2009.1.el7.centos.x86_64\r\n\r\nAdditional information\r\n\r\nYou can check the same behavior on https://huggingface.co/datasets/viewer/?dataset=timit_asr",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 831027021,
    "title": "Add MDD Dataset",
    "dateCreated": "2021-03-14T00:01:05Z",
    "dateModified": "2021-03-14T00:01:05Z",
    "description": "- **Name:** *MDD Dataset*\r\n- **Description:** The Movie Dialog dataset (MDD) is designed to measure how well models can perform at goal and non-goal orientated dialog centered around the topic of movies (question answering, recommendation and discussion), from various movie reviews sources such as MovieLens and OMDb.\r\n- **Paper:** [arXiv](https://arxiv.org/pdf/1511.06931.pdf)\r\n- **Data:**  https://research.fb.com/downloads/babi/\r\n- **Motivation:** This is one of the popular dialog datasets, a part of Facebook Research's \"bAbI project\".\r\n\r\n### Checkbox\r\n\r\n- [x] Create the dataset script `/datasets/my_dataset/my_dataset.py` using the template\r\n- [x] Fill the `_DESCRIPTION` and `_CITATION` variables\r\n- [x] Implement `_infos()`, `_split_generators()` and `_generate_examples()`\r\n- [x] Make sure that the `BUILDER_CONFIGS` class attribute is filled with the different configurations of the dataset and that the `BUILDER_CONFIG_CLASS` is specified if there is a custom config class.\r\n- [x] Generate the metadata file `dataset_infos.json` for all configurations\r\n- [x] Generate the dummy data `dummy_data.zip` files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card `README.md` using the template : fill the tags and the various paragraphs\r\n- [x] Both tests for the real data and the dummy data pass.\r\n\r\n\r\n**Note**: I haven't included the following from the data files: `entities` (the file containing list of all entities in the first three subtasks), `dictionary`(the dictionary of words they use in their models), `movie_kb`(contains the knowledge base of information about the movies, actors and other entities that are mentioned in the dialogs). Please let me know if those are needed, and if yes, should I make separate configurations for them?",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 831006551,
    "title": "Build custom dataset to fine-tune Wav2Vec2",
    "dateCreated": "2021-03-13T22:01:10Z",
    "dateModified": "2021-03-13T22:01:10Z",
    "description": "Thank you for your recent tutorial on how to finetune Wav2Vec2 on a custom dataset. The example you gave here (https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) was on the CommonVoice dataset. However, what if I want to load my own dataset?  I have a manifest (transcript and their audio files) in a JSON file. \r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 830978687,
    "title": "Fix text-classification tags",
    "dateCreated": "2021-03-13T19:51:42Z",
    "dateModified": "2021-03-13T19:51:42Z",
    "description": "There are different tags for text classification right now: `text-classification` and `text_classification`:\r\n![image](https://user-images.githubusercontent.com/29076344/111042457-856bdf00-8463-11eb-93c9-50a30106a1a1.png).\r\n\r\nThis PR fixes it.\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 830953431,
    "title": "github is not always available - probably need a back up",
    "dateCreated": "2021-03-13T18:03:32Z",
    "dateModified": "2021-03-13T18:03:32Z",
    "description": "Yesterday morning github wasn't working:\r\n\r\n```\r\n:/tmp$ wget https://raw.githubusercontent.com/huggingface/datasets/1.4.1/metrics/sacrebleu/sacrebleu.py--2021-03-12 18:35:59-- https://raw.githubusercontent.com/huggingface/datasets/1.4.1/metrics/sacrebleu/sacrebleu.py\r\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\r\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\r\nHTTP request sent, awaiting response... 500 Internal Server Error\r\n2021-03-12 18:36:11 ERROR 500: Internal Server Error.\r\n```\r\n\r\nSuggestion: have a failover system and replicate the data on another system and reach there if gh isn't reachable? perhaps gh can be a master and the replicate a slave - so there is only one true source.",
    "status": "open",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 830626430,
    "title": "Multilingual dIalogAct benchMark (miam)",
    "dateCreated": "2021-03-12T23:02:55Z",
    "dateModified": "2021-03-12T23:02:55Z",
    "description": "My collaborators (@EmileChapuis, @PierreColombo) and I within the Affective Computing team at Telecom Paris would like to anonymously publish the miam dataset. It is assocated with a publication currently under review. We will update the dataset with full citations once the review period is over.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 830423033,
    "title": "add_faisis_index  gets very slow when doing it interatively  ",
    "dateCreated": "2021-03-12T20:27:18Z",
    "dateModified": "2021-03-12T20:27:18Z",
    "description": "As the below code suggests, I want to run add_faisis_index in every nth interaction from the training loop. I have 7.2 million documents. Usually, it takes 2.5 hours (if I run an as a separate process similar to the script given in rag/use_own_knowleldge_dataset.py). Now, this takes usually 5hrs. Is this normal?   Any way to make this process faster? \r\n\r\n@lhoestq \r\n\r\n```\r\n   def training_step(self, batch, batch_idx) -> Dict:\r\n\r\n    \r\n        if (not batch_idx==0) and (batch_idx%5==0):\r\n\r\n            print(\"******************************************************\")\r\n            ctx_encoder=self.trainer.model.module.module.model.rag.ctx_encoder\r\n            model_copy =type(ctx_encoder)(self.config_dpr) # get a new instance  #this will be load in the CPU\r\n            model_copy.load_state_dict(ctx_encoder.state_dict()) # copy weights and stuff\r\n\r\n\r\n            list_of_gpus = ['cuda:2','cuda:3']\r\n            c_dir='/custom/cache/dir'\r\n\r\n            kb_dataset = load_dataset(\"csv\", data_files=[self.custom_config.csv_path], split=\"train\", delimiter=\"\\t\", column_names=[\"title\", \"text\"],cache_dir=c_dir) \r\n\r\n            print(kb_dataset)\r\n\r\n      \r\n            n=len(list_of_gpus) #nunber of dedicated GPUs\r\n            kb_list=[kb_dataset.shard(n, i, contiguous=True) for i in range(n)]\r\n\r\n            #kb_dataset.save_to_disk('/hpc/gsir059/MY-Test/RAY/transformers/examples/research_projects/rag/haha-dir')\r\n\r\n\r\n            print(self.trainer.global_rank)\r\n            dataset_shards = self.re_encode_kb(model_copy.to(device=list_of_gpus[self.trainer.global_rank]),kb_list[self.trainer.global_rank])\r\n            output = [None for _ in list_of_gpus]\r\n\r\n            #self.trainer.accelerator_connector.accelerator.barrier(\"embedding_process\")\r\n            dist.all_gather_object(output, dataset_shards)\r\n            \r\n\r\n            #This creation and re-initlaization of the new index\r\n            if (self.trainer.global_rank==0):  #saving will be done in the main process \r\n           \r\n                combined_dataset = concatenate_datasets(output)\r\n    \r\n                passages_path =self.config.passages_path\r\n\r\n                logger.info(\"saving the dataset with  \")\r\n                #combined_dataset.save_to_disk('/hpc/gsir059/MY-Test/RAY/transformers/examples/research_projects/rag/MY-Passage')\r\n                combined_dataset.save_to_disk(passages_path)\r\n                logger.info(\"Add faiss index to the dataset that consist of embeddings\") \r\n\r\n    \r\n                embedding_dataset=combined_dataset\r\n                index = faiss.IndexHNSWFlat(768, 128, faiss.METRIC_INNER_PRODUCT)\r\n                embedding_dataset.add_faiss_index(\"embeddings\", custom_index=index)\r\n\r\n                embedding_dataset.get_index(\"embeddings\").save(self.config.index_path)\r\n\r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 830351527,
    "title": "Preserve column ordering in Dataset.rename_column",
    "dateCreated": "2021-03-12T18:26:47Z",
    "dateModified": "2021-03-12T18:26:47Z",
    "description": "Currently `Dataset.rename_column` doesn't necessarily preserve the order of the columns:\r\n```python\r\n>>> from datasets import Dataset\r\n>>> d = Dataset.from_dict({'sentences': [\"s1\", \"s2\"], 'label': [0, 1]})\r\n>>> d\r\nDataset({\r\n    features: ['sentences', 'label'],\r\n    num_rows: 2\r\n})\r\n>>> d.rename_column('sentences', 'text')\r\nDataset({\r\n    features: ['label', 'text'],\r\n    num_rows: 2\r\n})\r\n```\r\nThis PR fixes this.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 830339905,
    "title": "Add CBT dataset",
    "dateCreated": "2021-03-12T18:04:19Z",
    "dateModified": "2021-03-12T18:04:19Z",
    "description": "This PR adds the [CBT Dataset](https://arxiv.org/abs/1511.02301).\r\n\r\nNote that I have also added the `raw` dataset as a separate configuration. I couldn't find a suitable \"task\" for it in YAML tags.\r\n\r\nThe dummy files have one example each, as the examples are slightly big. For `raw` dataset, I just used top few lines, because they are entire books and would take up a lot of space.\r\n\r\nLet me know in case of any issues.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 830279098,
    "title": "Support pickle protocol for dataset splits defined as ReadInstruction",
    "dateCreated": "2021-03-12T16:35:11Z",
    "dateModified": "2021-03-12T16:35:11Z",
    "description": "Fixes #2022 (+ some style fixes) ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 830190276,
    "title": "Fix arrow memory checks issue in tests",
    "dateCreated": "2021-03-12T14:49:52Z",
    "dateModified": "2021-03-12T14:49:52Z",
    "description": "The tests currently fail on `master` because the arrow memory verification doesn't return the expected memory evolution when loading an arrow table in memory.\r\nFrom my experiments, the tests fail only when the full test suite is ran.\r\nThis made me think that maybe some arrow objects from other tests were not freeing their memory until they do and cause the memory verifications to fail in other tests.\r\n\r\nCollecting the garbage collector before checking the arrow memory usage seems to fix this issue.\r\nI added a context manager `assert_arrow_memory_increases` that we can use in tests and that deals with the gc.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 830180803,
    "title": "Doc2dial update data_infos and data_loaders",
    "dateCreated": "2021-03-12T14:39:29Z",
    "dateModified": "2021-03-12T14:39:29Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 830169387,
    "title": "ValueError: datasets' indices [1] come from memory and datasets' indices [0] come from disk",
    "dateCreated": "2021-03-12T14:27:00Z",
    "dateModified": "2021-03-12T14:27:00Z",
    "description": "Hi there,\r\n\r\nI am trying to concat two datasets that I've previously saved to disk via `save_to_disk()` like so (note that both are saved as `DataDict`, `PATH_DATA_CLS_*` are `Path`-objects):\r\n```python\r\nconcatenate_datasets([load_from_disk(PATH_DATA_CLS_A)['train'], load_from_disk(PATH_DATA_CLS_B)['train']])\r\n```\r\nYielding the following error:\r\n```python\r\nValueError: Datasets' indices should ALL come from memory, or should ALL come from disk.\r\nHowever datasets' indices [1] come from memory and datasets' indices [0] come from disk.\r\n```\r\nBeen trying to solve this for quite some time now. Both `DataDict` have been created by reading in a `csv` via `load_dataset` and subsequently processed using the various `datasets` methods (i.e. filter, map, remove col, rename col). Can't figure out tho...\r\n\r\n`load_from_disk(PATH_DATA_CLS_A)['train']` yields:\r\n```python\r\nDataset({\r\n    features: ['labels', 'text'],\r\n    num_rows: 785\r\n})\r\n```\r\n`load_from_disk(PATH_DATA_CLS_B)['train']` yields:\r\n```python\r\nDataset({\r\n    features: ['labels', 'text'],\r\n    num_rows: 3341\r\n})\r\n```",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 830047652,
    "title": "Doc2dial rc",
    "dateCreated": "2021-03-12T11:56:28Z",
    "dateModified": "2021-03-12T11:56:28Z",
    "description": "Added fix to handle the last turn that is a user turn.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 830036875,
    "title": "outdated dataset_infos.json might fail verifications",
    "dateCreated": "2021-03-12T11:41:54Z",
    "dateModified": "2021-03-12T11:41:54Z",
    "description": "The [doc2dial/dataset_infos.json](https://github.com/huggingface/datasets/blob/master/datasets/doc2dial/dataset_infos.json) is outdated. It would fail data_loader when verifying download checksum etc..\r\n\r\nCould you please update this file or point me how to update this file?\r\n\r\nThank you.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 829919685,
    "title": "Fix: Wikipedia - save memory by replacing root.clear with elem.clear",
    "dateCreated": "2021-03-12T09:22:00Z",
    "dateModified": "2021-03-12T09:22:00Z",
    "description": "see: https://github.com/huggingface/datasets/issues/2031\r\n\r\nWhat I did:\r\n- replace root.clear with elem.clear\r\n- remove lines to get root element\r\n- $ make style\r\n- $ make test\r\n  - some tests required some pip packages, I installed them.\r\n\r\ntest results on origin/master and my branch are same. I think it's not related on my modification, isn't it?\r\n```\r\n==================================================================================== short test summary info ====================================================================================\r\nFAILED tests/test_arrow_writer.py::TypedSequenceTest::test_catch_overflow - AssertionError: OverflowError not raised\r\n============================================================= 1 failed, 2332 passed, 5138 skipped, 70 warnings in 91.75s (0:01:31) ==============================================================\r\nmake: *** [Makefile:19: test] Error 1\r\n\r\n```\r\n\r\nIs there anything else I should do?",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 829909258,
    "title": "Cannot load wikitext",
    "dateCreated": "2021-03-12T09:09:39Z",
    "dateModified": "2021-03-12T09:09:39Z",
    "description": "when I execute these codes\r\n```\r\n>>> from datasets import load_dataset\r\n>>> test_dataset = load_dataset(\"wikitext\")\r\n```\r\n\r\nI got an error,any help?\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/xxx/anaconda3/envs/transformer/lib/python3.7/site-packages/datasets/load.py\", line 589, in load_dataset\r\n    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n  File \"/home/xxx/anaconda3/envs/transformer/lib/python3.7/site-packages/datasets/load.py\", line 267, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/home/xxx/anaconda3/envs/transformer/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/home/xxx/anaconda3/envs/transformer/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 487, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/wikitext/wikitext.py\r\n```",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 829475544,
    "title": "wiki40b/wikipedia for almost all languages cannot be downloaded",
    "dateCreated": "2021-03-11T19:54:54Z",
    "dateModified": "2021-03-11T19:54:54Z",
    "description": "Hi\r\nI am trying to download the data as below:\r\n\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"wiki40b\", \"cs\")\r\nprint(dataset)\r\n```\r\n\r\nI am getting this error. @lhoestq I will be grateful if you could assist me with this error. For almost all languages except english I am getting this error.\r\n\r\nI really need majority of languages in this dataset to be able to train my models for a deadline and your great scalable super well-written library is my only hope to train the models at scale while being low on resources. \r\n\r\nthank you very much.\r\n\r\n```\r\n(fast) dara@vgne046:/user/dara/dev/codes/seq2seq$ python test_data.py\r\nDownloading and preparing dataset wiki40b/cs (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to temp/dara/cache_home_2/datasets/wiki40b/cs/1.1.0/063778187363ffb294896eaa010fc254b42b73e31117c71573a953b0b0bf010f...\r\nTraceback (most recent call last):\r\n  File \"test_data.py\", line 3, in <module>\r\n    dataset = load_dataset(\"wiki40b\", \"cs\")\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/load.py\", line 746, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/builder.py\", line 579, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/builder.py\", line 1105, in _download_and_prepare\r\n    import apache_beam as beam\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/apache_beam-2.28.0-py3.7-linux-x86_64.egg/apache_beam/__init__.py\", line 96, in <module>\r\n    from apache_beam import io\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/apache_beam-2.28.0-py3.7-linux-x86_64.egg/apache_beam/io/__init__.py\", line 23, in <module>\r\n    from apache_beam.io.avroio import *\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/apache_beam-2.28.0-py3.7-linux-x86_64.egg/apache_beam/io/avroio.py\", line 55, in <module>\r\n    import avro\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 668, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 638, in _load_backward_compatible\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/avro_python3-1.9.2.1-py3.7.egg/avro/__init__.py\", line 34, in <module>\r\n  File \"/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/avro_python3-1.9.2.1-py3.7.egg/avro/__init__.py\", line 30, in LoadResource\r\nNotADirectoryError: [Errno 20] Not a directory: '/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/avro_python3-1.9.2.1-py3.7.egg/avro/VERSION.txt'\r\n```",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 829381388,
    "title": "Fix typo",
    "dateCreated": "2021-03-11T17:46:13Z",
    "dateModified": "2021-03-11T17:46:13Z",
    "description": "Change `ENV_XDG_CACHE_HOME ` to `XDG_CACHE_HOME `",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 829295339,
    "title": "Raise an error for outdated sacrebleu versions",
    "dateCreated": "2021-03-11T16:08:00Z",
    "dateModified": "2021-03-11T16:08:00Z",
    "description": "The `sacrebleu` metric seem to only work for sacrecleu>=1.4.12\r\n\r\nFor example using sacrebleu==1.2.10, an error is raised (from metric/sacrebleu/sacrebleu.py):\r\n```python\r\n    def _compute(\r\n        self,\r\n        predictions,\r\n        references,\r\n        smooth_method=\"exp\",\r\n        smooth_value=None,\r\n        force=False,\r\n        lowercase=False,\r\n        tokenize=scb.DEFAULT_TOKENIZER,\r\n        use_effective_order=False,\r\n    ):\r\n        references_per_prediction = len(references[0])\r\n        if any(len(refs) != references_per_prediction for refs in references):\r\n            raise ValueError(\"Sacrebleu requires the same number of references for each prediction\")\r\n        transformed_references = [[refs[i] for refs in references] for i in range(references_per_prediction)]\r\n>       output = scb.corpus_bleu(\r\n            sys_stream=predictions,\r\n            ref_streams=transformed_references,\r\n            smooth_method=smooth_method,\r\n            smooth_value=smooth_value,\r\n            force=force,\r\n            lowercase=lowercase,\r\n            tokenize=tokenize,\r\n            use_effective_order=use_effective_order,\r\n        )\r\n\r\nE       TypeError: corpus_bleu() got an unexpected keyword argument 'smooth_method'\r\n/mnt/cache/modules/datasets_modules/metrics/sacrebleu/b390045b3d1dd4abf6a95c4a2a11ee3bcc2b7620b076204d0ddc353fa649fd86/sacrebleu.py:114: TypeError\r\n```\r\n\r\nI improved the error message when users have an outdated version of sacrebleu.\r\nThe new error message tells the user to update sacrebleu.\r\ncc @LysandreJik ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 829250912,
    "title": "Use Arrow filtering instead of writing a new arrow file for Dataset.filter",
    "dateCreated": "2021-03-11T15:18:50Z",
    "dateModified": "2021-03-11T15:18:50Z",
    "description": "Currently the filter method reads the dataset batch by batch to write a new, filtered, arrow file on disk. Therefore all the reading + writing can take some time.\r\n\r\nUsing a mask directly on the arrow table doesn't do any read or write operation therefore it's significantly quicker.\r\n\r\nI think there are two cases:\r\n- if the dataset doesn't have an indices mapping, then one can simply use the arrow filtering on the main arrow table `dataset._data.filter(...)`\r\n- if the dataset an indices mapping, then the mask should be applied on the indices mapping table `dataset._indices.filter(...)`\r\n\r\nThe indices mapping is used to map between the idx at `dataset[idx]` in `__getitem__` and the idx in the actual arrow table.\r\n\r\nThe new filter method should therefore be faster, and allow users to pass either a filtering function (that returns a boolean given an example), or directly a mask.\r\n\r\nFeel free to discuss this idea in this thread :)\r\n\r\nOne additional note: the refactor at #2025 would make all the pickle-related stuff work directly with the arrow filtering, so that we only need to change the Dataset.filter method without having to deal with pickle.\r\n\r\ncc @theo-m @gchhablani \r\n\r\nrelated issues: #1796 #1949 ",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 829122778,
    "title": "wikipedia.py generator that extracts XML doesn't release memory",
    "dateCreated": "2021-03-11T12:51:24Z",
    "dateModified": "2021-03-11T12:51:24Z",
    "description": "I tried downloading Japanese wikipedia, but it always failed because of out of memory maybe.\r\n\r\nI found that the generator function that extracts XML data in wikipedia.py doesn't release memory in the loop.\r\n\r\nhttps://github.com/huggingface/datasets/blob/13a5b7db992ad5cf77895e4c0f76595314390418/datasets/wikipedia/wikipedia.py#L464-L502\r\n\r\n`root.clear()` intend to clear memory, but it doesn't.\r\nhttps://github.com/huggingface/datasets/blob/13a5b7db992ad5cf77895e4c0f76595314390418/datasets/wikipedia/wikipedia.py#L490\r\nhttps://github.com/huggingface/datasets/blob/13a5b7db992ad5cf77895e4c0f76595314390418/datasets/wikipedia/wikipedia.py#L494\r\nI replaced them with `elem.clear()`, then it seems to work correctly.\r\n\r\nhere is the notebook to reproduce it.\r\nhttps://gist.github.com/miyamonz/dc06117302b6e85fa51cbf46dde6bb51#file-xtract_content-ipynb",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 829110803,
    "title": "Implement Dataset from text",
    "dateCreated": "2021-03-11T12:34:50Z",
    "dateModified": "2021-03-11T12:34:50Z",
    "description": "Implement `Dataset.from_text`.\r\n\r\nAnalogue to #1943, #1946.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 829097290,
    "title": "Loading a faiss index KeyError",
    "dateCreated": "2021-03-11T12:16:13Z",
    "dateModified": "2021-03-11T12:16:13Z",
    "description": "I've recently been testing out RAG and DPR embeddings, and I've run into an issue that is not apparent in the documentation.\r\n\r\nThe basic steps are:\r\n\r\n1. Create a dataset (dataset1)\r\n2. Create an embeddings column using DPR\r\n3. Add a faiss index to the dataset\r\n4. Save faiss index to a file\r\n5. Create a new dataset (dataset2) with the same text and label information as dataset1\r\n6. Try to load the faiss index from file to dataset2\r\n7. Get `KeyError: \"Column embeddings not in the dataset\"`\r\n\r\nI've made a colab notebook that should show exactly what I did. Please switch to GPU runtime; I didn't check on CPU.\r\n\r\nhttps://colab.research.google.com/drive/1X0S9ZuZ8k0ybcoei4w7so6dS_WrABmIx?usp=sharing\r\n\r\nUbuntu Version\r\nVERSION=\"18.04.5 LTS (Bionic Beaver)\"\r\n\r\ndatasets==1.4.1\r\nfaiss==1.5.3\r\nfaiss-gpu==1.7.0\r\ntorch==1.8.0+cu101\r\ntransformers==4.3.3\r\n\r\nNVIDIA-SMI 460.56\r\nDriver Version: 460.32.03\r\nCUDA Version: 11.2    \r\nTesla K80           \r\n\r\nI was basically following the steps here: https://huggingface.co/docs/datasets/faiss_and_ea.html#adding-a-faiss-index\r\n\r\nI included the exact code from the documentation at the end of the notebook to show that they don't work either.\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 828721393,
    "title": "Adding PersiNLU reading-comprehension",
    "dateCreated": "2021-03-11T04:41:13Z",
    "dateModified": "2021-03-11T04:41:13Z",
    "description": "",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 828490444,
    "title": "Update format columns in Dataset.rename_columns",
    "dateCreated": "2021-03-10T23:50:59Z",
    "dateModified": "2021-03-10T23:50:59Z",
    "description": "Fixes #2026 ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 828194467,
    "title": "KeyError on using map after renaming a column",
    "dateCreated": "2021-03-10T18:54:17Z",
    "dateModified": "2021-03-10T18:54:17Z",
    "description": "Hi,\r\n\r\nI'm trying to use `cifar10` dataset. I want to rename the `img` feature to `image` in order to make it consistent with `mnist`, which I'm also planning to use. By doing this, I was trying to avoid modifying `prepare_train_features` function.\r\n\r\nHere is what I try:\r\n\r\n```python\r\ntransform = Compose([ToPILImage(),ToTensor(),Normalize([0.0,0.0,0.0],[1.0,1.0,1.0])])\r\ndef prepare_features(examples):\r\n    images = []\r\n    labels = []\r\n    print(examples)\r\n    for example_idx, example in enumerate(examples[\"image\"]):\r\n        if transform is not None:\r\n            images.append(transform(examples[\"image\"][example_idx].permute(2,0,1)))\r\n        else:\r\n            images.append(examples[\"image\"][example_idx].permute(2,0,1))\r\n        labels.append(examples[\"label\"][example_idx])\r\n    output = {\"label\":labels, \"image\":images}\r\n    return output\r\n\r\nraw_dataset = load_dataset('cifar10')\r\nraw_dataset.set_format('torch',columns=['img','label'])\r\nraw_dataset = raw_dataset.rename_column('img','image')\r\n\r\nfeatures = datasets.Features({\r\n            \"image\": datasets.Array3D(shape=(3,32,32),dtype=\"float32\"),\r\n            \"label\": datasets.features.ClassLabel(names=[\r\n                            \"airplane\",\r\n                            \"automobile\",\r\n                            \"bird\",\r\n                            \"cat\",\r\n                            \"deer\",\r\n                            \"dog\",\r\n                            \"frog\",\r\n                            \"horse\",\r\n                            \"ship\",\r\n                            \"truck\",\r\n                        ]),\r\n        })\r\ntrain_dataset = raw_dataset.map(prepare_features, features = features,batched=True, batch_size=10000)\r\n```\r\nThe error:\r\n```python\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-54-bf29672c53ee> in <module>()\r\n     14                         ]),\r\n     15         })\r\n---> 16 train_dataset = raw_dataset.map(prepare_features, features = features,batched=True, batch_size=10000)\r\n\r\n2 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\r\n   1287         test_inputs = self[:2] if batched else self[0]\r\n   1288         test_indices = [0, 1] if batched else 0\r\n-> 1289         update_data = does_function_return_dict(test_inputs, test_indices)\r\n   1290         logger.info(\"Testing finished, running the mapping function on the dataset\")\r\n   1291 \r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in does_function_return_dict(inputs, indices)\r\n   1258             fn_args = [inputs] if input_columns is None else [inputs[col] for col in input_columns]\r\n   1259             processed_inputs = (\r\n-> 1260                 function(*fn_args, indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n   1261             )\r\n   1262             does_return_dict = isinstance(processed_inputs, Mapping)\r\n\r\n<ipython-input-52-b4dccbafb70d> in prepare_features(examples)\r\n      3     labels = []\r\n      4     print(examples)\r\n----> 5     for example_idx, example in enumerate(examples[\"image\"]):\r\n      6         if transform is not None:\r\n      7             images.append(transform(examples[\"image\"][example_idx].permute(2,0,1)))\r\n\r\nKeyError: 'image'\r\n```\r\n\r\nThe print statement inside returns this:\r\n```python\r\n{'label': tensor([6, 9])}\r\n```\r\nApparently, both `img` and `image` do not exist after renaming. \r\n\r\nNote that this code works fine with `img` everywhere.\r\n\r\nNotebook: https://colab.research.google.com/drive/1SzESAlz3BnVYrgQeJ838vbMp1OsukiA2?usp=sharing\r\n\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 828047476,
    "title": "[Refactor] Use in-memory/memory-mapped/concatenation tables in Dataset",
    "dateCreated": "2021-03-10T17:00:47Z",
    "dateModified": "2021-03-10T17:00:47Z",
    "description": "## Intro\r\n\r\nCurrently there is one assumption that we need to change: a dataset is either fully in memory (dataset._data_files is empty), or the dataset can be reloaded from disk with memory mapping (using the dataset._data_files).\r\nThis assumption is used for pickling for example:\r\n- in-memory dataset can just be pickled/unpickled in-memory\r\n- on-disk dataset can be unloaded to only keep the filepaths when pickling, and then reloaded from the disk when unpickling\r\n\r\n## Issues\r\n\r\nBecause of this assumption, we can't easily implement methods like `Dataset.add_item` to append more rows to a dataset, or `dataset.add_column` to add a column, since we can't mix data from memory and data from the disk.\r\nMoreover, `concatenate_datasets` doesn't work if the datasets to concatenate are not all from memory, or all form the disk.\r\n\r\n## Solution provided in this PR\r\n\r\nI changed this by allowing several types of Table to be used in the Dataset object.\r\nMore specifically I added three pyarrow Table wrappers: InMemoryTable, MemoryMappedTable and ConcatenationTable.\r\nThe in-memory and memory-mapped tables implement the pickling behavior described above.\r\nThe ConcatenationTable can be made from several tables (either in-memory or memory mapped) called \"blocks\". Pickling a ConcatenationTable simply pickles the underlying blocks.\r\n\r\n## Implementation details\r\n\r\nThe three tables classes mentioned above all inherit from a `Table` class defined in `table.py`, which is a wrapper of a pyarrow table. The `Table` wrapper implements all the attributes and methods of the underlying pyarrow table.\r\n\r\nRegarding the MemoryMappedTable:\r\nReloading a pyarrow table from the disk makes you lose all the changes you may have applied (slice, rename_columns, drop, cast etc.). Therefore the MemoryMappedTable implements a \"replay\" mechanism to re-apply the changes when reloading the pyarrow table from the disk.\r\n\r\n## Checklist\r\n\r\n- [x] add InMemoryTable\r\n- [x] add MemoryMappedTable\r\n- [x] add ConcatenationTable\r\n- [x] Update the ArrowReader to use these new tables depending on the `in_memory` parameter\r\n- [x] Update Dataset.from_xxx methods\r\n- [x] Update load_from_disk and save_to_disk\r\n- [x] Backward compatibility of load_from_disk\r\n- [x] Add tests for the new tables\r\n- [x] Update current tests\r\n- [ ] Documentation\r\n\r\n----------\r\n\r\nI would be happy to discuss the design of this PR :)\r\n\r\nClose #1877 ",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 827842962,
    "title": "Remove print statement from mnist.py",
    "dateCreated": "2021-03-10T14:39:58Z",
    "dateModified": "2021-03-10T14:39:58Z",
    "description": "",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 827819608,
    "title": "Add Romanian to XQuAD",
    "dateCreated": "2021-03-10T14:24:32Z",
    "dateModified": "2021-03-10T14:24:32Z",
    "description": "On Jan 18, XQuAD was updated with a new Romanian validation file ([xquad commit link](https://github.com/deepmind/xquad/commit/60cac411649156efb6aab9dd4c9cde787a2c0345))\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 827435033,
    "title": "ValueError when rename_column on splitted dataset",
    "dateCreated": "2021-03-10T09:40:38Z",
    "dateModified": "2021-03-10T09:40:38Z",
    "description": "Hi there,\r\nI am loading `.tsv` file via `load_dataset` and subsequently split the rows into training and test set via the `ReadInstruction` API like so:\r\n\r\n```python\r\nsplit = {\r\n    'train': ReadInstruction('train', to=90, unit='%'),\r\n    'test': ReadInstruction('train', from_=-10, unit='%')\r\n}\r\n\r\ndataset = load_dataset(\r\n    path='csv',               # use 'text' loading script to load from local txt-files\r\n    delimiter='\\t',           # xxx\r\n    data_files=text_files,    # list of paths to local text files\r\n    split=split,              # xxx\r\n)\r\n\r\ndataset\r\n```\r\n\r\nPart of output:\r\n```python\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['sentence', 'sentiment'],\r\n        num_rows: 900\r\n    })\r\n    test: Dataset({\r\n        features: ['sentence', 'sentiment'],\r\n        num_rows: 100\r\n    })\r\n})\r\n```\r\nAfterwards I'd like to rename the 'sentence' column to 'text' in order to be compatible with my modelin pipeline. If I run the following code I experience a `ValueError` however:\r\n```python\r\ndataset['train'].rename_column('sentence', 'text')\r\n```\r\n```python\r\n/usr/local/lib/python3.7/dist-packages/datasets/splits.py in __init__(self, name)\r\n    353         for split_name in split_names_from_instruction:\r\n    354             if not re.match(_split_re, split_name):\r\n--> 355                 raise ValueError(f\"Split name should match '{_split_re}'' but got '{split_name}'.\")\r\n    356 \r\n    357     def __str__(self):\r\n\r\nValueError: Split name should match '^\\w+(\\.\\w+)*$'' but got 'ReadInstruction('.\r\n```\r\nIn particular, these behavior does not arise if I use the deprecated `rename_column_` method. Any idea what causes the error? Would assume something in the way I defined the split.\r\n\r\nThanks in advance! :)",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 826988016,
    "title": "Interactively doing  save_to_disk and load_from_disk corrupts the datasets object?",
    "dateCreated": "2021-03-10T02:48:34Z",
    "dateModified": "2021-03-10T02:48:34Z",
    "description": " dataset_info.json file saved after using  save_to_disk gets corrupted as follows. \r\n \r\n \r\n![image](https://user-images.githubusercontent.com/16892570/110568474-ed969880-81b7-11eb-832f-2e5129656016.png)\r\n\r\nIs there a way to disable the cache that will save to /tmp/huggiface/datastes ? \r\nI have a feeling there is a serious issue with cashing.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 826961126,
    "title": "Remove unnecessary docstart check in conll-like datasets",
    "dateCreated": "2021-03-10T02:20:16Z",
    "dateModified": "2021-03-10T02:20:16Z",
    "description": "Related to this PR: #1998\r\n\r\nAdditionally, this PR adds the docstart note to the conll2002 dataset card ([link](https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/conll2002/ned.train) to the raw data with `DOCSTART` lines).\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 826625706,
    "title": "Replace print with logging in dataset scripts",
    "dateCreated": "2021-03-09T20:59:34Z",
    "dateModified": "2021-03-09T20:59:34Z",
    "description": "Replaces `print(...)` in the dataset scripts with the library logger.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 826473764,
    "title": "Md gender card update",
    "dateCreated": "2021-03-09T18:57:20Z",
    "dateModified": "2021-03-09T18:57:20Z",
    "description": "I updated the descriptions of the datasets as they appear in the HF repo and the descriptions of the source datasets according to what I could find from the paper and the references. I'm still a little unclear about some of the fields of the different configs, and there was little info on the word list and name list. I'll contact the authors to see if they have any additional information or suggested changes.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 826428578,
    "title": "Add TF-based Features to handle different modes of data",
    "dateCreated": "2021-03-09T18:29:52Z",
    "dateModified": "2021-03-09T18:29:52Z",
    "description": "Hi,\r\n\r\nI am creating this draft PR to work on add features similar to [TF datasets](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/core/features). I'll be starting with `Tensor` and `FeatureConnector` classes, and build upon them to add other features as well. This is a work in progress.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 825965493,
    "title": "Not all languages have 2 digit codes.",
    "dateCreated": "2021-03-09T13:53:39Z",
    "dateModified": "2021-03-09T13:53:39Z",
    "description": ".",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 825942108,
    "title": "Fix ipython function creation in tests",
    "dateCreated": "2021-03-09T13:36:59Z",
    "dateModified": "2021-03-09T13:36:59Z",
    "description": "The test at `tests/test_caching.py::RecurseDumpTest::test_dump_ipython_function` was failing in python 3.8 because the ipython function was not properly created.\r\n\r\nFix #2010 ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 825916531,
    "title": "more explicit method parameters",
    "dateCreated": "2021-03-09T13:18:29Z",
    "dateModified": "2021-03-09T13:18:29Z",
    "description": "re: #2009\n\nnot super convinced this is better, and while I usually fight against kwargs here it seems to me that it better conveys the relationship to the `_split_generator` method.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 825694305,
    "title": "Add Cryptonite dataset",
    "dateCreated": "2021-03-09T10:32:11Z",
    "dateModified": "2021-03-09T10:32:11Z",
    "description": "cc @aviaefrat who's the original author of the dataset & paper, see https://github.com/aviaefrat/cryptonite",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 825634064,
    "title": "No upstream branch",
    "dateCreated": "2021-03-09T09:48:55Z",
    "dateModified": "2021-03-09T09:48:55Z",
    "description": "Feels like the documentation on adding a new dataset is outdated?\r\n\r\nhttps://github.com/huggingface/datasets/blob/987df6b4e9e20fc0c92bc9df48137d170756fd7b/ADD_NEW_DATASET.md#L49-L54\r\n\r\nThere is no upstream branch on remote. ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 825621952,
    "title": "Add RoSent Dataset",
    "dateCreated": "2021-03-09T09:40:08Z",
    "dateModified": "2021-03-09T09:40:08Z",
    "description": "This PR adds a Romanian sentiment analysis dataset. This PR also closes pending PR #1529.\r\n\r\nI had to add an `original_id` feature because the dataset files have repeated IDs. I can remove them if needed. I have also added `id` which is unique.\r\n\r\nLet me know in case of any issues.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 825567635,
    "title": "Local testing fails",
    "dateCreated": "2021-03-09T09:01:38Z",
    "dateModified": "2021-03-09T09:01:38Z",
    "description": "I'm following the CI setup as described in \r\n\r\nhttps://github.com/huggingface/datasets/blob/8eee4fa9e133fe873a7993ba746d32ca2b687551/.circleci/config.yml#L16-L19\r\n\r\nin a new conda environment, at commit https://github.com/huggingface/datasets/commit/4de6dbf84e93dad97e1000120d6628c88954e5d4\r\n\r\nand getting\r\n\r\n```\r\nFAILED tests/test_caching.py::RecurseDumpTest::test_dump_ipython_function - TypeError: an integer is required (got type bytes)\r\n1 failed, 2321 passed, 5109 skipped, 10 warnings in 124.32s (0:02:04)\r\n```\r\n\r\nSeems like a discrepancy with CI, perhaps a lib version that's not controlled? \r\nTried with `pyarrow=={1.0.0,0.17.1,2.0.0}`",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 825541366,
    "title": "Ambiguous documentation",
    "dateCreated": "2021-03-09T08:42:11Z",
    "dateModified": "2021-03-09T08:42:11Z",
    "description": "https://github.com/huggingface/datasets/blob/2ac9a0d24a091989f869af55f9f6411b37ff5188/templates/new_dataset_script.py#L156-L158\r\n\r\nLooking at the template, I find this documentation line to be confusing, the method parameters don't include the `gen_kwargs` so I'm unclear where they're coming from.\r\n\r\nHappy to push a PR with a clearer statement when I understand the meaning.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 825153804,
    "title": "Fix various typos/grammer in the docs",
    "dateCreated": "2021-03-09T01:39:28Z",
    "dateModified": "2021-03-09T01:39:28Z",
    "description": "This PR:\r\n* fixes various typos/grammer I came across while reading the docs\r\n* adds the \"Install with conda\" installation instructions\r\n\r\nCloses #1959 ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 824518158,
    "title": "How to not load huggingface datasets into memory ",
    "dateCreated": "2021-03-08T12:35:26Z",
    "dateModified": "2021-03-08T12:35:26Z",
    "description": "Hi\r\nI am running this example from transformers library version 4.3.3:\r\n(Here is the full documentation https://github.com/huggingface/transformers/issues/8771 but the running command should work out of the box)\r\n\r\n USE_TF=0  deepspeed  run_seq2seq.py --model_name_or_path google/mt5-base --dataset_name wmt16 --dataset_config_name ro-en --source_prefix \"translate English to Romanian: \" --task translation_en_to_ro   --output_dir /test/test_large  --do_train --do_eval --predict_with_generate  --max_train_samples 500   --max_val_samples 500  --max_source_length 128 --max_target_length 128 --sortish_sampler --per_device_train_batch_size 8   --val_max_target_length 128 --deepspeed ds_config.json --num_train_epochs 1 --eval_steps 25000 --warmup_steps 500 --overwrite_output_dir\r\n\r\n(Here please find the script: https://github.com/huggingface/transformers/blob/master/examples/seq2seq/run_seq2seq.py)\r\n\r\nIf you do not pass max_train_samples in above command to load the full dataset, then I get memory issue on a gpu with 24 GigBytes of memory.\r\n \r\nI need to train large-scale mt5 model on large-scale datasets of wikipedia (multiple of them concatenated or other datasets in multiple languages like OPUS), could you help me how I can avoid loading the full data into memory? to make the scripts not related to data size? \r\n\r\nIn above example, I was hoping the script could work without relying on dataset size, so I can still train the model without subsampling training set.\r\n\r\nthank you so much @lhoestq for your great help in advance\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 824457794,
    "title": "Don't gitignore dvc.lock",
    "dateCreated": "2021-03-08T11:13:08Z",
    "dateModified": "2021-03-08T11:13:08Z",
    "description": "The benchmarks runs are [failing](https://github.com/huggingface/datasets/runs/2055534629?check_suite_focus=true) because of \r\n```\r\nERROR: 'dvc.lock' is git-ignored.\r\n```\r\n\r\nI removed the dvc.lock file from the gitignore to fix that",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 824275035,
    "title": "Setting to torch format not working with torchvision and MNIST",
    "dateCreated": "2021-03-08T07:38:11Z",
    "dateModified": "2021-03-08T07:38:11Z",
    "description": "Hi\r\n\r\nI am trying to use `torchvision.transforms` to handle the transformation of the image data in the `mnist` dataset. Assume I have a `transform` variable which contains the `torchvision.transforms` object.\r\n\r\nA snippet of what I am trying to do:\r\n```python\r\ndef prepare_features(examples):\r\n    images = []\r\n    labels = []\r\n    for example_idx, example in enumerate(examples[\"image\"]):\r\n        if transform is not None:\r\n            images.append(transform(\r\n                np.array(examples[\"image\"][example_idx], dtype=np.uint8)\r\n            ))\r\n        else:\r\n            images.append(torch.tensor(np.array(examples[\"image\"][example_idx], dtype=np.uint8)))\r\n        labels.append(torch.tensor(examples[\"label\"][example_idx]))\r\n    output = {\"label\":labels, \"image\":images}\r\n    return output\r\n\r\nraw_dataset = load_dataset('mnist')\r\ntrain_dataset = raw_dataset.map(prepare_features, batched=True, batch_size=10000)\r\ntrain_dataset.set_format(\"torch\",columns=[\"image\",\"label\"])\r\n```\r\n\r\nAfter this, I check the type of the following:\r\n```python\r\nprint(type(train_dataset[\"train\"][\"label\"]))\r\nprint(type(train_dataset[\"train\"][\"image\"][0]))\r\n```\r\nThis leads to the following output:\r\n\r\n```python\r\n<class 'torch.Tensor'>\r\n<class 'list'>\r\n```\r\nI use `torch.utils.DataLoader` for batches, the type of `batch[\"train\"][\"image\"]` is also `<class 'list'>`.\r\n\r\nI don't understand why only the `label` is converted to a torch tensor, why does the image not get converted? How can I fix this issue?\r\n\r\nThanks,\r\nGunjan\r\n\r\nEDIT:\r\nI just checked the shapes, and the types, `batch[image]` is a actually a list of list of tensors. Shape is (1,28,2,28), where `batch_size` is 2. I don't understand why this is happening. Ideally it should be a tensor of shape (2,1,28,28).\r\n\r\nEDIT 2:\r\nInside `prepare_train_features`, the shape of `images[0]` is `torch.Size([1,28,28])`, the conversion is working. However, the output of the `map` is a list of list of list of list.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 824080760,
    "title": "LaRoSeDa",
    "dateCreated": "2021-03-08T01:06:32Z",
    "dateModified": "2021-03-08T01:06:32Z",
    "description": "Add LaRoSeDa to huggingface datasets.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 824034678,
    "title": "Messages are being printed to the `stdout`",
    "dateCreated": "2021-03-07T22:09:34Z",
    "dateModified": "2021-03-07T22:09:34Z",
    "description": "In this code segment, we can see some messages are being printed to the `stdout`.\r\nhttps://github.com/huggingface/datasets/blob/7e60bb509b595e8edc60a87f32b2bacfc065d607/src/datasets/builder.py#L545-L554\r\nAccording to the comment, it is done intentionally, but I don't really understand why don't we log it with a higher level or print it directly to the `stderr`.\r\nIn my opinion, this kind of messages should never printed to the stdout. At least some configuration/flag should make it possible to provide in order to explicitly prevent the package to contaminate the stdout.\r\n",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 823955744,
    "title": "MOROCO",
    "dateCreated": "2021-03-07T16:22:17Z",
    "dateModified": "2021-03-07T16:22:17Z",
    "description": "Add MOROCO to huggingface datasets.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 823946706,
    "title": "Empty evidence document (\"provenance\") in KILT ELI5 dataset",
    "dateCreated": "2021-03-07T15:41:35Z",
    "dateModified": "2021-03-07T15:41:35Z",
    "description": "In the  original KILT benchmark(https://github.com/facebookresearch/KILT), \r\n\r\nall samples has its evidence document (i.e. wikipedia page id) for prediction.\r\n\r\nFor example, a sample in ELI5 dataset has the format including provenance (=evidence document) like this\r\n\r\n`{\"id\": \"1kiwfx\", \"input\": \"In Trading Places (1983, Akroyd/Murphy) how does the scheme at the end of the movie work? Why would buying a lot of OJ at a high price ruin the Duke Brothers?\", \"output\": [{\"answer\": \"I feel so old. People have been askinbg what happened at the end of this movie for what must be the last 15 years of my life. It never stops. Every year/month/fortnight, I see someone asking what happened, and someone explaining. Andf it will keep on happening, until I am 90yrs old, in a home, with nothing but the Internet and my bladder to keep me going. And there it will be: \\\"what happens at the end of Trading Places?\\\"\"}, {\"provenance\": [{\"wikipedia_id\": \"242855\", \"title\": \"Futures contract\", \"section\": \"Section::::Abstract.\", \"start_paragraph_id\": 1, \"start_character\": 14, \"end_paragraph_id\": 1, \"end_character\": 612, \"bleu_score\": 0.9232808519770748}]}], \"meta\": {\"partial_evidence\": [{\"wikipedia_id\": \"520990\", \"title\": \"Trading Places\", \"section\": \"Section::::Plot.\\n\", \"start_paragraph_id\": 7, \"end_paragraph_id\": 7, \"meta\": {\"evidence_span\": [\"On television, they learn that Clarence Beeks is transporting a secret USDA report on orange crop forecasts.\", \"On television, they learn that Clarence Beeks is transporting a secret USDA report on orange crop forecasts. Winthorpe and Valentine recall large payments made to Beeks by the Dukes and realize that the Dukes plan to obtain the report to corner the market on frozen orange juice.\", \"Winthorpe and Valentine recall large payments made to Beeks by the Dukes and realize that the Dukes plan to obtain the report to corner the market on frozen orange juice.\"]}}]}}`\r\n\r\nHowever, KILT ELI5 dataset from huggingface datasets library only contain empty list of provenance.\r\n\r\n`{'id': '1oy5tc', 'input': 'in football whats the point of wasting the first two plays with a rush - up the middle - not regular rush plays i get those', 'meta': {'left_context': '', 'mention': '', 'obj_surface': [], 'partial_evidence': [], 'right_context': '', 'sub_surface': [], 'subj_aliases': [], 'template_questions': []}, 'output': [{'answer': 'In most cases the O-Line is supposed to make a hole for the running back to go through. If you run too many plays to the outside/throws the defense will catch on.\\n\\nAlso, 2 5 yard plays gets you a new set of downs.', 'meta': {'score': 2}, 'provenance': []}, {'answer': \"I you don't like those type of plays, watch CFL.  We only get 3 downs so you can't afford to waste one.  Lots more passing.\", 'meta': {'score': 2}, 'provenance': []}]}\r\n`\r\n\r\nshould i perform other procedure to obtain evidence documents?",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 823899910,
    "title": "Windows Permission Error (most recent version of datasets)",
    "dateCreated": "2021-03-07T11:55:28Z",
    "dateModified": "2021-03-07T11:55:28Z",
    "description": "Hi everyone,\r\nCan anyone help me with why the dataset loading script below raises a Windows Permission Error? I stuck quite closely to https://github.com/huggingface/datasets/blob/master/datasets/conll2003/conll2003.py , only I want to load the data from three local three-column tsv-files (id\\ttokens\\tpos_tags\\n). I am using the most recent version of datasets. Thank you in advance!\r\nLuisa\r\n\r\nMy script:\r\n```\r\nimport datasets\r\nimport csv\r\n\r\nlogger = datasets.logging.get_logger(__name__)\r\n\r\n\r\nclass SampleConfig(datasets.BuilderConfig):\r\n\r\n    def __init__(self, **kwargs):\r\n        super(SampleConfig, self).__init__(**kwargs)\r\n\r\n\r\nclass Sample(datasets.GeneratorBasedBuilder):\r\n    BUILDER_CONFIGS = [\r\n        SampleConfig(name=\"conll2003\", version=datasets.Version(\"1.0.0\"), description=\"Conll2003 dataset\"),\r\n    ]\r\n\r\n    def _info(self):\r\n        return datasets.DatasetInfo(\r\n            description=\"Dataset with words and their POS-Tags\",\r\n            features=datasets.Features(\r\n                {\r\n                    \"id\": datasets.Value(\"string\"),\r\n                    \"tokens\": datasets.Sequence(datasets.Value(\"string\")),\r\n                    \"pos_tags\": datasets.Sequence(\r\n                        datasets.features.ClassLabel(\r\n                            names=[\r\n                                \"''\",\r\n                                \",\",\r\n                                \"-LRB-\",\r\n                                \"-RRB-\",\r\n                                \".\",\r\n                                \":\",\r\n                                \"CC\",\r\n                                \"CD\",\r\n                                \"DT\",\r\n                                \"EX\",\r\n                                \"FW\",\r\n                                \"HYPH\",\r\n                                \"IN\",\r\n                                \"JJ\",\r\n                                \"JJR\",\r\n                                \"JJS\",\r\n                                \"MD\",\r\n                                \"NN\",\r\n                                \"NNP\",\r\n                                \"NNPS\",\r\n                                \"NNS\",\r\n                                \"PDT\",\r\n                                \"POS\",\r\n                                \"PRP\",\r\n                                \"PRP$\",\r\n                                \"RB\",\r\n                                \"RBR\",\r\n                                \"RBS\",\r\n                                \"RP\",\r\n                                \"TO\",\r\n                                \"UH\",\r\n                                \"VB\",\r\n                                \"VBD\",\r\n                                \"VBG\",\r\n                                \"VBN\",\r\n                                \"VBP\",\r\n                                \"VBZ\",\r\n                                \"WDT\",\r\n                                \"WP\",\r\n                                \"WRB\",\r\n                                \"``\"\r\n                            ]\r\n                        )\r\n                    ),\r\n                }\r\n            ),\r\n            supervised_keys=None,\r\n            homepage=\"https://catalog.ldc.upenn.edu/LDC2011T03\",\r\n            citation=\"Weischedel, Ralph, et al. OntoNotes Release 4.0 LDC2011T03. Web Download. Philadelphia: Linguistic Data Consortium, 2011.\",\r\n        )\r\n\r\n    def _split_generators(self, dl_manager):\r\n        loaded_files = dl_manager.download_and_extract(self.config.data_files)\r\n        return [\r\n            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": loaded_files[\"train\"]}),\r\n            datasets.SplitGenerator(name=datasets.Split.TEST, gen_kwargs={\"filepath\": loaded_files[\"test\"]}),\r\n            datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={\"filepath\": loaded_files[\"val\"]})\r\n        ]\r\n\r\n    def _generate_examples(self, filepath):\r\n        logger.info(\"generating examples from = %s\", filepath)\r\n        with open(filepath, encoding=\"cp1252\") as f:\r\n            data = csv.reader(f, delimiter=\"\\t\")\r\n            ids = list()\r\n            tokens = list()\r\n            pos_tags = list()\r\n            for id_, line in enumerate(data):\r\n                #print(line)\r\n                if len(line) == 1:\r\n                    if tokens:\r\n                        yield id_, {\"id\": ids, \"tokens\": tokens, \"pos_tags\": pos_tags}\r\n                        ids = list()\r\n                        tokens = list()\r\n                        pos_tags = list()\r\n                else:\r\n                    ids.append(line[0])\r\n                    tokens.append(line[1])\r\n                    pos_tags.append(line[2])\r\n            # last example\r\n            yield id_, {\"id\": ids, \"tokens\": tokens, \"pos_tags\": pos_tags}\r\n\r\n\r\ndef main():\r\n    dataset = datasets.load_dataset(\r\n        \"data_loading.py\", data_files={\r\n            \"train\": \"train.tsv\",\r\n            \"test\": \"test.tsv\",\r\n            \"val\": \"val.tsv\"\r\n        }\r\n    )\r\n\r\n    #print(dataset)\r\n\r\nif __name__==\"__main__\":\r\n    main()\r\n```\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 823753591,
    "title": "Add FashionMNIST dataset",
    "dateCreated": "2021-03-06T21:36:57Z",
    "dateModified": "2021-03-06T21:36:57Z",
    "description": "This PR adds [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 823723960,
    "title": "Add -DOCSTART- note to dataset card of conll-like datasets",
    "dateCreated": "2021-03-06T19:08:29Z",
    "dateModified": "2021-03-06T19:08:29Z",
    "description": "Closes #1983",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 823679465,
    "title": "from datasets import MoleculeDataset, GEOMDataset",
    "dateCreated": "2021-03-06T15:50:19Z",
    "dateModified": "2021-03-06T15:50:19Z",
    "description": "I met the ImportError: cannot import name 'MoleculeDataset' from 'datasets'. Have anyone met the similar issues? Thanks!",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 823573410,
    "title": "Error when exploring `arabic_speech_corpus`",
    "dateCreated": "2021-03-06T05:55:20Z",
    "dateModified": "2021-03-06T05:55:20Z",
    "description": "Navigate to https://huggingface.co/datasets/viewer/?dataset=arabic_speech_corpus\r\n\r\nError:\r\n```\r\nImportError: To be able to use this dataset, you need to install the following dependencies['soundfile'] using 'pip install soundfile' for instance'\r\nTraceback:\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/streamlit/script_runner.py\", line 332, in _run_script\r\n    exec(code, module.__dict__)\r\nFile \"/home/sasha/nlp-viewer/run.py\", line 233, in <module>\r\n    configs = get_confs(option)\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/streamlit/caching.py\", line 604, in wrapped_func\r\n    return get_or_create_cached_value()\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/streamlit/caching.py\", line 588, in get_or_create_cached_value\r\n    return_value = func(*args, **kwargs)\r\nFile \"/home/sasha/nlp-viewer/run.py\", line 145, in get_confs\r\n    module_path = nlp.load.prepare_module(path, dataset=True\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/datasets/load.py\", line 342, in prepare_module\r\n    f\"To be able to use this {module_type}, you need to install the following dependencies\"\r\n```",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 822878431,
    "title": "[Timit_asr] Make sure not only the first sample is used ",
    "dateCreated": "2021-03-05T08:42:51Z",
    "dateModified": "2021-03-05T08:42:51Z",
    "description": "When playing around with timit I noticed that only the first sample is used for all indices. I corrected this typo so that the dataset is correctly loaded.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 822871238,
    "title": "not being able to get wikipedia es language",
    "dateCreated": "2021-03-05T08:31:48Z",
    "dateModified": "2021-03-05T08:31:48Z",
    "description": "Hi\r\nI am trying to run a code with wikipedia of config 20200501.es, getting:\r\n\r\nTraceback (most recent call last):\r\n  File \"run_mlm_t5.py\", line 608, in <module>\r\n    main()\r\n  File \"run_mlm_t5.py\", line 359, in main\r\n    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)\r\n  File \"/dara/libs/anaconda3/envs/success432/lib/python3.7/site-packages/datasets-1.2.1-py3.7.egg/datasets/load.py\", line 612, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/dara/libs/anaconda3/envs/success432/lib/python3.7/site-packages/datasets-1.2.1-py3.7.egg/datasets/builder.py\", line 527, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/dara/libs/anaconda3/envs/success432/lib/python3.7/site-packages/datasets-1.2.1-py3.7.egg/datasets/builder.py\", line 1050, in _download_and_prepare\r\n    \"\\n\\t`{}`\".format(usage_example)\r\ndatasets.builder.MissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https://beam.apache.org/documentation/runners/capability-matrix/\r\nIf you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). \r\nExample of usage: \r\n\t`load_dataset('wikipedia', '20200501.es', beam_runner='DirectRunner')`\r\n\r\nthanks @lhoestq  for any suggestion/help ",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 822758387,
    "title": "How to load a dataset with load_from disk and save it again after doing transformations without changing the original? ",
    "dateCreated": "2021-03-05T05:25:50Z",
    "dateModified": "2021-03-05T05:25:50Z",
    "description": "I am using the latest datasets library.  In my work, I first use **load_from_disk** to load a data set that contains 3.8Gb information. Then during my training process, I update that dataset object and add new elements and save it in a different place.  \r\n\r\nWhen I save the dataset with **save_to_disk**, the original dataset which is already in the disk also gets updated. I do not want to update it.  How to prevent from this?\r\n",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 822672238,
    "title": "`datasets.map` multi processing much slower than single processing ",
    "dateCreated": "2021-03-05T02:10:02Z",
    "dateModified": "2021-03-05T02:10:02Z",
    "description": "Hi, thank you for the great library.\r\n\r\nI've been using datasets to pretrain language models, and it often involves datasets as large as ~70G.\r\nMy data preparation step is roughly two steps: `load_dataset` which splits corpora into a table of sentences, and `map` converts a sentence into a list of integers, using a tokenizer.\r\n\r\nI noticed that `map` function with `num_proc=mp.cpu_count() //2` takes more than 20 hours to finish the job where as `num_proc=1` gets the job done in about 5 hours. The machine I used has 40 cores, with 126G of RAM. There were no other jobs when `map` function was running.\r\n\r\nWhat could be the reason? I would be happy to provide information necessary to spot the reason.\r\n\r\np.s. I was experiencing the imbalance issue mentioned in [here](https://github.com/huggingface/datasets/issues/610#issuecomment-705177036) when I was using multi processing.\r\np.s.2 When I run `map` with `num_proc=1`, I see one tqdm bar but all the cores are working. When `num_proc=20`, only 20 cores work. \r\n![Screen Shot 2021-03-05 at 11 04 59](https://user-images.githubusercontent.com/29157715/110056895-ef6cf000-7da2-11eb-8307-6698e9fb1ad4.png)\r\n",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 822554473,
    "title": "Adding the conllpp dataset",
    "dateCreated": "2021-03-04T22:19:43Z",
    "dateModified": "2021-03-04T22:19:43Z",
    "description": "Adding the conllpp dataset, is a revision from https://github.com/huggingface/datasets/pull/1910.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 822384502,
    "title": "OSError: Memory mapping file failed: Cannot allocate memory",
    "dateCreated": "2021-03-04T18:21:58Z",
    "dateModified": "2021-03-04T18:21:58Z",
    "description": "Hi,\r\nI am trying to run a code with a wikipedia dataset, here is the command to reproduce the error. You can find the codes for run_mlm.py in huggingface repo here: https://github.com/huggingface/transformers/blob/v4.3.2/examples/language-modeling/run_mlm.py \r\n```\r\npython run_mlm.py --model_name_or_path bert-base-multilingual-cased --dataset_name wikipedia --dataset_config_name 20200501.en --do_train --do_eval --output_dir /dara/test  --max_seq_length 128\r\n```\r\n\r\nI am using transformer version: 4.3.2 \r\n\r\nBut I got memory erorr using this dataset, is there a way I could save on memory with dataset library with wikipedia dataset?\r\nSpecially I need to train a model with multiple of wikipedia datasets concatenated. thank you very much @lhoestq  for your help and suggestions:\r\n\r\n```\r\n  File \"run_mlm.py\", line 441, in <module>\r\n    main()\r\n  File \"run_mlm.py\", line 233, in main\r\n    split=f\"train[{data_args.validation_split_percentage}%:]\",\r\n  File \"/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/load.py\", line 750, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications, in_memory=keep_in_memory)\r\n  File \"/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/builder.py\", line 740, in as_dataset\r\n    map_tuple=True,\r\n  File \"/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/utils/py_utils.py\", line 225, in map_nested\r\n    return function(data_struct)\r\n  File \"/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/builder.py\", line 757, in _build_single_dataset\r\n    in_memory=in_memory,\r\n  File \"/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/builder.py\", line 829, in _as_dataset\r\n    in_memory=in_memory,\r\n  File \"/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/arrow_reader.py\", line 215, in read\r\n    return self.read_files(files=files, original_instructions=instructions, in_memory=in_memory)\r\n  File \"/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/arrow_reader.py\", line 236, in read_files\r\n    pa_table = self._read_files(files, in_memory=in_memory)\r\n  File \"/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/arrow_reader.py\", line 171, in _read_files\r\n    pa_table: pa.Table = self._get_dataset_from_filename(f_dict, in_memory=in_memory)\r\n  File \"/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/arrow_reader.py\", line 302, in _get_dataset_from_filename\r\n    pa_table = ArrowReader.read_table(filename, in_memory=in_memory)\r\n  File \"/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/arrow_reader.py\", line 322, in read_table\r\n    stream = stream_from(filename)\r\n  File \"pyarrow/io.pxi\", line 782, in pyarrow.lib.memory_map\r\n  File \"pyarrow/io.pxi\", line 743, in pyarrow.lib.MemoryMappedFile._open\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\nOSError: Memory mapping file failed: Cannot allocate memory\r\n```\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 822328147,
    "title": "Question/problem with dataset labels",
    "dateCreated": "2021-03-04T17:06:53Z",
    "dateModified": "2021-03-04T17:06:53Z",
    "description": "Hi, I'm using a dataset with two labels \"nurse\" and \"not nurse\". For whatever reason (that I don't understand), I get an error that I think comes from the datasets package (using csv). Everything works fine if the labels are \"nurse\" and \"surgeon\". \r\n\r\nThis is the trace I get:\r\n\r\n```\r\nFile \"../../../models/tr-4.3.2/run_puppets.py\", line 523, in <module>\r\n    main()\r\n  File \"../../../models/tr-4.3.2/run_puppets.py\", line 249, in main\r\n    datasets = load_dataset(\"csv\", data_files=data_files)\r\n  File \"/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/load.py\", line 740, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/builder.py\", line 572, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/builder.py\", line 650, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/builder.py\", line 1028, in _prepare_split\r\n    writer.write_table(table)\r\n  File \"/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 292, in write_table\r\n    pa_table = pa_table.cast(self._schema)\r\n  File \"pyarrow/table.pxi\", line 1311, in pyarrow.lib.Table.cast\r\n  File \"pyarrow/table.pxi\", line 265, in pyarrow.lib.ChunkedArray.cast\r\n  File \"/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/pyarrow/compute.py\", line 87, in cast\r\n    return call_function(\"cast\", [arr], options)\r\n  File \"pyarrow/_compute.pyx\", line 298, in pyarrow._compute.call_function\r\n  File \"pyarrow/_compute.pyx\", line 192, in pyarrow._compute.Function.call\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Failed to parse string: not nurse\r\n```\r\n\r\nAny ideas how to fix this? For now, I'll probably make them numeric. ",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 822324605,
    "title": "Readme.md is misleading about kinds of datasets?",
    "dateCreated": "2021-03-04T17:04:20Z",
    "dateModified": "2021-03-04T17:04:20Z",
    "description": "Hi!\r\n\r\nAt the README.MD, you say: \"efficient data pre-processing: simple, fast and reproducible data pre-processing for the above public datasets as well as your own local datasets in CSV/JSON/text. \"\r\n\r\nBut here:\r\nhttps://github.com/huggingface/datasets/blob/master/templates/new_dataset_script.py#L82-L117\r\n\r\nYou mention other kinds of datasets, with images and so on. I'm confused. \r\n\r\nIs it possible to use it to store, say, imagenet locally? ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 822308956,
    "title": "wmt15 is broken",
    "dateCreated": "2021-03-04T16:46:25Z",
    "dateModified": "2021-03-04T16:46:25Z",
    "description": "While testing the hotfix, I tried a random other wmt release and found wmt15 to be broken:\r\n```\r\npython -c 'from datasets import load_dataset; load_dataset(\"wmt15\", \"de-en\")' \r\nDownloading: 2.91kB [00:00, 818kB/s]\r\nDownloading: 3.02kB [00:00, 897kB/s]\r\nDownloading: 41.1kB [00:00, 19.1MB/s]\r\nDownloading and preparing dataset wmt15/de-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/stas/.cache/huggingface/datasets/wmt15/de-en/1.0.0/39ad5f9262a0910a8ad7028ad432731ad23fdf91f2cebbbf2ba4776b9859e87f...\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/datasets/load.py\", line 740, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/datasets/builder.py\", line 578, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/datasets/builder.py\", line 634, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/stas/.cache/huggingface/modules/datasets_modules/datasets/wmt15/39ad5f9262a0910a8ad7028ad432731ad23fdf91f2cebbbf2ba4776b9859e87f/wmt_utils.py\", line 757, in _split_generators\r\n    downloaded_files = dl_manager.download_and_extract(urls_to_download)\r\n  File \"/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 283, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 191, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File \"/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 203, in map_nested\r\n    mapped = [\r\n  File \"/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 204, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 160, in _single_map_nested\r\n    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]\r\n  File \"/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 160, in <listcomp>\r\n    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]\r\n  File \"/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 142, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 214, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 274, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 614, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://huggingface.co/datasets/wmt/wmt15/resolve/main/training-parallel-nc-v10.tgz\r\n```",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 822176290,
    "title": "wmt datasets fail to load",
    "dateCreated": "2021-03-04T14:18:55Z",
    "dateModified": "2021-03-04T14:18:55Z",
    "description": "~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\wmt14\\43e717d978d2261502b0194999583acb874ba73b0f4aed0ada2889d1bb00f36e\\wmt_utils.py in _split_generators(self, dl_manager)\r\n    758         # Extract manually downloaded files.\r\n    759         manual_files = dl_manager.extract(manual_paths_dict)\r\n--> 760         extraction_map = dict(downloaded_files, **manual_files)\r\n    761 \r\n    762         for language in self.config.language_pair:\r\n\r\nTypeError: type object argument after ** must be a mapping, not list",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 822170651,
    "title": "Optimize int precision",
    "dateCreated": "2021-03-04T14:12:23Z",
    "dateModified": "2021-03-04T14:12:23Z",
    "description": "Optimize int precision to reduce dataset file size.\r\n\r\nClose #1973, close #1825, close #861.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 821816588,
    "title": "Add tests for WMT datasets",
    "dateCreated": "2021-03-04T06:46:42Z",
    "dateModified": "2021-03-04T06:46:42Z",
    "description": "As requested in #1981, we need tests for WMT datasets, using dummy data.",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 821746008,
    "title": "The size of CoNLL-2003 is not consistant with the official release.",
    "dateCreated": "2021-03-04T04:41:34Z",
    "dateModified": "2021-03-04T04:41:34Z",
    "description": "Thanks for the dataset sharing! But when I use conll-2003, I meet some questions.\r\nThe statistics of conll-2003 in this repo is  : \r\n\\#train 14041  \\#dev 3250 \\#test 3453\r\nWhile the official statistics is:\r\n\\#train 14987 \\#dev 3466 \\#test 3684\r\nWish for your reply~",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 821448791,
    "title": "Fix NestedDataStructure.data for empty dict",
    "dateCreated": "2021-03-03T20:16:51Z",
    "dateModified": "2021-03-03T20:16:51Z",
    "description": "Fix #1981",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 821411109,
    "title": "wmt datasets fail to load",
    "dateCreated": "2021-03-03T19:21:39Z",
    "dateModified": "2021-03-03T19:21:39Z",
    "description": "on master:\r\n```\r\npython -c 'from datasets import load_dataset; load_dataset(\"wmt14\", \"de-en\")'\r\nDownloading and preparing dataset wmt14/de-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/stas/.cache/huggingface/datasets/wmt14/de-en/1.0.0/43e717d978d2261502b0194999583acb874ba73b0f4aed0ada2889d1bb00f36e...\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/load.py\", line 740, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py\", line 578, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py\", line 634, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/stas/.cache/huggingface/modules/datasets_modules/datasets/wmt14/43e717d978d2261502b0194999583acb874ba73b0f4aed0ada2889d1bb00f36e/wmt_utils.py\", line 760, in _split_generators\r\n    extraction_map = dict(downloaded_files, **manual_files)\r\n```\r\n\r\nit worked fine recently. same problem if I try wmt16.\r\n\r\ngit bisect points to this commit from Feb 25 as the culprit https://github.com/huggingface/datasets/commit/792f1d9bb1c5361908f73e2ef7f0181b2be409fa\r\n\r\n@albertvillanova ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 821312810,
    "title": "Loading all answers from drop",
    "dateCreated": "2021-03-03T17:13:07Z",
    "dateModified": "2021-03-03T17:13:07Z",
    "description": "Hello all,\r\n\r\nI propose this change to the DROP loading script so that all answers are loaded no matter their type. Currently, only \"span\" answers are loaded, which excludes a significant amount of answers from drop (i.e. \"number\" and \"date\").\r\n\r\nI updated the script with the version I use for my work. However, I couldn't find a way to verify that all is working when integrated with the datasets repo, since the `load_dataset` method seems to always download the script from github and not local files.\r\n\r\nNote that 9 items from the train set have no answers, as well as 1 from the validation set. The script I propose simply do not load them.\r\n\r\nLet me know if there is anything else I can do,\r\nCl\u00e9ment",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 820977853,
    "title": "Add article_id and process test set template for semeval 2020 task 11\u2026",
    "dateCreated": "2021-03-03T10:34:32Z",
    "dateModified": "2021-03-03T10:34:32Z",
    "description": "\u2026 dataset\r\n\r\n- `article_id` is needed to create the submission file for the task at https://propaganda.qcri.org/semeval2020-task11/\r\n- The `technique classification` task provides the span indices in a template for the test set that is necessary to complete the task. This PR implements processing of that template for the dataset.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 820956806,
    "title": "Adding ro sts dataset",
    "dateCreated": "2021-03-03T10:08:53Z",
    "dateModified": "2021-03-03T10:08:53Z",
    "description": "Adding [RO-STS](https://github.com/dumitrescustefan/RO-STS) dataset",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 820312022,
    "title": "ModuleNotFoundError: No module named 'apache_beam' for wikipedia datasets ",
    "dateCreated": "2021-03-02T19:21:28Z",
    "dateModified": "2021-03-02T19:21:28Z",
    "description": "Hi\r\nI am trying to run run_mlm.py code [1] of huggingface with following \"wikipedia\"/ \"20200501.aa\"  dataset:\r\n\r\n`python run_mlm.py     --model_name_or_path bert-base-multilingual-cased --dataset_name wikipedia     --dataset_config_name 20200501.aa     --do_train     --do_eval     --output_dir /tmp/test-mlm --max_seq_length 256\r\n`\r\n\r\nI am getting this error, but as per documentation, huggingface dataset provide processed version of this dataset and users can load it without requiring setup extra settings for apache-beam. could you help me please to load this dataset? \r\nDo you think I can run run_ml.py with this dataset? or anyway I could subsample and train the model? I greatly appreciate providing the processed version of all languages for this dataset, which allow the user to use them without setting up apache-beam,. thanks \r\n\r\nI really appreciate your help.\r\n@lhoestq \r\n\r\nthanks.\r\n\r\n[1] https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\r\n\r\nerror I get: \r\n\r\n```\r\n>>> import datasets \r\n>>> datasets.load_dataset(\"wikipedia\", \"20200501.aa\")\r\nDownloading and preparing dataset wikipedia/20200501.aa (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /dara/temp/cache_home_2/datasets/wikipedia/20200501.aa/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/dara/temp/libs/anaconda3/envs/codes/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/load.py\", line 746, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/dara/temp/libs/anaconda3/envs/codes/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/builder.py\", line 573, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/dara/temp/libs/anaconda3/envs/codes/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/builder.py\", line 1099, in _download_and_prepare\r\n    import apache_beam as beam\r\nModuleNotFoundError: No module named 'apache_beam'\r\n\r\n```",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 820228538,
    "title": "Add datasets full offline mode with HF_DATASETS_OFFLINE",
    "dateCreated": "2021-03-02T17:26:59Z",
    "dateModified": "2021-03-02T17:26:59Z",
    "description": "Add the HF_DATASETS_OFFLINE environment variable for users who want to use `datasets` offline without having to wait for the network timeouts/retries to happen. This was requested in https://github.com/huggingface/datasets/issues/1939\r\n\r\ncc @stas00 ",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 820205485,
    "title": "Fix flake8",
    "dateCreated": "2021-03-02T16:59:13Z",
    "dateModified": "2021-03-02T16:59:13Z",
    "description": "Fix flake8 style.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 820122223,
    "title": "feat(docs): navigate with left/right arrow keys",
    "dateCreated": "2021-03-02T15:24:50Z",
    "dateModified": "2021-03-02T15:24:50Z",
    "description": "Enables docs navigation with left/right arrow keys. It can be useful for the ones who navigate with keyboard a lot.\r\nMore info : https://github.com/sphinx-doc/sphinx/pull/2064\r\n\r\nYou can try here : https://29353-250213286-gh.circle-artifacts.com/0/docs/_build/html/index.html",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 820077312,
    "title": "Question: what gets stored in the datasets cache and why is it so huge?",
    "dateCreated": "2021-03-02T14:35:53Z",
    "dateModified": "2021-03-02T14:35:53Z",
    "description": "I'm running several training jobs (around 10) with a relatively large dataset (3M samples). The datasets cache reached 178G and it seems really large. What is it stored in there and why is it so large? I don't think I noticed this problem before and seems to be related to the new version of the datasets library. Any insight? Thank you!",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 819752761,
    "title": "'Dataset' object has no attribute 'rename_column'",
    "dateCreated": "2021-03-02T08:01:49Z",
    "dateModified": "2021-03-02T08:01:49Z",
    "description": "'Dataset' object has no attribute 'rename_column'",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 819714231,
    "title": "Fix ArrowWriter closes stream at exit",
    "dateCreated": "2021-03-02T07:12:34Z",
    "dateModified": "2021-03-02T07:12:34Z",
    "description": "Current implementation of ArrowWriter does not properly release the `stream` resource (by closing it) if its `finalize()` method is not called and/or an Exception is raised before/during the call to its `finalize()` method.\r\n\r\nTherefore, ArrowWriter should be used as a context manager that properly closes its `stream` resource at exit.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 819500620,
    "title": "Fixing the URL filtering for bad MLSUM examples in GEM",
    "dateCreated": "2021-03-02T01:22:58Z",
    "dateModified": "2021-03-02T01:22:58Z",
    "description": "This updates the code and metadata to use the updated `gem_mlsum_bad_ids_fixed.json` file provided by @juand-r\r\n\r\ncc @sebastianGehrmann ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 819129568,
    "title": "Add Turkish News Category Dataset - 270K - Lite Version",
    "dateCreated": "2021-03-01T18:21:59Z",
    "dateModified": "2021-03-01T18:21:59Z",
    "description": "This PR adds the Turkish News Categories Dataset (270K - Lite Version) dataset which is a text classification dataset by me, @basakbuluz and @serdarakyol.\r\nThis dataset contains the same news from the current [interpress_news_category_tr dataset](https://huggingface.co/datasets/interpress_news_category_tr) but contains less information, OCR errors are reduced, can be easily separated, and can be divided into 10 classes (\"k\u00fclt\u00fcrsanat\", \"ekonomi\", \"siyaset\", \"e\u011fitim\", \"d\u00fcnya\", \"spor\", \"teknoloji\", \"magazin\", \"sa\u011fl\u0131k\", \"g\u00fcndem\") were rearranged.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 819101253,
    "title": "Fix metrics collision in separate multiprocessed experiments",
    "dateCreated": "2021-03-01T17:45:18Z",
    "dateModified": "2021-03-01T17:45:18Z",
    "description": "As noticed in #1942 , there's a issue with locks if you run multiple separate evaluation experiments in a multiprocessed setup.\r\n\r\nIndeed there is a time span in Metric._finalize() where the process 0 loses its lock before re-acquiring it. This is bad since the lock of the process 0 tells the other process that the corresponding cache file is available for writing/reading/deleting: we end up having one metric cache that collides with another one. This can raise FileNotFound errors when a metric tries to read the cache file and if the second conflicting metric deleted it.\r\n\r\nTo fix that I made sure that the lock file of the process 0 stays acquired from the cache file creation to the end of the metric computation. This way the other metrics can simply sample a new hashing name in order to avoid the collision.\r\n\r\nFinally I added missing tests for separate experiments in distributed setup.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 818833460,
    "title": "Can we parallelized the add_faiss_index process over dataset shards ?",
    "dateCreated": "2021-03-01T12:47:34Z",
    "dateModified": "2021-03-01T12:47:34Z",
    "description": "I am thinking of making the  **add_faiss_index** process faster. What if we run the add_faiss_index process on separate dataset shards and then combine them before (dataset.concatenate) saving the faiss.index file ?\r\n\r\nI feel theoretically this will reduce the accuracy of retrieval since it affects the indexing process.\r\n\r\n@lhoestq\r\n",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 818624864,
    "title": "Datasets.py function load_dataset does not match squad dataset",
    "dateCreated": "2021-03-01T08:41:31Z",
    "dateModified": "2021-03-01T08:41:31Z",
    "description": "### 1 When I try to train lxmert,and follow the code in README that --dataset name:\r\n```shell \r\npython examples/question-answering/run_qa.py --model_name_or_path unc-nlp/lxmert-base-uncased --dataset_name squad --do_train --do_eval --per_device_train_batch_size 12 --learning_rate 3e-5 --num_train_epochs 2 --max_seq_length 384 --doc_stride 128 --output_dir /home2/zhenggo1/checkpoint/lxmert_squad\r\n```\r\nthe bug is that:\r\n```\r\nDownloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.75 MiB, post-processed: Unknown size, total: 119.27 MiB) to /home2/zhenggo1/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7...\r\nTraceback (most recent call last):\r\n  File \"examples/question-answering/run_qa.py\", line 501, in <module>\r\n    main()\r\n  File \"examples/question-answering/run_qa.py\", line 217, in main\r\n    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)\r\n  File \"/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/load.py\", line 746, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/builder.py\", line 573, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/builder.py\", line 633, in _download_and_prepare\r\n    self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n  File \"/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/utils/info_utils.py\", line 39, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json']\r\n```\r\nAnd I try to find the [checksum link](https://github.com/huggingface/datasets/blob/master/datasets/squad/dataset_infos.json)\r\n,is the problem plain_text do not have a checksum?\r\n\r\n### 2 When I try to train lxmert,and use local dataset:\r\n```\r\npython examples/question-answering/run_qa.py --model_name_or_path unc-nlp/lxmert-base-uncased --train_file $SQUAD_DIR/train-v1.1.json --validation_file $SQUAD_DIR/dev-v1.1.json --do_train --do_eval --per_device_train_batch_size 12 --learning_rate 3e-5 --num_train_epochs 2 --max_seq_length 384 --doc_stride 128 --output_dir /home2/zhenggo1/checkpoint/lxmert_squad\r\n```\r\nThe bug is that \r\n```\r\n['title', 'paragraphs']\r\nTraceback (most recent call last):\r\n  File \"examples/question-answering/run_qa.py\", line 501, in <module>\r\n    main()\r\n  File \"examples/question-answering/run_qa.py\", line 273, in main\r\n    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\r\nIndexError: list index out of range\r\n```\r\nI print the answer_column_name and find that local squad dataset need the package datasets to preprocessing so that the code below can work:\r\n```\r\nif training_args.do_train:\r\n        column_names = datasets[\"train\"].column_names\r\n    else:\r\n        column_names = datasets[\"validation\"].column_names\r\n    print(datasets[\"train\"].column_names)\r\n    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\r\n    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\r\n    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\r\n``` \r\n## Please tell me how to fix the bug,thks a lot!",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 818289967,
    "title": "bug in SNLI dataset ",
    "dateCreated": "2021-02-28T19:36:20Z",
    "dateModified": "2021-02-28T19:36:20Z",
    "description": "Hi\r\nThere is label of -1 in train set of SNLI dataset, please find the code below:\r\n\r\n```\r\nimport numpy as np \r\nimport datasets \r\ndata = datasets.load_dataset(\"snli\")[\"train\"]\r\nlabels = []\r\nfor d in data:\r\n   labels.append(d[\"label\"])\r\nprint(np.unique(labels))\r\n```\r\n\r\nand results:\r\n\r\n`[-1  0  1  2]`\r\n\r\nversion of datasets used:\r\n`datasets                  1.2.1                     <pip>\r\n`\r\n\r\nthanks for your help. @lhoestq ",
    "status": "open",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 818089156,
    "title": "Fix unused arguments",
    "dateCreated": "2021-02-28T02:47:07Z",
    "dateModified": "2021-02-28T02:47:07Z",
    "description": "Noticed some args in the codebase are not used, so managed to find all such occurrences with Pylance and fix them.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 818077947,
    "title": "Add sst dataset",
    "dateCreated": "2021-02-28T02:08:29Z",
    "dateModified": "2021-02-28T02:08:29Z",
    "description": "Related to #1934&mdash;Add the Stanford Sentiment Treebank dataset.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 818073154,
    "title": "Allow stateful function in dataset.map",
    "dateCreated": "2021-02-28T01:29:05Z",
    "dateModified": "2021-02-28T01:29:05Z",
    "description": "Removes the \"test type\" section in Dataset.map which would modify the state of the stateful function. Now, the return type of the map function is inferred after processing the first example.\r\n\r\nFixes #1940 \r\n\r\n@lhoestq Not very happy with the usage of `nonlocal`. Would like to hear your opinion on this.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 818055644,
    "title": "Bug in skip_rows argument of load_dataset function ?",
    "dateCreated": "2021-02-27T23:32:54Z",
    "dateModified": "2021-02-27T23:32:54Z",
    "description": "Hello everyone,\r\n\r\nI'm quite new to Git so sorry in advance if I'm breaking some ground rules of issues posting... :/\r\nI tried to use the load_dataset function, from Huggingface datasets library, on a csv file using the skip_rows argument described on Huggingface page to skip the first row containing column names\r\n\r\n`test_dataset = load_dataset('csv', data_files=['test_wLabel.tsv'], delimiter='\\t', column_names=[\"id\", \"sentence\", \"label\"], skip_rows=1)`\r\n\r\nBut I got the following error message\r\n\r\n`__init__() got an unexpected keyword argument 'skip_rows'`\r\n\r\nHave I used the wrong argument ? Am I missing something or is this a bug ?\r\n\r\nThank you very much for your time,\r\nBest regards,\r\nArthur",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 818037548,
    "title": "XSum dataset download link broken",
    "dateCreated": "2021-02-27T21:47:56Z",
    "dateModified": "2021-02-27T21:47:56Z",
    "description": "I did \r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"xsum\")\r\n```\r\n\r\nThis returns\r\n`ConnectionError: Couldn't reach http://bollin.inf.ed.ac.uk/public/direct/XSUM-EMNLP18-Summary-Data-Original.tar.gz`",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 818014624,
    "title": "[request] make load_metric api intutive",
    "dateCreated": "2021-02-27T20:43:54Z",
    "dateModified": "2021-02-27T20:43:54Z",
    "description": "```\r\nmetric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)\r\n```\r\n\r\nMay I suggest that `num_process` is confusing as it's singular yet expects a plural value and either \r\n* be deprecated in favor of `num_processes` which is more intuitive since it's plural as its expected value\r\n* or even better why not mimic the established dist environment convention for that purpose, which uses `world_size`. \r\n\r\nSame for `process_id` - why reinvent the naming and needing to explain that this is **NOT** `PID`, when we have `rank` already. That is:\r\n\r\n```\r\nmetric = load_metric('glue', 'mrpc', world_size=world_size, rank=rank)\r\n```\r\n\r\nThis then fits like a glove into the pytorch DDP and alike envs. and we just need to call:\r\n\r\n* `dist.get_world_size()`\r\n* `dist.get_rank()`\r\n\r\nSo it'd be as simple as:\r\n\r\n```\r\nmetric = load_metric('glue', 'mrpc', world_size=dist.get_world_size(), rank=dist.get_rank())\r\n```\r\n\r\nFrom: https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group\r\n\r\n* `world_size (int, optional)` \u2013 Number of processes participating in the job. Required if store is specified.\r\n* `rank (int, optional)` \u2013 Rank of the current process. Required if store is specified.\r\n\r\nAnd may be an example would be useful, so that the user doesn't even need to think about where to get `dist`:\r\n```\r\nimport torch.distributed as dist\r\nif dist.is_initialized():\r\n    metric = load_metric(metric_name, world_size=dist.get_world_size(), rank=dist.get_rank())\r\nelse:\r\n    metric = load_metric(metric_name)\r\n```\r\n\r\nI'm aware this is pytorch-centric, but it's better than no examples, IMHO.\r\n\r\nThank you.",
    "status": "open",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 818013741,
    "title": "[distributed env] potentially unsafe parallel execution",
    "dateCreated": "2021-02-27T20:38:45Z",
    "dateModified": "2021-02-27T20:38:45Z",
    "description": "```\r\nmetric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)\r\n```\r\n\r\npresumes that there is only one set of parallel processes running - and will intermittently fail if you have multiple sets running as they will surely overwrite each other. Similar to https://github.com/huggingface/datasets/issues/1942 (but for a different reason).\r\nThat's why dist environments use some unique to a group identifier so that each group is dealt with separately. \r\n\r\ne.g. the env-way of pytorch dist syncing is done with a unique per set `MASTER_ADDRESS+MASTER_PORT`\r\n\r\nSo ideally this interface should ask for a shared secret to do the right thing.\r\n\r\nI'm not reporting an immediate need, but am only flagging that this will hit someone down the road.\r\n\r\nThis problem can be remedied by adding a new optional `shared_secret` option, which can then be used to differentiate different groups of processes. and this secret should be part of the file lock name and the experiment.\r\n\r\nThank you",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 818010664,
    "title": "typos + grammar",
    "dateCreated": "2021-02-27T20:21:43Z",
    "dateModified": "2021-02-27T20:21:43Z",
    "description": "This PR proposes a few typo + grammar fixes, and rewrites some sentences in an attempt to improve readability.\r\n\r\nN.B. When referring to the library `datasets` in the docs it is typically used as a singular, and it definitely is a singular when written as \"`datasets` library\", that is \"`datasets` library is ...\" and not \"are ...\".",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 817565563,
    "title": "add a new column ",
    "dateCreated": "2021-02-26T18:17:27Z",
    "dateModified": "2021-02-26T18:17:27Z",
    "description": "Hi\r\nI'd need to add a new column to the dataset, I was wondering how this can be done? thanks \r\n@lhoestq ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 817498869,
    "title": "Documentation for to_csv, to_pandas and to_dict",
    "dateCreated": "2021-02-26T16:35:49Z",
    "dateModified": "2021-02-26T16:35:49Z",
    "description": "I added these methods to the documentation with a small paragraph.\r\n\r\nI also fixed some formatting issues in the docstrings",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 817428160,
    "title": "Handle timeouts",
    "dateCreated": "2021-02-26T15:02:07Z",
    "dateModified": "2021-02-26T15:02:07Z",
    "description": "As noticed in https://github.com/huggingface/datasets/issues/1939, timeouts were not properly handled when loading a dataset.\r\nThis caused the connection to hang indefinitely when working in a firewalled environment cc @stas00 \r\n\r\nI added a default timeout, and included an option to our offline environment for tests to be able to simulate both connection errors and timeout errors (previously it was simulating connection errors only).\r\n\r\nNow networks calls don't hang indefinitely.\r\nThe default timeout is set to 10sec (we might reduce it).",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 817423573,
    "title": "Add cross-platform support for datasets-cli",
    "dateCreated": "2021-02-26T14:56:25Z",
    "dateModified": "2021-02-26T14:56:25Z",
    "description": "One thing I've noticed while going through the codebase is the usage of `scripts` in `setup.py`. This [answer](https://stackoverflow.com/a/28119736/14095927) on SO explains it nicely why it's better to use `entry_points` instead of `scripts`. To add cross-platform support to the CLI, this PR replaces `scripts` with `entry_points` in `setup.py` and moves datasets-cli to src/datasets/commands/datasets_cli.py. All *.md and *.rst files are updated accordingly. The same changes were made in the transformers repo to add cross-platform ([link to PR](https://github.com/huggingface/transformers/pull/4131)).",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 817295235,
    "title": "updated multi_nli dataset with missing fields",
    "dateCreated": "2021-02-26T11:54:36Z",
    "dateModified": "2021-02-26T11:54:36Z",
    "description": "1) updated fields which were missing earlier\r\n2) added tags to README\r\n3) updated a few fields of README \r\n4) new dataset_infos.json and dummy files",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 816986936,
    "title": "Enable Fast Filtering using Arrow Dataset",
    "dateCreated": "2021-02-26T02:53:37Z",
    "dateModified": "2021-02-26T02:53:37Z",
    "description": "Hi @lhoestq,\r\n\r\nAs mentioned in Issue #1796, I would love to work on enabling fast filtering/mapping. Can you please share the expectations? It would be great if you could point me to the relevant methods/files involved. Or the docs or maybe an overview of `arrow_dataset.py`. I only ask this because I am having trouble getting started ;-;\r\n\r\nAny help would be appreciated.\r\n\r\nThanks,\r\nGunjan",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 816689329,
    "title": "dataset loading logger level",
    "dateCreated": "2021-02-25T18:33:37Z",
    "dateModified": "2021-02-25T18:33:37Z",
    "description": "on master I get this with `--dataset_name wmt16 --dataset_config ro-en`:\r\n\r\n```\r\nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/stas/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f/cache-2e01bead8cf42e26.arrow\r\nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/stas/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f/cache-ac3bebaf4f91f776.arrow\r\nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/stas/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f/cache-810c3e61259d73a9.arrow\r\n```\r\n\r\nwhy are those WARNINGs? Should be INFO, no?\r\n\r\nwarnings should only be used when a user needs to pay attention to something, this is just informative - I'd even say it should be DEBUG, but definitely not WARNING.\r\n\r\nThank you.\r\n",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 816590299,
    "title": "Update documentation with not in place transforms and update DatasetDict",
    "dateCreated": "2021-02-25T16:23:18Z",
    "dateModified": "2021-02-25T16:23:18Z",
    "description": "In #1883 were added the not in-place transforms `flatten`, `remove_columns`, `rename_column` and `cast`.\r\n\r\nI added them to the documentation and added a paragraph on how to use them\r\n\r\nYou can preview the documentation [here](https://28862-250213286-gh.circle-artifacts.com/0/docs/_build/html/processing.html#renaming-removing-casting-and-flattening-columns)\r\n\r\nI also added these methods to the DatasetDict class.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 816526294,
    "title": "Implement Dataset from CSV",
    "dateCreated": "2021-02-25T15:10:13Z",
    "dateModified": "2021-02-25T15:10:13Z",
    "description": "Implement `Dataset.from_csv`.\r\n\r\nAnalogue to #1943.\r\n\r\nIf finally, the scripts should be used instead, at least we can reuse the tests here. ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 816421966,
    "title": "AttributeError: 'DatasetDict' object has no attribute 'concatenate_datasets'",
    "dateCreated": "2021-02-25T13:09:45Z",
    "dateModified": "2021-02-25T13:09:45Z",
    "description": "Hi\r\nI am trying to concatenate a list of huggingface datastes as:\r\n\r\n` train_dataset = datasets.concatenate_datasets(train_datasets)\r\n`\r\nHere is the `train_datasets` when I print:\r\n\r\n```\r\n[Dataset({\r\n    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],\r\n    num_rows: 120361\r\n}), Dataset({\r\n    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],\r\n    num_rows: 2670\r\n}), Dataset({\r\n    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],\r\n    num_rows: 6944\r\n}), Dataset({\r\n    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],\r\n    num_rows: 38140\r\n}), Dataset({\r\n    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],\r\n    num_rows: 173711\r\n}), Dataset({\r\n    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],\r\n    num_rows: 1655\r\n}), Dataset({\r\n    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],\r\n    num_rows: 4274\r\n}), Dataset({\r\n    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],\r\n    num_rows: 2019\r\n}), Dataset({\r\n    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],\r\n    num_rows: 2109\r\n}), Dataset({\r\n    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],\r\n    num_rows: 11963\r\n})]\r\n```\r\n\r\nI am getting the following error:\r\n\r\n`AttributeError: 'DatasetDict' object has no attribute 'concatenate_datasets'\r\n`\r\n\r\nI was wondering if you could help me with this issue, thanks a lot ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 816267216,
    "title": "Add Turkish News Category Dataset (270K - Lite Version)",
    "dateCreated": "2021-02-25T09:45:22Z",
    "dateModified": "2021-02-25T09:45:22Z",
    "description": "This PR adds the Turkish News Categories Dataset (270K - Lite Version) dataset which is a text classification dataset by me,  @basakbuluz and @serdarakyol. \r\nThis dataset contains the same news from the current [interpress_news_category_tr dataset](https://huggingface.co/datasets/interpress_news_category_tr) but contains less information, OCR errors are reduced, can be easily separated, and can be divided into 10 classes (\"k\u00fclt\u00fcrsanat\", \"ekonomi\", \"siyaset\", \"e\u011fitim\", \"d\u00fcnya\", \"spor\", \"teknoloji\", \"magazin\", \"sa\u011fl\u0131k\", \"g\u00fcndem\") were rearranged.\r\n\r\n@SBrandeis @lhoestq, can you please review this PR?\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 816160453,
    "title": "Implement Dataset from JSON and JSON Lines",
    "dateCreated": "2021-02-25T07:17:33Z",
    "dateModified": "2021-02-25T07:17:33Z",
    "description": "Implement `Dataset.from_jsonl`.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 816037520,
    "title": "[experiment] missing default_experiment-1-0.arrow",
    "dateCreated": "2021-02-25T03:02:15Z",
    "dateModified": "2021-02-25T03:02:15Z",
    "description": "the original report was pretty bad and incomplete - my apologies!\r\n\r\nPlease see the complete version here: https://github.com/huggingface/datasets/issues/1942#issuecomment-786336481\r\n\r\n------------\r\n\r\nAs mentioned here https://github.com/huggingface/datasets/issues/1939 metrics don't get cached, looking at my local `~/.cache/huggingface/metrics` - there are many `*.arrow.lock` files but zero metrics files.\r\n\r\nw/o the network I get:\r\n```\r\nFileNotFoundError: [Errno 2] No such file or directory: '~/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow\r\n```\r\nthere is just `~/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow.lock`\r\n\r\nI did run the same `run_seq2seq.py` script on the instance with network and it worked just fine, but only the lock file was left behind.\r\n\r\nthis is with master.\r\n\r\nThank you.",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 815985167,
    "title": "Loading of FAISS index fails for index_name = 'exact'",
    "dateCreated": "2021-02-25T01:30:54Z",
    "dateModified": "2021-02-25T01:30:54Z",
    "description": "Hi,\r\n\r\nIt looks like loading of FAISS index now fails when using index_name = 'exact'.\r\n\r\nFor example, from the RAG [model card](https://huggingface.co/facebook/rag-token-nq?fbclid=IwAR3bTfhls5U_t9DqsX2Vzb7NhtRHxJxfQ-uwFT7VuCPMZUM2AdAlKF_qkI8#usage).\r\n\r\nRunning `transformers==4.3.2` and datasets installed from source on latest `master` branch.\r\n\r\n```bash\r\n(venv) sergey_mkrtchyan datasets (master) $ python\r\nPython 3.8.6 (v3.8.6:db455296be, Sep 23 2020, 13:31:39)\r\n[Clang 6.0 (clang-600.0.57)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\r\n>>> tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\r\n>>> retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\r\nUsing custom data configuration dummy.psgs_w100.nq.no_index-dummy=True,with_index=False\r\nReusing dataset wiki_dpr (/Users/sergey_mkrtchyan/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.no_index-dummy=True,with_index=False/0.0.0/8a97e0f4fa5bc46e179474db6a61b09d5d2419d2911835bd3f91d110c936d8bb)\r\nUsing custom data configuration dummy.psgs_w100.nq.exact-50b6cda57ff32ab4\r\nReusing dataset wiki_dpr (/Users/sergey_mkrtchyan/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.exact-50b6cda57ff32ab4/0.0.0/8a97e0f4fa5bc46e179474db6a61b09d5d2419d2911835bd3f91d110c936d8bb)\r\n  0%|                                                                                                                                                                                                                   | 0/10 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/sergey_mkrtchyan/workspace/cformers/venv/lib/python3.8/site-packages/transformers/models/rag/retrieval_rag.py\", line 425, in from_pretrained\r\n    return cls(\r\n  File \"/Users/sergey_mkrtchyan/workspace/cformers/venv/lib/python3.8/site-packages/transformers/models/rag/retrieval_rag.py\", line 387, in __init__\r\n    self.init_retrieval()\r\n  File \"/Users/sergey_mkrtchyan/workspace/cformers/venv/lib/python3.8/site-packages/transformers/models/rag/retrieval_rag.py\", line 458, in init_retrieval\r\n    self.index.init_index()\r\n  File \"/Users/sergey_mkrtchyan/workspace/cformers/venv/lib/python3.8/site-packages/transformers/models/rag/retrieval_rag.py\", line 284, in init_index\r\n    self.dataset = load_dataset(\r\n  File \"/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/load.py\", line 750, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications, in_memory=keep_in_memory)\r\n  File \"/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/builder.py\", line 734, in as_dataset\r\n    datasets = utils.map_nested(\r\n  File \"/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/utils/py_utils.py\", line 195, in map_nested\r\n    return function(data_struct)\r\n  File \"/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/builder.py\", line 769, in _build_single_dataset\r\n    post_processed = self._post_process(ds, resources_paths)\r\n  File \"/Users/sergey_mkrtchyan/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/8a97e0f4fa5bc46e179474db6a61b09d5d2419d2911835bd3f91d110c936d8bb/wiki_dpr.py\", line 205, in _post_process\r\n    dataset.add_faiss_index(\"embeddings\", custom_index=index)\r\n  File \"/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/arrow_dataset.py\", line 2516, in add_faiss_index\r\n    super().add_faiss_index(\r\n  File \"/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/search.py\", line 416, in add_faiss_index\r\n    faiss_index.add_vectors(self, column=column, train_size=train_size, faiss_verbose=faiss_verbose)\r\n  File \"/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/search.py\", line 281, in add_vectors\r\n    self.faiss_index.add(vecs)\r\n  File \"/Users/sergey_mkrtchyan/workspace/cformers/venv/lib/python3.8/site-packages/faiss/__init__.py\", line 104, in replacement_add\r\n    self.add_c(n, swig_ptr(x))\r\n  File \"/Users/sergey_mkrtchyan/workspace/cformers/venv/lib/python3.8/site-packages/faiss/swigfaiss.py\", line 3263, in add\r\n    return _swigfaiss.IndexHNSW_add(self, n, x)\r\nRuntimeError: Error in virtual void faiss::IndexHNSW::add(faiss::Index::idx_t, const float *) at /Users/runner/work/faiss-wheels/faiss-wheels/faiss/faiss/IndexHNSW.cpp:356: Error: 'is_trained' failed\r\n>>>\r\n```\r\n\r\nThe issue seems to be related to the scalar quantization in faiss added in this commit: 8c5220307c33f00e01c3bf7b8. Reverting it fixes the issue.\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 815770012,
    "title": "Side effect when filtering data due to `does_function_return_dict` call in `Dataset.map()`",
    "dateCreated": "2021-02-24T19:18:56Z",
    "dateModified": "2021-02-24T19:18:56Z",
    "description": "Hi there!\r\n\r\nIn my codebase I have a function to filter rows in a dataset, selecting only a certain number of examples per class. The function passes a extra argument to maintain a counter of the number of dataset rows/examples already selected per each class, which are the ones I want to keep in the end:\r\n\r\n```python\r\n      def fill_train_examples_per_class(example, per_class_limit: int, counter: collections.Counter):\r\n          label = int(example['label'])\r\n          current_counter = counter.get(label, 0)\r\n          if current_counter < per_class_limit:\r\n              counter[label] = current_counter + 1\r\n              return True\r\n          return False\r\n```\r\n\r\nAt some point I invoke it through the `Dataset.filter()` method in the `arrow_dataset.py` module like this:\r\n\r\n```python\r\n...\r\nkwargs = {\"per_class_limit\": train_examples_per_class_limit, \"counter\": Counter()}\r\ndatasets['train'] = datasets['train'].filter(fill_train_examples_per_class,  num_proc=1, fn_kwargs=kwargs)\r\n...\r\n```\r\n\r\nThe problem is that, passing a stateful container (the counter,) provokes a side effect in the new filtered dataset obtained. This is due to the fact that at some point in `filter()`, the `map()`'s function `does_function_return_dict` is invoked in  line [1290](https://github.com/huggingface/datasets/blob/96578adface7e4bc1f3e8bafbac920d72ca1ca60/src/datasets/arrow_dataset.py#L1290). \r\n\r\nWhen this occurs, the state of the counter is initially modified by the effects of the function call on the 1 or 2 rows selected in lines 1288 and 1289 of the same file (which are marked as `test_inputs` & `test_indices` respectively in lines 1288 and 1289. This happens out of the control of the user (which for example can't reset the state of the counter before continuing the execution,) provoking in the end an undesired side effect in the results obtained. \r\n\r\nIn my case, the resulting dataset -despite of the counter results are ok- lacks an instance of the classes 0 and 1 (which happen to be the classes of the first two examples of my dataset.) The rest of the classes I have in my dataset, contain the right number of examples as they were not affected by the effects of `does_function_return_dict` call.\r\n\r\nI've debugged my code extensively and made a workaround myself hardcoding the necessary stuff (basically putting `update_data=True` in line 1290,) and then I obtain the results I expected without the side effect.\r\n\r\nIs there a way to avoid that call to `does_function_return_dict` in map()'s line 1290 ? (e.g. extracting the required information that `does_function_return_dict` returns without making the testing calls to the user function on dataset rows 0 & 1) \r\n\r\nThanks in advance,\r\n\r\nFrancisco Perez-Sorrosal\r\n\r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 815680510,
    "title": "[firewalled env] OFFLINE mode",
    "dateCreated": "2021-02-24T17:13:42Z",
    "dateModified": "2021-02-24T17:13:42Z",
    "description": "This issue comes from a need to be able to run `datasets` in a firewalled env, which currently makes the software hang until it times out, as it's unable to complete the network calls.\r\n\r\nI propose the following approach to solving this problem, using the example of `run_seq2seq.py` as a sample program. There are 2 possible ways to going about it.\r\n\r\n## 1. Manual\r\n\r\nmanually prepare data and metrics files, that is transfer to the firewalled instance the dataset and the metrics and run:\r\n\r\n```\r\nDATASETS_OFFLINE=1 run_seq2seq.py  --train_file xyz.csv --validation_file xyz.csv ...\r\n```\r\n\r\n`datasets` must not make any network calls and if there is a logic to do that and something is missing it should assert that this or that action requires network and therefore it can't proceed.\r\n\r\n## 2. Automatic\r\n\r\nIn some clouds one can prepare a datastorage ahead of time with a normal networked environment but which doesn't have gpus and then one switches to the gpu instance which is firewalled, but it can access all the cached data. This is the ideal situation, since in this scenario we don't have to do anything manually, but simply run the same application twice:\r\n\r\n1. on the non-firewalled instance:\r\n```\r\nrun_seq2seq.py  --dataset_name wmt16 --dataset_config ro-en ...\r\n```\r\n\r\nwhich should download and cached everything.\r\n\r\n2. and then immediately after on the firewalled instance, which shares the same filesystem\r\n```\r\nDATASETS_OFFLINE=1 run_seq2seq.py  --dataset_name wmt16 --dataset_config ro-en ...\r\n```\r\n\r\nand the metrics and datasets should be cached by the invocation number 1 and any network calls be skipped and if the logic is missing data it should assert and not try to fetch any data from online.\r\n\r\n## Common Issues\r\n\r\n1. for example currently `datasets` tries to look up online datasets if the files contain json or csv, despite the paths already provided\r\n\r\n```\r\n     if dataset and path in _PACKAGED_DATASETS_MODULES:\r\n```\r\n\r\n2. it has an issue with metrics. e.g. I had to manually copy `rouge/rouge.py` from the `datasets` repo to the current dir - or it was hanging.\r\n\r\nI had to comment out `head_hf_s3(...)` calls to make things work. So all those `try: head_hf_s3(...)` shouldn't be tried with `DATASETS_OFFLINE=1`\r\n\r\nHere is the corresponding issue for `transformers`: https://github.com/huggingface/transformers/issues/10379\r\n\r\nThanks.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 815647774,
    "title": "Disallow ClassLabel with no names",
    "dateCreated": "2021-02-24T16:37:57Z",
    "dateModified": "2021-02-24T16:37:57Z",
    "description": "It was possible to create a ClassLabel without specifying the names or the number of classes.\r\nThis was causing silent issues as in #1936 and breaking the conversion methods str2int and int2str.\r\n\r\ncc @justin-yan ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 815163943,
    "title": "CommonGen dataset page shows an error OSError: [Errno 28] No space left on device",
    "dateCreated": "2021-02-24T06:47:33Z",
    "dateModified": "2021-02-24T06:47:33Z",
    "description": "The page of the CommonGen data https://huggingface.co/datasets/viewer/?dataset=common_gen  shows \r\n![image](https://user-images.githubusercontent.com/10104354/108959311-1865e600-7629-11eb-868c-cf4cb27034ea.png)\r\n",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 814726512,
    "title": "[WIP] Adding Support for Reading Pandas Category",
    "dateCreated": "2021-02-23T18:32:54Z",
    "dateModified": "2021-02-23T18:32:54Z",
    "description": "@lhoestq - continuing our conversation from https://github.com/huggingface/datasets/issues/1906#issuecomment-784247014\r\n\r\nThe goal of this PR is to support `Dataset.from_pandas(df)` where the dataframe contains a Category.\r\n\r\nJust the 4 line change below actually does seem to work:\r\n\r\n```\r\n>>> from datasets import Dataset\r\n>>> import pandas as pd\r\n>>> df = pd.DataFrame(pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\"))\r\n>>> ds = Dataset.from_pandas(df)\r\n>>> ds.to_pandas()\r\n   0\r\n0  a\r\n1  b\r\n2  c\r\n3  a\r\n>>> ds.to_pandas().dtypes\r\n0    category\r\ndtype: object\r\n```\r\n\r\nsave_to_disk, etc. all seem to work as well.  The main things that are theoretically \"incorrect\" if we leave this are:\r\n\r\n```\r\n>>> ds.features.type\r\nStructType(struct<0: int64>)\r\n```\r\nthere are a decent number of references to this property in the library, but I can't find anything that seems to actually break as a result of this being int64 vs. dictionary?  I think the gist of my question is: a) do we *need* to change the dtype of Classlabel and have get_nested_type return a pyarrow.DictionaryType instead of int64? and b) do you *want* it to change?  The biggest challenge I see to implementing this correctly is that the data will need to be passed in along with the pyarrow schema when instantiating the Classlabel (I *think* this is unavoidable, since the type itself doesn't contain the actual label values) which could be a fairly intrusive change - e.g. `from_arrow_schema`'s interface would need to change to include optional arrow data?  Once we start going down this path of modifying the public interfaces I am admittedly feeling a little bit outside of my comfort zone\r\n\r\nAdditionally I think `int2str`, `str2int`, and `encode_example` probably won't work - but I can't find any usages of them in the library itself.",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 814623827,
    "title": "add CoVoST2",
    "dateCreated": "2021-02-23T16:28:16Z",
    "dateModified": "2021-02-23T16:28:16Z",
    "description": "This PR adds the CoVoST2 dataset for speech translation and ASR.\r\nhttps://github.com/facebookresearch/covost#covost-2\r\n\r\nThe dataset requires manual download as the download page requests an email address and the URLs are temporary.\r\n\r\nThe dummy data is a bit bigger because of the mp3 files and 36 configs.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 814437190,
    "title": "Add Stanford Sentiment Treebank (SST)",
    "dateCreated": "2021-02-23T12:53:16Z",
    "dateModified": "2021-02-23T12:53:16Z",
    "description": "I am going to add SST:\r\n\r\n- **Name:** The Stanford Sentiment Treebank\r\n- **Description:** The first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language\r\n- **Paper:** [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\r\n- **Data:** https://nlp.stanford.edu/sentiment/index.html\r\n- **Motivation:** Already requested in #353, SST is a popular dataset for Sentiment Classification\r\n\r\nWhat's the difference with the [_SST-2_](https://huggingface.co/datasets/viewer/?dataset=glue&config=sst2) dataset included in GLUE? Essentially, SST-2 is a version of SST where:\r\n- the labels were mapped from real numbers in [0.0, 1.0] to a binary label: {0, 1}\r\n- the labels of the *sub-sentences* were included only in the training set\r\n- the labels in the test set are obfuscated\r\n\r\nSo there is a lot more information in the original SST. The tricky bit is, the data is scattered into many text files and, for one in particular, I couldn't find the original encoding ([*but I'm not the only one*](https://groups.google.com/g/word2vec-toolkit/c/QIUjLw6RqFk/m/_iEeyt428wkJ) \ud83c\udfb5). The only solution I found was to manually replace all the \u00e8, \u00eb, \u00e7 and so on into an `utf-8` copy of the text file. I uploaded the result in my Dropbox and I am using that as the main repo for the dataset.\r\n\r\nAlso, the _sub-sentences_ are built at run-time from the information encoded in several text files, so generating the examples is a bit more cumbersome than usual. Luckily, the dataset is not enormous.\r\n\r\nI plan to divide the dataset in 2 configs: one with just whole sentences with their labels, the other with sentences _and their sub-sentences_ with their labels. Each config will be split in train, validation and test. Hopefully this makes sense, we may discuss it in the PR I'm going to submit.\r\n\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 814335846,
    "title": "Use arrow ipc file format",
    "dateCreated": "2021-02-23T10:38:24Z",
    "dateModified": "2021-02-23T10:38:24Z",
    "description": "According to the [documentation](https://arrow.apache.org/docs/format/Columnar.html?highlight=arrow1#ipc-file-format), it's identical to the streaming format except that it contains the memory offsets of each sample:\r\n\r\n> We define a \u201cfile format\u201d supporting random access that is build with the stream format. The file starts and ends with a magic string ARROW1 (plus padding). What follows in the file is identical to the stream format. At the end of the file, we write a footer containing a redundant copy of the schema (which is a part of the streaming format) plus memory offsets and sizes for each of the data blocks in the file. This enables random access any record batch in the file. See File.fbs for the precise details of the file footer.\r\n\r\nSince it stores more metadata regarding the positions of the examples in the file, it should enable better example retrieval performances. However from the discussion in https://github.com/huggingface/datasets/issues/1803 it looks like it's not the case unfortunately. Maybe in the future this will allow speed gains.\r\n\r\nI think it's still a good idea to start using it anyway for these reasons:\r\n- in the future we may have speed gains\r\n- it contains the arrow streaming format data\r\n- it's compatible with the pyarrow Dataset implementation (it allows to load remote dataframes for example) if we want to use it in the future\r\n- it's also the format used by arrow feather if we want to use it in the future\r\n- it's roughly the same size as the streaming format\r\n- it's easy to have backward compatibility with the streaming format\r\n\r\n",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 814326116,
    "title": "Fix builder config creation with data_dir",
    "dateCreated": "2021-02-23T10:26:02Z",
    "dateModified": "2021-02-23T10:26:02Z",
    "description": "The data_dir parameter wasn't taken into account to create the config_id, therefore the resulting builder config was considered not custom. However a builder config that is non-custom must not have a name that collides with the predefined builder config names. Therefore it resulted in a `ValueError(\"Cannot name a custom BuilderConfig the same as an available...\")`\r\n\r\nI fixed that by commenting the line that used to ignore the data_dir when creating the config.\r\n\r\nIt was previously ignored before the introduction of config id because we didn't want to change the config name. Now it's fine to take it into account for the config id.\r\n\r\nNow creating a config with a data_dir works again @patrickvonplaten ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 814225074,
    "title": "add m_lama (multilingual lama) dataset",
    "dateCreated": "2021-02-23T08:11:57Z",
    "dateModified": "2021-02-23T08:11:57Z",
    "description": "Add a multilingual (machine translated and automatically generated) version of the LAMA benchmark. For details see the paper https://arxiv.org/pdf/2102.00894.pdf ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 814055198,
    "title": "updated the wino_bias dataset",
    "dateCreated": "2021-02-23T03:07:40Z",
    "dateModified": "2021-02-23T03:07:40Z",
    "description": "Updated the wino_bias.py script.\r\n- updated the data_url\r\n- added different configurations for different data splits\r\n- added the coreference_cluster to the data features",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 813929669,
    "title": "Improve typing and style and fix some inconsistencies",
    "dateCreated": "2021-02-22T22:47:41Z",
    "dateModified": "2021-02-22T22:47:41Z",
    "description": "This PR:\r\n* improves typing (mostly more consistent use of `typing.Optional`)\r\n* `DatasetDict.cleanup_cache_files` now correctly returns a dict \r\n* replaces `dict()` with the corresponding literal\r\n* uses `dict_to_copy.copy()` instead of `dict(dict_to_copy)` for shallow copying",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 813793434,
    "title": "Updating old cards",
    "dateCreated": "2021-02-22T19:26:04Z",
    "dateModified": "2021-02-22T19:26:04Z",
    "description": "Updated the cards for [Allocine](https://github.com/mcmillanmajora/datasets/tree/updating-old-cards/datasets/allocine), [CNN/DailyMail](https://github.com/mcmillanmajora/datasets/tree/updating-old-cards/datasets/cnn_dailymail), and [SNLI](https://github.com/mcmillanmajora/datasets/tree/updating-old-cards/datasets/snli). For the most part, the information was just rearranged or rephrased, but the social impact statements are new. ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 813768935,
    "title": "Update README.md",
    "dateCreated": "2021-02-22T18:51:34Z",
    "dateModified": "2021-02-22T18:51:34Z",
    "description": "Updated the info for the wino_bias dataset.",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 813607994,
    "title": "Fix: Wiki_dpr - add missing scalar quantizer",
    "dateCreated": "2021-02-22T15:32:05Z",
    "dateModified": "2021-02-22T15:32:05Z",
    "description": "All the prebuilt wiki_dpr indexes already use SQ8, I forgot to update the wiki_dpr script after building them. Now it's finally done.\r\n\r\nThe scalar quantizer SQ8 doesn't reduce the performance of the index as shown in retrieval experiments on RAG.\r\nThe quantizer reduces the size of the index a lot but increases index building time.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 813600902,
    "title": "Fix: Wiki_dpr - fix when with_embeddings is False or index_name is \"no_index\"",
    "dateCreated": "2021-02-22T15:23:46Z",
    "dateModified": "2021-02-22T15:23:46Z",
    "description": "Fix the bugs noticed in #1915 \r\n\r\nThere was a bug when `with_embeddings=False` where the configuration name was the same as if `with_embeddings=True`, which led the dataset builder to do bad verifications (for example it used to expect to download the embeddings for `with_embeddings=False`).\r\n\r\nAnother issue was that setting `index_name=\"no_index\"` didn't set `with_index` to False.\r\n\r\nI fixed both of them and added dummy data for those configurations for testing.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 813599733,
    "title": "Anonymous Dataset Addition (i.e Anonymous PR?)",
    "dateCreated": "2021-02-22T15:22:30Z",
    "dateModified": "2021-02-22T15:22:30Z",
    "description": "Hello,\r\nThanks a lot for your librairy.\r\nWe plan to submit a paper on OpenReview using the Anonymous setting. Is it possible to add a new dataset without breaking the anonimity, with a link to the paper ? \r\nCheers \r\n@eusip",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 813363472,
    "title": "Fix save_to_disk with relative path",
    "dateCreated": "2021-02-22T10:27:19Z",
    "dateModified": "2021-02-22T10:27:19Z",
    "description": "As noticed in #1919 and #1920 the target directory was not created using `makedirs` so saving to it raises `FileNotFoundError`. For absolute paths it works but not for the good reason. This is because the target path was the same as the temporary path where in-memory data are written as an intermediary step.\r\n\r\nI added the `makedirs` call using `fs.makedirs` in order to support remote filesystems.\r\nI also fixed the issue with the target path being the temporary path.\r\n\r\nI added a test case for relative paths as well for save_to_disk.\r\n\r\nThanks to @M-Salti for reporting and investigating",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 813140806,
    "title": "How to update the \"wino_bias\" dataset",
    "dateCreated": "2021-02-22T05:39:39Z",
    "dateModified": "2021-02-22T05:39:39Z",
    "description": "Hi all,\r\n\r\nThanks for the efforts to collect all the datasets! But I think there is a problem with the wino_bias dataset. The current link is not correct. How can I update that?\r\n\r\nThanks!",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 812716042,
    "title": "Standardizing datasets dtypes",
    "dateCreated": "2021-02-20T22:04:01Z",
    "dateModified": "2021-02-20T22:04:01Z",
    "description": "This PR follows up on discussion in #1900 to have an explicit set of basic dtypes for datasets.\r\n\r\nThis moves away from str(pyarrow.DataType) as the method of choice for creating dtypes, favoring an explicit mapping to a list of supported Value dtypes.\r\n\r\nI believe in practice this should be backward compatible, since anyone previously using Value() would only have been able to use dtypes that had an identically named pyarrow factory function, which are all explicitly supported here, with `float32` and `float64` acting as the official datasets dtypes, which resolves the tension between `double` being the pyarrow dtype and `float64` being the pyarrow type factory function.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 812628220,
    "title": "Fix save_to_disk issue",
    "dateCreated": "2021-02-20T14:22:39Z",
    "dateModified": "2021-02-20T14:22:39Z",
    "description": "Fixes #1919 \r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 812626872,
    "title": "Failure to save with save_to_disk",
    "dateCreated": "2021-02-20T14:18:10Z",
    "dateModified": "2021-02-20T14:18:10Z",
    "description": "When I try to save a dataset locally using the `save_to_disk` method I get the error:\r\n\r\n```bash\r\nFileNotFoundError: [Errno 2] No such file or directory: '/content/squad/train/squad-train.arrow'\r\n```\r\n\r\nTo replicate:\r\n\r\n1. Install `datasets` from master\r\n2. Run this code:\r\n\r\n    ```python\r\n    from datasets import load_dataset\r\n    squad = load_dataset(\"squad\")   # or any other dataset\r\n    squad.save_to_disk(\"squad\")     # error here\r\n    ```\r\n\r\nThe problem is that the method is not creating a directory with the name `dataset_path` for saving the dataset in (i.e. it's not creating the *train* and *validation* directories in this case). After creating the directory the problem resolves.\r\nI'll open a PR soon doing that and linking this issue.\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 812541510,
    "title": "Fix QA4MRE download URLs",
    "dateCreated": "2021-02-20T07:32:17Z",
    "dateModified": "2021-02-20T07:32:17Z",
    "description": "The URLs in the `dataset_infos` and `README` are correct, only the ones in the download script needed updating.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 812390178,
    "title": "UnicodeDecodeError: windows 10 machine",
    "dateCreated": "2021-02-19T22:13:05Z",
    "dateModified": "2021-02-19T22:13:05Z",
    "description": "Windows 10\r\nPhp 3.6.8\r\n\r\nwhen running\r\n\r\n```\r\nimport datasets\r\n\r\noscar_am = datasets.load_dataset(\"oscar\", \"unshuffled_deduplicated_am\")\r\nprint(oscar_am[\"train\"][0])\r\n```\r\nI get the following error\r\n\r\n```\r\nfile \"C:\\PYTHON\\3.6.8\\lib\\encodings\\cp1252.py\", line 23, in decode\r\n    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\r\nUnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 58: character maps to <undefined>\r\n```",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 812291984,
    "title": "Remove unused py_utils objects",
    "dateCreated": "2021-02-19T19:51:25Z",
    "dateModified": "2021-02-19T19:51:25Z",
    "description": "Remove unused/unnecessary py_utils functions/classes.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 812229654,
    "title": "Unable to download `wiki_dpr`",
    "dateCreated": "2021-02-19T18:11:32Z",
    "dateModified": "2021-02-19T18:11:32Z",
    "description": "I am trying to download the `wiki_dpr` dataset. Specifically, I want to download `psgs_w100.multiset.no_index` with no embeddings/no index. In order to do so, I ran:\r\n\r\n`curr_dataset = load_dataset(\"wiki_dpr\", embeddings_name=\"multiset\", index_name=\"no_index\")` \r\n\r\nHowever, I got the following error:\r\n`datasets.utils.info_utils.UnexpectedDownloadedFile: {'embeddings_index'}`\r\n\r\nI tried adding in flags `with_embeddings=False` and `with_index=False`:\r\n\r\n`curr_dataset = load_dataset(\"wiki_dpr\", with_embeddings=False, with_index=False, embeddings_name=\"multiset\", index_name=\"no_index\")`\r\n\r\nBut I got the following error:\r\n`raise ExpectedMoreDownloadedFiles(str(set(expected_checksums) - set(recorded_checksums)))\r\ndatasets.utils.info_utils.ExpectedMoreDownloadedFiles: {\u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_5\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_15\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_30\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_36\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_18\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_41\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_13\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_48\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_10\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_23\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_14\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_34\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_43\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_40\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_47\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_3\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_24\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_7\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_33\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_46\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_42\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_27\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_29\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_26\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_22\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_4\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_20\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_39\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_6\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_16\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_8\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_35\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_49\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_17\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_25\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_0\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_38\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_12\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_44\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_1\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_32\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_19\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_31\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_37\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_9\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_11\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_21\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_28\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_45\u2019, \u2018https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_2\u2019}`\r\n\r\nIs there anything else I need to set to download the dataset?\r\n\r\n**UPDATE**: just running `curr_dataset = load_dataset(\"wiki_dpr\", with_embeddings=False, with_index=False)` gives me the same error.\r\n",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 812149201,
    "title": "Fix logging imports and make all datasets use library logger",
    "dateCreated": "2021-02-19T16:12:34Z",
    "dateModified": "2021-02-19T16:12:34Z",
    "description": "Fix library relative logging imports and make all datasets use library logger.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 812127307,
    "title": "Add keep_linebreaks parameter to text loader",
    "dateCreated": "2021-02-19T15:43:45Z",
    "dateModified": "2021-02-19T15:43:45Z",
    "description": "As asked in #870 and https://github.com/huggingface/transformers/issues/10269 there should be a parameter to keep the linebreaks when loading a text dataset.\r\ncc @sgugger @jncasey",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 812034140,
    "title": "Update: WMT - use mirror links",
    "dateCreated": "2021-02-19T13:42:34Z",
    "dateModified": "2021-02-19T13:42:34Z",
    "description": "As asked in #1892 I created mirrors of the data hosted on statmt.org and updated the wmt scripts.\r\nNow downloading the wmt datasets is blazing fast :)\r\n\r\ncc @stas00 @patrickvonplaten ",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 812009956,
    "title": "Saving processed dataset running infinitely",
    "dateCreated": "2021-02-19T13:09:19Z",
    "dateModified": "2021-02-19T13:09:19Z",
    "description": "I have a text dataset of size 220M.\r\n\r\nFor pre-processing, I need to tokenize this and filter rows with the large sequence.\r\n\r\nMy tokenization took roughly 3hrs. I used map() with batch size 1024 and multi-process with 96 processes.\r\n\r\nfilter() function was way to slow, so I used a hack to use pyarrow filter table function, which is damm fast. Mentioned [here](https://github.com/huggingface/datasets/issues/1796)\r\n\r\n```dataset._data = dataset._data.filter(...)```\r\nIt took 1 hr for the filter.\r\n\r\nThen i use `save_to_disk()` on processed dataset and it is running forever.\r\n\r\nI have been waiting since 8 hrs, it has not written a single byte. \r\n\r\nInfact it has actually read from disk more than 100GB, screenshot below shows the stats using `iotop`. \r\nSecond process is the one.\r\n<img width=\"1672\" alt=\"Screenshot 2021-02-19 at 6 36 53 PM\" src=\"https://user-images.githubusercontent.com/20911334/108508197-7325d780-72e1-11eb-8369-7c057d137d81.png\">\r\n\r\n\r\nI am not able to figure out, whether this is some issue with dataset library or that it is due to my hack for filter() function.",
    "status": "open",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 811697108,
    "title": "Adding CoNLLpp dataset.",
    "dateCreated": "2021-02-19T05:12:30Z",
    "dateModified": "2021-02-19T05:12:30Z",
    "description": "",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 811520569,
    "title": "DBPedia14 Dataset Checksum bug?",
    "dateCreated": "2021-02-18T22:25:48Z",
    "dateModified": "2021-02-18T22:25:48Z",
    "description": "Hi there!!!\r\n\r\nI've been using successfully the DBPedia dataset (https://huggingface.co/datasets/dbpedia_14) with my codebase in the last couple of weeks, but in the last couple of days now I get this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./conditional_classification/basic_pipeline.py\", line 178, in <module>\r\n    main()\r\n  File \"./conditional_classification/basic_pipeline.py\", line 128, in main\r\n    corpus.load_data(limit_train_examples_per_class=args.data_args.train_examples_per_class,\r\n  File \"/home/fp/dev/conditional_classification/conditional_classification/datasets_base.py\", line 83, in load_data\r\n    datasets = load_dataset(self.name, split=dataset_split)\r\n  File \"/home/fp/anaconda3/envs/conditional/lib/python3.8/site-packages/datasets/load.py\", line 609, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/fp/anaconda3/envs/conditional/lib/python3.8/site-packages/datasets/builder.py\", line 526, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/fp/anaconda3/envs/conditional/lib/python3.8/site-packages/datasets/builder.py\", line 586, in _download_and_prepare\r\n    verify_checksums(\r\n  File \"/home/fp/anaconda3/envs/conditional/lib/python3.8/site-packages/datasets/utils/info_utils.py\", line 39, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbQ2Vic1kxMmZZQ1k']\r\n```\r\n\r\nI've seen this has happened before in other datasets as reported in #537.\r\n\r\nI've tried clearing my cache and call again `load_dataset` but still is not working. My same codebase is successfully downloading and using other datasets (e.g. AGNews) without any problem, so I guess something has happened specifically to the DBPedia dataset in the last few days. \r\n\r\nCan you please check if there's a problem with the checksums? \r\n\r\nOr this is related to any other stuff? I've seen that the path in the cache for the dataset is `/home/fp/.cache/huggingface/datasets/d_bpedia14/dbpedia_14/2.0.0/a70413e39e7a716afd0e90c9e53cb053691f56f9ef5fe317bd07f2c368e8e897...` and includes `d_bpedia14` instead maybe of `dbpedia_14`. Was this maybe a bug introduced recently?\r\n\r\nThanks!",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 811405274,
    "title": "Feature Request: Support for Pandas `Categorical`",
    "dateCreated": "2021-02-18T19:46:05Z",
    "dateModified": "2021-02-18T19:46:05Z",
    "description": "```\r\nfrom datasets import Dataset\r\nimport pandas as pd\r\nimport pyarrow\r\n\r\ndf = pd.DataFrame(pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\"))\r\npyarrow.Table.from_pandas(df)\r\nDataset.from_pandas(df)\r\n# Throws NotImplementedError\r\n# TODO(thom) this will need access to the dictionary as well (for labels). I.e. to the py_table\r\n```\r\n\r\nI'm curious if https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L796 could be built out in a way similar to `Sequence`?\r\n\r\ne.g. a `Map` class (or whatever name the maintainers might prefer) that can accept:\r\n\r\n```\r\nindex_type = generate_from_arrow_type(pa_type.index_type)\r\nvalue_type = generate_from_arrow_type(pa_type.value_type)\r\n```\r\n\r\nand then additional code points to modify:\r\n\r\n- FeatureType: https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L694\r\n- A branch to handle Map in get_nested_type: https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L719\r\n- I don't quite understand what `encode_nested_example` does but perhaps a branch there? https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L755\r\n- Similarly, I don't quite understand why `Sequence` is used this way in `generate_from_dict`, but perhaps a branch here? https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L775\r\n\r\nI couldn't find other usages of `Sequence` outside of defining specific datasets, so I'm not sure if that's a comprehensive set of touchpoints.",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 811384174,
    "title": "Standardizing datasets.dtypes",
    "dateCreated": "2021-02-18T19:15:31Z",
    "dateModified": "2021-02-18T19:15:31Z",
    "description": "This PR was further branched off of jdy-str-to-pyarrow-parsing, so it depends on https://github.com/huggingface/datasets/pull/1900 going first for the diff to be up-to-date (I'm not sure if there's a way for me to use jdy-str-to-pyarrow-parsing as a base branch while having it appear in the pull requests here).\r\n\r\nThis moves away from `str(pyarrow.DataType)` as the method of choice for creating dtypes, favoring an explicit mapping to a list of supported Value dtypes.\r\n\r\nI believe in practice this should be backward compatible, since anyone previously using Value() would only have been able to use dtypes that had an identically named pyarrow factory function, which are all explicitly supported here.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 811260904,
    "title": "Fix to_pandas for boolean ArrayXD",
    "dateCreated": "2021-02-18T16:30:46Z",
    "dateModified": "2021-02-18T16:30:46Z",
    "description": "As noticed in #1887 the conversion of a dataset with a boolean ArrayXD feature types fails because of the underlying ListArray conversion to numpy requires `zero_copy_only=False`.\r\n\r\nzero copy is available for all primitive types except booleans\r\nsee https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array.to_numpy\r\nand https://issues.apache.org/jira/browse/ARROW-2871?jql=text%20~%20%22boolean%20to_numpy%22\r\n\r\ncc @SBrandeis ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 811145531,
    "title": "Initial commit for the addition of TIMIT dataset",
    "dateCreated": "2021-02-18T14:23:12Z",
    "dateModified": "2021-02-18T14:23:12Z",
    "description": "Below points needs to be addressed:\r\n\r\n- Creation of dummy dataset is failing\r\n- Need to check on the data representation\r\n- License is not creative commons. Copyright: Portions \u00a9 1993 Trustees of the University of Pennsylvania\r\n\r\nAlso the links (_except the download_) point to the ami corpus! ;-)\r\n\r\n@patrickvonplaten  Requesting your comments, will be happy to address them!",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 810931171,
    "title": "Fix setimes_2 wmt urls",
    "dateCreated": "2021-02-18T09:42:26Z",
    "dateModified": "2021-02-18T09:42:26Z",
    "description": "Continuation of #1901 \r\nSome other urls were missing https",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 810845605,
    "title": "Fix OPUS dataset download errors",
    "dateCreated": "2021-02-18T07:39:41Z",
    "dateModified": "2021-02-18T07:39:41Z",
    "description": "Replace http to https.\r\n\r\nhttps://github.com/huggingface/datasets/issues/854\r\n\r\nhttps://discuss.huggingface.co/t/cannot-download-wmt16/2081\r\n\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 810512488,
    "title": "Issue #1895: Bugfix for string_to_arrow timestamp[ns] support",
    "dateCreated": "2021-02-17T20:26:04Z",
    "dateModified": "2021-02-17T20:26:04Z",
    "description": "Should resolve https://github.com/huggingface/datasets/issues/1895\r\n\r\nThe main part of this PR adds additional parsing in `string_to_arrow` to convert the timestamp dtypes that result from `str(pa_type)` back into the pa.DataType TimestampType.\r\n\r\nWhile adding unit-testing, I noticed that support for the double/float types also don't invert correctly, so I added them, which I believe would hypothetically make this section of `Value` redundant:\r\n\r\n```\r\n    def __post_init__(self):\r\n        if self.dtype == \"double\":  # fix inferred type\r\n            self.dtype = \"float64\"\r\n        if self.dtype == \"float\":  # fix inferred type\r\n            self.dtype = \"float32\"\r\n```\r\n\r\nHowever, since I think Value.dtype is part of the public interface, removing that would result in a backward-incompatible change, so I didn't muck with that.\r\n\r\nThe rest of the PR consists of docstrings that I added while developing locally so I could keep track of which functions were supposed to be inverses of each other, and thought I'd include them initially in case you want to keep them around, but I'm happy to delete or remove any of them at your request!",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 810308332,
    "title": "Fix: ALT - fix duplicated examples in alt-parallel",
    "dateCreated": "2021-02-17T15:53:56Z",
    "dateModified": "2021-02-17T15:53:56Z",
    "description": "As noticed in #1898  by @10-zin the examples of the `alt-paralel` configurations have all the same values for the `translation` field.\r\nThis was due to a bad copy of a python dict.\r\n\r\nThis PR fixes that.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 810157251,
    "title": "ALT dataset has repeating instances in all splits",
    "dateCreated": "2021-02-17T12:51:42Z",
    "dateModified": "2021-02-17T12:51:42Z",
    "description": "The [ALT](https://huggingface.co/datasets/alt) dataset has all the same instances within each split :/\r\nSeemed like a great dataset for some experiments I wanted to carry out, especially since its medium-sized, and has all splits.\r\n\r\nWould be great if this could be fixed :)\r\n\r\nAdded a snapshot of the contents from `explore-datset` feature, for quick reference.\r\n\r\n![image](https://user-images.githubusercontent.com/33179372/108206321-442a2d00-714c-11eb-882f-b4b6e708ef9c.png)\r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 810113263,
    "title": "Fix PandasArrayExtensionArray conversion to native type",
    "dateCreated": "2021-02-17T11:48:24Z",
    "dateModified": "2021-02-17T11:48:24Z",
    "description": "To make the conversion to csv work in #1887 , we need  PandasArrayExtensionArray used for multidimensional numpy arrays to be converted to pandas native types.\r\nHowever previously pandas.core.internals.ExtensionBlock.to_native_types would fail with an PandasExtensionArray because\r\n1. the PandasExtensionArray.isna method was wrong\r\n2. the conversion of a PandasExtensionArray to a numpy array with dtype=object was returning a multidimensional array while pandas excepts a 1D array in this case (more info [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.api.extensions.ExtensionArray.html#pandas.api.extensions.ExtensionArray))\r\n\r\nI fixed these two issues and now the conversion to native types works, and so is the export to csv.\r\ncc @SBrandeis ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 809630271,
    "title": "Bug Report: timestamp[ns] not recognized",
    "dateCreated": "2021-02-16T20:38:04Z",
    "dateModified": "2021-02-16T20:38:04Z",
    "description": "Repro:\r\n\r\n```\r\nfrom datasets import Dataset\r\nimport pandas as pd\r\nimport pyarrow\r\n\r\ndf = pd.DataFrame(pd.date_range(\"2018-01-01\", periods=3, freq=\"H\"))\r\npyarrow.Table.from_pandas(df)\r\nDataset.from_pandas(df)\r\n# Throws ValueError: Neither timestamp[ns] nor timestamp[ns]_ seems to be a pyarrow data type.\r\n```\r\n\r\nThe factory function seems to be just \"timestamp\": https://arrow.apache.org/docs/python/generated/pyarrow.timestamp.html#pyarrow.timestamp\r\n\r\nIt seems like https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L36-L43 could have a little bit of additional structure for handling these cases?  I'd be happy to take a shot at opening a PR if I could receive some guidance on whether parsing something like `timestamp[ns]` and resolving it to timestamp('ns') is the goal of this method.\r\n\r\nAlternatively, if I'm using this incorrectly (e.g. is the expectation that we always provide a schema when timestamps are involved?), that would be very helpful to know as well!\r\n\r\n```\r\n$ pip list  # only the relevant libraries/versions\r\ndatasets                      1.2.1\r\npandas                        1.0.3\r\npyarrow                       3.0.0\r\n```",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 809609654,
    "title": "benchmarking against MMapIndexedDataset",
    "dateCreated": "2021-02-16T20:04:58Z",
    "dateModified": "2021-02-16T20:04:58Z",
    "description": "I am trying to benchmark my datasets based implementation against fairseq's [`MMapIndexedDataset`](https://github.com/pytorch/fairseq/blob/master/fairseq/data/indexed_dataset.py#L365) and finding that, according to psrecord, my `datasets` implem uses about 3% more CPU memory and runs 1% slower for `wikitext103` (~1GB of tokens).\r\n\r\nQuestions:\r\n1) Is this (basically identical) performance expected? \r\n2) Is there a scenario where this library will outperform `MMapIndexedDataset`? (maybe more examples/larger examples?)\r\n3) Should I be using different benchmarking tools than `psrecord`/how do you guys do benchmarks?\r\n\r\nThanks in advance! Sam",
    "status": "open",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 809556503,
    "title": "wmt19 is broken",
    "dateCreated": "2021-02-16T18:39:58Z",
    "dateModified": "2021-02-16T18:39:58Z",
    "description": "1. Check which lang pairs we have: `--dataset_name wmt19`:\r\n\r\nPlease pick one among the available configs: ['cs-en', 'de-en', 'fi-en', 'gu-en', 'kk-en', 'lt-en', 'ru-en', 'zh-en', 'fr-de']\r\n\r\n \r\n2. OK, let's pick `ru-en`:\r\n\r\n`--dataset_name wmt19 --dataset_config \"ru-en\"`\r\n\r\nno cookies:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./run_seq2seq.py\", line 661, in <module>\r\n    main()\r\n  File \"./run_seq2seq.py\", line 317, in main\r\n    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/load.py\", line 740, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py\", line 572, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py\", line 628, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/stas/.cache/huggingface/modules/datasets_modules/datasets/wmt19/436092de5f3faaf0fc28bc84875475b384e90a5470fa6afaee11039ceddc5052/wmt_utils.py\", line 755, in _split_generators\r\n    downloaded_files = dl_manager.download_and_extract(urls_to_download)\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/download_manager.py\", line 276, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/download_manager.py\", line 191, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/py_utils.py\", line 233, in map_nested\r\n    mapped = [\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/py_utils.py\", line 234, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/py_utils.py\", line 190, in _single_map_nested\r\n    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/py_utils.py\", line 190, in <listcomp>\r\n    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/py_utils.py\", line 172, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/download_manager.py\", line 211, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/file_utils.py\", line 274, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/file_utils.py\", line 584, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-ru.tar.gz\r\n```",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 809554174,
    "title": "request to mirror wmt datasets, as they are really slow to download",
    "dateCreated": "2021-02-16T18:36:11Z",
    "dateModified": "2021-02-16T18:36:11Z",
    "description": "Would it be possible to mirror the wmt data files under hf? Some of them take hours to download and not because of the local speed. They are all quite small datasets, just extremely slow to download.\r\n\r\nThank you!",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 809550001,
    "title": "suggestion to improve a missing dataset error",
    "dateCreated": "2021-02-16T18:29:13Z",
    "dateModified": "2021-02-16T18:29:13Z",
    "description": "I was using  `--dataset_name wmt19` all was good. Then thought perhaps wmt20 is out, so I tried to use `--dataset_name wmt20`, got 3 different errors (1 repeated twice), none telling me the real issue - that `wmt20` isn't in the `datasets`:\r\n\r\n```\r\nTrue, predict_with_generate=True)\r\nTraceback (most recent call last):\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/load.py\", line 323, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/file_utils.py\", line 274, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/file_utils.py\", line 584, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/master/datasets/wmt20/wmt20.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/load.py\", line 335, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/file_utils.py\", line 274, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/file_utils.py\", line 584, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/master/datasets/wmt20/wmt20.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./run_seq2seq.py\", line 661, in <module>\r\n    main()\r\n  File \"./run_seq2seq.py\", line 317, in main\r\n    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/load.py\", line 706, in load_dataset\r\n    module_path, hash, resolved_file_path = prepare_module(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/load.py\", line 343, in prepare_module\r\n    raise FileNotFoundError(\r\nFileNotFoundError: Couldn't find file locally at wmt20/wmt20.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/master/datasets/wmt20/wmt20.py.\r\nThe file is also not present on the master branch on github.\r\n```\r\n\r\nSuggestion: if it is not in a local path, check that there is an actual `https://github.com/huggingface/datasets/tree/master/datasets/wmt20` first and assert \"dataset `wmt20` doesn't exist in datasets\", rather than trying to find a load script - since the whole repo is not there.\r\n\r\nThe error occured when running:\r\n```\r\ncd examples/seq2seq\r\nexport BS=16; rm -r output_dir; PYTHONPATH=../../src USE_TF=0 CUDA_VISIBLE_DEVICES=0 python ./run_seq2seq.py --model_name_or_path t5-small --output_dir output_dir --adam_eps 1e-06 --do_eval --evaluation_strategy=steps  --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 --max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir --per_device_eval_batch_size $BS --predict_with_generate --eval_steps 25000  --sortish_sampler --task translation_en_to_ro  --val_max_target_length 128 --warmup_steps 500  --max_val_samples 500 --dataset_name wmt20 --dataset_config \"ro-en\" --source_prefix \"translate English to Romanian: \"\r\n```\r\n\r\nThanks.",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 809395586,
    "title": "Reformat dataset cards section titles",
    "dateCreated": "2021-02-16T15:11:47Z",
    "dateModified": "2021-02-16T15:11:47Z",
    "description": "Titles are formatted like [Foo](#foo) instead of just Foo",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 809276015,
    "title": "Implement to_dict and to_pandas for Dataset",
    "dateCreated": "2021-02-16T12:38:19Z",
    "dateModified": "2021-02-16T12:38:19Z",
    "description": "With options to return a generator or the full dataset",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 809241123,
    "title": "Docs for adding new column on formatted dataset",
    "dateCreated": "2021-02-16T11:45:00Z",
    "dateModified": "2021-02-16T11:45:00Z",
    "description": "As mentioned in #1872 we should add in the documentation how the format gets updated when new columns are added\r\n\r\nClose #1872",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 809229809,
    "title": "Implement to_csv for Dataset",
    "dateCreated": "2021-02-16T11:27:29Z",
    "dateModified": "2021-02-16T11:27:29Z",
    "description": "cc @thomwolf \r\n\r\n`to_csv` supports passing either a file path or a *binary* file object\r\nThe writing is batched to avoid loading the whole table in memory",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 809221885,
    "title": "Common voice",
    "dateCreated": "2021-02-16T11:16:10Z",
    "dateModified": "2021-02-16T11:16:10Z",
    "description": "Started filling out information about the dataset and a dataset card.\r\n\r\nTo do\r\nCreate tagging file\r\nUpdate the common_voice.py file with more information",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 808881501,
    "title": "add missing info on how to add large files",
    "dateCreated": "2021-02-15T23:46:39Z",
    "dateModified": "2021-02-15T23:46:39Z",
    "description": "Thanks to @lhoestq's instructions I was able to add data files to a custom dataset repo. This PR is attempting to tell others how to do the same if they need to.\r\n\r\n@lhoestq ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 808755894,
    "title": "dtype fix when using numpy arrays",
    "dateCreated": "2021-02-15T18:55:25Z",
    "dateModified": "2021-02-15T18:55:25Z",
    "description": "As discussed in #625 this fix lets the user preserve the dtype of numpy array to pyarrow array which was getting lost due to conversion of numpy array -> list -> pyarrow array",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 808750623,
    "title": "Add not-in-place implementations for several dataset transforms",
    "dateCreated": "2021-02-15T18:44:26Z",
    "dateModified": "2021-02-15T18:44:26Z",
    "description": "Should we deprecate in-place versions of such methods?",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 808716576,
    "title": "Create Remote Manager",
    "dateCreated": "2021-02-15T17:36:24Z",
    "dateModified": "2021-02-15T17:36:24Z",
    "description": "Refactoring to separate the concern of remote (HTTP/FTP requests) management.",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 808578200,
    "title": "`list_datasets()` returns a list of strings, not objects",
    "dateCreated": "2021-02-15T14:20:15Z",
    "dateModified": "2021-02-15T14:20:15Z",
    "description": "Here and there in the docs there is still stuff like this:\r\n\r\n```python\r\n>>> datasets_list = list_datasets()\r\n>>> print(', '.join(dataset.id for dataset in datasets_list))\r\n```\r\n\r\nHowever, my understanding is that `list_datasets()` returns a list of strings rather than a list of objects.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 808563439,
    "title": "Update multi_woz_v22 checksums",
    "dateCreated": "2021-02-15T14:00:18Z",
    "dateModified": "2021-02-15T14:00:18Z",
    "description": "As noticed in #1876 the checksums of this dataset are outdated.\r\nI updated them in this PR",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 808541442,
    "title": "Replace flatten_nested",
    "dateCreated": "2021-02-15T13:29:40Z",
    "dateModified": "2021-02-15T13:29:40Z",
    "description": "Replace `flatten_nested` with `NestedDataStructure.flatten`.\r\n\r\nThis is a first step towards having all NestedDataStructure logic as a separated concern, independent of the caller/user of the data structure.\r\n\r\nEventually, all checks (whether the underlying data is list, dict, etc.) will be only inside this class.\r\n\r\nI have also generalized the flattening, and now it handles multiple levels of nesting.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 808526883,
    "title": "Add LJ Speech dataset",
    "dateCreated": "2021-02-15T13:10:42Z",
    "dateModified": "2021-02-15T13:10:42Z",
    "description": "This PR adds the LJ Speech dataset (https://keithito.com/LJ-Speech-Dataset/)\r\nAs requested by #1841 \r\nThe ASR format is based on #1767 \r\n\r\nThere are a couple of quirks that should be addressed:\r\n- I tagged this dataset as `other-other-automatic-speech-recognition` and `other-other-text-to-speech` (as classified by paperswithcode). Since the number of speech datasets is about to grow, maybe these categories should be added to the main list? \r\n- Similarly to #1767 this dataset uses only a single dummy sample to reduce the zip size (`wav`s are quite heavy). Is there a plan to allow LFS or S3 usage for dummy data in the repo?\r\n- The dataset is distributed under the Public Domain license, which is not used anywhere else in the repo, AFAIK. Do you think Public Domain is worth adding to the tagger app as well?\r\n\r\nPinging @patrickvonplaten to review",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 808462272,
    "title": "Allow concatenation of both in-memory and on-disk datasets",
    "dateCreated": "2021-02-15T11:39:46Z",
    "dateModified": "2021-02-15T11:39:46Z",
    "description": "This is a prerequisite for the addition of the `add_item` feature (see #1870).\r\nCurrently there is one assumption that we would need to change: a dataset is either fully in memory (dataset._data_files is empty), or the dataset can be reloaded from disk (using the dataset._data_files).\r\nThis assumption is used for pickling for example:\r\n- in-memory dataset can just be pickled/unpickled in-memory\r\n- on-disk dataset can be unloaded to only keep the filepaths when pickling, and then reloaded from the disk when unpickling\r\n\r\nMaybe let's have a design that allows a Dataset to have a Table that can be rebuilt from heterogenous sources like in-memory tables or on-disk tables ? This could also be further extended in the future\r\n\r\nOne idea would be to define a list of sources and each source implements a way to reload its corresponding pyarrow Table.\r\nThen the dataset would be the concatenation of all these tables.\r\n\r\nDepending on the source type, the serialization using pickle would be different. In-memory data would be copied while on-disk data would simply be replaced by the path to these data.\r\n\r\nIf you have some ideas you would like to share about the design/API feel free to do so :)\r\n\r\ncc @albertvillanova ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 808025859,
    "title": " load_dataset(\"multi_woz_v22\") NonMatchingChecksumError",
    "dateCreated": "2021-02-14T19:14:48Z",
    "dateModified": "2021-02-14T19:14:48Z",
    "description": "Hi, it seems that loading the multi_woz_v22 dataset gives a NonMatchingChecksumError.\r\n\r\nTo reproduce:\r\n\r\n`dataset = load_dataset('multi_woz_v22','v2.2_active_only',split='train')`\r\n\r\n\r\nThis will give the following error:\r\n\r\n```\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/dialog_acts.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_001.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_003.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_004.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_005.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_006.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_007.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_008.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_009.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_010.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_012.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_013.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_014.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_015.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_016.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_017.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/dev/dialogues_001.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/dev/dialogues_002.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/test/dialogues_001.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/test/dialogues_002.json']\r\n```\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 807887267,
    "title": "Adding sari metric",
    "dateCreated": "2021-02-14T04:38:35Z",
    "dateModified": "2021-02-14T04:38:35Z",
    "description": "Adding SARI metric that is used in evaluation of text simplification. This is required as part of the GEM benchmark.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 807786094,
    "title": "Adding Europarl Bilingual dataset",
    "dateCreated": "2021-02-13T17:02:04Z",
    "dateModified": "2021-02-13T17:02:04Z",
    "description": "Implementation of Europarl bilingual dataset from described [here](https://opus.nlpl.eu/Europarl.php).\r\n\r\nThis dataset allows to use every language pair detailed in the original dataset. The loading script manages also the small errors contained in the original dataset (in very rare cases (1 over 10M) there are some keys that references to inexistent sentences).\r\nI chose to follow the the style of a similar dataset available in this repository: `multi_para_crawl`.\r\n",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 807750745,
    "title": "add iapp_wiki_qa_squad",
    "dateCreated": "2021-02-13T13:34:27Z",
    "dateModified": "2021-02-13T13:34:27Z",
    "description": "`iapp_wiki_qa_squad` is an extractive question answering dataset from Thai Wikipedia articles.\r\nIt is adapted from [the original iapp-wiki-qa-dataset](https://github.com/iapp-technology/iapp-wiki-qa-dataset)\r\nto [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) format, resulting in\r\n5761/742/739 questions from 1529/191/192 articles.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 807711935,
    "title": "Adding a new column to the dataset after set_format was called",
    "dateCreated": "2021-02-13T09:14:35Z",
    "dateModified": "2021-02-13T09:14:35Z",
    "description": "Hi, \r\n\r\nthanks for the nice library. I'm in the process of creating a custom dataset, which has a mix of tensors and lists of strings. I stumbled upon an error and want to know if its a problem on my side. \r\n\r\nI load some lists of strings and integers, then call `data.set_format(\"torch\", columns=[\"some_integer_column1\", \"some_integer_column2\"], output_all_columns=True)`. This converts the integer columns into tensors, but keeps the lists of strings as they are. I then call `map` to add a new column to my dataset, which is a **list of strings**. Once I iterate through my dataset, I get an error that the new column can't be converted into a tensor (which is probably caused by `set_format`). \r\n\r\nBelow some pseudo code:\r\n```python\r\n    def augment_func(sample: Dict) -> Dict:\r\n        # do something\r\n        return {\r\n         \"some_integer_column1\" : augmented_data[\"some_integer_column1\"],  # <-- tensor\r\n         \"some_integer_column2\" : augmented_data[\"some_integer_column2\"],  # <-- tensor\r\n         \"NEW_COLUMN\": targets,  # <-- list of strings\r\n        }\r\n\r\n\r\n    data = datasets.load_dataset(__file__, data_dir=\"...\", split=\"train\")\r\n    data.set_format(\"torch\", columns=[\"some_integer_column1\", \"some_integer_column2\"], output_all_columns=True)\r\n\r\n    augmented_dataset = data.map(augment_func, batched=False)\r\n    \r\n    for sample in augmented_dataset:\r\n        print(sample)  # fails\r\n\r\n```\r\n\r\nand the exception:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"dataset.py\", line 487, in <module>\r\n    main()\r\n  File \"dataset.py\", line 471, in main\r\n    for sample in augmented_dataset:\r\n  File \"lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 697, in __iter__\r\n    yield self._getitem(\r\n  File \"lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1069, in _getitem\r\n    outputs = self._convert_outputs(\r\n  File \"lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 890, in _convert_outputs\r\n    v = map_nested(command, v, **map_nested_kwargs)\r\n  File \"lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 225, in map_nested\r\n    return function(data_struct)\r\n  File \"lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 850, in command\r\n    return [map_nested(command, i, **map_nested_kwargs) for i in x]\r\n  File \"lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 850, in <listcomp>\r\n    return [map_nested(command, i, **map_nested_kwargs) for i in x]\r\n  File \"lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 225, in map_nested\r\n    return function(data_struct)\r\n  File \"lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 850, in command\r\n    return [map_nested(command, i, **map_nested_kwargs) for i in x]\r\n  File \"lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 850, in <listcomp>\r\n    return [map_nested(command, i, **map_nested_kwargs) for i in x]\r\n  File \"lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 225, in map_nested\r\n    return function(data_struct)\r\n  File \"lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 851, in command\r\n    return torch.tensor(x, **format_kwargs)\r\nTypeError: new(): invalid data type 'str'\r\n```\r\n\r\nThanks!\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 807697671,
    "title": "Add newspop dataset",
    "dateCreated": "2021-02-13T07:31:23Z",
    "dateModified": "2021-02-13T07:31:23Z",
    "description": "",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 807306564,
    "title": "Implement Dataset add_item",
    "dateCreated": "2021-02-12T15:03:46Z",
    "dateModified": "2021-02-12T15:03:46Z",
    "description": "Implement `Dataset.add_item`.\r\n\r\nClose #1854.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 807159835,
    "title": "Remove outdated commands in favor of huggingface-cli",
    "dateCreated": "2021-02-12T11:28:10Z",
    "dateModified": "2021-02-12T11:28:10Z",
    "description": "Removing the old user commands since `huggingface_hub` is going to be used instead.\r\ncc @julien-c ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 807138159,
    "title": "Update oscar sizes",
    "dateCreated": "2021-02-12T10:55:35Z",
    "dateModified": "2021-02-12T10:55:35Z",
    "description": "This commit https://github.com/huggingface/datasets/commit/837a152e4724adc5308e2c4481908c00a8d93383 removed empty lines from the oscar deduplicated datasets. This PR updates the size of each deduplicated dataset to fix possible `NonMatchingSplitsSizesError` errors. cc @cahya-wirawan",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 807127181,
    "title": "ERROR WHEN USING SET_TRANSFORM() ",
    "dateCreated": "2021-02-12T10:38:31Z",
    "dateModified": "2021-02-12T10:38:31Z",
    "description": "Hi, I'm trying to use dataset.set_transform(encode) as @lhoestq told me in this issue: https://github.com/huggingface/datasets/issues/1825#issuecomment-774202797\r\n\r\nHowever, when I try to use Trainer from transformers with such dataset, it throws an error:\r\n\r\n```\r\nTypeError: __init__() missing 1 required positional argument: 'transform'\r\n[INFO|trainer.py:357] 2021-02-12 10:18:09,893 >> The following columns in the training set don't have a corresponding argument in `AlbertForMaskedLM.forward` and have been ignored: text.\r\nException in device=TPU:0: __init__() missing 1 required positional argument: 'transform'\r\nTraceback (most recent call last):\r\n  File \"/anaconda3/envs/torch-xla-1.7/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\r\n    _start_fn(index, pf_cfg, fn, args)\r\n  File \"/anaconda3/envs/torch-xla-1.7/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 324, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/home/alejandro_vaca/transformers/examples/language-modeling/run_mlm_wwm.py\", line 368, in _mp_fn\r\n    main()\r\n  File \"/home/alejandro_vaca/transformers/examples/language-modeling/run_mlm_wwm.py\", line 332, in main\r\n    data_collator=data_collator,\r\n  File \"/anaconda3/envs/torch-xla-1.7/lib/python3.6/site-packages/transformers/trainer.py\", line 286, in __init__\r\n    self._remove_unused_columns(self.train_dataset, description=\"training\")\r\n  File \"/anaconda3/envs/torch-xla-1.7/lib/python3.6/site-packages/transformers/trainer.py\", line 359, in _remove_unused_columns\r\n    dataset.set_format(type=dataset.format[\"type\"], columns=columns)\r\n  File \"/home/alejandro_vaca/datasets/src/datasets/fingerprint.py\", line 312, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/home/alejandro_vaca/datasets/src/datasets/arrow_dataset.py\", line 818, in set_format\r\n    _ = get_formatter(type, **format_kwargs)\r\n  File \"/home/alejandro_vaca/datasets/src/datasets/formatting/__init__.py\", line 112, in get_formatter\r\n    return _FORMAT_TYPES[format_type](**format_kwargs)\r\nTypeError: __init__() missing 1 required positional argument: 'transform'\r\n```\r\n\r\nThe code I'm using:\r\n\r\n```{python}\r\n\r\n    def tokenize_function(examples):\r\n        # Remove empty lines\r\n        examples[\"text\"] = [line for line in examples[\"text\"] if len(line) > 0 and not line.isspace()]\r\n        return tokenizer(examples[\"text\"], padding=padding, truncation=True, max_length=data_args.max_seq_length)\r\n\r\n    datasets.set_transform(tokenize_function)\r\n\r\n    data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm_probability=data_args.mlm_probability)\r\n\r\n    # Initialize our Trainer\r\n    trainer = Trainer(\r\n        model=model,\r\n        args=training_args,\r\n        train_dataset=datasets[\"train\"] if training_args.do_train else None,\r\n        eval_dataset=datasets[\"val\"] if training_args.do_eval else None,\r\n        tokenizer=tokenizer,\r\n        data_collator=data_collator,\r\n    )\r\n```\r\n\r\nI've installed from source, master branch.\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 807017816,
    "title": "Add dataset for Financial PhraseBank",
    "dateCreated": "2021-02-12T07:30:56Z",
    "dateModified": "2021-02-12T07:30:56Z",
    "description": "",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 806388290,
    "title": "Updated OPUS Open Subtitles Dataset with metadata information",
    "dateCreated": "2021-02-11T13:26:26Z",
    "dateModified": "2021-02-11T13:26:26Z",
    "description": "Close #1844 \r\n\r\nProblems:\r\n- I ran `python datasets-cli test datasets/open_subtitles --save_infos --all_configs`, hence the change in `dataset_infos.json`, but it appears that the metadata features have not been added for all pairs. Any idea why that might be?\r\n- Possibly related to the above, I tried doing `pip uninstall datasets && pip install -e \".[dev]\"` after the changes, and loading the dataset via `load_dataset(\"open_subtitles\", lang1='hi', lang2='it')` to check if the update worked, but the loaded dataset did not contain the metadata fields (neither in the features nor doing `next(iter(dataset['train']))`). What step(s) did I miss?\r\n\r\nQuestions:\r\n- Is it ok to have a `classmethod` in there? I have not seen any in the few other datasets I have checked. I could make it a local method of the `_generate_examples` method, but I'd rather not duplicate the logic...",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 806172843,
    "title": "Add Winogender Schemas",
    "dateCreated": "2021-02-11T08:18:38Z",
    "dateModified": "2021-02-11T08:18:38Z",
    "description": "## Adding a Dataset\r\n- **Name:** Winogender Schemas\r\n- **Description:** Winogender Schemas (inspired by Winograd Schemas) are minimal pairs of sentences that differ only by the gender of one pronoun in the sentence, designed to test for the presence of gender bias in automated coreference resolution systems.\r\n- **Paper:** https://arxiv.org/abs/1804.09301\r\n- **Data:** https://github.com/rudinger/winogender-schemas (see data directory)\r\n- **Motivation:** Testing gender bias in automated coreference resolution systems, improve coreference resolution in general.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 806171311,
    "title": "Add WikiCREM",
    "dateCreated": "2021-02-11T08:16:00Z",
    "dateModified": "2021-02-11T08:16:00Z",
    "description": "## Adding a Dataset\r\n- **Name:** WikiCREM\r\n- **Description:** A large unsupervised corpus for coreference resolution.\r\n- **Paper:** https://arxiv.org/abs/1905.06290\r\n- **Github repo:**: https://github.com/vid-koci/bert-commonsense\r\n- **Data:** https://ora.ox.ac.uk/objects/uuid:c83e94bb-7584-41a1-aef9-85b0e764d9e3\r\n- **Motivation:** Coreference resolution, common sense reasoning\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 805722293,
    "title": "Fix writing GPU Faiss index",
    "dateCreated": "2021-02-10T17:32:03Z",
    "dateModified": "2021-02-10T17:32:03Z",
    "description": "As reported in by @corticalstack there is currently an error when we try to save a faiss index on GPU.\r\n\r\nI fixed that by checking the index `getDevice()` method before calling `index_gpu_to_cpu`\r\n\r\nClose #1859 ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 805631215,
    "title": "Fix Limit url",
    "dateCreated": "2021-02-10T15:44:56Z",
    "dateModified": "2021-02-10T15:44:56Z",
    "description": "The test.json file of the Literal-Motion-in-Text (LiMiT) dataset was removed recently on the master branch of the repo at https://github.com/ilmgut/limit_dataset\r\n\r\nThis PR uses the previous commit sha to download the file instead, as suggested by @Paethon\r\n\r\nClose #1836 ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 805510037,
    "title": "Add loading from the Datasets Hub + add relative paths in download manager",
    "dateCreated": "2021-02-10T13:24:11Z",
    "dateModified": "2021-02-10T13:24:11Z",
    "description": "With the new Datasets Hub on huggingface.co it's now possible to have a dataset repo with your own script and data.\r\nFor example: https://huggingface.co/datasets/lhoestq/custom_squad/tree/main contains one script and two json files.\r\n\r\nYou can load it using\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nd = load_dataset(\"lhoestq/custom_squad\")\r\n```\r\n\r\nTo be able to use the data files that live right next to the dataset script on the repo in the hub, I added relative paths support for the DownloadManager. For example in the repo mentioned above, there are two json files that can be downloaded via\r\n```python\r\n_URLS = {\r\n    \"train\": \"train-v1.1.json\",\r\n    \"dev\": \"dev-v1.1.json\",\r\n}\r\ndownloaded_files = dl_manager.download_and_extract(_URLS)\r\n```\r\n\r\nTo make it work, I set the `base_path` of the DownloadManager to be the parent path of the dataset script (which comes from either a local path or a remote url).\r\n\r\nI also had to add the auth header of the requests to huggingface.co for private datasets repos. The token is fetched from [huggingface_hub](https://github.com/huggingface/huggingface_hub).",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 805479025,
    "title": "Error \"in void don't know how to serialize this type of index\" when saving index to disk when device=0 (GPU)",
    "dateCreated": "2021-02-10T12:41:00Z",
    "dateModified": "2021-02-10T12:41:00Z",
    "description": "Error serializing faiss index.  Error as follows:\r\n\r\n`Error in void faiss::write_index(const faiss::Index*, faiss::IOWriter*) at /home/conda/feedstock_root/build_artifacts/faiss-split_1612472484670/work/faiss/impl/index_write.cpp:453: don't know how to serialize this type of index`\r\n\r\n\r\nNote:\r\n\r\n`torch.cuda.is_available()` reports:\r\n\r\n```\r\nCuda is available\r\ncuda:0\r\n\r\n```\r\n\r\nAdding index, device=0 for GPU.\r\n\r\n`dataset.add_faiss_index(column='embeddings', index_name='idx_embeddings', device=0)`\r\n\r\nHowever, during a quick debug, self.faiss_index has no attr \"device\" when checked in` search.py, method save`, so fails to transform gpu index to cpu index.  If I add index without device, index is saved OK.\r\n\r\n\r\n```\r\ndef save(self, file: str):\r\n        \"\"\"Serialize the FaissIndex on disk\"\"\"\r\n        import faiss  # noqa: F811\r\n\r\n        if (\r\n            hasattr(self.faiss_index, \"device\")\r\n            and self.faiss_index.device is not None\r\n            and self.faiss_index.device > -1\r\n        ):\r\n            index = faiss.index_gpu_to_cpu(self.faiss_index)\r\n        else:\r\n            index = self.faiss_index\r\n        faiss.write_index(index, file)\r\n```\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 805477774,
    "title": "Clean config getenvs",
    "dateCreated": "2021-02-10T12:39:14Z",
    "dateModified": "2021-02-10T12:39:14Z",
    "description": "Following #1848 \r\nRemove double getenv calls and fix one issue with rarfile\r\n\r\ncc @albertvillanova ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 805391107,
    "title": "Unable to upload \"community provided\" dataset - 400 Client Error",
    "dateCreated": "2021-02-10T10:39:01Z",
    "dateModified": "2021-02-10T10:39:01Z",
    "description": "Hi,\r\ni'm trying to a upload a dataset as described [here](https://huggingface.co/docs/datasets/v1.2.0/share_dataset.html#sharing-a-community-provided-dataset). This is what happens:\r\n\r\n``` \r\n$ datasets-cli login\r\n$ datasets-cli upload_dataset my_dataset\r\nAbout to upload file /path/to/my_dataset/dataset_infos.json to S3 under filename my_dataset/dataset_infos.json and namespace username\r\nAbout to upload file /path/to/my_dataset/my_dataset.py to S3 under filename my_dataset/my_dataset.py and namespace username\r\nProceed? [Y/n] Y\r\nUploading... This might take a while if files are large\r\n400 Client Error: Bad Request for url: https://huggingface.co/api/datasets/presign\r\nhuggingface.co migrated to a new model hosting system.\r\nYou need to upgrade to transformers v3.5+ to upload new models.\r\nMore info at https://discuss.hugginface.co or https://twitter.com/julien_c. Thank you! \r\n```\r\nI'm using the latest releases of datasets and transformers.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 805360200,
    "title": "load_dataset(\"amazon_polarity\") NonMatchingChecksumError",
    "dateCreated": "2021-02-10T10:00:56Z",
    "dateModified": "2021-02-10T10:00:56Z",
    "description": "Hi, it seems that loading the amazon_polarity dataset gives a NonMatchingChecksumError.\r\n\r\nTo reproduce:\r\n```\r\nload_dataset(\"amazon_polarity\")\r\n```\r\nThis will give the following error:\r\n```\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-3-8559a03fe0f8> in <module>()\r\n----> 1 dataset = load_dataset(\"amazon_polarity\")\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     37     if len(bad_urls) > 0:\r\n     38         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 39         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     40     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     41 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/u/0/uc?id=0Bz8a_Dbh9QhbaW12WVVZS2drcnM&export=download']\r\n```",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 805256579,
    "title": "Minor fix in the docs",
    "dateCreated": "2021-02-10T07:27:43Z",
    "dateModified": "2021-02-10T07:27:43Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 805204397,
    "title": "Feature Request: Dataset.add_item",
    "dateCreated": "2021-02-10T06:06:00Z",
    "dateModified": "2021-02-10T06:06:00Z",
    "description": "I'm trying to integrate `huggingface/datasets` functionality into `fairseq`, which requires (afaict) being able to build a dataset through an `add_item` method, such as https://github.com/pytorch/fairseq/blob/master/fairseq/data/indexed_dataset.py#L318, as opposed to loading all the text into arrow, and then `dataset.map(binarizer)`.\r\nIs this possible at the moment? Is there an example? I'm happy to use raw `pa.Table` but not sure whether it will support uneven length entries.\r\n\r\n### Desired API\r\n\r\n```python\r\nimport numpy as np\r\ntokenized: List[np.NDArray[np.int64]] = [np.array([4,4,2]), np.array([8,6,5,5,2]), np.array([3,3,31,5])\r\n\r\ndef build_dataset_from_tokenized(tokenized: List[np.NDArray[int]]) -> Dataset:\r\n   \"\"\"FIXME\"\"\"\r\n   dataset = EmptyDataset()\r\n   for t in tokenized: dataset.append(t)\r\n   return dataset\r\nds = build_dataset_from_tokenized(tokenized)\r\nassert (ds[0] == np.array([4,4,2])).all()\r\n```\r\n\r\n### What I tried\r\ngrep, google for \"add one entry at a time\", \"datasets.append\"\r\n\r\n### Current Code\r\nThis code achieves the same result but doesn't fit into the `add_item` abstraction.\r\n\r\n```python\r\n    dataset = load_dataset('text', data_files={'train': 'train.txt'})\r\n    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length=4096)\r\n    def tokenize_function(examples):\r\n        ids = tokenizer(examples['text'], return_attention_mask=False)['input_ids']\r\n        return {'input_ids': [x[1:] for x in ids]}\r\n    ds = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=['text'], load_from_cache_file=not overwrite_cache)\r\n\tprint(ds['train'][0]) => np array\r\n```\r\n\r\nThanks in advance!",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 804791166,
    "title": "Configure library root logger at the module level",
    "dateCreated": "2021-02-09T18:11:12Z",
    "dateModified": "2021-02-09T18:11:12Z",
    "description": "Configure library root logger at the datasets.logging module level (singleton-like).\r\n\r\nBy doing it this way:\r\n- we are sure configuration is done only once: module level code is only runned once\r\n- no need of global variable\r\n- no need of threading lock",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 804633033,
    "title": "Add Arabic Speech Corpus ",
    "dateCreated": "2021-02-09T15:02:26Z",
    "dateModified": "2021-02-09T15:02:26Z",
    "description": "",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 804523174,
    "title": "set bert_score version dependency",
    "dateCreated": "2021-02-09T12:51:07Z",
    "dateModified": "2021-02-09T12:51:07Z",
    "description": "Set the bert_score version in requirements since previous versions of bert_score will fail with datasets (closes #843)",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 804412249,
    "title": "Add cord 19 dataset",
    "dateCreated": "2021-02-09T10:22:08Z",
    "dateModified": "2021-02-09T10:22:08Z",
    "description": "Initial version only reading the metadata in CSV.\r\n\r\n### Checklist:\r\n- [x] Create the dataset script /datasets/my_dataset/my_dataset.py using the template\r\n- [x] Fill the _DESCRIPTION and _CITATION variables\r\n- [x] Implement _infos(), _split_generators() and _generate_examples()\r\n- [x] Make sure that the BUILDER_CONFIGS class attribute is filled with the different configurations of the dataset and that the BUILDER_CONFIG_CLASS is specified if there is a custom config class.\r\n- [x] Generate the metadata file dataset_infos.json for all configurations\r\n- [x] Generate the dummy data dummy_data.zip files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card README.md using the template and at least fill the tags\r\n- [x] Both tests for the real data and the dummy data pass.\r\n\r\n### Extras:\r\n- [x] add more metadata\r\n- [x] add full text\r\n- [x] add pre-computed document embedding",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 804292971,
    "title": "Add TIMIT",
    "dateCreated": "2021-02-09T07:29:41Z",
    "dateModified": "2021-02-09T07:29:41Z",
    "description": "## Adding a Dataset\r\n- **Name:** *TIMIT*\r\n- **Description:** *The TIMIT corpus of read speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems*\r\n\r\n- **Paper:** *Homepage*: http://groups.inf.ed.ac.uk/ami/corpus/ / *Wikipedia*: https://en.wikipedia.org/wiki/TIMIT\r\n- **Data:** *https://deepai.org/dataset/timit*\r\n- **Motivation:** Important speech dataset\r\n\r\n\r\nIf interested in tackling this issue, feel free to tag @patrickvonplaten\r\n\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 803826506,
    "title": "Refactoring: Create config module",
    "dateCreated": "2021-02-08T18:43:51Z",
    "dateModified": "2021-02-08T18:43:51Z",
    "description": "Refactorize configuration settings into their own module.\r\n\r\nThis could be seen as a Pythonic singleton-like approach. Eventually a config instance class might be created.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 803824694,
    "title": "[Metrics] Add word error metric metric",
    "dateCreated": "2021-02-08T18:41:15Z",
    "dateModified": "2021-02-08T18:41:15Z",
    "description": "This PR adds the word error rate metric to datasets. \r\nWER: https://en.wikipedia.org/wiki/Word_error_rate\r\nfor speech recognition. WER is the main metric used in ASR. \r\n\r\n`jiwer` seems to be a solid library (see https://github.com/asteroid-team/asteroid/pull/329#discussion_r525158939)",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 803806380,
    "title": "Make DownloadManager downloaded/extracted paths accessible",
    "dateCreated": "2021-02-08T18:14:42Z",
    "dateModified": "2021-02-08T18:14:42Z",
    "description": "Make accessible the file paths downloaded/extracted by DownloadManager.\r\n\r\nClose #1831.\r\n\r\nThe approach:\r\n- I set these paths as DownloadManager attributes: these are DownloadManager's concerns\r\n- To access to these from DatasetBuilder, I set the DownloadManager instance as DatasetBuilder attribute: object composition",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 803714493,
    "title": "Enable logging propagation and remove logging handler",
    "dateCreated": "2021-02-08T16:22:13Z",
    "dateModified": "2021-02-08T16:22:13Z",
    "description": "We used to have logging propagation disabled because of this issue: https://github.com/tensorflow/tensorflow/issues/26691\r\nBut since it's now fixed we should re-enable it. This is important to keep the default logging behavior for users, and propagation is also needed for pytest fixtures as asked in #1826 \r\n\r\nI also removed the handler that was added since, according to the logging [documentation](https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library):\r\n> It is strongly advised that you do not add any handlers other than NullHandler to your library\u2019s loggers. This is because the configuration of handlers is the prerogative of the application developer who uses your library. The application developer knows their target audience and what handlers are most appropriate for their application: if you add handlers \u2018under the hood\u2019, you might well interfere with their ability to carry out unit tests and deliver logs which suit their requirements.\r\n\r\nIt could have been useful if we wanted to have a custom formatter for the logging but I think it's more important to keep the logging as default to not interfere with the users' logging management.\r\n\r\nTherefore I also removed the two methods `datasets.logging.enable_default_handler` and `datasets.logging.disable_default_handler`.\r\n\r\ncc @albertvillanova this should let you use capsys/caplog in pytest\r\ncc @LysandreJik @sgugger if you want to do the same in `transformers`",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 803588125,
    "title": "Update Open Subtitles corpus with original sentence IDs",
    "dateCreated": "2021-02-08T13:55:13Z",
    "dateModified": "2021-02-08T13:55:13Z",
    "description": "Hi! It would be great if you could add the original sentence ids to [Open Subtitles](https://huggingface.co/datasets/open_subtitles).\r\n\r\nI can think of two reasons: first, it's possible to gather sentences for an entire document (the original ids contain media id, subtitle file id and sentence id), therefore somewhat allowing for document-level machine translation (and other document-level stuff which could be cool to have); second, it's possible to have parallel sentences in multiple languages, as they share the same ids across bitexts.\r\n\r\nI think I should tag @abhishekkrthakur as he's the one who added it in the first place.\r\n\r\nThanks!",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 803565393,
    "title": "MustC Speech Translation",
    "dateCreated": "2021-02-08T13:27:45Z",
    "dateModified": "2021-02-08T13:27:45Z",
    "description": "## Adding a Dataset\r\n- **Name:** *IWSLT19*\r\n- **Description:** *The Speech Translation Task addresses the translation of English audio into German and Portuguese text.*\r\n- **Hompage:** *https://sites.google.com/view/iwslt-evaluation-2019/speech-translation*\r\n- **Data:** *https://sites.google.com/view/iwslt-evaluation-2019/speech-translation* - all data under \"Allowed Training Data\" and \"Development and Evalutaion Data for TED/How2\"\r\n- **Motivation:** Important speech dataset\r\n\r\nIf interested in tackling this issue, feel free to tag @patrickvonplaten\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 803563149,
    "title": "Add AMI Corpus",
    "dateCreated": "2021-02-08T13:25:00Z",
    "dateModified": "2021-02-08T13:25:00Z",
    "description": "## Adding a Dataset\r\n- **Name:** *AMI*\r\n- **Description:** *The AMI Meeting Corpus is a multi-modal data set consisting of 100 hours of meeting recordings. For a gentle introduction to the corpus, see the corpus overview. To access the data, follow the directions given there. Around two-thirds of the data has been elicited using a scenario in which the participants play different roles in a design team, taking a design project from kick-off to completion over the course of a day. The rest consists of naturally occurring meetings in a range of domains. Detailed information can be found in the documentation section.*\r\n\r\n- **Paper:** *Homepage*: http://groups.inf.ed.ac.uk/ami/corpus/\r\n- **Data:** *http://groups.inf.ed.ac.uk/ami/download/* - Select all cases in 1) and select \"Individual Headsets\" & \"Microphone array\" for 2)\r\n- **Motivation:** Important speech dataset\r\n\r\n\r\nIf interested in tackling this issue, feel free to tag @patrickvonplaten\r\n\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 803561123,
    "title": "Add ljspeech",
    "dateCreated": "2021-02-08T13:22:26Z",
    "dateModified": "2021-02-08T13:22:26Z",
    "description": "## Adding a Dataset\r\n- **Name:** *ljspeech*\r\n- **Description:** *This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours.\r\n\r\nThe texts were published between 1884 and 1964, and are in the public domain. The audio was recorded in 2016-17 by the LibriVox project and is also in the public domain.)*\r\n- **Paper:** *Homepage*: https://keithito.com/LJ-Speech-Dataset/\r\n- **Data:** *https://keithito.com/LJ-Speech-Dataset/*\r\n- **Motivation:** Important speech dataset\r\n- **TFDatasets Implementation**: https://www.tensorflow.org/datasets/catalog/ljspeech\r\nIf interested in tackling this issue, feel free to tag @patrickvonplaten\r\n\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 803560039,
    "title": "Add common voice",
    "dateCreated": "2021-02-08T13:21:05Z",
    "dateModified": "2021-02-08T13:21:05Z",
    "description": "## Adding a Dataset\r\n- **Name:** *common voice*\r\n- **Description:** *Mozilla Common Voice Dataset*\r\n- **Paper:** Homepage: https://voice.mozilla.org/en/datasets\r\n- **Data:** https://voice.mozilla.org/en/datasets\r\n- **Motivation:** Important speech dataset\r\n- **TFDatasets Implementation**: https://www.tensorflow.org/datasets/catalog/common_voice\r\nIf interested in tackling this issue, feel free to tag @patrickvonplaten\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 803559164,
    "title": "Add Voxforge",
    "dateCreated": "2021-02-08T13:19:56Z",
    "dateModified": "2021-02-08T13:19:56Z",
    "description": "## Adding a Dataset\r\n- **Name:** *voxforge* \r\n- **Description:** *VoxForge is a language classification dataset. It consists of user submitted audio clips submitted to the website. In this release, data from 6 languages is collected - English, Spanish, French, German, Russian, and Italian. Since the website is constantly updated, and for the sake of reproducibility, this release contains only recordings submitted prior to 2020-01-01. The samples are splitted between train, validation and testing so that samples from each speaker belongs to exactly one split.*\r\n- **Paper:** *Homepage*: http://www.voxforge.org/\r\n- **Data:** *http://www.voxforge.org/home/downloads*\r\n- **Motivation:** Important speech dataset\r\n- **TFDatasets Implementation**: https://www.tensorflow.org/datasets/catalog/voxforge\r\nIf interested in tackling this issue, feel free to tag @patrickvonplaten\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 803557521,
    "title": "Add tedlium",
    "dateCreated": "2021-02-08T13:17:52Z",
    "dateModified": "2021-02-08T13:17:52Z",
    "description": "## Adding a Dataset\r\n- **Name:** *tedlium*\r\n- **Description:** *The TED-LIUM 1-3 corpus is English-language TED talks, with transcriptions, sampled at 16kHz. It contains about 118 hours of speech.*\r\n- **Paper:** Homepage: http://www.openslr.org/7/, https://lium.univ-lemans.fr/en/ted-lium2/ &, https://www.openslr.org/51/\r\n- **Data:** http://www.openslr.org/7/\r\n- **Motivation:** Important speech dataset\r\n- **TFDatasets Implementation**: https://www.tensorflow.org/datasets/catalog/tedlium\r\nIf interested in tackling this issue, feel free to tag @patrickvonplaten\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 803555650,
    "title": "Add VCTK",
    "dateCreated": "2021-02-08T13:15:28Z",
    "dateModified": "2021-02-08T13:15:28Z",
    "description": "## Adding a Dataset\r\n- **Name:** *VCTK*\r\n- **Description:** *This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive.*\r\n- **Paper:** Homepage: https://datashare.ed.ac.uk/handle/10283/3443\r\n- **Data:** https://datashare.ed.ac.uk/handle/10283/3443\r\n- **Motivation:** Important speech dataset\r\n- **TFDatasets Implementation**: https://www.tensorflow.org/datasets/catalog/vctk\r\n\r\nIf interested in tackling this issue, feel free to tag @patrickvonplaten\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 803531837,
    "title": "test.json has been removed from the limit dataset repo (breaks dataset)",
    "dateCreated": "2021-02-08T12:45:53Z",
    "dateModified": "2021-02-08T12:45:53Z",
    "description": "https://github.com/huggingface/datasets/blob/16042b233dbff2a7585110134e969204c69322c3/datasets/limit/limit.py#L51\r\n\r\nThe URL is not valid anymore since test.json has been removed in master for some reason. Directly referencing the last commit works:\r\n\r\n`https://raw.githubusercontent.com/ilmgut/limit_dataset/0707d3989cd8848f0f11527c77dcf168fefd2b23/data`",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 803524790,
    "title": "Add CHiME4 dataset",
    "dateCreated": "2021-02-08T12:36:38Z",
    "dateModified": "2021-02-08T12:36:38Z",
    "description": "## Adding a Dataset\r\n- **Name:** Chime4\r\n- **Description:** Chime4 is a dataset for automatic speech recognition. It is especially useful for evaluating models in a noisy environment and for multi-channel ASR\r\n- **Paper:** Dataset comes from a channel: http://spandh.dcs.shef.ac.uk/chime_challenge/CHiME4/ . Results paper: \r\n- **Data:** http://spandh.dcs.shef.ac.uk/chime_challenge/CHiME4/download.html\r\n- **Motivation:** So far there are very little datasets for speech in `datasets`. Only `lbirispeech_asr` so far.\r\n\r\nIf interested in tackling this issue, feel free to tag @patrickvonplaten\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 803517094,
    "title": "Fixes base_url of limit dataset",
    "dateCreated": "2021-02-08T12:26:35Z",
    "dateModified": "2021-02-08T12:26:35Z",
    "description": "`test.json` is not available in the master branch of the repository anymore. Linking to a specific commit.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 803120978,
    "title": "Add OSCAR dataset card",
    "dateCreated": "2021-02-08T01:39:49Z",
    "dateModified": "2021-02-08T01:39:49Z",
    "description": "I added more information and completed the dataset card for OSCAR which was started by @lhoestq in his previous [PR](https://github.com/huggingface/datasets/pull/1824).",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 802880897,
    "title": "Looks like nokogumbo is up-to-date now, so this is no longer needed.",
    "dateCreated": "2021-02-07T06:52:07Z",
    "dateModified": "2021-02-07T06:52:07Z",
    "description": "Looks like nokogumbo is up-to-date now, so this is no longer needed.\n\n__Originally posted by @dependabot in https://github.com/discourse/discourse/pull/11373#issuecomment-738993432__",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 802868854,
    "title": "Some question about raw dataset download info in the project .",
    "dateCreated": "2021-02-07T05:33:36Z",
    "dateModified": "2021-02-07T05:33:36Z",
    "description": "Hi , i review the code in \r\nhttps://github.com/huggingface/datasets/blob/master/datasets/conll2003/conll2003.py\r\nin the _split_generators function is the truly logic of download raw datasets with dl_manager\r\nand use Conll2003 cls by use import_main_class in load_dataset function\r\nMy question is that , with this logic it seems that i can not have the raw dataset download location\r\nin variable in downloaded_files in _split_generators.\r\nIf someone also want use huggingface datasets as raw dataset downloader,\r\nhow can he retrieve the raw dataset download path from attributes in \r\ndatasets.dataset_dict.DatasetDict ?",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 802790075,
    "title": "using map on loaded Tokenizer 10x - 100x slower than default Tokenizer?",
    "dateCreated": "2021-02-06T21:00:26Z",
    "dateModified": "2021-02-06T21:00:26Z",
    "description": "This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I'm only showing snippets but I can share more) and the map function ran much slower: \r\n\r\n````\r\ndef save_tokenizer(original_tokenizer,text,path=\"simpledata/tokenizer\"):\r\n    words_unique = set(text.split(\" \"))\r\n    for i in words_unique:\r\n        original_tokenizer.add_tokens(i)\r\n    original_tokenizer.save_pretrained(path)\r\n\r\ntokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,\"tokenizer_squad\"))\r\n\r\ntrain_set_baby=Dataset.from_dict({\"text\":[train_set[\"text\"][0][0:50]]})\r\n````\r\n\r\nI then applied the dataset map function on a fairly small set of text:\r\n\r\n```\r\n%%time\r\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\r\n\r\n```\r\n\r\n\r\nThe run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds\r\n\r\n**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**\r\n\r\nIn comparison using (even after adding additional tokens): \r\n`\r\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")`\r\n\r\n```\r\n%%time\r\ntrain_set_baby = train_set_baby.map(lambda d:tokenizer2(d[\"text\"]),batched=True)\r\n\r\n```\r\nThe time is \r\n**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 \u00b5s, total: 68.1 ms Wall time: 62.9 ms**\r\n\r\nIt seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. \r\n\r\nI should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. \r\n\r\n\r\n",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 802693600,
    "title": "Add Tweet Eval Dataset",
    "dateCreated": "2021-02-06T12:36:25Z",
    "dateModified": "2021-02-06T12:36:25Z",
    "description": "Closes Draft PR #1407. \r\n\r\nNotes:\r\n1. I have excluded `mapping.txt` from the dataset at it only contained the name mappings, which are already present in the ClassLabels.\r\n2. I have also exluded the textual names for the emojis mentioned in the [mapping](https://github.com/cardiffnlp/tweeteval/blob/main/datasets/emoji/mapping.txt).\r\n3. I do not understand @abhishekkrthakur's example generator on #1407. Maybe he was trying to build up on code from some other dataset.\r\n\r\nRequesting @lhoestq  to review.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 802449234,
    "title": "Add CelebA Dataset",
    "dateCreated": "2021-02-05T20:20:55Z",
    "dateModified": "2021-02-05T20:20:55Z",
    "description": "Trying to add CelebA Dataset. \r\nNeed help with testing. Loading examples takes a lot of time so I am unable to generate the `dataset_infos.json` and unable to test. Also, need help with creating `dummy_data.zip`.\r\n\r\nAdditionally, trying to load a few examples using `load_dataset('./datasets/celeb_a',split='train[10:20]')` still loads all the examples (doesn't stop at 10).",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 802353974,
    "title": "Regarding On-the-fly Data Loading",
    "dateCreated": "2021-02-05T17:43:48Z",
    "dateModified": "2021-02-05T17:43:48Z",
    "description": "Hi,\r\n\r\nI was wondering if it is possible to load images/texts as a batch during the training process, without loading the entire dataset on the RAM at any given point.\r\n\r\nThanks,\r\nGunjan",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 802074744,
    "title": "Print error message with filename when malformed CSV",
    "dateCreated": "2021-02-05T11:07:59Z",
    "dateModified": "2021-02-05T11:07:59Z",
    "description": "Print error message specifying filename when malformed CSV file.\r\n\r\nClose #1821",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 802073925,
    "title": "Datasets library not suitable for huge text datasets.",
    "dateCreated": "2021-02-05T11:06:50Z",
    "dateModified": "2021-02-05T11:06:50Z",
    "description": "Hi,\r\n\r\nI'm trying to use datasets library to load a 187GB dataset of pure text, with the intention of building a Language Model. The problem is that from the 187GB it goes to some TB when processed by Datasets. First of all, I think the pre-tokenizing step (with tokenizer.map()) is not really thought for datasets this big, but for fine-tuning datasets, as this process alone takes so much time, usually in expensive machines (due to the need of tpus - gpus) which is not being used for training. It would possibly be more efficient in such cases to tokenize each batch at training time (receive batch - tokenize batch - train with batch), so that the whole time the machine is up it's being used for training. \r\nMoreover, the pyarrow objects created from a 187 GB datasets are huge, I mean, we always receive OOM, or No Space left on device errors when only 10-12% of the dataset has been processed, and only that part occupies 2.1TB in disk, which is so many times the disk  usage of the pure text (and this doesn't make sense, as tokenized texts should be lighter than pure texts).\r\n\r\nAny suggestions??",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 802048281,
    "title": "Add OSCAR dataset card",
    "dateCreated": "2021-02-05T10:30:26Z",
    "dateModified": "2021-02-05T10:30:26Z",
    "description": "I started adding the dataset card for OSCAR !\r\n\r\nFor now it's just basic info for all the different configurations in `Dataset Structure`.\r\nIn particular the Data Splits section tells how may samples there are for each config. The Data Instances section show an example for each config, and it also shows the size in MB. Since the Data Instances section is very long the user has to click to expand the info. I was able to generate it thanks to the tools made by  @madlag and @yjernite :D\r\n\r\nCc @pjox could you help me with the other sections ? (Dataset Description, Dataset Creation, Considerations for Using the Data, Additional Information)\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 802042181,
    "title": "Add FewRel Dataset",
    "dateCreated": "2021-02-05T10:22:03Z",
    "dateModified": "2021-02-05T10:22:03Z",
    "description": "Hi,\r\n\r\nThis PR closes this [Card](https://github.com/huggingface/datasets/projects/1#card-53285184) and Issue #1757.\r\n\r\nI wasn't sure how to add `pid2name` along with the dataset so I added it as a separate configuration. For each (head, tail, tokens) triplet, I have created one example. I have added the dictionary key as `\"relation\"` in the dataset. Additionally, for `pubmed_unsupervised`, I kept `\"relation\":\"\"` in the dictionary.\r\n\r\nPlease recommend better alternatives, if any.\r\n\r\nThanks,\r\nGunjan",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 802003835,
    "title": "Add Hindi Discourse Analysis Natural Language Inference Dataset",
    "dateCreated": "2021-02-05T09:30:54Z",
    "dateModified": "2021-02-05T09:30:54Z",
    "description": "# Dataset Card for Hindi Discourse Analysis Dataset\r\n\r\n## Table of Contents\r\n- [Dataset Description](#dataset-description)\r\n  - [Dataset Summary](#dataset-summary)\r\n  - [Supported Tasks](#supported-tasks-and-leaderboards)\r\n  - [Languages](#languages)\r\n- [Dataset Structure](#dataset-structure)\r\n  - [Data Instances](#data-instances)\r\n  - [Data Fields](#data-fields)\r\n  - [Data Splits](#data-splits)\r\n- [Dataset Creation](#dataset-creation)\r\n  - [Curation Rationale](#curation-rationale)\r\n  - [Source Data](#source-data)\r\n  - [Annotations](#annotations)\r\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\r\n- [Considerations for Using the Data](#considerations-for-using-the-data)\r\n  - [Social Impact of Dataset](#social-impact-of-dataset)\r\n  - [Discussion of Biases](#discussion-of-biases)\r\n  - [Other Known Limitations](#other-known-limitations)\r\n- [Additional Information](#additional-information)\r\n  - [Dataset Curators](#dataset-curators)\r\n  - [Licensing Information](#licensing-information)\r\n  - [Citation Information](#citation-information)\r\n  - [Contributions](#contributions)\r\n\r\n## Dataset Description\r\n\r\n- HomePage : https://github.com/midas-research/hindi-nli-data\r\n- Paper : https://www.aclweb.org/anthology/2020.aacl-main.71\r\n- Point of Contact : https://github.com/midas-research/hindi-nli-data\r\n\r\n### Dataset Summary\r\n\r\n- Dataset for Natural Language Inference in Hindi Language. Hindi Discourse Analysis (HDA) Dataset consists of textual-entailment pairs.\r\n- Each row of the Datasets if made up of 4 columns - Premise, Hypothesis, Label and Topic.\r\n- Premise and Hypothesis is written in Hindi while Entailment_Label is in English.\r\n- Entailment_label is of 2 types - entailed and not-entailed.\r\n- Entailed means that hypotheis can be inferred from premise and not-entailed means vice versa\r\n- Dataset can be used to train models for Natural Language Inference tasks in Hindi Language.\r\n\r\n### Supported Tasks and Leaderboards\r\n\r\n- Natural Language Inference for Hindi\r\n\r\n### Languages\r\n\r\n- Dataset is in Hindi\r\n\r\n## Dataset Structure\r\n\r\n- Data is structured in TSV format. \r\n- train, test and dev files are in seperate files\r\n\r\n\r\n### Dataset Instances\r\n\r\nAn example of 'train' looks as follows.\r\n\r\n```\r\n{'hypothesis': '\u092f\u0939 \u090f\u0915 \u0935\u0930\u094d\u0923\u0928\u093e\u0924\u094d\u092e\u0915 \u0915\u0925\u0928 \u0939\u0948\u0964', 'label': 1, 'premise': '\u091c\u0948\u0938\u0947 \u0909\u0938 \u0915\u093e \u0938\u093e\u0930\u093e \u091a\u0947\u0939\u0930\u093e \u0905\u092a\u0928\u093e \u0939\u094b \u0914\u0930 \u0906\u0901\u0916\u0947\u0902 \u0915\u093f\u0938\u0940 \u0926\u0942\u0938\u0930\u0947 \u0915\u0940 \u091c\u094b \u091a\u0947\u0939\u0930\u0947 \u092a\u0930 \u092a\u092a\u094b\u091f\u094b\u0902 \u0915\u0947 \u092a\u0940\u091b\u0947 \u092e\u0939\u0938\u0942\u0930 \u0915\u0930 \u0926\u0940 \u0917\u0908\u0902\u0964', 'topic': 1}\r\n\r\n\r\n```\r\n### Data Fields\r\n\r\n- Each row contatins 4 columns - premise, hypothesis, label and topic.\r\n\r\n### Data Splits\r\n\r\n- Train : 31892\r\n- Valid : 9460\r\n- Test : 9970\r\n\r\n## Dataset Creation\r\n\r\n- We employ a recasting technique from Poliak et al. (2018a,b) to convert publicly available Hindi Discourse Analysis classification datasets in Hindi and pose them as TE problems\r\n- In this recasting process, we build template hypotheses for each class in the label taxonomy\r\n- Then, we pair the original annotated sentence with each of the template hypotheses to create TE samples.\r\n- For more information on the recasting process, refer to paper https://www.aclweb.org/anthology/2020.aacl-main.71\r\n\r\n### Source Data\r\n\r\nSource Dataset for the recasting process is the BBC Hindi Headlines Dataset(https://github.com/NirantK/hindi2vec/releases/tag/bbc-hindi-v0.1)\r\n\r\n#### Initial Data Collection and Normalization\r\n\r\n-  Initial Data was collected by members of MIDAS Lab from Hindi Websites. They crowd sourced the data annotation process and selected two random stories from our corpus and had the three annotators work on them independently and classify each sentence based on the discourse mode.\r\n- Please refer to this paper for detailed information: https://www.aclweb.org/anthology/2020.lrec-1.149/\r\n- The Discourse is further classified into \"Argumentative\" , \"Descriptive\" , \"Dialogic\" , \"Informative\"  and  \"Narrative\" - 5 Clases.\r\n\r\n#### Who are the source language producers?\r\n\r\nPlease refer to this paper for detailed information: https://www.aclweb.org/anthology/2020.lrec-1.149/\r\n\r\n### Annotations\r\n\r\n#### Annotation process\r\n\r\nAnnotation process has been described in Dataset Creation Section.\r\n\r\n#### Who are the annotators?\r\n\r\nAnnotation is done automatically by machine and corresponding recasting process.\r\n\r\n### Personal and Sensitive Information\r\n\r\nNo Personal and Sensitive Information is mentioned in the Datasets.\r\n\r\n## Considerations for Using the Data\r\n\r\nPls refer to this paper: https://www.aclweb.org/anthology/2020.aacl-main.71\r\n\r\n### Discussion of Biases\r\n\r\nNo known bias exist in the dataset.\r\nPls refer to this paper: https://www.aclweb.org/anthology/2020.aacl-main.71\r\n\r\n### Other Known Limitations\r\n\r\nNo other known limitations . Size of data may not be enough to train large models\r\n\r\n## Additional Information\r\n\r\nPls refer to this link: https://github.com/midas-research/hindi-nli-data\r\n\r\n### Dataset Curators\r\n\r\nIt is written in the repo : https://github.com/midas-research/hindi-nli-data that \r\n- This corpus can be used freely for research purposes.\r\n- The paper listed below provide details of the creation and use of the corpus. If you use the corpus, then please cite the paper.\r\n- If interested in commercial use of the corpus, send email to midas@iiitd.ac.in.\r\n- If you use the corpus in a product or application, then please credit the authors and Multimodal Digital Media Analysis Lab - Indraprastha Institute of Information Technology, New Delhi appropriately. Also, if you send us an email, we will be thrilled to know about how you have used the corpus.\r\n- Multimodal Digital Media Analysis Lab - Indraprastha Institute of Information Technology, New Delhi, India disclaims any responsibility for the use of the corpus and does not provide technical support. However, the contact listed above will be happy to respond to queries and clarifications.\r\n- Rather than redistributing the corpus, please direct interested parties to this page\r\n- Please feel free to send us an email:\r\n  - with feedback regarding the corpus.\r\n  - with information on how you have used the corpus.\r\n  - if interested in having us analyze your data for natural language inference.\r\n  - if interested in a collaborative research project.\r\n\r\n\r\n### Licensing Information\r\n\r\nCopyright (C) 2019 Multimodal Digital Media Analysis Lab - Indraprastha Institute of Information Technology, New Delhi (MIDAS, IIIT-Delhi).\r\nPls contact authors for any information on the dataset.\r\n\r\n### Citation Information\r\n\r\n```\r\n    @inproceedings{uppal-etal-2020-two,\r\n    title = \"Two-Step Classification using Recasted Data for Low Resource Settings\",\r\n    author = \"Uppal, Shagun  and\r\n      Gupta, Vivek  and\r\n      Swaminathan, Avinash  and\r\n      Zhang, Haimin  and\r\n      Mahata, Debanjan  and\r\n      Gosangi, Rakesh  and\r\n      Shah, Rajiv Ratn  and\r\n      Stent, Amanda\",\r\n    booktitle = \"Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing\",\r\n    month = dec,\r\n    year = \"2020\",\r\n    address = \"Suzhou, China\",\r\n    publisher = \"Association for Computational Linguistics\",\r\n    url = \"https://www.aclweb.org/anthology/2020.aacl-main.71\",\r\n    pages = \"706--719\",\r\n    abstract = \"An NLP model{'}s ability to reason should be independent of language. Previous works utilize Natural Language Inference (NLI) to understand the reasoning ability of models, mostly focusing on high resource languages like English. To address scarcity of data in low-resource languages such as Hindi, we use data recasting to create NLI datasets for four existing text classification datasets. Through experiments, we show that our recasted dataset is devoid of statistical irregularities and spurious patterns. We further study the consistency in predictions of the textual entailment models and propose a consistency regulariser to remove pairwise-inconsistencies in predictions. We propose a novel two-step classification method which uses textual-entailment predictions for classification task. We further improve the performance by using a joint-objective for classification and textual entailment. We therefore highlight the benefits of data recasting and improvements on classification performance using our approach with supporting experimental results.\",\r\n}\r\n```\r\n\r\n### Contributions\r\n\r\nThanks to [@avinsit123](https://github.com/avinsit123) for adding this dataset.\r\n",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 801747647,
    "title": "Provide better exception message when one of many files results in an exception",
    "dateCreated": "2021-02-05T00:49:03Z",
    "dateModified": "2021-02-05T00:49:03Z",
    "description": "I find when I process many files, i.e.\r\n\r\n```\r\ntrain_files = glob.glob('rain*.csv')\r\nvalidation_files = glob.glob(validation*.csv')\r\ndatasets = load_dataset(\"csv\", data_files=dict(train=train_files, validation=validation_files))\r\n```\r\n\r\nI sometimes encounter an error due to one of the files being misformed (i.e. no data, or a comma in a field that isn't quoted, etc).\r\n\r\nFor example, this is the tail of an exception which I suspect is due to a stray comma.\r\n\r\n>   File \"pandas/_libs/parsers.pyx\", line 756, in pandas._libs.parsers.TextReader.read\r\n>   File \"pandas/_libs/parsers.pyx\", line 783, in pandas._libs.parsers.TextReader._read_low_memory\r\n>   File \"pandas/_libs/parsers.pyx\", line 827, in pandas._libs.parsers.TextReader._read_rows\r\n>   File \"pandas/_libs/parsers.pyx\", line 814, in pandas._libs.parsers.TextReader._tokenize_rows\r\n>   File \"pandas/_libs/parsers.pyx\", line 1951, in pandas._libs.parsers.raise_parser_error\r\n> pandas.errors.ParserError: Error tokenizing data. C error: Expected 2 fields in line 559, saw 3\r\n\r\nIt would be nice if the exception trace contained the name of the file being processed (I have 250 separate files!)",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 801529936,
    "title": "Add metrics usage examples and tests",
    "dateCreated": "2021-02-04T18:23:50Z",
    "dateModified": "2021-02-04T18:23:50Z",
    "description": "All metrics finally have usage examples and proper fast + slow tests :)\r\n\r\nI added examples of usage for every metric, and I use doctest to make sure they all work as expected.\r\n\r\nFor \"slow\" metrics such as bert_score or bleurt which require to download + run a transformer model, the download + forward pass are only done in the slow test.\r\nIn the fast test on the other hand, the download + forward pass are monkey patched.\r\n\r\nMetrics that need to be installed from github are not added to setup.py because it prevents uploading the `datasets` package to pypi.\r\nAn additional-test-requirements.txt file is used instead. This file also include `comet` in order to not have to resolve its *impossible* dependencies.\r\n\r\nAlso `comet` is not tested on windows because one of its dependencies (fairseq) can't be installed in the CI for some reason.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 801448670,
    "title": "Fixed spelling `S3Fileystem` to `S3FileSystem`",
    "dateCreated": "2021-02-04T16:36:46Z",
    "dateModified": "2021-02-04T16:36:46Z",
    "description": "Fixed documentation spelling errors. \r\nWrong `S3Fileystem`\r\nRight `S3FileSystem`",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 800958776,
    "title": "Loading local dataset raise requests.exceptions.ConnectTimeout",
    "dateCreated": "2021-02-04T05:55:23Z",
    "dateModified": "2021-02-04T05:55:23Z",
    "description": "Load local dataset:\r\n```\r\ndataset = load_dataset('json', data_files=[\"../../data/json.json\"])\r\ntrain = dataset[\"train\"]\r\nprint(train.features)\r\ntrain1 = train.map(lambda x: {\"labels\": 1})\r\nprint(train1[:2])\r\n```\r\n\r\nbut it raised requests.exceptions.ConnectTimeout:\r\n\r\n```\r\n/Users/littlely/myvirtual/tf2/bin/python3.7 /Users/littlely/projects/python_projects/pytorch_learning/nlp/dataset/transformers_datasets.py\r\nTraceback (most recent call last):\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connection.py\", line 160, in _new_conn\r\n    (self._dns_host, self.port), self.timeout, **extra_kw\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/util/connection.py\", line 84, in create_connection\r\n    raise err\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/util/connection.py\", line 74, in create_connection\r\n    sock.connect(sa)\r\nsocket.timeout: timed out\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 677, in urlopen\r\n    chunked=chunked,\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\r\n    self._validate_conn(conn)\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\r\n    conn.connect()\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connection.py\", line 309, in connect\r\n    conn = self._new_conn()\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connection.py\", line 167, in _new_conn\r\n    % (self.host, self.timeout),\r\nurllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x1181e9940>, 'Connection to s3.amazonaws.com timed out. (connect timeout=10)')\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\r\n    timeout=timeout\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 727, in urlopen\r\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/util/retry.py\", line 439, in increment\r\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/json/json.py (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x1181e9940>, 'Connection to s3.amazonaws.com timed out. (connect timeout=10)'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/littlely/projects/python_projects/pytorch_learning/nlp/dataset/transformers_datasets.py\", line 12, in <module>\r\n    dataset = load_dataset('json', data_files=[\"../../data/json.json\"])\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/datasets/load.py\", line 591, in load_dataset\r\n    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/datasets/load.py\", line 263, in prepare_module\r\n    head_hf_s3(path, filename=name, dataset=dataset, max_retries=download_config.max_retries)\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 232, in head_hf_s3\r\n    max_retries=max_retries,\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 523, in http_head\r\n    max_retries=max_retries,\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 458, in _request_with_retry\r\n    raise err\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 454, in _request_with_retry\r\n    response = requests.request(verb.upper(), url, **params)\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/requests/api.py\", line 61, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/requests/sessions.py\", line 530, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/requests/sessions.py\", line 643, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/requests/adapters.py\", line 504, in send\r\n    raise ConnectTimeout(e, request=request)\r\nrequests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/json/json.py (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x1181e9940>, 'Connection to s3.amazonaws.com timed out. (connect timeout=10)'))\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\nWhy it want to connect a remote url when I load local datasets, and how can I fix it?",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 800870652,
    "title": "pyarrow.lib.ArrowInvalid: Column 1 named input_ids expected length 599 but got length 1500",
    "dateCreated": "2021-02-04T02:30:23Z",
    "dateModified": "2021-02-04T02:30:23Z",
    "description": "I am trying to preprocess any dataset in this package with GPT-2 tokenizer, so I need to structure the datasets as long sequences of text without padding. I've been following a couple of your tutorials and here you can find the script that is failing right at the end\r\n\r\nhttps://github.com/LuCeHe/GenericTools/blob/master/KerasTools/lm_preprocessing.py\r\n\r\nIn the last iteration of the last dset.map, it gives the error that I copied in the title. Another issue that I have, if I leave the batch_size set as 1000 in the last .map, I'm afraid it's going to lose most text, so I'm considering setting both writer_batch_size and batch_size to 300 K, but I'm not sure it's the best way to go.\r\n\r\nCan you help me?\r\nThanks!",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 800660995,
    "title": "Doc2dial rc update to latest version",
    "dateCreated": "2021-02-03T20:08:54Z",
    "dateModified": "2021-02-03T20:08:54Z",
    "description": "",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 800610017,
    "title": "Add CCAligned Multilingual Dataset",
    "dateCreated": "2021-02-03T18:59:52Z",
    "dateModified": "2021-02-03T18:59:52Z",
    "description": "Hello,\r\n\r\nI'm trying to add [CCAligned Multilingual Dataset](http://www.statmt.org/cc-aligned/). This has the potential to close #1756.\r\n\r\nThis dataset has two types - Document-Pairs, and Sentence-Pairs.\r\n\r\nThe datasets are huge, so I won't be able to test all of them. At the same time, a user might only want to download one particular language and not all. To provide this feature, `load_dataset`'s `**config_kwargs` should allow some random keyword args, in this case -`language_code`. This will be needed before the dataset is downloaded and extracted.\r\n\r\nI'm expecting the usage to be something like - \r\n`load_dataset('ccaligned_multilingual','documents',language_code='en_XX-af_ZA')`. Ofcourse, at a later stage we can provide just two character language codes. This also has an issue where one language has multiple files (`my_MM` and `my_MM_zaw` on the link), but before that the required functionality must be added to `load_dataset`.\r\n\r\nIt would be great if someone could either tell me an alternative way to do this, or point me to where changes need to be made, if any, apart from the `BuilderConfig` definition. \r\n\r\nAdditionally, I believe the tests will also have to be modified if this change is made, since it would not be possible to test for any random keyword arguments. \r\n\r\nA decent way to go about this would be to provide all the options in a list/dictionary for `language_code` and use that to test the arguments. In essence, this is similar to the pre-trained checkpoint dictionary as `transformers`. That means writing dataset specific tests, or adding something new to dataset generation script to make it easier for everyone to add keyword arguments without having to worry about the tests.\r\n\r\nThanks,\r\nGunjan\r\n\r\nRequesting @lhoestq / @yjernite to review.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 800516236,
    "title": "Add Freebase QA Dataset",
    "dateCreated": "2021-02-03T16:57:49Z",
    "dateModified": "2021-02-03T16:57:49Z",
    "description": "Closes PR #1435. Fixed issues with PR #1809.\r\n\r\nRequesting @lhoestq to review.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 800435973,
    "title": "Support future datasets",
    "dateCreated": "2021-02-03T15:26:49Z",
    "dateModified": "2021-02-03T15:26:49Z",
    "description": "If a dataset is available at the version of the local installation of `datasets` (e.g. 1.2.0), then loading this dataset means loading the script at this version.\r\n\r\nHowever when trying to load a dataset that is only available on master, currently users have to specify `script_version=\"master\"` in `load_dataset` to make it work.\r\n\r\nHowever we could automatically get the dataset from master instead in this case.\r\n\r\nI added this feature in this PR.\r\nI also added a warning if a dataset is not available at the version of the local installation of `datasets` but is loaded from master:\r\n```python\r\n>>> load_dataset(\"silicone\", \"dyda_da\")\r\nCouldn't find file locally at silicone/silicone.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.2.0/datasets/silicone/silicone.py.\r\nThe file was picked from the master branch on github instead at https://raw.githubusercontent.com/huggingface/datasets/master/datasets/silicone/silicone.py.\r\nDownloading and preparing dataset silicone/dyda_da (download: 8.46 MiB, generated: 9.39 MiB, post-processed: Unknown size, total: 17.86 MiB) to /Users/quentinlhoest/.cache/huggingface/datasets/silicone/dyda_da/1.0.0/d41d8c0b73c6df035b1369c45774418f0051163ea689b5502b8bda783adf6342...\r\n...\r\n```\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 799379178,
    "title": "Add CIFAR-100 Dataset",
    "dateCreated": "2021-02-02T15:22:59Z",
    "dateModified": "2021-02-02T15:22:59Z",
    "description": "Adding CIFAR-100 Dataset.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 799211060,
    "title": "Unable to add Multi-label Datasets",
    "dateCreated": "2021-02-02T11:50:56Z",
    "dateModified": "2021-02-02T11:50:56Z",
    "description": "I am trying to add [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. The dataset contains two labels per image - `fine label` and `coarse label`. Using just one label in supervised keys as \r\n`supervised_keys=(\"img\", \"fine_label\")` raises no issue. But trying `supervised_keys=(\"img\", \"fine_label\",\"coarse_label\")` leads to this error : \r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"test_script.py\", line 2, in <module>\r\n    d = load_dataset('./datasets/cifar100')\r\n  File \"~/datasets/src/datasets/load.py\", line 668, in load_dataset\r\n    **config_kwargs,\r\n  File \"~/datasets/src/datasets/builder.py\", line 896, in __init__\r\n    super(GeneratorBasedBuilder, self).__init__(*args, **kwargs)\r\n  File \"~/datasets/src/datasets/builder.py\", line 247, in __init__\r\n    info.update(self._info())\r\n  File \"~/.cache/huggingface/modules/datasets_modules/datasets/cifar100/61d2489b2d4a4abc34201432541b7380984ec714e290817d9a1ee318e4b74e0f/cifar100.py\", line 79, in _info\r\n    citation=_CITATION,\r\n  File \"<string>\", line 19, in __init__\r\n  File \"~/datasets/src/datasets/info.py\", line 136, in __post_init__\r\n    self.supervised_keys = SupervisedKeysData(*self.supervised_keys)\r\nTypeError: __init__() takes from 1 to 3 positional arguments but 4 were given\r\n```\r\nIs there a way I can fix this?\r\n\r\nAlso, what does adding `supervised_keys` do? Is it necessary? How would I specify `supervised_keys` for a multi-input, multi-label dataset?\r\n\r\nThanks,\r\nGunjan",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 799168650,
    "title": "Add Hateful Memes Dataset",
    "dateCreated": "2021-02-02T10:53:59Z",
    "dateModified": "2021-02-02T10:53:59Z",
    "description": "## Add Hateful Memes Dataset\r\n- **Name:** Hateful Memes\r\n- **Description:** [https://ai.facebook.com/blog/hateful-memes-challenge-and-data-set]( https://ai.facebook.com/blog/hateful-memes-challenge-and-data-set)\r\n- **Paper:** [https://arxiv.org/pdf/2005.04790.pdf](https://arxiv.org/pdf/2005.04790.pdf)\r\n- **Data:** [This link](https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/XjiOc5ycDBRRNwbhRlgH.zip?AWSAccessKeyId=AKIARVBOBDCY4MWEDJKS&Signature=DaUuGgZWUgDHzEPPbyJ2PhSJ56Q%3D&Expires=1612816874)\r\n- **Motivation:** Including multi-modal datasets to \ud83e\udd17 datasets.\r\n\r\nI will be adding this dataset. It requires the user to sign an agreement on DrivenData. So, it will be used with a manual download.\r\n\r\nThe issue with this dataset is that the images are of different sizes. The image datasets added so far (CIFAR-10 and MNIST) have a uniform shape throughout.\r\nSo something like \r\n```python\r\n datasets.Array2D(shape=(28, 28), dtype=\"uint8\")\r\n```\r\nwon't work for the images. How would I add image features then? I checked `datasets/features.py` but couldn't figure out the appropriate class for this. I'm assuming I would want to avoid re-sizing at all since we want the user to be able to access the original images.\r\n\r\nAlso, in case I want to load only a subset of the data, since the actual data is around 8.8GB, how would that be possible?\r\n\r\nThanks,\r\nGunjan",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 799059141,
    "title": "Add FreebaseQA dataset",
    "dateCreated": "2021-02-02T08:35:53Z",
    "dateModified": "2021-02-02T08:35:53Z",
    "description": "Adding FreebaseQA dataset suggested in PR #1435 with minor edits. Also closes that PR.\r\nRequesting @lhoestq to review.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 798879180,
    "title": "writing Datasets in a human readable format",
    "dateCreated": "2021-02-02T02:55:40Z",
    "dateModified": "2021-02-02T02:55:40Z",
    "description": "Hi\r\nI see there is a save_to_disk function to save data, but this is not human readable format, is there a way I could save a Dataset object in a human readable  format to a file like json? thanks @lhoestq ",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 798823591,
    "title": "Adding an aggregated dataset for the GEM benchmark",
    "dateCreated": "2021-02-02T00:39:53Z",
    "dateModified": "2021-02-02T00:39:53Z",
    "description": "This dataset gathers modified versions of several other conditional text generation datasets which together make up the shared task for the Generation Evaluation and Metrics workshop (think GLUE for text generation)\r\n\r\nThe changes from the original datasets are detailed in the Dataset Cards on the GEM website, which are linked to in this dataset card.\r\n\r\ncc @sebastianGehrmann\r\n",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 798607869,
    "title": "Update details to MLSUM dataset",
    "dateCreated": "2021-02-01T18:35:12Z",
    "dateModified": "2021-02-01T18:35:12Z",
    "description": "Update details to MLSUM dataset",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 798498053,
    "title": "can't pickle SwigPyObject objects when calling dataset.get_nearest_examples from FAISS index",
    "dateCreated": "2021-02-01T16:14:17Z",
    "dateModified": "2021-02-01T16:14:17Z",
    "description": "So, I have the following instances in my dataset\r\n\r\n```\r\n{'question': 'An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of \r\nthis increase in rotation?', \r\n'answer': 'C', \r\n'example_id': 'ARCCH_Mercury_7175875', \r\n'options':[{'option_context': 'One effect of increased amperage in the planetary world (..)', 'option_id': 'A', 'option_text': 'Planetary density will decrease.'},\r\n (...)]}\r\n```\r\n\r\nThe `options` value is always an list with 4 options, each one is a dict with `option_context`; `option_id` and `option_text`.\r\n\r\nI would like to overwrite the `option_context` of each instance of my dataset for a dpr result that I am developing. Then, I trained a model already and save it in a FAISS index\r\n```\r\ndpr_dataset = load_dataset(\r\n            \"text\",\r\n            data_files=ARC_CORPUS_TEXT,\r\n            cache_dir=CACHE_DIR,\r\n            split=\"train[:100%]\",\r\n        )\r\ndpr_dataset.load_faiss_index(\"embeddings\", f\"{ARC_CORPUS_FAISS}\")\r\ntorch.set_grad_enabled(False)\r\n```\r\n\r\nThen, as a processor of my dataset, I created a map function that calls the `dpr_dataset` for each _option_\r\n\r\n```\r\ndef generate_context(example):\r\n    question_text = example['question']\r\n    for option in example['options']:\r\n        question_with_option = question_text + \" \" + option['option_text']\r\n        tokenize_text =  question_tokenizer(question_with_option, return_tensors=\"pt\").to(device)\r\n        question_embed = (\r\n            question_encoder(**tokenize_text)\r\n        )[0][0].cpu().numpy()\r\n        _, retrieved_examples = dpr_dataset.get_nearest_examples(\r\n            \"embeddings\", question_embed, k=10\r\n        )\r\n    #    option[\"option_context\"] = retrieved_examples[\"text\"]\r\n    #    option[\"option_context\"] = \" \".join(option[\"option_context\"]).strip()\r\n    #result_dict = {\r\n    #    'example_id': example['example_id'],\r\n    #    'answer': example['answer'],\r\n    #    'question': question_text,\r\n        #options': example['options']\r\n    # }\r\n    return example\r\n```\r\n\r\nI intentionally commented on this portion of the code.\r\n\r\nBut when I call the `map` method, `ds_with_context = dataset.map(generate_context,load_from_cache_file=False)`\r\n\r\nIt calls the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-55-75a458ce205c> in <module>\r\n----> 1 ds_with_context = dataset.map(generate_context,load_from_cache_file=False)\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/dataset_dict.py in map(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc)\r\n    301                     num_proc=num_proc,\r\n    302                 )\r\n--> 303                 for k, dataset in self.items()\r\n    304             }\r\n    305         )\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/dataset_dict.py in <dictcomp>(.0)\r\n    301                     num_proc=num_proc,\r\n    302                 )\r\n--> 303                 for k, dataset in self.items()\r\n    304             }\r\n    305         )\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\r\n   1257                 fn_kwargs=fn_kwargs,\r\n   1258                 new_fingerprint=new_fingerprint,\r\n-> 1259                 update_data=update_data,\r\n   1260             )\r\n   1261         else:\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    155         }\r\n    156         # apply actual function\r\n--> 157         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    158         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    159         # re-apply format to the output\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    156                         kwargs_for_fingerprint[\"fingerprint_name\"] = fingerprint_name\r\n    157                         kwargs[fingerprint_name] = update_fingerprint(\r\n--> 158                             self._fingerprint, transform, kwargs_for_fingerprint\r\n    159                         )\r\n    160 \r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/fingerprint.py in update_fingerprint(fingerprint, transform, transform_args)\r\n    103     for key in sorted(transform_args):\r\n    104         hasher.update(key)\r\n--> 105         hasher.update(transform_args[key])\r\n    106     return hasher.hexdigest()\r\n    107 \r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/fingerprint.py in update(self, value)\r\n     55     def update(self, value):\r\n     56         self.m.update(f\"=={type(value)}==\".encode(\"utf8\"))\r\n---> 57         self.m.update(self.hash(value).encode(\"utf-8\"))\r\n     58 \r\n     59     def hexdigest(self):\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/fingerprint.py in hash(cls, value)\r\n     51             return cls.dispatch[type(value)](cls, value)\r\n     52         else:\r\n---> 53             return cls.hash_default(value)\r\n     54 \r\n     55     def update(self, value):\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/fingerprint.py in hash_default(cls, value)\r\n     44     @classmethod\r\n     45     def hash_default(cls, value):\r\n---> 46         return cls.hash_bytes(dumps(value))\r\n     47 \r\n     48     @classmethod\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/utils/py_utils.py in dumps(obj)\r\n    387     file = StringIO()\r\n    388     with _no_cache_fields(obj):\r\n--> 389         dump(obj, file)\r\n    390     return file.getvalue()\r\n    391 \r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/utils/py_utils.py in dump(obj, file)\r\n    359 def dump(obj, file):\r\n    360     \"\"\"pickle an object to a file\"\"\"\r\n--> 361     Pickler(file, recurse=True).dump(obj)\r\n    362     return\r\n    363 \r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/dill/_dill.py in dump(self, obj)\r\n    452             raise PicklingError(msg)\r\n    453         else:\r\n--> 454             StockPickler.dump(self, obj)\r\n    455         stack.clear()  # clear record of 'recursion-sensitive' pickled objects\r\n    456         return\r\n\r\n/usr/lib/python3.7/pickle.py in dump(self, obj)\r\n    435         if self.proto >= 4:\r\n    436             self.framer.start_framing()\r\n--> 437         self.save(obj)\r\n    438         self.write(STOP)\r\n    439         self.framer.end_framing()\r\n\r\n/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    502         f = self.dispatch.get(t)\r\n    503         if f is not None:\r\n--> 504             f(self, obj) # Call unbound method with explicit self\r\n    505             return\r\n    506 \r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/utils/py_utils.py in save_function(pickler, obj)\r\n    554                 dill._dill._create_function,\r\n    555                 (obj.__code__, globs, obj.__name__, obj.__defaults__, obj.__closure__, obj.__dict__, fkwdefaults),\r\n--> 556                 obj=obj,\r\n    557             )\r\n    558         else:\r\n\r\n/usr/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)\r\n    636         else:\r\n    637             save(func)\r\n--> 638             save(args)\r\n    639             write(REDUCE)\r\n    640 \r\n\r\n/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    502         f = self.dispatch.get(t)\r\n    503         if f is not None:\r\n--> 504             f(self, obj) # Call unbound method with explicit self\r\n    505             return\r\n    506 \r\n\r\n/usr/lib/python3.7/pickle.py in save_tuple(self, obj)\r\n    784         write(MARK)\r\n    785         for element in obj:\r\n--> 786             save(element)\r\n    787 \r\n    788         if id(obj) in memo:\r\n\r\n/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    502         f = self.dispatch.get(t)\r\n    503         if f is not None:\r\n--> 504             f(self, obj) # Call unbound method with explicit self\r\n    505             return\r\n    506 \r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)\r\n    939             # we only care about session the first pass thru\r\n    940             pickler._session = False\r\n--> 941         StockPickler.save_dict(pickler, obj)\r\n    942         log.info(\"# D2\")\r\n    943     return\r\n\r\n/usr/lib/python3.7/pickle.py in save_dict(self, obj)\r\n    854 \r\n    855         self.memoize(obj)\r\n--> 856         self._batch_setitems(obj.items())\r\n    857 \r\n    858     dispatch[dict] = save_dict\r\n\r\n/usr/lib/python3.7/pickle.py in _batch_setitems(self, items)\r\n    880                 for k, v in tmp:\r\n    881                     save(k)\r\n--> 882                     save(v)\r\n    883                 write(SETITEMS)\r\n    884             elif n:\r\n\r\n/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    547 \r\n    548         # Save the reduce() output and finally memoize the object\r\n--> 549         self.save_reduce(obj=obj, *rv)\r\n    550 \r\n    551     def persistent_id(self, obj):\r\n\r\n/usr/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)\r\n    660 \r\n    661         if state is not None:\r\n--> 662             save(state)\r\n    663             write(BUILD)\r\n    664 \r\n\r\n/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    502         f = self.dispatch.get(t)\r\n    503         if f is not None:\r\n--> 504             f(self, obj) # Call unbound method with explicit self\r\n    505             return\r\n    506 \r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)\r\n    939             # we only care about session the first pass thru\r\n    940             pickler._session = False\r\n--> 941         StockPickler.save_dict(pickler, obj)\r\n    942         log.info(\"# D2\")\r\n    943     return\r\n\r\n/usr/lib/python3.7/pickle.py in save_dict(self, obj)\r\n    854 \r\n    855         self.memoize(obj)\r\n--> 856         self._batch_setitems(obj.items())\r\n    857 \r\n    858     dispatch[dict] = save_dict\r\n\r\n/usr/lib/python3.7/pickle.py in _batch_setitems(self, items)\r\n    880                 for k, v in tmp:\r\n    881                     save(k)\r\n--> 882                     save(v)\r\n    883                 write(SETITEMS)\r\n    884             elif n:\r\n\r\n/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    502         f = self.dispatch.get(t)\r\n    503         if f is not None:\r\n--> 504             f(self, obj) # Call unbound method with explicit self\r\n    505             return\r\n    506 \r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)\r\n    939             # we only care about session the first pass thru\r\n    940             pickler._session = False\r\n--> 941         StockPickler.save_dict(pickler, obj)\r\n    942         log.info(\"# D2\")\r\n    943     return\r\n\r\n/usr/lib/python3.7/pickle.py in save_dict(self, obj)\r\n    854 \r\n    855         self.memoize(obj)\r\n--> 856         self._batch_setitems(obj.items())\r\n    857 \r\n    858     dispatch[dict] = save_dict\r\n\r\n/usr/lib/python3.7/pickle.py in _batch_setitems(self, items)\r\n    885                 k, v = tmp[0]\r\n    886                 save(k)\r\n--> 887                 save(v)\r\n    888                 write(SETITEM)\r\n    889             # else tmp is empty, and we're done\r\n\r\n/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    547 \r\n    548         # Save the reduce() output and finally memoize the object\r\n--> 549         self.save_reduce(obj=obj, *rv)\r\n    550 \r\n    551     def persistent_id(self, obj):\r\n\r\n/usr/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)\r\n    660 \r\n    661         if state is not None:\r\n--> 662             save(state)\r\n    663             write(BUILD)\r\n    664 \r\n\r\n/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    502         f = self.dispatch.get(t)\r\n    503         if f is not None:\r\n--> 504             f(self, obj) # Call unbound method with explicit self\r\n    505             return\r\n    506 \r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)\r\n    939             # we only care about session the first pass thru\r\n    940             pickler._session = False\r\n--> 941         StockPickler.save_dict(pickler, obj)\r\n    942         log.info(\"# D2\")\r\n    943     return\r\n\r\n/usr/lib/python3.7/pickle.py in save_dict(self, obj)\r\n    854 \r\n    855         self.memoize(obj)\r\n--> 856         self._batch_setitems(obj.items())\r\n    857 \r\n    858     dispatch[dict] = save_dict\r\n\r\n/usr/lib/python3.7/pickle.py in _batch_setitems(self, items)\r\n    880                 for k, v in tmp:\r\n    881                     save(k)\r\n--> 882                     save(v)\r\n    883                 write(SETITEMS)\r\n    884             elif n:\r\n\r\n/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    547 \r\n    548         # Save the reduce() output and finally memoize the object\r\n--> 549         self.save_reduce(obj=obj, *rv)\r\n    550 \r\n    551     def persistent_id(self, obj):\r\n\r\n/usr/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)\r\n    660 \r\n    661         if state is not None:\r\n--> 662             save(state)\r\n    663             write(BUILD)\r\n    664 \r\n\r\n/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    502         f = self.dispatch.get(t)\r\n    503         if f is not None:\r\n--> 504             f(self, obj) # Call unbound method with explicit self\r\n    505             return\r\n    506 \r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)\r\n    939             # we only care about session the first pass thru\r\n    940             pickler._session = False\r\n--> 941         StockPickler.save_dict(pickler, obj)\r\n    942         log.info(\"# D2\")\r\n    943     return\r\n\r\n/usr/lib/python3.7/pickle.py in save_dict(self, obj)\r\n    854 \r\n    855         self.memoize(obj)\r\n--> 856         self._batch_setitems(obj.items())\r\n    857 \r\n    858     dispatch[dict] = save_dict\r\n\r\n/usr/lib/python3.7/pickle.py in _batch_setitems(self, items)\r\n    885                 k, v = tmp[0]\r\n    886                 save(k)\r\n--> 887                 save(v)\r\n    888                 write(SETITEM)\r\n    889             # else tmp is empty, and we're done\r\n\r\n/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    522             reduce = getattr(obj, \"__reduce_ex__\", None)\r\n    523             if reduce is not None:\r\n--> 524                 rv = reduce(self.proto)\r\n    525             else:\r\n    526                 reduce = getattr(obj, \"__reduce__\", None)\r\n\r\nTypeError: can't pickle SwigPyObject objects\r\n```\r\n\r\nWhich I have no idea how to solve/deal with it\r\n\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 798483881,
    "title": "Add SICK dataset",
    "dateCreated": "2021-02-01T15:57:44Z",
    "dateModified": "2021-02-01T15:57:44Z",
    "description": "Adds the SICK dataset (http://marcobaroni.org/composes/sick.html).\r\n\r\nCloses #1772.\r\n\r\nEdit: also closes #1632, which is the original issue requesting the dataset. The newer one is a duplicate.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 798243904,
    "title": "Querying examples from big datasets is slower than small datasets",
    "dateCreated": "2021-02-01T11:08:23Z",
    "dateModified": "2021-02-01T11:08:23Z",
    "description": "After some experiments with bookcorpus I noticed that querying examples from big datasets is slower than small datasets.\r\nFor example\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nb1 = load_dataset(\"bookcorpus\", split=\"train[:1%]\")\r\nb50 = load_dataset(\"bookcorpus\", split=\"train[:50%]\")\r\nb100 = load_dataset(\"bookcorpus\", split=\"train[:100%]\")\r\n\r\n%timeit _ = b1[-1]                                                                     \r\n# 12.2 \u00b5s \u00b1 70.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\r\n\r\n%timeit _ = b50[-1]                                                                    \r\n# 92.5 \u00b5s \u00b1 1.24 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n\r\n%timeit _ = b100[-1]                                                                      \r\n# 177 \u00b5s \u00b1 3.13 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n\r\n```\r\n\r\nIt looks like the time to fetch the example increases with the size of the dataset.\r\n\r\nThis is maybe due to the use of the Arrow streaming format to store the data on disk. I guess pyarrow needs to iterate through the file as a stream to find the queried sample.\r\n\r\nMaybe switching to the Arrow IPC file format could help fixing this issue.\r\n\r\nIndeed according to the [documentation](https://arrow.apache.org/docs/format/Columnar.html?highlight=arrow1#ipc-file-format), it's identical to the streaming format except that it contains the memory offsets of each sample, which could fix the issue:\r\n> We define a \u201cfile format\u201d supporting random access that is build with the stream format. The file starts and ends with a magic string ARROW1 (plus padding). What follows in the file is identical to the stream format. At the end of the file, we write a footer containing a redundant copy of the schema (which is a part of the streaming format) plus memory offsets and sizes for each of the data blocks in the file. This enables random access any record batch in the file. See File.fbs for the precise details of the file footer.\r\n\r\ncc @gaceladri since it can help speed up your training when this one is fixed.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 797924468,
    "title": "add github of contributors",
    "dateCreated": "2021-02-01T03:49:19Z",
    "dateModified": "2021-02-01T03:49:19Z",
    "description": "This PR will add contributors GitHub id at the end of every dataset cards.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 797814275,
    "title": "[GEM] Updated the source link of the data to update correct tokenized version.",
    "dateCreated": "2021-01-31T21:17:19Z",
    "dateModified": "2021-01-31T21:17:19Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 797798689,
    "title": "Add DuoRC Dataset",
    "dateCreated": "2021-01-31T20:01:59Z",
    "dateModified": "2021-01-31T20:01:59Z",
    "description": "Hi,\r\n\r\nDuoRC SelfRC is one type of the [DuoRC Dataset](https://duorc.github.io/). DuoRC SelfRC is a crowdsourced Abstractive/Extractive Question-Answering dataset based on Wikipedia movie plots. It contains examples that may have answers in the movie plot, synthesized answers which are not present in the movie plot, or no answers.  I have also added ParaphraseRC - the other type of DuoRC dataset where questions are based on Wikipedia movie plots and answers are based on corresponding IMDb movie plots.\r\n\r\nPaper :  [https://arxiv.org/abs/1804.07927](https://arxiv.org/abs/1804.07927)\r\n\r\nI want to add this to \ud83e\udd17 datasets to make it more accessible to the community. I have added  all the details that I could find. Please let me know if anything else is needed from my end.\r\n\r\nThanks,\r\nGunjan\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 797789439,
    "title": "Update: SWDA - Fixed code to use all metadata features. Added comments and cleaned c\u2026",
    "dateCreated": "2021-01-31T19:18:55Z",
    "dateModified": "2021-01-31T19:18:55Z",
    "description": "This is a dataset I currently use my research and I realized some features are not being returned.\r\n\r\nPrevious code was not using all available metadata and was kind of messy\r\n\r\nI fixed code to use all metadata and made some modification to be more efficient and better formatted.\r\n\r\n\r\nPlease let me know if I need to make any changes.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 797766818,
    "title": "Add Arabic sarcasm dataset",
    "dateCreated": "2021-01-31T17:38:55Z",
    "dateModified": "2021-01-31T17:38:55Z",
    "description": "This MIT license dataset: https://github.com/iabufarha/ArSarcasm\r\n\r\nVia https://sites.google.com/view/ar-sarcasm-sentiment-detection/",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 797357901,
    "title": "Connection error",
    "dateCreated": "2021-01-30T07:32:45Z",
    "dateModified": "2021-01-30T07:32:45Z",
    "description": "Hi\r\nI am hitting to the error, help me and thanks.\r\n\r\n`train_data = datasets.load_dataset(\"xsum\", split=\"train\")`\r\n`ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/xsum/xsum.py`",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 797329905,
    "title": "Filter on dataset too much slowww",
    "dateCreated": "2021-01-30T04:09:19Z",
    "dateModified": "2021-01-30T04:09:19Z",
    "description": "I have a dataset with 50M rows.\r\nFor pre-processing, I need to tokenize this and filter rows with the large sequence.\r\n\r\nMy tokenization took roughly 12mins. I used `map()` with batch size 1024 and multi-process with 96 processes.\r\n\r\nWhen I applied the `filter()` function it is taking too much time. I need to filter sequences based on a boolean column.\r\nBelow are the variants I tried.\r\n1. filter() with batch size 1024, single process (takes roughly 3 hr)\r\n2. filter() with batch size 1024, 96 processes (takes 5-6 hrs \u00af\\\\\\_(\u30c4)\\_/\u00af)\r\n3. filter() with loading all data in memory, only a single boolean column (never ends).\r\n\r\nCan someone please help?\r\n\r\nBelow is a sample code for small dataset.\r\n\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('glue', 'mrpc', split='train')\r\ndataset = dataset.map(lambda x: {'flag': random.randint(0,1)==1})\r\n\r\ndef _amplify(data):\r\n        return data\r\n\r\ndataset = dataset.filter(_amplify, batch_size=1024, keep_in_memory=False, input_columns=['flag'])\r\n```\r\n",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 797021730,
    "title": "Custom formatting for lazy map + arrow data extraction refactor",
    "dateCreated": "2021-01-29T16:35:53Z",
    "dateModified": "2021-01-29T16:35:53Z",
    "description": "Hi !\r\n\r\nThis PR refactors the way data are extracted from pyarrow tables to extend it to the use of custom formatting functions.\r\n\r\nWhile the internal storage of the dataset is always the Apache Arrow format, by setting a specific format on a dataset, you can cast the output of `datasets.Dataset.__getitem__` in NumPy/pandas/PyTorch/TensorFlow, on-the-fly.\r\n\r\nA specific format can be activated with `datasets.Dataset.set_format`. For example: `dataset.set_format(type='torch', columns=['label'])`.\r\n\r\n### What's new:\r\n\r\nYou can now also define your own formatting function that is applied on-the-fly. To do so you can pass your formatting function in the `transform` parameter of `datasets.Dataset.set_format`, and keep `type` to `None`.\r\nA formatting function is a callable that takes a batch (as a dict, formatted as python) as input and returns a batch.\r\n\r\nHere is an example to tokenize and pad tokens on-the-fly when accessing the samples:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nfrom transformers import BertTokenizer\r\n\r\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\r\ndef encode(batch):\r\n    return tokenizer(batch[\"sentence1\"], padding=\"longest\", truncation=True, max_length=512, return_tensors=\"pt\")\r\n\r\ndataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\r\ndataset.set_format(transform=encode)\r\ndataset.format\r\n# {'type': 'custom', 'format_kwargs': {'transform': <function __main__.encode(batch)>}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}\r\ndataset[:2]\r\n# {'input_ids': tensor([[  101,  2572,  3217, ... 102]]), 'token_type_ids': tensor([[0, 0, 0, ... 0]]), 'attention_mask': tensor([[1, 1, 1, ... 1]])}\r\n```\r\n\r\nLet me know what you think of this API !\r\nWe can still change it if we want to.\r\n\r\nEspecially @sgugger since this may be useful when using `datasets` to train models.\r\n\r\nEDIT: this was changed to `dataset.set_transform(encode)`\r\n\r\n-------------------\r\n\r\nNote:\r\n\r\nI had to refactor the way data are extracted and formatted from pyarrow tables and I made it more robust and flexible. In particular I modularized it to be able to unit-test it properly. This was very helpful since I detected some bugs in the previous implementation and was able to fix them.\r\n\r\nSome bugs I found and fixed:\r\n- certain slices/ranges were not supported because negative ids were passed to pyarrow\r\n- formatting as numpy/torch/tensorflow a column would make it lose its precision information (for example a column as `Value(\"float32\")`) would be returned as a tensor of float64 (default behavior for numpy)\r\n- on windows  integers formatted as numpy/torch/tensorflow were not always int64 tensors by default but were int32 \r\n\r\nThe unit tests for those are now really extensive :)",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 796975588,
    "title": "Move silicone directory",
    "dateCreated": "2021-01-29T15:33:15Z",
    "dateModified": "2021-01-29T15:33:15Z",
    "description": "The dataset was added in #1761 but not in the right directory. I'm moving it to /datasets",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 796940299,
    "title": "Minor fix the docstring of load_metric",
    "dateCreated": "2021-01-29T14:47:35Z",
    "dateModified": "2021-01-29T14:47:35Z",
    "description": "Minor fix:\r\n- duplicated attributes\r\n- format fix",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 796934627,
    "title": "Allow loading dataset in-memory",
    "dateCreated": "2021-01-29T14:39:50Z",
    "dateModified": "2021-01-29T14:39:50Z",
    "description": "Allow loading datasets either from:\r\n- memory-mapped file (current implementation)\r\n- from file descriptor, copying data to physical memory\r\n\r\nClose #708",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 796924519,
    "title": "Small fix with corrected logging of train vectors",
    "dateCreated": "2021-01-29T14:26:06Z",
    "dateModified": "2021-01-29T14:26:06Z",
    "description": "Now you can set `train_size` to the whole dataset size via `train_size = -1` and login writes not `Training the index with the first -1 vectors` but (for example) `Training the index with the first 16123 vectors`. And maybe more than dataset length. Logging will be correct",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 796678157,
    "title": "ModuleNotFoundError: No module named 'apache_beam', when specific languages.",
    "dateCreated": "2021-01-29T08:17:24Z",
    "dateModified": "2021-01-29T08:17:24Z",
    "description": "```py\r\nimport datasets\r\nwiki = datasets.load_dataset('wikipedia', '20200501.ja', cache_dir='./datasets')\r\n```\r\nthen `ModuleNotFoundError: No module named 'apache_beam'` happend.\r\n\r\nThe error doesn't appear when it's '20200501.en'.\r\nI don't know Apache Beam, but according to #498 it isn't necessary when it's saved to local. is it correct?",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 796229721,
    "title": "[BUG FIX] typo in the import path for metrics",
    "dateCreated": "2021-01-28T18:01:37Z",
    "dateModified": "2021-01-28T18:01:37Z",
    "description": "This tiny PR fixes a typo introduced in https://github.com/huggingface/datasets/pull/1726 which prevents loading new metrics",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 795544422,
    "title": "Doc2dial rc",
    "dateCreated": "2021-01-27T23:51:00Z",
    "dateModified": "2021-01-27T23:51:00Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 795485842,
    "title": "Update the CommonGen citation information",
    "dateCreated": "2021-01-27T22:12:47Z",
    "dateModified": "2021-01-27T22:12:47Z",
    "description": "",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 795462816,
    "title": "How to use split dataset ",
    "dateCreated": "2021-01-27T21:37:47Z",
    "dateModified": "2021-01-27T21:37:47Z",
    "description": "![Capture1](https://user-images.githubusercontent.com/78090287/106057436-cb6a1f00-6111-11eb-8c9c-3658065b1fdf.PNG)\r\n\r\nHey,\r\nI want to split the lambada dataset into corpus, test, train and valid txt files (like penn treebank) but I am not able to achieve this. What I am doing is, executing the lambada.py file in my project but its not giving desired results. Any help will be appreciated!",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 795458856,
    "title": "Not enough disk space (Needed: Unknown size) when caching on a cluster",
    "dateCreated": "2021-01-27T21:30:59Z",
    "dateModified": "2021-01-27T21:30:59Z",
    "description": "I'm running some experiments where I'm caching datasets on a cluster and accessing it through multiple compute nodes. However, I get an error when loading the cached dataset from the shared disk.\r\n\r\nThe exact error thrown:\r\n\r\n```bash\r\n>>> load_dataset(dataset, cache_dir=\"/path/to/cluster/shared/path\")\r\nOSError: Not enough disk space. Needed: Unknown size (download: Unknown size, generated: Unknown size, post-processed: Unknown size)\r\n```\r\n\r\n\r\n[`utils.has_sufficient_disk_space`](https://github.com/huggingface/datasets/blob/8a03ab7d123a76ee744304f21ce868c75f411214/src/datasets/utils/py_utils.py#L332) fails on each job because of how the cluster system is designed (`disk_usage(\".\").free` can't compute on the cluster's shared disk).\r\n\r\n\r\nThis is exactly where the error gets thrown:\r\nhttps://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L502\r\n\r\n```python\r\nif not utils.has_sufficient_disk_space(self.info.size_in_bytes or 0, directory=self._cache_dir_root):\r\n    raise IOError(\r\n          \"Not enough disk space. Needed: {} (download: {}, generated: {}, post-processed: {})\".format(\r\n          utils.size_str(self.info.size_in_bytes or 0),\r\n          utils.size_str(self.info.download_size or 0),\r\n          utils.size_str(self.info.dataset_size or 0),\r\n          utils.size_str(self.info.post_processing_size or 0),\r\n       )\r\n    )\r\n\r\n```\r\n\r\nWhat would be a good way to circumvent this? my current fix is to manually comment out that part, but that is not ideal. \r\nWould it be possible to pass a flag to skip this check on disk space?",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 794659174,
    "title": "JSONDecodeError on JSON with multiple lines",
    "dateCreated": "2021-01-27T00:19:22Z",
    "dateModified": "2021-01-27T00:19:22Z",
    "description": "Hello :),\r\n\r\nI have been trying to load data using a JSON file. Based on the [docs](https://huggingface.co/docs/datasets/loading_datasets.html#json-files), the following format is supported:\r\n\r\n```json\r\n{\"key1\":11, \"key2\":12, \"key3\":13}\r\n{\"key1\":21, \"key2\":22, \"key3\":23}\r\n```\r\n But, when I try loading a dataset with the same format, I get a JSONDecodeError : `JSONDecodeError: Extra data: line 2 column 1 (char 7142)`. Now, this is expected when using `json` to load a JSON file. But I was wondering if there are any special arguments to pass when using `load_dataset` as the docs suggest that this format is supported.\r\n\r\nWhen I convert the JSON file to a list of dictionaries format, I get AttributeError: `AttributeError: 'list' object has no attribute 'keys'`. So, I can't convert them to list of dictionaries either.\r\n\r\nPlease let me know :)\r\n\r\nThanks,\r\nGunjan",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 794544495,
    "title": "Dataset Examples Explorer",
    "dateCreated": "2021-01-26T20:39:02Z",
    "dateModified": "2021-01-26T20:39:02Z",
    "description": "In the Older version of the Dataset, there are a useful Dataset Explorer that allow user to visualize the examples (training, test and validation) of a particular dataset, it is no longer there in current version.\r\n\r\nHope HuggingFace can re-enable the feature that at least allow viewing of  the first 20  examples of a particular dataset, or alternatively can extract 20 examples for each datasets and make those part of the Dataset Card Documentation.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 794167920,
    "title": "Update pyarrow import warning",
    "dateCreated": "2021-01-26T11:47:11Z",
    "dateModified": "2021-01-26T11:47:11Z",
    "description": "Update the minimum version to >=0.17.1 in the pyarrow version check and update the message.\r\n\r\nI also moved the check at the top of the __init__.py",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 793914556,
    "title": "AttributeError: module 'pyarrow' has no attribute 'PyExtensionType' during import ",
    "dateCreated": "2021-01-26T04:18:35Z",
    "dateModified": "2021-01-26T04:18:35Z",
    "description": "I'm using Colab. And suddenly this morning, there is this error. Have a look below!\r\n\r\n![screenshot-colab research google com-2021 01 26-08-15-36](https://user-images.githubusercontent.com/45964869/105799890-fdaf3b80-5fae-11eb-8f06-11b65cdccc30.png)\r\n",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 793882132,
    "title": "Update SciFact URL",
    "dateCreated": "2021-01-26T02:49:06Z",
    "dateModified": "2021-01-26T02:49:06Z",
    "description": "Hi,\r\n\r\nI'm following up this [issue](https://github.com/huggingface/datasets/issues/1717). I'm the SciFact dataset creator, and I'm trying to update the SciFact data url in your repo. Thanks again for adding the dataset!\r\n\r\nBasically, I'd just like to change the `_URL` to `\"https://scifact.s3-us-west-2.amazonaws.com/release/latest/data.tar.gz\"`. I changed `scifact.py` appropriately and tried running\r\n\r\n```\r\npython datasets-cli test datasets/scifact --save_infos --all_configs\r\n```\r\nwhich I was hoping would update the `dataset_infos.json` for SciFact. But for some reason the code still seems to be looking for the old version of the dataset. Full stack trace below. I've tried to clear all my Huggingface-related caches, and I've `git grep`'d to make sure that the old path to the dataset isn't floating around somewhere. So I'm not sure why this is happening?\r\n\r\nCan you help me switch the download URL?\r\n\r\n```\r\n(datasets) $ python datasets-cli test datasets/scifact --save_infos --all_configs\r\nChecking datasets/scifact/scifact.py for additional imports.\r\nFound main folder for dataset datasets/scifact/scifact.py at /Users/dwadden/.cache/huggingface/modules/datasets_modules/datasets/scifact\r\nFound specific version folder for dataset datasets/scifact/scifact.py at /Users/dwadden/.cache/huggingface/modules/datasets_modules/datasets/scifact/2b43b4e125ce3369da7d6353961d9d315e6593f24cc7bbe9ede5e5c911d11534\r\nFound script file from datasets/scifact/scifact.py to /Users/dwadden/.cache/huggingface/modules/datasets_modules/datasets/scifact/2b43b4e125ce3369da7d6353961d9d315e6593f24cc7bbe9ede5e5c911d11534/scifact.py\r\nFound dataset infos file from datasets/scifact/dataset_infos.json to /Users/dwadden/.cache/huggingface/modules/datasets_modules/datasets/scifact/2b43b4e125ce3369da7d6353961d9d315e6593f24cc7bbe9ede5e5c911d11534/dataset_infos.json\r\nFound metadata file for dataset datasets/scifact/scifact.py at /Users/dwadden/.cache/huggingface/modules/datasets_modules/datasets/scifact/2b43b4e125ce3369da7d6353961d9d315e6593f24cc7bbe9ede5e5c911d11534/scifact.json\r\nLoading Dataset Infos from /Users/dwadden/.cache/huggingface/modules/datasets_modules/datasets/scifact/2b43b4e125ce3369da7d6353961d9d315e6593f24cc7bbe9ede5e5c911d11534\r\nTesting builder 'corpus' (1/2)\r\nGenerating dataset scifact (/Users/dwadden/.cache/huggingface/datasets/scifact/corpus/1.0.0/2b43b4e125ce3369da7d6353961d9d315e6593f24cc7bbe9ede5e5c911d11534)\r\nDownloading and preparing dataset scifact/corpus (download: 2.72 MiB, generated: 7.63 MiB, post-processed: Unknown size, total: 10.35 MiB) to /Users/dwadden/.cache/huggingface/datasets/scifact/corpus/1.0.0/2b43b4e125ce3369da7d6353961d9d315e6593f24cc7bbe9ede5e5c911d11534...\r\nDownloading took 0.0 min\r\nChecksum Computation took 0.0 min\r\nTraceback (most recent call last):\r\n  File \"/Users/dwadden/proj/datasets/datasets-cli\", line 36, in <module>\r\n    service.run()\r\n  File \"/Users/dwadden/proj/datasets/src/datasets/commands/test.py\", line 139, in run\r\n    builder.download_and_prepare(\r\n  File \"/Users/dwadden/proj/datasets/src/datasets/builder.py\", line 562, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Users/dwadden/proj/datasets/src/datasets/builder.py\", line 622, in _download_and_prepare\r\n    verify_checksums(\r\n  File \"/Users/dwadden/proj/datasets/src/datasets/utils/info_utils.py\", line 32, in verify_checksums\r\n    raise ExpectedMoreDownloadedFiles(str(set(expected_checksums) - set(recorded_checksums)))\r\ndatasets.utils.info_utils.ExpectedMoreDownloadedFiles: {'https://ai2-s2-scifact.s3-us-west-2.amazonaws.com/release/2020-05-01/data.tar.gz'}\r\n```",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 793539703,
    "title": "Ignore definition line number of functions for caching",
    "dateCreated": "2021-01-25T16:42:29Z",
    "dateModified": "2021-01-25T16:42:29Z",
    "description": "As noticed in #1718 , when a function used for processing with `map` is moved inside its python file, then the change of line number causes the caching mechanism to consider it as a different function. Therefore in this case, it recomputes everything.\r\n\r\nThis is because we were not ignoring the line number definition for such functions (even though we're doing it for lambda functions).\r\n\r\nFor example this code currently prints False:\r\n```python\r\nfrom datasets.fingerprint import Hasher\r\n\r\n# define once\r\ndef foo(x):\r\n    return x\r\n\r\nh = Hasher.hash(foo)\r\n\r\n# define a second time elsewhere\r\ndef foo(x):\r\n    return x\r\n\r\nprint(h == Hasher.hash(foo))\r\n```\r\n\r\nI changed this by ignoring the line number for all functions.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 793474507,
    "title": "Narrative QA Manual",
    "dateCreated": "2021-01-25T15:22:31Z",
    "dateModified": "2021-01-25T15:22:31Z",
    "description": "Submitting the manual version of Narrative QA script which requires a manual download from the original repository",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 793273770,
    "title": "GPT2 MNLI training using run_glue.py",
    "dateCreated": "2021-01-25T10:53:52Z",
    "dateModified": "2021-01-25T10:53:52Z",
    "description": "Edit: I'm closing this because I actually meant to post this in `transformers `not `datasets`\r\n\r\nRunning this on Google Colab,\r\n\r\n```\r\n!python run_glue.py \\\r\n  --model_name_or_path gpt2 \\\r\n  --task_name mnli \\\r\n  --do_train \\\r\n  --do_eval \\\r\n  --max_seq_length 128 \\\r\n  --per_gpu_train_batch_size 10 \\\r\n  --gradient_accumulation_steps 32\\\r\n  --learning_rate 2e-5 \\\r\n  --num_train_epochs 3.0 \\\r\n  --output_dir models/gpt2/mnli/\r\n```\r\n\r\nI get the following error,\r\n\r\n```\r\n \"Asking to pad but the tokenizer does not have a padding token. \"\r\nValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\r\n```\r\n\r\nDo I need to modify the trainer to work with GPT2 ?",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 792755249,
    "title": "[Question & Bug Report] Can we preprocess a dataset on the fly?",
    "dateCreated": "2021-01-24T09:28:24Z",
    "dateModified": "2021-01-24T09:28:24Z",
    "description": "I know we can use `Datasets.map` to preprocess a dataset, but I'm using it with very large corpus which generates huge cache file (several TB cache from a 400 GB text file). I have no disk large enough to save it.  Can we preprocess a dataset on the fly without generating cache?\r\n\r\nBTW, I tried raising `writer_batch_size`. Seems that argument doesn't have any effect when it's larger than `batch_size`, because you are saving all the batch instantly after it's processed. Please check the following code:\r\n\r\nhttps://github.com/huggingface/datasets/blob/0281f9d881f3a55c89aeaa642f1ba23444b64083/src/datasets/arrow_dataset.py#L1532",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 792742120,
    "title": "Efficient ways to iterate the dataset",
    "dateCreated": "2021-01-24T07:54:31Z",
    "dateModified": "2021-01-24T07:54:31Z",
    "description": "For a large dataset that does not fits the memory, how can I select only a subset of features from each example?\r\n\r\nIf I iterate over the dataset and then select the subset of features one by one, the resulted memory usage will be huge. Any ways to solve this?\r\n\r\nThanks",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 792730559,
    "title": "is it possible to make slice to be more compatible like python list and numpy?",
    "dateCreated": "2021-01-24T06:15:52Z",
    "dateModified": "2021-01-24T06:15:52Z",
    "description": "Hi,\r\nsee below error:\r\n```\r\nAssertionError: Requested slice [:10000000000000000] incompatible with 20 examples.\r\n```",
    "status": "open",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 792708160,
    "title": "bug in loading datasets ",
    "dateCreated": "2021-01-24T02:53:45Z",
    "dateModified": "2021-01-24T02:53:45Z",
    "description": "Hi,\r\nI need to load a dataset, I use these commands:\r\n\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('csv', data_files={'train': 'sick/train.csv',\r\n                                          'test':  'sick/test.csv',\r\n                                          'validation': 'sick/validation.csv'})\r\nprint(dataset['validation'])\r\n```\r\nthe dataset in sick/train.csv are simple csv files representing the data. I am getting this error, do you have an idea how I can solve this? thank you @lhoestq \r\n\r\n                            \r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset csv/default-61468fc71a743ec1 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /julia/cache_home_2/datasets/csv/default-61468fc71a743ec1/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\r\nTraceback (most recent call last):\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/builder.py\", line 485, in incomplete_dir\r\n    yield tmp_dir\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/builder.py\", line 527, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/builder.py\", line 604, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/builder.py\", line 959, in _prepare_split\r\n    for key, table in utils.tqdm(generator, unit=\" tables\", leave=False, disable=not_verbose):\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/tqdm-4.49.0-py3.7.egg/tqdm/std.py\", line 1133, in __iter__\r\n    for obj in iterable:\r\n  File \"/julia/cache_home_2/modules/datasets_modules/datasets/csv/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2/csv.py\", line 129, in _generate_tables\r\n    for batch_idx, df in enumerate(csv_file_reader):\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/pandas-1.2.0-py3.7-linux-x86_64.egg/pandas/io/parsers.py\", line 1029, in __next__\r\n    return self.get_chunk()\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/pandas-1.2.0-py3.7-linux-x86_64.egg/pandas/io/parsers.py\", line 1079, in get_chunk\r\n    return self.read(nrows=size)\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/pandas-1.2.0-py3.7-linux-x86_64.egg/pandas/io/parsers.py\", line 1052, in read\r\n    index, columns, col_dict = self._engine.read(nrows)\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/pandas-1.2.0-py3.7-linux-x86_64.egg/pandas/io/parsers.py\", line 2056, in read\r\n    data = self._reader.read(nrows)\r\n  File \"pandas/_libs/parsers.pyx\", line 756, in pandas._libs.parsers.TextReader.read\r\n  File \"pandas/_libs/parsers.pyx\", line 783, in pandas._libs.parsers.TextReader._read_low_memory\r\n  File \"pandas/_libs/parsers.pyx\", line 827, in pandas._libs.parsers.TextReader._read_rows\r\n  File \"pandas/_libs/parsers.pyx\", line 814, in pandas._libs.parsers.TextReader._tokenize_rows\r\n  File \"pandas/_libs/parsers.pyx\", line 1951, in pandas._libs.parsers.raise_parser_error\r\npandas.errors.ParserError: Error tokenizing data. C error: Expected 1 fields in line 37, saw 2\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"write_sick.py\", line 19, in <module>\r\n    'validation': 'sick/validation.csv'})\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/load.py\", line 612, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/builder.py\", line 534, in download_and_prepare\r\n    self._save_info()\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/contextlib.py\", line 130, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/builder.py\", line 491, in incomplete_dir\r\n    shutil.rmtree(tmp_dir)\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/shutil.py\", line 498, in rmtree\r\n    onerror(os.rmdir, path, sys.exc_info())\r\n  File \"/julia/libs/anaconda3/envs/success/lib/python3.7/shutil.py\", line 496, in rmtree\r\n    os.rmdir(path)\r\nOSError: [Errno 39] Directory not empty: '/julia/cache_home_2/datasets/csv/default-61468fc71a743ec1/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2.incomplete'\r\n```\r\n\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 792703797,
    "title": "Adding SICK dataset",
    "dateCreated": "2021-01-24T02:15:31Z",
    "dateModified": "2021-01-24T02:15:31Z",
    "description": "Hi\r\nIt would be great to include SICK dataset.\r\n\r\n## Adding a Dataset\r\n- **Name:** SICK\r\n- **Description:** a well known entailment dataset \r\n- **Paper:** http://marcobaroni.org/composes/sick.html\r\n- **Data:** http://marcobaroni.org/composes/sick.html\r\n- **Motivation:** this is an important NLI benchmark\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n\r\n\r\n\r\nthanks",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 792701276,
    "title": "Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.2.1/datasets/csv/csv.py",
    "dateCreated": "2021-01-24T01:53:52Z",
    "dateModified": "2021-01-24T01:53:52Z",
    "description": "Hi,\r\nWhen I load_dataset from local csv files, below error happened, looks raw.githubusercontent.com was blocked by the chinese government. But why it need to download csv.py? should it include when pip install the dataset?\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/tom/pyenv/pystory/lib/python3.6/site-packages/datasets/load.py\", line 267, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/home/tom/pyenv/pystory/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 343, in cached_path\r\n    max_retries=download_config.max_retries,\r\n  File \"/home/tom/pyenv/pystory/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 617, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.2.1/datasets/csv/csv.py\r\n\r\n```",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 792698148,
    "title": "how can I combine 2 dataset with different/same features?",
    "dateCreated": "2021-01-24T01:26:06Z",
    "dateModified": "2021-01-24T01:26:06Z",
    "description": "to combine 2 dataset by one-one map like ds = zip(ds1, ds2):\r\nds1: {'text'}, ds2: {'text'}, combine ds:{'src', 'tgt'} \r\nor different feature:\r\nds1: {'src'}, ds2: {'tgt'}, combine ds:{'src', 'tgt'}",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 792523284,
    "title": "_pickle.PicklingError: Can't pickle typing.Union[str, NoneType]: it's not the same object as typing.Union when calling datasets.map with num_proc=2",
    "dateCreated": "2021-01-23T10:13:00Z",
    "dateModified": "2021-01-23T10:13:00Z",
    "description": "It may be a bug of multiprocessing with Datasets, when I disable the multiprocessing by set num_proc to None, everything works fine.\r\n\r\nThe script I use is https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm_wwm.py\r\n\r\nScript args:\r\n\r\n```\r\n--model_name_or_path\r\n../../../model/chinese-roberta-wwm-ext\r\n--train_file\r\n/nfs/volume-377-2/bert/data/test/train.txt\r\n--output_dir\r\ntest\r\n--do_train\r\n--per_device_train_batch_size\r\n2\r\n--gradient_accumulation_steps\r\n2\r\n--learning_rate\r\n1e-4\r\n--max_steps\r\n1000\r\n--warmup_steps\r\n10\r\n--save_steps\r\n1000\r\n--save_total_limit\r\n1\r\n--seed\r\n23333\r\n--max_seq_length\r\n512\r\n--preprocessing_num_workers\r\n2\r\n--cache_dir\r\n/nfs/volume-377-2/bert/data/test/cache\r\n```\r\n\r\nWhere the `/nfs/volume-377-2/bert/data/test/train.txt` is just a toy example with 10000 lines of random string, you should be able to reproduce this error esaily.\r\n\r\nFull Traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/nfs/volume-377-2/bert/transformers/examples/language-modeling/run_mlm_wwm.py\", line 398, in <module>\r\n    main()\r\n  File \"/nfs/volume-377-2/bert/transformers/examples/language-modeling/run_mlm_wwm.py\", line 325, in main\r\n    load_from_cache_file=not data_args.overwrite_cache,\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/datasets/dataset_dict.py\", line 303, in map\r\n    for k, dataset in self.items()\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/datasets/dataset_dict.py\", line 303, in <dictcomp>\r\n    for k, dataset in self.items()\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/datasets/arrow_dataset.py\", line 1318, in map\r\n    transformed_shards = [r.get() for r in results]\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/datasets/arrow_dataset.py\", line 1318, in <listcomp>\r\n    transformed_shards = [r.get() for r in results]\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/multiprocess/pool.py\", line 644, in get\r\n    raise self._value\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/multiprocess/pool.py\", line 424, in _handle_tasks\r\n    put(task)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/multiprocess/connection.py\", line 209, in send\r\n    self._send_bytes(_ForkingPickler.dumps(obj))\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/multiprocess/reduction.py\", line 54, in dumps\r\n    cls(buf, protocol, *args, **kwds).dump(obj)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py\", line 446, in dump\r\n    StockPickler.dump(self, obj)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 409, in dump\r\n    self.save(obj)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 751, in save_tuple\r\n    save(element)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py\", line 933, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py\", line 1438, in save_function\r\n    obj.__dict__, fkwdefaults), obj=obj)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 610, in save_reduce\r\n    save(args)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 751, in save_tuple\r\n    save(element)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 736, in save_tuple\r\n    save(element)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py\", line 1170, in save_cell\r\n    pickler.save_reduce(_create_cell, (f,), obj=obj)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 610, in save_reduce\r\n    save(args)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 736, in save_tuple\r\n    save(element)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 521, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 605, in save_reduce\r\n    save(cls)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py\", line 1365, in save_type\r\n    obj.__bases__, _dict), obj=obj)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 610, in save_reduce\r\n    save(args)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 751, in save_tuple\r\n    save(element)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py\", line 933, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py\", line 933, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 507, in save\r\n    self.save_global(obj, rv)\r\n  File \"/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py\", line 927, in save_global\r\n    (obj, module_name, name))\r\n_pickle.PicklingError: Can't pickle typing.Union[str, NoneType]: it's not the same object as typing.Union\r\n```\r\n",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 792150745,
    "title": "Mention kwargs in the Dataset Formatting docs",
    "dateCreated": "2021-01-22T16:43:20Z",
    "dateModified": "2021-01-22T16:43:20Z",
    "description": "Hi,\r\n\r\nThis was discussed in Issue #1762 where the docs didn't mention that keyword arguments to `datasets.Dataset.set_format()` are allowed. \r\nTo prevent people from having to check the code/method docs, I just added a couple of lines in the docs.\r\n\r\nPlease let me know your thoughts on this.\r\n\r\nThanks,\r\nGunjan\r\n\r\n@lhoestq ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 792068497,
    "title": "Add Librispeech ASR",
    "dateCreated": "2021-01-22T14:54:37Z",
    "dateModified": "2021-01-22T14:54:37Z",
    "description": "This PR adds the librispeech asr dataset: https://www.tensorflow.org/datasets/catalog/librispeech\r\n\r\nThere are 2 configs: \"clean\" and \"other\" whereas there are two \"train\" datasets for \"clean\", hence the name \"train.100\" and \"train.360\".\r\n\r\nAs suggested by @lhoestq, due to the enormous size of the dataset in `.arrow` format, the speech files are not directly prepared to a float32-array, but instead just the path to the array file is stored.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 792044105,
    "title": "Issues when run two programs compute the same metrics",
    "dateCreated": "2021-01-22T14:22:55Z",
    "dateModified": "2021-01-22T14:22:55Z",
    "description": "I got the following error when running two different programs that both compute sacreblue metrics. It seems that both read/and/write to the same location (.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow) where it caches the batches:\r\n\r\n```\r\nFile \"train_matching_min.py\", line 160, in <module>ch_9_label\r\n    avg_loss = valid(epoch, args.batch, args.validation, args.with_label)\r\n  File \"train_matching_min.py\", line 93, in valid\r\n    bleu += eval.compute()\r\n  File \"/u/tlhoang/projects/seal/match/models/eval.py\", line 23, in compute\r\n    return self.metric.compute()['score']\r\n  File \"/dccstor/know/anaconda3/lib/python3.7/site-packages/datasets/metric.py\", line 387, in compute\r\n    self._finalize()\r\n  File \"/dccstor/know/anaconda3/lib/python3.7/site-packages/datasets/metric.py\", line 355, in _finalize\r\n    self.data = Dataset(**reader.read_files([{\"filename\": f} for f in file_paths]))\r\n  File \"/dccstor/know/anaconda3/lib/python3.7/site-packages/datasets/arrow_reader.py\", line 231, in read_files\r\n    pa_table = self._read_files(files)\r\n  File \"/dccstor/know/anaconda3/lib/python3.7/site-packages/datasets/arrow_reader.py\", line 170, in _read_files\r\n    pa_table: pa.Table = self._get_dataset_from_filename(f_dict)\r\n  File \"/dccstor/know/anaconda3/lib/python3.7/site-packages/datasets/arrow_reader.py\", line 299, in _get_dataset_from_filename\r\n    pa_table = f.read_all()\r\n  File \"pyarrow/ipc.pxi\", line 481, in pyarrow.lib.RecordBatchReader.read_all\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Expected to read 1819307375 metadata bytes, but only read 454396\r\n``` ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 791553065,
    "title": "Error iterating over Dataset with DataLoader",
    "dateCreated": "2021-01-21T22:56:45Z",
    "dateModified": "2021-01-21T22:56:45Z",
    "description": "I have a Dataset that I've mapped a tokenizer over:\r\n\r\n```\r\nencoded_dataset.set_format(type='torch',columns=['attention_mask','input_ids','token_type_ids'])\r\nencoded_dataset[:1]\r\n```\r\n```\r\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\r\n 'input_ids': tensor([[  101,   178,  1198,  1400,  1714, 22233, 21365,  4515,  8618,  1113,\r\n            102]]),\r\n 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\r\n```\r\n\r\nWhen I try to iterate as in the docs, I get errors:\r\n\r\n```\r\ndataloader = torch.utils.data.DataLoader(encoded_dataset, batch_sampler=32)\r\nnext(iter(dataloader))\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-45-05180ba8aa35> in <module>()\r\n      1 dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_sampler=32)\r\n----> 2 next(iter(dataloader))\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py in __init__(self, loader)\r\n    411         self._timeout = loader.timeout\r\n    412         self._collate_fn = loader.collate_fn\r\n--> 413         self._sampler_iter = iter(self._index_sampler)\r\n    414         self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()\r\n    415         self._persistent_workers = loader.persistent_workers\r\n\r\nTypeError: 'int' object is not iterable\r\n\r\n\r\n```",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 791486860,
    "title": "Connection Issues",
    "dateCreated": "2021-01-21T20:56:09Z",
    "dateModified": "2021-01-21T20:56:09Z",
    "description": "Today, I am getting connection issues while loading a dataset and the metric.\r\n```\r\nTraceback (most recent call last):\r\n  File \"src/train.py\", line 180, in <module>\r\n    train_dataset, dev_dataset, test_dataset = create_race_dataset()\r\n  File \"src/train.py\", line 130, in create_race_dataset\r\n    train_dataset = load_dataset(\"race\", \"all\", split=\"train\")\r\n  File \"/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/load.py\", line 591, in load_dataset\r\n    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n  File \"/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/load.py\", line 267, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 343, in cached_path\r\n    max_retries=download_config.max_retries,\r\n  File \"/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 617, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.2.1/datasets/race/race.py\r\n```\r\n\r\nOr\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"src/train.py\", line 105, in <module>\r\n    rouge = datasets.load_metric(\"rouge\")\r\n  File \"/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/load.py\", line 500, in load_metric\r\n    dataset=False,\r\n  File \"/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/load.py\", line 267, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 343, in cached_path\r\n    max_retries=download_config.max_retries,\r\n  File \"/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 617, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.2.1/metrics/rouge/rouge.py\r\n```",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 791389763,
    "title": "PAWS-X: Fix csv Dictreader splitting data on quotes",
    "dateCreated": "2021-01-21T18:21:01Z",
    "dateModified": "2021-01-21T18:21:01Z",
    "description": "\r\n```python\r\nfrom datasets import load_dataset\r\n# load english paws-x dataset \r\ndatasets = load_dataset('paws-x', 'en')\r\nprint(len(datasets['train']))                     # outputs 49202 but official dataset has 49401 pairs\r\nprint(datasets['train'].unique('label'))     # outputs [1, 0, -1] but labels are binary [0,1]\r\n```\r\n\r\nchanged `data = csv.DictReader(f, delimiter=\"\\t\")` to `data = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)` in the dataloader to make csv module not split by quotes.\r\n\r\nThe results are as expected for all languages after the change.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 791226007,
    "title": "Unable to format dataset to CUDA Tensors",
    "dateCreated": "2021-01-21T15:31:23Z",
    "dateModified": "2021-01-21T15:31:23Z",
    "description": "Hi,\r\n\r\nI came across this [link](https://huggingface.co/docs/datasets/torch_tensorflow.html) where the docs show show to convert a dataset to a particular format. I see that there is an option to convert it to tensors, but I don't see any option to convert it to CUDA tensors.\r\n\r\nI tried this, but Dataset doesn't support assignment:\r\n```\r\n  columns=['input_ids', 'token_type_ids', 'attention_mask', 'start_positions','end_positions']\r\n\r\n        samples.set_format(type='torch', columns = columns)\r\n        for column in columns:\r\n            samples[column].to(torch.device(self.config.device))\r\n```\r\nThere should be an option to do so, or if there is already a way to do this, please let me know.\r\n\r\nThanks,\r\nGunjan",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 791150858,
    "title": "Add SILICONE benchmark",
    "dateCreated": "2021-01-21T14:29:12Z",
    "dateModified": "2021-01-21T14:29:12Z",
    "description": "My collaborators and I within the Affective Computing team at Telecom Paris would like to re-submit our spoken dialogue dataset for publication.\r\n\r\nThis is a new pull request relative to the [previously closed request](https://github.com/huggingface/datasets/pull/1712) which was reviewed by @lhoestq.\r\n\r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 791110857,
    "title": "More tags",
    "dateCreated": "2021-01-21T13:50:10Z",
    "dateModified": "2021-01-21T13:50:10Z",
    "description": "Since the hub v2 is going to be released soon I figured it would be great to add the missing tags at least for some of the datasets of reference listed [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md#write-the-loadingprocessing-code)",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 790992226,
    "title": "wikipedia dataset incomplete",
    "dateCreated": "2021-01-21T11:47:15Z",
    "dateModified": "2021-01-21T11:47:15Z",
    "description": "Hey guys,\r\n\r\nI am using the https://github.com/huggingface/datasets/tree/master/datasets/wikipedia dataset.\r\nUnfortunately, I found out that there is an incompleteness for the German dataset.\r\n For reasons unknown to me, the number of inhabitants has been removed from many pages:\r\nThorey-sur-Ouche has 128 inhabitants according to the webpage (https://de.wikipedia.org/wiki/Thorey-sur-Ouche).\r\nThe pickle file however shows: franz\u00f6sische Gemeinde mit  Einwohnern (Stand).\r\n Is it possible to fix this?\r\n\r\nBest regards \r\nChris\r\n",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 790626116,
    "title": "dataset.search() (elastic) cannot reliably retrieve search results",
    "dateCreated": "2021-01-21T02:26:37Z",
    "dateModified": "2021-01-21T02:26:37Z",
    "description": "I am trying to use elastic search to retrieve the indices of items in the dataset in their precise order, given shuffled training indices.\r\n\r\nThe problem I have is that I cannot retrieve reliable results with my data on my first search. I have to run the search **twice** to get the right answer.\r\n\r\nI am indexing data that looks like the following from the HF SQuAD 2.0 data set:\r\n\r\n```\r\n['57318658e6313a140071d02b',\r\n '56f7165e3d8e2e1400e3733a',\r\n '570e2f6e0b85d914000d7d21',\r\n '5727e58aff5b5019007d97d0',\r\n '5a3b5a503ff257001ab8441f',\r\n '57262fab271a42140099d725']\r\n```\r\n\r\n\r\n\r\nTo reproduce the issue, try:\r\n\r\n```\r\nfrom datasets import load_dataset, load_metric\r\nfrom transformers import BertTokenizerFast, BertForQuestionAnswering\r\nfrom elasticsearch import Elasticsearch\r\nimport numpy as np\r\nimport collections\r\nfrom tqdm.auto import tqdm\r\nimport torch\r\n\r\n# from https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=941LPhDWeYv-\r\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\r\nmax_length = 384 # The maximum length of a feature (question and context)\r\ndoc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\r\npad_on_right = tokenizer.padding_side == \"right\"\r\nsquad_v2 = True\r\n\r\n# from https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=941LPhDWeYv-\r\ndef prepare_validation_features(examples):\r\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\r\n    # in one example possible giving several features when a context is long, each of those features having a\r\n    # context that overlaps a bit the context of the previous feature.\r\n    tokenized_examples = tokenizer(\r\n        examples[\"question\" if pad_on_right else \"context\"],\r\n        examples[\"context\" if pad_on_right else \"question\"],\r\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\r\n        max_length=max_length,\r\n        stride=doc_stride,\r\n        return_overflowing_tokens=True,\r\n        return_offsets_mapping=True,\r\n        padding=\"max_length\",\r\n    )\r\n\r\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\r\n    # its corresponding example. This key gives us just that.\r\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\r\n\r\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\r\n    tokenized_examples[\"example_id\"] = []\r\n\r\n    for i in range(len(tokenized_examples[\"input_ids\"])):\r\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\r\n        sequence_ids = tokenized_examples.sequence_ids(i)\r\n        context_index = 1 if pad_on_right else 0\r\n\r\n        # One example can give several spans, this is the index of the example containing this span of text.\r\n        sample_index = sample_mapping[i]\r\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\r\n\r\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\r\n        # position is part of the context or not.\r\n        tokenized_examples[\"offset_mapping\"][i] = [\r\n            (list(o) if sequence_ids[k] == context_index else None)\r\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\r\n        ]\r\n\r\n    return tokenized_examples\r\n\r\n\r\n\r\n\r\n\r\n# build base examples, features set of training data\r\nshuffled_idx = pd.read_csv('https://raw.githubusercontent.com/afogarty85/temp/main/idx.csv')['idx'].to_list()\r\nexamples = load_dataset(\"squad_v2\").shuffle(seed=1)['train']\r\nfeatures = load_dataset(\"squad_v2\").shuffle(seed=1)['train'].map(\r\n    prepare_validation_features,\r\n    batched=True,\r\n    remove_columns=['answers', 'context', 'id', 'question', 'title'])\r\n# reorder features by the training process\r\nfeatures = features.select(indices=shuffled_idx)\r\n# get the example ids to match with the \"example\" data; get unique entries\r\nid_list = list(dict.fromkeys(features['example_id']))\r\n# now search for their index positions in the examples data set; load elastic search\r\nes = Elasticsearch([{'host': 'localhost'}]).ping()\r\n# add an index to the id column for the examples\r\nexamples.add_elasticsearch_index(column='id')\r\n# retrieve the example index\r\nexample_idx_k1 = [examples.search(index_name='id', query=i, k=1).indices for i in id_list]\r\nexample_idx_k1 = [item for sublist in example_idx_k1 for item in sublist]\r\n\r\nexample_idx_k2 = [examples.search(index_name='id', query=i, k=3).indices for i in id_list]\r\nexample_idx_k2 = [item for sublist in example_idx_k2 for item in sublist]\r\n\r\nlen(example_idx_k1)  # should be 130319\r\nlen(example_idx_k2)  # should be 130319\r\n\r\n#trial 1 lengths:\r\n# k=1: 130314\r\n# k=3: 130319\r\n\r\n# trial 2:\r\n# just run k=3 first: 130310\r\n# try k=1 after k=3: 130319\r\n```\r\n\r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 790466509,
    "title": "FewRel",
    "dateCreated": "2021-01-20T23:56:03Z",
    "dateModified": "2021-01-20T23:56:03Z",
    "description": "## Adding a Dataset\r\n- **Name:** FewRel\r\n- **Description:**  Large-Scale Supervised Few-Shot Relation Classification Dataset\r\n- **Paper:** @inproceedings{han2018fewrel,\r\n               title={FewRel:A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation},\r\n               author={Han, Xu and Zhu, Hao and Yu, Pengfei and Wang, Ziyun and Yao, Yuan and Liu, Zhiyuan and Sun, Maosong},\r\n               booktitle={EMNLP},\r\n               year={2018}}\r\n- **Data:** https://github.com/ProKil/FewRel\r\n- **Motivation:** relationship extraction dataset that's been used by some state of the art systems that should be incorporated.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 790380028,
    "title": "Ccaligned multilingual translation dataset",
    "dateCreated": "2021-01-20T22:18:44Z",
    "dateModified": "2021-01-20T22:18:44Z",
    "description": "## Adding a Dataset\r\n- **Name:** *name of the dataset*\r\n- **Description:** *short description of the dataset (or link to social media or blog post)*\r\n- CCAligned consists of parallel or comparable web-document pairs in 137 languages aligned with English. These web-document pairs were constructed by performing language identification on raw web-documents, and ensuring corresponding language codes were corresponding in the URLs of web documents. This pattern matching approach yielded more than 100 million aligned documents paired with English. Recognizing that each English document was often aligned to mulitple documents in different target language, we can join on English documents to obtain aligned documents that directly pair two non-English documents (e.g., Arabic-French).\r\n- **Paper:** *link to the dataset paper if available*\r\n- https://www.aclweb.org/anthology/2020.emnlp-main.480.pdf\r\n- **Data:** *link to the Github repository or current dataset location*\r\n- http://www.statmt.org/cc-aligned/\r\n- **Motivation:** *what are some good reasons to have this dataset*\r\n- The authors says it's an high quality dataset.\r\n- it's pretty large and includes many language pairs. It could be interesting training mt5 on this task.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 790324734,
    "title": "Using select/reordering datasets slows operations down immensely",
    "dateCreated": "2021-01-20T21:12:12Z",
    "dateModified": "2021-01-20T21:12:12Z",
    "description": "I am using portions of HF's helpful work in preparing / scoring the SQuAD 2.0 data. The problem I have is that after using `select` to re-ordering the dataset, computations slow down immensely where the total scoring process on 131k training examples would take maybe 3 minutes, now take over an hour.\r\n\r\nThe below example should be reproducible and I have ran myself down this path because I want to use HF's scoring functions and helpful data preparation, but use my own trainer. The training process uses shuffle and therefore the order I trained on no longer matches the original data set order. So, to score my results correctly, the original data set needs to match the order of the training. This requires that I: (1) collect the index for each row of data emitted during training, and (2) use this index information to re-order the datasets correctly so the orders match when I go to score.\r\n\r\n\r\nThe problem is, the dataset class starts performing very poorly as soon as you start manipulating its order by immense magnitudes.\r\n\r\n\r\n\r\n```\r\nfrom datasets import load_dataset, load_metric\r\nfrom transformers import BertTokenizerFast, BertForQuestionAnswering\r\nfrom elasticsearch import Elasticsearch\r\nimport numpy as np\r\nimport collections\r\nfrom tqdm.auto import tqdm\r\nimport torch\r\n\r\n# from https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=941LPhDWeYv-\r\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\r\nmax_length = 384 # The maximum length of a feature (question and context)\r\ndoc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\r\npad_on_right = tokenizer.padding_side == \"right\"\r\nsquad_v2 = True\r\n\r\n# from https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=941LPhDWeYv-\r\ndef prepare_validation_features(examples):\r\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\r\n    # in one example possible giving several features when a context is long, each of those features having a\r\n    # context that overlaps a bit the context of the previous feature.\r\n    tokenized_examples = tokenizer(\r\n        examples[\"question\" if pad_on_right else \"context\"],\r\n        examples[\"context\" if pad_on_right else \"question\"],\r\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\r\n        max_length=max_length,\r\n        stride=doc_stride,\r\n        return_overflowing_tokens=True,\r\n        return_offsets_mapping=True,\r\n        padding=\"max_length\",\r\n    )\r\n\r\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\r\n    # its corresponding example. This key gives us just that.\r\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\r\n\r\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\r\n    tokenized_examples[\"example_id\"] = []\r\n\r\n    for i in range(len(tokenized_examples[\"input_ids\"])):\r\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\r\n        sequence_ids = tokenized_examples.sequence_ids(i)\r\n        context_index = 1 if pad_on_right else 0\r\n\r\n        # One example can give several spans, this is the index of the example containing this span of text.\r\n        sample_index = sample_mapping[i]\r\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\r\n\r\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\r\n        # position is part of the context or not.\r\n        tokenized_examples[\"offset_mapping\"][i] = [\r\n            (list(o) if sequence_ids[k] == context_index else None)\r\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\r\n        ]\r\n\r\n    return tokenized_examples\r\n\r\n# from https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=941LPhDWeYv-\r\ndef postprocess_qa_predictions(examples, features, starting_logits, ending_logits, n_best_size = 20, max_answer_length = 30):\r\n    all_start_logits, all_end_logits = starting_logits, ending_logits\r\n    # Build a map example to its corresponding features.\r\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\r\n    features_per_example = collections.defaultdict(list)\r\n\r\n    for i, feature in enumerate(features):\r\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\r\n\r\n    # The dictionaries we have to fill.\r\n    predictions = collections.OrderedDict()\r\n\r\n    # Logging.\r\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\r\n\r\n    # Let's loop over all the examples!\r\n    for example_index, example in enumerate(tqdm(examples)):\r\n        # Those are the indices of the features associated to the current example.\r\n        feature_indices = features_per_example[example_index]\r\n\r\n        min_null_score = None # Only used if squad_v2 is True.\r\n        valid_answers = []\r\n\r\n        context = example[\"context\"]\r\n        # Looping through all the features associated to the current example.\r\n        for feature_index in feature_indices:\r\n\r\n            # We grab the predictions of the model for this feature.\r\n            start_logits = all_start_logits[feature_index]\r\n            end_logits = all_end_logits[feature_index]\r\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\r\n            # context.\r\n            offset_mapping = features[feature_index][\"offset_mapping\"]\r\n\r\n            # Update minimum null prediction.\r\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\r\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\r\n            if min_null_score is None or min_null_score < feature_null_score:\r\n                min_null_score = feature_null_score\r\n\r\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\r\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\r\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\r\n            for start_index in start_indexes:\r\n                for end_index in end_indexes:\r\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\r\n                    # to part of the input_ids that are not in the context.\r\n                    if (\r\n                        start_index >= len(offset_mapping)\r\n                        or end_index >= len(offset_mapping)\r\n                        or offset_mapping[start_index] is None\r\n                        or offset_mapping[end_index] is None\r\n                    ):\r\n                        continue\r\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\r\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\r\n                        continue\r\n\r\n                    start_char = offset_mapping[start_index][0]\r\n                    end_char = offset_mapping[end_index][1]\r\n                    valid_answers.append(\r\n                        {\r\n                            \"score\": start_logits[start_index] + end_logits[end_index],\r\n                            \"text\": context[start_char: end_char]\r\n                        }\r\n                    )\r\n\r\n\r\n        if len(valid_answers) > 0:\r\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\r\n        else:\r\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\r\n            # failure.\r\n            best_answer = {\"text\": \"\", \"score\": 0.0}\r\n\r\n        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\r\n        if not squad_v2:\r\n            predictions[example[\"id\"]] = best_answer[\"text\"]\r\n        else:\r\n            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\r\n            predictions[example[\"id\"]] = answer\r\n\r\n    return predictions\r\n\r\n\r\n\r\n# build base examples, features from training data\r\nexamples = load_dataset(\"squad_v2\").shuffle(seed=5)['train']\r\nfeatures = load_dataset(\"squad_v2\").shuffle(seed=5)['train'].map(\r\n    prepare_validation_features,\r\n    batched=True,\r\n    remove_columns=['answers', 'context', 'id', 'question', 'title'])\r\n\r\n# sim some shuffled training indices that we want to use to re-order the data to compare how we did\r\nshuffle_idx = np.arange(0, 131754)\r\nnp.random.shuffle(shuffle_idx)\r\n# create a new dataset with rows selected following the training shuffle\r\nfeatures = features.select(indices=shuffle_idx)\r\n# get unique example ids to match with the \"example\" data\r\nid_list = list(dict.fromkeys(features['example_id']))\r\n# now search for their index positions; load elastic search\r\nes = Elasticsearch([{'host': 'localhost'}]).ping()\r\n# add an index to the id column for the examples\r\nexamples.add_elasticsearch_index(column='id')\r\n# search the examples for their index position\r\nexample_idx = [examples.search(index_name='id', query=i, k=1).indices for i in id_list]\r\n# drop the elastic search\r\nexamples.drop_index(index_name='id')\r\n# put examples in the right order\r\nexamples = examples.select(indices=example_idx)\r\n\r\n# generate some fake data\r\nlogits = {'starting_logits': torch.randn(131754, 384), 'ending_logits': torch.randn(131754, 384)}\r\n\r\n\r\ndef score_squad(logits, n_best_size, max_answer):\r\n    # proceed with QA calculation\r\n    final_predictions = postprocess_qa_predictions(examples=examples,\r\n                                                   features=features,\r\n                                                   starting_logits=logits['starting_logits'],\r\n                                                   ending_logits=logits['ending_logits'],\r\n                                                   n_best_size=20,\r\n                                                   max_answer_length=30)\r\n    metric = load_metric(\"squad_v2\")\r\n    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\r\n    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\r\n    metrics = metric.compute(predictions=formatted_predictions, references=references)\r\n    return metrics\r\n\r\nmetrics = score_squad(logits, n_best_size=20, max_answer=30)\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 789881730,
    "title": "Use a config id in the cache directory names for custom configs",
    "dateCreated": "2021-01-20T11:11:00Z",
    "dateModified": "2021-01-20T11:11:00Z",
    "description": "As noticed by @JetRunner there was some issues when trying to generate a dataset using a custom config that is based on an existing config.\r\n\r\nFor example in the following code the `mnli_custom` would reuse the cache used to create `mnli` instead of generating a new dataset with the new label classes:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nmnli = load_dataset(\"glue\", \"mnli\")\r\nmnli_custom = load_dataset(\"glue\", \"mnli\", label_classes=[\"contradiction\", \"entailment\", \"neutral\"])\r\n```\r\n\r\nI fixed that by extending the cache directory definition of a dataset that is being generated.\r\nInstead of using the config name in the cache directory name, I switched to using a `config_id`.\r\n\r\nBy default it is equal to the config name.\r\nHowever the name of a config is not sufficent to have a unique identifier for the dataset being generated since it doesn't take into account:\r\n- the config kwargs that can be used to overwrite attributes\r\n- the custom features used to write the dataset\r\n- the data_files for json/text/csv/pandas datasets\r\n\r\nTherefore the config id is just the config name with an optional suffix based on these.\r\n\r\nIn particular taking into account the config kwargs fixes the issue with the `label_classes` above.\r\n\r\nI completed the current test cases by adding the case that was missing: overwriting an already existing config.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 789867685,
    "title": "fix comet citations",
    "dateCreated": "2021-01-20T10:52:38Z",
    "dateModified": "2021-01-20T10:52:38Z",
    "description": "I realized COMET citations were not showing in the hugging face metrics page:\r\n\r\n<img width=\"814\" alt=\"Screenshot 2021-01-20 at 09 48 44\" src=\"https://user-images.githubusercontent.com/17256847/105164848-8b9da900-5b0d-11eb-9e20-a38f559d2037.png\">\r\n\r\nThis pull request is intended to fix that.\r\n\r\nThanks!",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 789822459,
    "title": "COMET metric citation",
    "dateCreated": "2021-01-20T09:54:43Z",
    "dateModified": "2021-01-20T09:54:43Z",
    "description": "In my last pull request to add COMET metric, the citations where not following the usual \"format\". Because of that they where not correctly displayed on the website: \r\n\r\n<img width=\"814\" alt=\"Screenshot 2021-01-20 at 09 48 44\" src=\"https://user-images.githubusercontent.com/17256847/105158000-686efb80-5b05-11eb-8bb0-9c85fdac2938.png\">\r\n\r\nThis pull request is only intended to fix that.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 789232980,
    "title": "Updated README for the Social Bias Frames dataset",
    "dateCreated": "2021-01-19T17:53:00Z",
    "dateModified": "2021-01-19T17:53:00Z",
    "description": "See the updated card at https://github.com/mcmillanmajora/datasets/tree/add-SBIC-card/datasets/social_bias_frames. I incorporated information from the [SBIC data statement](https://homes.cs.washington.edu/~msap/social-bias-frames/DATASTATEMENT.html), paper, and the corpus README file included with the dataset download.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 788668085,
    "title": "Fix typo in README.md of cnn_dailymail",
    "dateCreated": "2021-01-19T03:06:05Z",
    "dateModified": "2021-01-19T03:06:05Z",
    "description": "When I read the README.md of `CNN/DailyMail Dataset`, there seems to be a typo `CCN`.\r\n\r\nI am afraid this is a trivial matter, but I would like to make a suggestion for revision.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 788476639,
    "title": "Added metadata and correct splits for swda.",
    "dateCreated": "2021-01-18T18:36:32Z",
    "dateModified": "2021-01-18T18:36:32Z",
    "description": "Switchboard Dialog Act Corpus\r\n\r\nI made some changes following  @bhavitvyamalik recommendation in #1678:\r\n\r\n* Contains all metadata.\r\n* Used official implementation from the [/swda](https://github.com/cgpotts/swda) repo.\r\n* Add official train and test splits used in [Stolcke et al. (2000)](https://web.stanford.edu/~jurafsky/ws97) and validation split used in [Probabilistic-RNN-DA-Classifier](https://github.com/NathanDuran/Probabilistic-RNN-DA-Classifier).",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 788431642,
    "title": "add Stuctured Argument Extraction for Korean dataset",
    "dateCreated": "2021-01-18T17:14:19Z",
    "dateModified": "2021-01-18T17:14:19Z",
    "description": "",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 788299775,
    "title": "datasets slicing with seed ",
    "dateCreated": "2021-01-18T14:08:55Z",
    "dateModified": "2021-01-18T14:08:55Z",
    "description": "Hi\r\nI need to slice a dataset with random seed, I looked into documentation here https://huggingface.co/docs/datasets/splits.html \r\nI could not find a seed option, could you assist me please how I can get a slice for different seeds?\r\nthank you.\r\n@lhoestq  ",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 788188184,
    "title": "Fix release conda worflow",
    "dateCreated": "2021-01-18T11:29:10Z",
    "dateModified": "2021-01-18T11:29:10Z",
    "description": "The current workflow yaml file is not valid according to https://github.com/huggingface/datasets/actions/runs/487638110",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 787838256,
    "title": "difference between wsc and wsc.fixed for superglue",
    "dateCreated": "2021-01-18T00:50:19Z",
    "dateModified": "2021-01-18T00:50:19Z",
    "description": "Hi\r\nI see two versions of wsc in superglue, and I am not sure what is the differences and which one is the original one. could you help to discuss the differences? thanks @lhoestq ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 787649811,
    "title": "Add missing \"brief\" entries to reuters",
    "dateCreated": "2021-01-17T07:58:49Z",
    "dateModified": "2021-01-17T07:58:49Z",
    "description": "This brings the number of examples for ModApte to match the stated `Training set (9,603 docs)...Test Set (3,299 docs)`",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 787631412,
    "title": "Issue while Creating Custom Metric",
    "dateCreated": "2021-01-17T07:01:14Z",
    "dateModified": "2021-01-17T07:01:14Z",
    "description": "Hi Team,\r\n\r\nI am trying to create a custom metric for my training as follows, where f1 is my own metric:\r\n\r\n```python\r\n def _info(self):\r\n        # TODO: Specifies the datasets.MetricInfo object\r\n        return datasets.MetricInfo(\r\n            # This is the description that will appear on the metrics page.\r\n            description=_DESCRIPTION,\r\n            citation=_CITATION,\r\n            inputs_description=_KWARGS_DESCRIPTION,\r\n            # This defines the format of each prediction and reference\r\n            features = datasets.Features({'predictions':datasets.Sequence(datasets.Value(\"int32\")), \"references\": datasets.Sequence(datasets.Value(\"int32\")),\"offset_mapping\":datasets.Sequence(datasets.Value(\"int32\")),'text':datasets.Sequence(datasets.Value('string')),\"ground\":datasets.Sequence(datasets.Value(\"int32\")),}),\r\n            # Homepage of the metric for documentation\r\n            homepage=\"http://metric.homepage\",\r\n            # Additional links to the codebase or references\r\n            codebase_urls=[\"http://github.com/path/to/codebase/of/new_metric\"],\r\n            reference_urls=[\"http://path.to.reference.url/new_metric\"]\r\n        )\r\n\r\n    def _compute(self,predictions,references,text,offset_mapping,spans):\r\n\r\n        pred_spans = []\r\n\r\n        for i,preds in enumerate(predictions):\r\n            current_preds = []\r\n            for j,token_preds in enumerate(preds):\r\n                if (preds>0.5):\r\n                    current_preds+=list(range(offset_mapping[i][j][0],offset_mapping[i][j][1]))\r\n            pred_spans.append(current_spans)\r\n        \r\n        return {\r\n            \"Token Wise F1\": f1_score(references,predictions,labels=[0,1]),\r\n            \"Offset Wise F1\": np.mean([f1(preds,gold) for preds,fold in zip(pred_spans,ground)])\r\n        }\r\n\r\n```\r\n\r\nI believe this is not correct. But that's not the issue I am facing right now. I get this error :\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-144-ed7349b50821> in <module>()\r\n----> 1 new_metric.compute(predictions=inputs[\"labels\"],references=inputs[\"labels\"], text=inputs[\"text\"], offset_mapping=inputs[\"offset_mapping\"],ground=inputs[\"ground\"] )\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/datasets/features.py in encode_batch(self, batch)\r\n    802         encoded_batch = {}\r\n    803         if set(batch) != set(self):\r\n--> 804             print(batch)\r\n    805             print(self)\r\n    806             raise ValueError(\"Column mismatch between batch {} and features {}\".format(set(batch), set(self)))\r\n\r\nValueError: Column mismatch between batch {'references', 'predictions'} and features {'ground', 'predictions', 'offset_mapping', 'text', 'references'}\r\n```\r\nOn checking the features.py file, I see the call is made from add_batch() in metrics.py which only takes in predictions and references.\r\n\r\nHow do I make my custom metric work? Will it work with a trainer even if I am able to make this metric work?\r\n\r\nThanks,\r\nGunjan",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 787623640,
    "title": "Add GLUE Compat (compatible with transformers<3.5.0)",
    "dateCreated": "2021-01-17T05:54:25Z",
    "dateModified": "2021-01-17T05:54:25Z",
    "description": "Link to our discussion on Slack (HF internal)\r\nhttps://huggingface.slack.com/archives/C014N4749J9/p1609668119337400\r\n\r\nThe next step is to add a compatible option in the new `run_glue.py`\r\n\r\nI duplicated `glue` and made the following changes:\r\n1. Change the name to `glue_compat`.\r\n2. Change the label assignments for MNLI and AX.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 787327060,
    "title": "error when run fine_tuning on text_classification",
    "dateCreated": "2021-01-16T02:23:19Z",
    "dateModified": "2021-01-16T02:23:19Z",
    "description": "dataset:sem_eval_2014_task_1\r\npretrained_model:bert-base-uncased\r\n\r\nerror description:\r\nwhen i use these resoruce to train fine_tuning a text_classification on sem_eval_2014_task_1,there always be some problem(when i use other dataset ,there exist the error too). And i followed the colab code (url:https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=TlqNaB8jIrJW).\r\n\r\n\r\nthe error is like this :\r\n`File \"train.py\", line 69, in <module>\r\n    trainer.train()\r\n  File \"/home/projects/anaconda3/envs/calibration/lib/python3.7/site-packages/transformers/trainer.py\", line 784, in train\r\n    for step, inputs in enumerate(epoch_iterator):\r\n  File \"/home/projects/anaconda3/envs/calibration/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 435, in __next__\r\n    data = self._next_data()\r\n  File \"/home/projects/anaconda3/envs/calibration/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 475, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"/home/projects/anaconda3/envs/calibration/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/home/projects/anaconda3/envs/calibration/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\nKeyError: 2`\r\n\r\nthis is my code :\r\n```dataset_name = 'sem_eval_2014_task_1'\r\nnum_labels_size = 3\r\nbatch_size = 4\r\nmodel_checkpoint = 'bert-base-uncased'\r\nnumber_train_epoch = 5\r\n\r\ndef tokenize(batch):\r\nreturn tokenizer(batch['premise'], batch['hypothesis'], truncation=True, )\r\n\r\ndef compute_metrics(pred):\r\nlabels = pred.label_ids\r\npreds = pred.predictions.argmax(-1)\r\nprecision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\r\nacc = accuracy_score(labels, preds)\r\nreturn {\r\n'accuracy': acc,\r\n'f1': f1,\r\n'precision': precision,\r\n'recall': recall\r\n}\r\n\r\nmodel = BertForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels_size)\r\ntokenizer = BertTokenizerFast.from_pretrained(model_checkpoint, use_fast=True)\r\n\r\ntrain_dataset = load_dataset(dataset_name, split='train')\r\ntest_dataset = load_dataset(dataset_name, split='test')\r\n\r\ntrain_encoded_dataset = train_dataset.map(tokenize, batched=True)\r\ntest_encoded_dataset = test_dataset.map(tokenize, batched=True)\r\n\r\nargs = TrainingArguments(\r\noutput_dir='./results',\r\nevaluation_strategy=\"epoch\",\r\nlearning_rate=2e-5,\r\nper_device_train_batch_size=batch_size,\r\nper_device_eval_batch_size=batch_size,\r\nnum_train_epochs=number_train_epoch,\r\nweight_decay=0.01,\r\ndo_predict=True,\r\n)\r\ntrainer = Trainer(\r\nmodel=model,\r\nargs=args,\r\ncompute_metrics=compute_metrics,\r\ntrain_dataset=train_encoded_dataset,\r\neval_dataset=test_encoded_dataset,\r\ntokenizer=tokenizer\r\n)\r\n\r\ntrainer.train()\r\ntrainer.evaluate()\r\n\r\n",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 787264605,
    "title": "add id_liputan6 dataset",
    "dateCreated": "2021-01-15T22:58:34Z",
    "dateModified": "2021-01-15T22:58:34Z",
    "description": "id_liputan6 is a large-scale Indonesian summarization dataset. The articles were harvested from an online news portal, and obtain 215,827 document-summary pairs: https://arxiv.org/abs/2011.00679",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 787219138,
    "title": "fixes and improvements for the WebNLG loader",
    "dateCreated": "2021-01-15T21:45:23Z",
    "dateModified": "2021-01-15T21:45:23Z",
    "description": "- fixes test sets loading in v3.0\r\n- adds additional fields for v3.0_ru\r\n- adds info to the WebNLG data card",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 786068440,
    "title": "Conda support",
    "dateCreated": "2021-01-14T15:11:25Z",
    "dateModified": "2021-01-14T15:11:25Z",
    "description": "Will push a new version on anaconda cloud every time a tag starting with `v` is pushed (like `v1.2.2`).\r\n\r\nWill appear here: https://anaconda.org/huggingface/datasets\r\n\r\nDepends on `conda-forge` for now, so the following is required for installation:\r\n\r\n```\r\nconda install -c huggingface -c conda-forge datasets\r\n```",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 785606286,
    "title": "update link in TLC to be github links",
    "dateCreated": "2021-01-14T02:49:21Z",
    "dateModified": "2021-01-14T02:49:21Z",
    "description": "Base on this issue https://github.com/huggingface/datasets/issues/1064, I can now use the official links.\r\n\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 785433854,
    "title": "Adjust BrWaC dataset features name",
    "dateCreated": "2021-01-13T20:39:04Z",
    "dateModified": "2021-01-13T20:39:04Z",
    "description": "I added this dataset some days ago, and today I used it to train some models and realized that the names of the features aren't so good.\r\n\r\nLooking at the current features hierarchy, we have \"paragraphs\" with a list of \"sentences\" with a list of \"sentences?!\". But the actual hierarchy is a \"text\" with a list of \"paragraphs\" with a list of \"sentences\".\r\n\r\nI confused myself trying to use the dataset with these names. So I think it's better to change it.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 785184740,
    "title": "Update add new dataset template",
    "dateCreated": "2021-01-13T15:08:09Z",
    "dateModified": "2021-01-13T15:08:09Z",
    "description": "This PR fixes a few typos in the \"Add new dataset template\" and clarifies a bit what to do for the dummy data creation when the `auto_generate` flag can't work.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 784956707,
    "title": "Fix empty token bug for `thainer` and `lst20`",
    "dateCreated": "2021-01-13T09:55:09Z",
    "dateModified": "2021-01-13T09:55:09Z",
    "description": "add a condition to check if tokens exist before yielding in `thainer` and `lst20`",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 784903002,
    "title": "connection issue with glue, what is the data url for glue? ",
    "dateCreated": "2021-01-13T08:37:40Z",
    "dateModified": "2021-01-13T08:37:40Z",
    "description": "Hi\r\nmy codes sometimes fails due to connection issue with glue, could you tell me how I can have the URL datasets library is trying to read GLUE from to test the machines I am working on if there is an issue on my side or not\r\nthanks ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 784874490,
    "title": "[GEM Dataset] Added TurkCorpus, an evaluation dataset for sentence simplification.",
    "dateCreated": "2021-01-13T07:50:19Z",
    "dateModified": "2021-01-13T07:50:19Z",
    "description": "We want to use TurkCorpus for validation and testing of the sentence simplification task. ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 784744674,
    "title": "Couldn't reach swda.py",
    "dateCreated": "2021-01-13T02:57:40Z",
    "dateModified": "2021-01-13T02:57:40Z",
    "description": "ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.2.0/datasets/swda/swda.py\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 784617525,
    "title": "Add MNIST dataset",
    "dateCreated": "2021-01-12T21:48:02Z",
    "dateModified": "2021-01-12T21:48:02Z",
    "description": "This PR adds the MNIST dataset to the library.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 784565898,
    "title": "Is there support for Deep learning datasets?",
    "dateCreated": "2021-01-12T20:22:41Z",
    "dateModified": "2021-01-12T20:22:41Z",
    "description": "I looked around this repository and looking the datasets I think that there's no support for images-datasets. Or am I missing something? For example to add a repo like this https://github.com/DZPeru/fish-datasets",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 784458342,
    "title": "Add an entry to an arrow dataset",
    "dateCreated": "2021-01-12T18:01:47Z",
    "dateModified": "2021-01-12T18:01:47Z",
    "description": "Is it possible to add an entry to a dataset object?\r\n\r\n**Motivation: I want to transform the sentences in the dataset and add them to the original dataset**\r\n\r\nFor example, say we have the following code:\r\n\r\n``` python\r\nfrom datasets import load_dataset\r\n\r\n# Load a dataset and print the first examples in the training set\r\nsquad_dataset = load_dataset('squad')\r\nprint(squad_dataset['train'][0])\r\n```\r\n\r\nIs it possible to add an entry to `squad_dataset`? Something like the following?\r\n\r\n``` python\r\nsquad_dataset.append({'text': \"This is a new sentence\"})\r\n```\r\n\r\nThe motivation for doing this is that I want to transform the sentences in the squad dataset and add them to the original dataset.\r\n\r\nIf the above doesn't work, is there any other way of achieving the motivation mentioned above? Perhaps by creating a new arrow dataset by using the older one and the transformer sentences?\r\n",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 784435131,
    "title": "BLEURT score calculation raises UnrecognizedFlagError",
    "dateCreated": "2021-01-12T17:27:02Z",
    "dateModified": "2021-01-12T17:27:02Z",
    "description": "Calling the `compute` method for **bleurt** metric fails with an `UnrecognizedFlagError` for `FLAGS.bleurt_batch_size`. \r\n\r\nMy environment:\r\n```\r\npython==3.8.5\r\ndatasets==1.2.0\r\ntensorflow==2.3.1\r\ncudatoolkit==11.0.221\r\n```\r\n\r\nTest code for reproducing the error:\r\n```\r\nfrom datasets import load_metric\r\nbleurt = load_metric('bleurt')\r\ngen_text = \"I am walking on the promenade today\"\r\nref_text = \"I am walking along the promenade on this sunny day\"\r\nbleurt.compute(predictions=[test_text], references=[test_text])\r\n```\r\n\r\nError Output:\r\n```\r\nUsing default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: datasets.load_metric('bleurt', 'bleurt-large-512').\r\nINFO:tensorflow:Reading checkpoint /home/ubuntu/.cache/huggingface/metrics/bleurt/default/downloads/extracted/9aee35580225730ac5422599f35c4986e4c49cafd08082123342b1019720dac4/bleurt-base-128.\r\nINFO:tensorflow:Config file found, reading.\r\nINFO:tensorflow:Will load checkpoint bert_custom\r\nINFO:tensorflow:Performs basic checks...\r\nINFO:tensorflow:... name:bert_custom\r\nINFO:tensorflow:... vocab_file:vocab.txt\r\nINFO:tensorflow:... bert_config_file:bert_config.json\r\nINFO:tensorflow:... do_lower_case:True\r\nINFO:tensorflow:... max_seq_length:128\r\nINFO:tensorflow:Creating BLEURT scorer.\r\nINFO:tensorflow:Loading model...\r\nINFO:tensorflow:BLEURT initialized.\r\n---------------------------------------------------------------------------\r\nUnrecognizedFlagError                     Traceback (most recent call last)\r\n<ipython-input-12-8b3f4322318a> in <module>\r\n      2 gen_text = \"I am walking on the promenade today\"\r\n      3 ref_text = \"I am walking along the promenade on this sunny day\"\r\n----> 4 bleurt.compute(predictions=[gen_text], references=[ref_text])\r\n\r\n~/anaconda3/envs/noved/lib/python3.8/site-packages/datasets/metric.py in compute(self, *args, **kwargs)\r\n    396             references = self.data[\"references\"]\r\n    397             with temp_seed(self.seed):\r\n--> 398                 output = self._compute(predictions=predictions, references=references, **kwargs)\r\n    399 \r\n    400             if self.buf_writer is not None:\r\n\r\n~/.cache/huggingface/modules/datasets_modules/metrics/bleurt/b1de33e1cbbcb1dbe276c887efa1fad68c6aff913885108078fa1ad408908778/bleurt.py in _compute(self, predictions, references)\r\n    103 \r\n    104     def _compute(self, predictions, references):\r\n--> 105         scores = self.scorer.score(references=references, candidates=predictions)\r\n    106         return {\"scores\": scores}\r\n\r\n~/anaconda3/envs/noved/lib/python3.8/site-packages/bleurt/score.py in score(self, references, candidates, batch_size)\r\n    164     \"\"\"\r\n    165     if not batch_size:\r\n--> 166       batch_size = FLAGS.bleurt_batch_size\r\n    167 \r\n    168     candidates, references = list(candidates), list(references)\r\n\r\n~/anaconda3/envs/noved/lib/python3.8/site-packages/tensorflow/python/platform/flags.py in __getattr__(self, name)\r\n     83     # a flag.\r\n     84     if not wrapped.is_parsed():\r\n---> 85       wrapped(_sys.argv)\r\n     86     return wrapped.__getattr__(name)\r\n     87 \r\n\r\n~/anaconda3/envs/noved/lib/python3.8/site-packages/absl/flags/_flagvalues.py in __call__(self, argv, known_only)\r\n    643     for name, value in unknown_flags:\r\n    644       suggestions = _helpers.get_flag_suggestions(name, list(self))\r\n--> 645       raise _exceptions.UnrecognizedFlagError(\r\n    646           name, value, suggestions=suggestions)\r\n    647 \r\n\r\nUnrecognizedFlagError: Unknown command line flag 'f'\r\n```\r\n\r\nPossible Fix:\r\nModify `_compute` method https://github.com/huggingface/datasets/blob/7e64851a12263dc74d41c668167918484c8000ab/metrics/bleurt/bleurt.py#L104\r\nto receive a `batch_size` argument, for example:\r\n```\r\ndef _compute(self, predictions, references, batch_size=1):\r\n    scores = self.scorer.score(references=references, candidates=predictions, batch_size=batch_size)\r\n    return {\"scores\": scores}\r\n```",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 784336370,
    "title": "Offline loading",
    "dateCreated": "2021-01-12T15:21:57Z",
    "dateModified": "2021-01-12T15:21:57Z",
    "description": "As discussed in #824 it would be cool to make the library work in offline mode.\r\nCurrently if there's not internet connection then modules (datasets or metrics) that have already been loaded in the past can't be loaded and it raises a ConnectionError.\r\nThis is because `prepare_module` fetches online for the latest version of the module.\r\n\r\nTo make it work in offline mode one suggestion was to reload the latest local version of the module.\r\nI implemented that and I also raise a warning saying that the module that is loaded is the latest local version.\r\n```python\r\nlogger.warning(\r\n    f\"Using the latest cached version of the module from {cached_module_path} since it \"\r\n    f\"couldn't be found locally at {input_path} or remotely ({error_type_that_prevented_reaching_out_remote_stuff}).\"\r\n)\r\n```\r\n\r\nI added tests to make sure it works as expected and I needed to do a few changes in the code to be able to test things properly. In particular I added a parameter `hf_modules_cache` to `init_dynamic_modules` for testing purposes. It makes it possible to have temporary modules caches for testing.\r\n\r\nI also added a `offline` context utility that allows to test part of the code by making all the requests fail as if there was no internet.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 784182273,
    "title": "load the local dataset",
    "dateCreated": "2021-01-12T12:12:55Z",
    "dateModified": "2021-01-12T12:12:55Z",
    "description": "your guidebook's example is like\r\n>>>from datasets import load_dataset\r\n>>> dataset = load_dataset('json', data_files='my_file.json')\r\nbut the first arg is path...\r\nso how should i do if i want to load the local dataset for model training?\r\ni will be grateful if you can help me handle this problem!\r\nthanks a lot!",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 783982100,
    "title": "ADD S3 support for downloading and uploading processed datasets",
    "dateCreated": "2021-01-12T07:17:34Z",
    "dateModified": "2021-01-12T07:17:34Z",
    "description": "# What does this PR do?\r\n\r\nThis PR adds the functionality to load and save `datasets` from and to s3. \r\nYou can save `datasets` with either `Dataset.save_to_disk()` or `DatasetDict.save_to_disk`. \r\nYou can load `datasets` with either `load_from_disk` or `Dataset.load_from_disk()`, `DatasetDict.load_from_disk()`. \r\n\r\nLoading `csv` or `json` datasets from s3 is not implemented. \r\n\r\nTo save/load datasets to s3 you either need to provide an `aws_profile`, which is set up on your machine, per default it uses the `default` profile or you have to pass an `aws_access_key_id` and `aws_secret_access_key`. \r\n\r\nThe implementation was done with the `fsspec` and `boto3`.\r\n\r\n\r\n### Example `aws_profile` :\r\n\r\n<details>\r\n\r\n```python\r\ndataset.save_to_disk(\"s3://moto-mock-s3-bucket/datasets/sdk\", aws_profile=\"hf-sm\")\r\n\r\nload_from_disk(\"s3://moto-mock-s3-bucket/datasets/sdk\", aws_profile=\"hf-sm\")\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Example `aws_access_key_id` and `aws_secret_access_key` :\r\n\r\n<details>\r\n\r\n```python\r\ndataset.save_to_disk(\"s3://moto-mock-s3-bucket/datasets/sdk\",\r\n                            aws_access_key_id=\"fake_access_key\", \r\n                            aws_secret_access_key=\"fake_secret_key\"\r\n                           )\r\n\r\nload_from_disk(\"s3://moto-mock-s3-bucket/datasets/sdk\",\r\n                            aws_access_key_id=\"fake_access_key\", \r\n                            aws_secret_access_key=\"fake_secret_key\"\r\n                           )\r\n```\r\n\r\n</details>\r\n\r\nIf you want to load a dataset from a public s3 bucket you can pass `anon=True` \r\n\r\n### Example  `anon=True` :\r\n\r\n<details>\r\n\r\n```python\r\ndataset.save_to_disk(\"s3://moto-mock-s3-bucket/datasets/sdk\", aws_profile=\"hf-sm\")\r\n\r\nload_from_disk(\"s3://moto-mock-s3-bucketdatasets/sdk\",anon=True)\r\n```\r\n\r\n</details>\r\n\r\n### Full Example\r\n\r\n```python\r\nimport datasets\r\n\r\ndataset = datasets.load_dataset(\"imdb\")\r\nprint(f\"DatasetDict contains {len(dataset)} datasets\")\r\nprint(f\"train Dataset has the size of: {len(dataset['train'])}\")\r\n\r\ndataset.save_to_disk(\"s3://moto-mock-s3-bucket/datasets/sdk\", aws_profile=\"hf-sm\")\r\n\r\nremote_dataset = datasets.load_from_disk(\"s3://moto-mock-s3-bucket/datasets/sdk\", aws_profile=\"hf-sm\")\r\nprint(f\"DatasetDict contains {len(remote_dataset)} datasets\")\r\nprint(f\"train Dataset has the size of: {len(remote_dataset['train'])}\")\r\n```\r\n\r\nRelated to #878 \r\n\r\n\r\nI would also adjust the documentation after the code would be reviewed, as long as I leave the PR in \"draft\" status. Something that we can consider is renaming the functions and changing the `_disk` maybe to `_filesystem` \r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 784023338,
    "title": "could not run models on a offline server successfully",
    "dateCreated": "2021-01-12T06:08:06Z",
    "dateModified": "2021-01-12T06:08:06Z",
    "description": "Hi, I really need your help about this.\r\nI am trying to fine-tuning a RoBERTa on a remote server, which is strictly banning internet. I try to install all the packages by hand and try to run run_mlm.py on the server. It works well on colab, but when I try to run it on this offline server, it shows:\r\n![image](https://user-images.githubusercontent.com/49967236/104276256-25a88600-546a-11eb-9776-8ec695dfa24e.png)\r\n\r\nis there anything I can do? Is it possible to download all the things in cache and upload it to the server? Please help me out...",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 783921679,
    "title": "Added unfiltered versions of the Wiki-Auto training data for the GEM simplification task.",
    "dateCreated": "2021-01-12T05:26:04Z",
    "dateModified": "2021-01-12T05:26:04Z",
    "description": "",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 783828428,
    "title": "[Scientific papers] Mirror datasets zip",
    "dateCreated": "2021-01-12T01:15:40Z",
    "dateModified": "2021-01-12T01:15:40Z",
    "description": "Datasets were uploading to https://s3.amazonaws.com/datasets.huggingface.co/scientific_papers/1.1.1/arxiv-dataset.zip and https://s3.amazonaws.com/datasets.huggingface.co/scientific_papers/1.1.1/pubmed-dataset.zip respectively to escape google drive quota and enable faster download. ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 783721833,
    "title": "Adding the NorNE dataset for NER",
    "dateCreated": "2021-01-11T21:34:13Z",
    "dateModified": "2021-01-11T21:34:13Z",
    "description": "NorNE is a manually annotated corpus of named entities which extends the annotation of the existing Norwegian Dependency Treebank. Comprising both of the official standards of written Norwegian (Bokm\u00e5l and Nynorsk), the corpus contains around 600,000 tokens and annotates a rich set of entity types including persons, organizations, locations, geo-political entities, products, and events, in addition to a class corresponding to nominals derived from names.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 783557542,
    "title": "Fix column list comparison in transmit format",
    "dateCreated": "2021-01-11T17:23:56Z",
    "dateModified": "2021-01-11T17:23:56Z",
    "description": "As noticed in #1718 the cache might not reload the cache files when new columns were added.\r\nThis is because of an issue in `transmit_format` where the column list comparison fails because the order was not deterministic. This causes the `transmit_format` to apply an unnecessary `set_format` transform with shuffled column names.\r\n\r\nI fixed that by sorting the columns for the comparison and added a test.\r\n\r\nTo properly test that I added a third column `col_3` to the dummy_dataset used for tests.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 783474753,
    "title": "Possible cache miss in datasets",
    "dateCreated": "2021-01-11T15:37:31Z",
    "dateModified": "2021-01-11T15:37:31Z",
    "description": "Hi,\r\n\r\nI am using the datasets package and even though I run the same data processing functions, datasets always recomputes the function instead of using cache.\r\nI have attached an example script that for me reproduces the problem.\r\nIn the attached example the second map function always recomputes instead of loading from cache.\r\nIs this a bug or am I doing something wrong?\r\nIs there a way for fix this and avoid all the recomputation?\r\n\r\nThanks\r\n\r\nEdit:\r\ntransformers==3.5.1\r\ndatasets==1.2.0\r\n\r\n```\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer\r\n\r\ndatasets = load_dataset('wikitext', 'wikitext-103-raw-v1')\r\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\r\n\r\n\r\ncolumn_names = datasets[\"train\"].column_names\r\ntext_column_name = \"text\" if \"text\" in column_names else column_names[0]\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\r\n\r\ntokenized_datasets = datasets.map(\r\n    tokenize_function,\r\n    batched=True,\r\n    num_proc=60,\r\n    remove_columns=[text_column_name],\r\n    load_from_cache_file=True,\r\n)\r\nmax_seq_length = tokenizer.model_max_length\r\ndef group_texts(examples):\r\n    # Concatenate all texts.\r\n    concatenated_examples = {\r\n        k: sum(examples[k], []) for k in examples.keys()}\r\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\r\n    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\r\n    # customize this part to your needs.\r\n    total_length = (total_length // max_seq_length) * max_seq_length\r\n    # Split by chunks of max_len.\r\n    result = {\r\n        k: [t[i: i + max_seq_length]\r\n            for i in range(0, total_length, max_seq_length)]\r\n        for k, t in concatenated_examples.items()\r\n    }\r\n    return result\r\n\r\ntokenized_datasets = tokenized_datasets.map(\r\n    group_texts,\r\n    batched=True,\r\n    num_proc=60,\r\n    load_from_cache_file=True,\r\n)\r\nprint(tokenized_datasets)\r\n\r\nprint('finished')\r\n```",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 783074255,
    "title": "SciFact dataset - minor changes",
    "dateCreated": "2021-01-11T05:26:40Z",
    "dateModified": "2021-01-11T05:26:40Z",
    "description": "Hi,\r\n\r\nSciFact dataset creator here. First of all, thanks for adding the dataset to Huggingface, much appreciated!\r\n\r\nI'd like to make a few minor changes, including the citation information and the `_URL` from which to download the dataset. Can I submit a PR for this?\r\n\r\nIt also looks like the dataset is being downloaded directly from Huggingface's Google cloud account rather than via the `_URL` in [scifact.py](https://github.com/huggingface/datasets/blob/master/datasets/scifact/scifact.py). Can you help me update the version on gcloud?\r\n\r\nThanks,\r\n\r\nDave",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 782819006,
    "title": "Add Hatexplain Dataset",
    "dateCreated": "2021-01-10T13:30:01Z",
    "dateModified": "2021-01-10T13:30:01Z",
    "description": "Adding Hatexplain - the first benchmark hate speech dataset covering multiple aspects of the issue",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 782754441,
    "title": "add Korean intonation-aided intention identification dataset",
    "dateCreated": "2021-01-10T06:29:04Z",
    "dateModified": "2021-01-10T06:29:04Z",
    "description": "",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 782416276,
    "title": "Adding adversarialQA dataset",
    "dateCreated": "2021-01-08T21:46:09Z",
    "dateModified": "2021-01-08T21:46:09Z",
    "description": "Adding the adversarialQA dataset (https://adversarialqa.github.io/) from Beat the AI (https://arxiv.org/abs/2002.00293)",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 782337723,
    "title": "Installation using conda",
    "dateCreated": "2021-01-08T19:12:15Z",
    "dateModified": "2021-01-08T19:12:15Z",
    "description": "Will a conda package for installing datasets be added to the huggingface conda channel? I have installed transformers using conda and would like to use the datasets library to use some of the scripts in the transformers/examples folder but am unable to do so at the moment as datasets can only be installed using pip and using pip in a conda environment is generally a bad idea in my experience.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 782313097,
    "title": "Silicone",
    "dateCreated": "2021-01-08T18:24:18Z",
    "dateModified": "2021-01-08T18:24:18Z",
    "description": "My collaborators and I within the Affective Computing team at Telecom Paris would like to push our spoken dialogue dataset for publication.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 782129083,
    "title": "Fix windows path scheme in cached path",
    "dateCreated": "2021-01-08T13:45:56Z",
    "dateModified": "2021-01-08T13:45:56Z",
    "description": "As noticed in #807 there's currently an issue with `cached_path` not raising `FileNotFoundError` on windows for absolute paths. This is due to the way we check for a path to be local or not. The check on the scheme using urlparse was incomplete.\r\n\r\nI fixed this and added tests",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 781914951,
    "title": "IsADirectoryError when trying to download C4",
    "dateCreated": "2021-01-08T07:31:30Z",
    "dateModified": "2021-01-08T07:31:30Z",
    "description": "**TLDR**:\r\n\r\nI fail to download C4 and see a stacktrace originating in `IsADirectoryError` as an explanation for failure.\r\n\r\nHow can the problem be fixed? \r\n\r\n**VERBOSE**:\r\n\r\nI use Python version 3.7 and have the following dependencies listed in my project:\r\n\r\n```\r\ndatasets==1.2.0\r\napache-beam==2.26.0\r\n```\r\n\r\nWhen running the following code, where `/data/huggingface/unpacked/` contains a single unzipped `wet.paths` file manually downloaded as per the instructions for C4:\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(\"c4\", \"en\", data_dir=\"/data/huggingface/unpacked\", beam_runner='DirectRunner')\r\n```\r\n\r\nI get the following stacktrace:\r\n\r\n```\r\n/Users/fredriko/venv/misc/bin/python /Users/fredriko/source/misc/main.py\r\nDownloading and preparing dataset c4/en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/fredriko/.cache/huggingface/datasets/c4/en/2.3.0/8304cf264cc42bdebcb13fca4b9cb36368a96f557d36f9dc969bebbe2568b283...\r\nTraceback (most recent call last):\r\n  File \"/Users/fredriko/source/misc/main.py\", line 3, in <module>\r\n    load_dataset(\"c4\", \"en\", data_dir=\"/data/huggingface/unpacked\", beam_runner='DirectRunner')\r\n  File \"/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/load.py\", line 612, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/builder.py\", line 527, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/builder.py\", line 1066, in _download_and_prepare\r\n    pipeline=pipeline,\r\n  File \"/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/builder.py\", line 582, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/Users/fredriko/.cache/huggingface/modules/datasets_modules/datasets/c4/8304cf264cc42bdebcb13fca4b9cb36368a96f557d36f9dc969bebbe2568b283/c4.py\", line 190, in _split_generators\r\n    file_paths = dl_manager.download_and_extract(files_to_download)\r\n  File \"/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 258, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 189, in download\r\n    self._record_sizes_checksums(url_or_urls, downloaded_path_or_paths)\r\n  File \"/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 117, in _record_sizes_checksums\r\n    self._recorded_sizes_checksums[str(url)] = get_size_checksum_dict(path)\r\n  File \"/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/utils/info_utils.py\", line 80, in get_size_checksum_dict\r\n    with open(path, \"rb\") as f:\r\nIsADirectoryError: [Errno 21] Is a directory: '/'\r\n\r\nProcess finished with exit code 1\r\n```",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 781875640,
    "title": "Databases",
    "dateCreated": "2021-01-08T06:14:03Z",
    "dateModified": "2021-01-08T06:14:03Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 781631455,
    "title": "<html dir=\"ltr\" lang=\"en\" class=\"focus-outline-visible\"><head><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">",
    "dateCreated": "2021-01-07T21:45:24Z",
    "dateModified": "2021-01-07T21:45:24Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 781507545,
    "title": "Added generated READMEs for datasets that were missing one.",
    "dateCreated": "2021-01-07T18:10:06Z",
    "dateModified": "2021-01-07T18:10:06Z",
    "description": "This is it: we worked on a generator with Yacine @yjernite , and we generated dataset cards for all missing ones (161), with all the information we could gather from datasets repository, and using dummy_data to generate examples when possible.\r\n\r\nCode is available here for the moment: https://github.com/madlag/datasets_readme_generator .\r\nWe will move it to a Hugging Face repository and to https://huggingface.co/datasets/card-creator/ later.\r\n",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 781494476,
    "title": "Error when downloading a large dataset on slow connection.",
    "dateCreated": "2021-01-07T17:48:15Z",
    "dateModified": "2021-01-07T17:48:15Z",
    "description": "I receive the following error after about an hour trying to download the `openwebtext` dataset.\r\n\r\nThe code used is:\r\n```python\r\nimport datasets\r\ndatasets.load_dataset(\"openwebtext\")\r\n```\r\n\r\n> Traceback (most recent call last):                                                                                                                                                                                                                             [4/28]\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/load.py\", line 610, in load_dataset\r\n>     ignore_verifications=ignore_verifications,\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/builder.py\", line 515, in download_and_prepare\r\n>     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/builder.py\", line 570, in _download_and_prepare\r\n>     split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n>   File \"/home/lucadiliello/.cache/huggingface/modules/datasets_modules/datasets/openwebtext/5c636399c7155da97c982d0d70ecdce30fbca66a4eb4fc768ad91f8331edac02/openwebtext.py\", line 62, in _split_generators\r\n>     dl_dir = dl_manager.download_and_extract(_URL)\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 254, in download_and_extract\r\n>     return self.extract(self.download(url_or_urls))\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 235, in extract\r\n>     num_proc=num_proc,\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 225, in map_nested\r\n>     return function(data_struct)\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 343, in cached_path\r\n>     tar_file.extractall(output_path_extracted)\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/tarfile.py\", line 2000, in extractall\r\n>     numeric_owner=numeric_owner)\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/tarfile.py\", line 2042, in extract\r\n>     numeric_owner=numeric_owner)\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/tarfile.py\", line 2112, in _extract_member\r\n>     self.makefile(tarinfo, targetpath)\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/tarfile.py\", line 2161, in makefile\r\n>     copyfileobj(source, target, tarinfo.size, ReadError, bufsize)\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/tarfile.py\", line 253, in copyfileobj\r\n>     buf = src.read(remainder)\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/lzma.py\", line 200, in read\r\n>     return self._buffer.read(size)\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/_compression.py\", line 68, in readinto\r\n>     data = self.read(len(byte_view))\r\n>   File \"/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/_compression.py\", line 99, in read\r\n>     raise EOFError(\"Compressed file ended before the \"\r\n> EOFError: Compressed file ended before the end-of-stream marker was reached\r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 781474949,
    "title": "Add information about caching and verifications in \"Load a Dataset\" docs",
    "dateCreated": "2021-01-07T17:18:44Z",
    "dateModified": "2021-01-07T17:18:44Z",
    "description": "Related to #215.\r\n\r\nMissing improvements from @lhoestq's #1703.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 781402757,
    "title": "Update XSUM Factuality DatasetCard",
    "dateCreated": "2021-01-07T15:37:14Z",
    "dateModified": "2021-01-07T15:37:14Z",
    "description": "Update XSUM Factuality DatasetCard",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 781395146,
    "title": "Improvements regarding caching and fingerprinting",
    "dateCreated": "2021-01-07T15:26:29Z",
    "dateModified": "2021-01-07T15:26:29Z",
    "description": "This PR adds these features:\r\n- Enable/disable caching\r\n    If disabled, the library will no longer reload cached datasets files when applying transforms to the datasets.\r\n    It is equivalent to setting `load_from_cache` to `False` in dataset transforms.\r\n```python\r\nfrom datasets import set_caching_enabled\r\n\r\nset_caching_enabled(False)\r\n```\r\n- Allow unpicklable functions in `map`\r\n    If an unpicklable function is used, then it's not possible to hash it to update the dataset fingerprint that is used to name cache files. To workaround that, a random fingerprint is generated instead and a warning is raised.\r\n```python\r\nlogger.warning(\r\n    f\"Transform {transform} couldn't be hashed properly, a random hash was used instead. \"\r\n    \"Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. \"\r\n    \"If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything.\"\r\n)\r\n```\r\n\r\nand also (open to discussion, EDIT: actually NOT included):\r\n- Enable/disable fingerprinting\r\n    Fingerprinting allows to have one deterministic fingerprint per dataset state.\r\n    A dataset fingerprint is updated after each transform.\r\n    Re-running the same transforms on a dataset in a different session results in the same fingerprint.\r\n    Disabling the fingerprinting mechanism makes all the fingerprints random.\r\n    Since the caching mechanism uses fingerprints to name the cache files, then cache file names will be different.\r\n    Therefore disabling fingerprinting will prevent the caching mechanism from reloading datasets files that have already been computed.\r\n    Disabling fingerprinting may speed up the lib for users that don't care about this feature and don't want to use caching.\r\n```python\r\nfrom datasets import set_fingerprinting_enabled\r\n\r\nset_fingerprinting_enabled(False)\r\n```\r\n\r\nOther details:\r\n- I renamed the `fingerprint` decorator to `fingerprint_transform` since the name was clearly not explicit. This decorator is used on dataset transform functions to allow them to update fingerprints.\r\n- I added some `ignore_kwargs` when decorating transforms with `fingerprint_transform`, to make the fingerprint update not sensible to kwargs like `load_from_cache` or `cache_file_name`.\r\n\r\nTodo: tests for set_fingerprinting_enabled + documentation for all the above features",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 781383277,
    "title": "Fix importlib metdata import in py38",
    "dateCreated": "2021-01-07T15:10:30Z",
    "dateModified": "2021-01-07T15:10:30Z",
    "description": "In Python 3.8 there's no need to install `importlib_metadata` since it already exists as `importlib.metadata` in the standard lib.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 781345717,
    "title": "Some datasets miss dataset_infos.json or dummy_data.zip",
    "dateCreated": "2021-01-07T14:17:13Z",
    "dateModified": "2021-01-07T14:17:13Z",
    "description": "While working on dataset REAME generation script at https://github.com/madlag/datasets_readme_generator , I noticed that some datasets miss a dataset_infos.json : \r\n\r\n```\r\nc4\r\nlm1b\r\nreclor\r\nwikihow\r\n```\r\n\r\nAnd some does not have a dummy_data.zip : \r\n\r\n```\r\nkor_nli\r\nmath_dataset\r\nmlqa\r\nms_marco\r\nnewsgroup\r\nqa4mre\r\nqangaroo\r\nreddit_tifu\r\nsuper_glue\r\ntrivia_qa\r\nweb_of_science\r\nwmt14\r\nwmt15\r\nwmt16\r\nwmt17\r\nwmt18\r\nwmt19\r\nxtreme\r\n```\r\n\r\nBut it seems that some of those last do have a \"dummy\" directory .\r\n\r\n",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 781333589,
    "title": "Update Curiosity dialogs DatasetCard",
    "dateCreated": "2021-01-07T13:59:27Z",
    "dateModified": "2021-01-07T13:59:27Z",
    "description": "Update Curiosity dialogs DatasetCard\r\n\r\nThere are some entries in the data fields section yet to be filled. There is little information regarding those fields.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 781271558,
    "title": "Update DBRD dataset card and download URL",
    "dateCreated": "2021-01-07T12:16:43Z",
    "dateModified": "2021-01-07T12:16:43Z",
    "description": "I've added the Dutch Bood Review Dataset (DBRD) during the recent sprint. This pull request makes two minor changes:\r\n\r\n1.  I'm changing the download URL from Google Drive to the dataset's GitHub release package. This is now possible because of PR #1316.\r\n2. I've updated the dataset card.\r\n\r\nCheers! \ud83d\ude04",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 781152561,
    "title": "Update Coached Conv Pref DatasetCard",
    "dateCreated": "2021-01-07T09:07:16Z",
    "dateModified": "2021-01-07T09:07:16Z",
    "description": "Update Coached Conversation Preferance DatasetCard",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 781126579,
    "title": "Update DialogRE DatasetCard",
    "dateCreated": "2021-01-07T08:22:33Z",
    "dateModified": "2021-01-07T08:22:33Z",
    "description": "Update the information in the dataset card for the Dialog RE dataset. ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 781096918,
    "title": "Unable to install datasets",
    "dateCreated": "2021-01-07T07:24:37Z",
    "dateModified": "2021-01-07T07:24:37Z",
    "description": "** Edit **\r\nI believe there's a bug with the package when you're installing it with Python 3.9. I recommend sticking with previous versions. Thanks, @thomwolf for the insight! \r\n\r\n**Short description**\r\n\r\nI followed the instructions for installing datasets (https://huggingface.co/docs/datasets/installation.html). However, while I tried to download datasets using `pip install datasets` I got a massive error message after getting stuck at \"Installing build dependencies...\" \r\n\r\nI was wondering if this problem can be fixed by creating a virtual environment, but it didn't help. Can anyone offer some advice on how to fix this issue? \r\n\r\nHere's an error message: \r\n\r\n`(env) Gas-MacBook-Pro:Downloads destiny$ pip install datasets\r\nCollecting datasets\r\n  Using cached datasets-1.2.0-py3-none-any.whl (159 kB)\r\nCollecting numpy>=1.17\r\n  Using cached numpy-1.19.5-cp39-cp39-macosx_10_9_x86_64.whl (15.6 MB)\r\nCollecting pyarrow>=0.17.1\r\n  Using cached pyarrow-2.0.0.tar.gz (58.9 MB)\r\n....\r\n\r\n      _configtest.c:9:5: warning: incompatible redeclaration of library function 'ceilf' [-Wincompatible-library-redeclaration]\r\n      int ceilf (void);\r\n          ^\r\n      _configtest.c:9:5: note: 'ceilf' is a builtin with type 'float (float)'\r\n      _configtest.c:10:5: warning: incompatible redeclaration of library function 'rintf' [-Wincompatible-library-redeclaration]\r\n      int rintf (void);\r\n          ^\r\n      _configtest.c:10:5: note: 'rintf' is a builtin with type 'float (float)'\r\n      _configtest.c:11:5: warning: incompatible redeclaration of library function 'truncf' [-Wincompatible-library-redeclaration]\r\n      int truncf (void);\r\n          ^\r\n      _configtest.c:11:5: note: 'truncf' is a builtin with type 'float (float)'\r\n      _configtest.c:12:5: warning: incompatible redeclaration of library function 'sqrtf' [-Wincompatible-library-redeclaration]\r\n      int sqrtf (void);\r\n          ^\r\n      _configtest.c:12:5: note: 'sqrtf' is a builtin with type 'float (float)'\r\n      _configtest.c:13:5: warning: incompatible redeclaration of library function 'log10f' [-Wincompatible-library-redeclaration]\r\n      int log10f (void);\r\n          ^\r\n      _configtest.c:13:5: note: 'log10f' is a builtin with type 'float (float)'\r\n      _configtest.c:14:5: warning: incompatible redeclaration of library function 'logf' [-Wincompatible-library-redeclaration]\r\n      int logf (void);\r\n          ^\r\n      _configtest.c:14:5: note: 'logf' is a builtin with type 'float (float)'\r\n      _configtest.c:15:5: warning: incompatible redeclaration of library function 'log1pf' [-Wincompatible-library-redeclaration]\r\n      int log1pf (void);\r\n          ^\r\n      _configtest.c:15:5: note: 'log1pf' is a builtin with type 'float (float)'\r\n      _configtest.c:16:5: warning: incompatible redeclaration of library function 'expf' [-Wincompatible-library-redeclaration]\r\n      int expf (void);\r\n          ^\r\n      _configtest.c:16:5: note: 'expf' is a builtin with type 'float (float)'\r\n      _configtest.c:17:5: warning: incompatible redeclaration of library function 'expm1f' [-Wincompatible-library-redeclaration]\r\n      int expm1f (void);\r\n          ^\r\n      _configtest.c:17:5: note: 'expm1f' is a builtin with type 'float (float)'\r\n      _configtest.c:18:5: warning: incompatible redeclaration of library function 'asinf' [-Wincompatible-library-redeclaration]\r\n      int asinf (void);\r\n          ^\r\n      _configtest.c:18:5: note: 'asinf' is a builtin with type 'float (float)'\r\n      _configtest.c:19:5: warning: incompatible redeclaration of library function 'acosf' [-Wincompatible-library-redeclaration]\r\n      int acosf (void);\r\n          ^\r\n      _configtest.c:19:5: note: 'acosf' is a builtin with type 'float (float)'\r\n      _configtest.c:20:5: warning: incompatible redeclaration of library function 'atanf' [-Wincompatible-library-redeclaration]\r\n      int atanf (void);\r\n          ^\r\n      _configtest.c:20:5: note: 'atanf' is a builtin with type 'float (float)'\r\n      _configtest.c:21:5: warning: incompatible redeclaration of library function 'asinhf' [-Wincompatible-library-redeclaration]\r\n      int asinhf (void);\r\n          ^\r\n      _configtest.c:21:5: note: 'asinhf' is a builtin with type 'float (float)'\r\n      _configtest.c:22:5: warning: incompatible redeclaration of library function 'acoshf' [-Wincompatible-library-redeclaration]\r\n      int acoshf (void);\r\n          ^\r\n      _configtest.c:22:5: note: 'acoshf' is a builtin with type 'float (float)'\r\n      _configtest.c:23:5: warning: incompatible redeclaration of library function 'atanhf' [-Wincompatible-library-redeclaration]\r\n      int atanhf (void);\r\n          ^\r\n      _configtest.c:23:5: note: 'atanhf' is a builtin with type 'float (float)'\r\n      _configtest.c:24:5: warning: incompatible redeclaration of library function 'hypotf' [-Wincompatible-library-redeclaration]\r\n      int hypotf (void);\r\n          ^\r\n      _configtest.c:24:5: note: 'hypotf' is a builtin with type 'float (float, float)'\r\n      _configtest.c:25:5: warning: incompatible redeclaration of library function 'atan2f' [-Wincompatible-library-redeclaration]\r\n      int atan2f (void);\r\n          ^\r\n      _configtest.c:25:5: note: 'atan2f' is a builtin with type 'float (float, float)'\r\n      _configtest.c:26:5: warning: incompatible redeclaration of library function 'powf' [-Wincompatible-library-redeclaration]\r\n      int powf (void);\r\n          ^\r\n      _configtest.c:26:5: note: 'powf' is a builtin with type 'float (float, float)'\r\n      _configtest.c:27:5: warning: incompatible redeclaration of library function 'fmodf' [-Wincompatible-library-redeclaration]\r\n      int fmodf (void);\r\n          ^\r\n      _configtest.c:27:5: note: 'fmodf' is a builtin with type 'float (float, float)'\r\n      _configtest.c:28:5: warning: incompatible redeclaration of library function 'modff' [-Wincompatible-library-redeclaration]\r\n      int modff (void);\r\n          ^\r\n      _configtest.c:28:5: note: 'modff' is a builtin with type 'float (float, float *)'\r\n      _configtest.c:29:5: warning: incompatible redeclaration of library function 'frexpf' [-Wincompatible-library-redeclaration]\r\n      int frexpf (void);\r\n          ^\r\n      _configtest.c:29:5: note: 'frexpf' is a builtin with type 'float (float, int *)'\r\n      _configtest.c:30:5: warning: incompatible redeclaration of library function 'ldexpf' [-Wincompatible-library-redeclaration]\r\n      int ldexpf (void);\r\n          ^\r\n      _configtest.c:30:5: note: 'ldexpf' is a builtin with type 'float (float, int)'\r\n      _configtest.c:31:5: warning: incompatible redeclaration of library function 'exp2f' [-Wincompatible-library-redeclaration]\r\n      int exp2f (void);\r\n          ^\r\n      _configtest.c:31:5: note: 'exp2f' is a builtin with type 'float (float)'\r\n      _configtest.c:32:5: warning: incompatible redeclaration of library function 'log2f' [-Wincompatible-library-redeclaration]\r\n      int log2f (void);\r\n          ^\r\n      _configtest.c:32:5: note: 'log2f' is a builtin with type 'float (float)'\r\n      _configtest.c:33:5: warning: incompatible redeclaration of library function 'copysignf' [-Wincompatible-library-redeclaration]\r\n      int copysignf (void);\r\n          ^\r\n      _configtest.c:33:5: note: 'copysignf' is a builtin with type 'float (float, float)'\r\n      _configtest.c:34:5: warning: incompatible redeclaration of library function 'nextafterf' [-Wincompatible-library-redeclaration]\r\n      int nextafterf (void);\r\n          ^\r\n      _configtest.c:34:5: note: 'nextafterf' is a builtin with type 'float (float, float)'\r\n      _configtest.c:35:5: warning: incompatible redeclaration of library function 'cbrtf' [-Wincompatible-library-redeclaration]\r\n      int cbrtf (void);\r\n          ^\r\n      _configtest.c:35:5: note: 'cbrtf' is a builtin with type 'float (float)'\r\n      35 warnings generated.\r\n      clang _configtest.o -o _configtest\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d _configtest\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      _configtest.c:1:5: warning: incompatible redeclaration of library function 'sinl' [-Wincompatible-library-redeclaration]\r\n      int sinl (void);\r\n          ^\r\n      _configtest.c:1:5: note: 'sinl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:2:5: warning: incompatible redeclaration of library function 'cosl' [-Wincompatible-library-redeclaration]\r\n      int cosl (void);\r\n          ^\r\n      _configtest.c:2:5: note: 'cosl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:3:5: warning: incompatible redeclaration of library function 'tanl' [-Wincompatible-library-redeclaration]\r\n      int tanl (void);\r\n          ^\r\n      _configtest.c:3:5: note: 'tanl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:4:5: warning: incompatible redeclaration of library function 'sinhl' [-Wincompatible-library-redeclaration]\r\n      int sinhl (void);\r\n          ^\r\n      _configtest.c:4:5: note: 'sinhl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:5:5: warning: incompatible redeclaration of library function 'coshl' [-Wincompatible-library-redeclaration]\r\n      int coshl (void);\r\n          ^\r\n      _configtest.c:5:5: note: 'coshl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:6:5: warning: incompatible redeclaration of library function 'tanhl' [-Wincompatible-library-redeclaration]\r\n      int tanhl (void);\r\n          ^\r\n      _configtest.c:6:5: note: 'tanhl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:7:5: warning: incompatible redeclaration of library function 'fabsl' [-Wincompatible-library-redeclaration]\r\n      int fabsl (void);\r\n          ^\r\n      _configtest.c:7:5: note: 'fabsl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:8:5: warning: incompatible redeclaration of library function 'floorl' [-Wincompatible-library-redeclaration]\r\n      int floorl (void);\r\n          ^\r\n      _configtest.c:8:5: note: 'floorl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:9:5: warning: incompatible redeclaration of library function 'ceill' [-Wincompatible-library-redeclaration]\r\n      int ceill (void);\r\n          ^\r\n      _configtest.c:9:5: note: 'ceill' is a builtin with type 'long double (long double)'\r\n      _configtest.c:10:5: warning: incompatible redeclaration of library function 'rintl' [-Wincompatible-library-redeclaration]\r\n      int rintl (void);\r\n          ^\r\n      _configtest.c:10:5: note: 'rintl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:11:5: warning: incompatible redeclaration of library function 'truncl' [-Wincompatible-library-redeclaration]\r\n      int truncl (void);\r\n          ^\r\n      _configtest.c:11:5: note: 'truncl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:12:5: warning: incompatible redeclaration of library function 'sqrtl' [-Wincompatible-library-redeclaration]\r\n      int sqrtl (void);\r\n          ^\r\n      _configtest.c:12:5: note: 'sqrtl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:13:5: warning: incompatible redeclaration of library function 'log10l' [-Wincompatible-library-redeclaration]\r\n      int log10l (void);\r\n          ^\r\n      _configtest.c:13:5: note: 'log10l' is a builtin with type 'long double (long double)'\r\n      _configtest.c:14:5: warning: incompatible redeclaration of library function 'logl' [-Wincompatible-library-redeclaration]\r\n      int logl (void);\r\n          ^\r\n      _configtest.c:14:5: note: 'logl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:15:5: warning: incompatible redeclaration of library function 'log1pl' [-Wincompatible-library-redeclaration]\r\n      int log1pl (void);\r\n          ^\r\n      _configtest.c:15:5: note: 'log1pl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:16:5: warning: incompatible redeclaration of library function 'expl' [-Wincompatible-library-redeclaration]\r\n      int expl (void);\r\n          ^\r\n      _configtest.c:16:5: note: 'expl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:17:5: warning: incompatible redeclaration of library function 'expm1l' [-Wincompatible-library-redeclaration]\r\n      int expm1l (void);\r\n          ^\r\n      _configtest.c:17:5: note: 'expm1l' is a builtin with type 'long double (long double)'\r\n      _configtest.c:18:5: warning: incompatible redeclaration of library function 'asinl' [-Wincompatible-library-redeclaration]\r\n      int asinl (void);\r\n          ^\r\n      _configtest.c:18:5: note: 'asinl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:19:5: warning: incompatible redeclaration of library function 'acosl' [-Wincompatible-library-redeclaration]\r\n      int acosl (void);\r\n          ^\r\n      _configtest.c:19:5: note: 'acosl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:20:5: warning: incompatible redeclaration of library function 'atanl' [-Wincompatible-library-redeclaration]\r\n      int atanl (void);\r\n          ^\r\n      _configtest.c:20:5: note: 'atanl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:21:5: warning: incompatible redeclaration of library function 'asinhl' [-Wincompatible-library-redeclaration]\r\n      int asinhl (void);\r\n          ^\r\n      _configtest.c:21:5: note: 'asinhl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:22:5: warning: incompatible redeclaration of library function 'acoshl' [-Wincompatible-library-redeclaration]\r\n      int acoshl (void);\r\n          ^\r\n      _configtest.c:22:5: note: 'acoshl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:23:5: warning: incompatible redeclaration of library function 'atanhl' [-Wincompatible-library-redeclaration]\r\n      int atanhl (void);\r\n          ^\r\n      _configtest.c:23:5: note: 'atanhl' is a builtin with type 'long double (long double)'\r\n      _configtest.c:24:5: warning: incompatible redeclaration of library function 'hypotl' [-Wincompatible-library-redeclaration]\r\n      int hypotl (void);\r\n          ^\r\n      _configtest.c:24:5: note: 'hypotl' is a builtin with type 'long double (long double, long double)'\r\n      _configtest.c:25:5: warning: incompatible redeclaration of library function 'atan2l' [-Wincompatible-library-redeclaration]\r\n      int atan2l (void);\r\n          ^\r\n      _configtest.c:25:5: note: 'atan2l' is a builtin with type 'long double (long double, long double)'\r\n      _configtest.c:26:5: warning: incompatible redeclaration of library function 'powl' [-Wincompatible-library-redeclaration]\r\n      int powl (void);\r\n          ^\r\n      _configtest.c:26:5: note: 'powl' is a builtin with type 'long double (long double, long double)'\r\n      _configtest.c:27:5: warning: incompatible redeclaration of library function 'fmodl' [-Wincompatible-library-redeclaration]\r\n      int fmodl (void);\r\n          ^\r\n      _configtest.c:27:5: note: 'fmodl' is a builtin with type 'long double (long double, long double)'\r\n      _configtest.c:28:5: warning: incompatible redeclaration of library function 'modfl' [-Wincompatible-library-redeclaration]\r\n      int modfl (void);\r\n          ^\r\n      _configtest.c:28:5: note: 'modfl' is a builtin with type 'long double (long double, long double *)'\r\n      _configtest.c:29:5: warning: incompatible redeclaration of library function 'frexpl' [-Wincompatible-library-redeclaration]\r\n      int frexpl (void);\r\n          ^\r\n      _configtest.c:29:5: note: 'frexpl' is a builtin with type 'long double (long double, int *)'\r\n      _configtest.c:30:5: warning: incompatible redeclaration of library function 'ldexpl' [-Wincompatible-library-redeclaration]\r\n      int ldexpl (void);\r\n          ^\r\n      _configtest.c:30:5: note: 'ldexpl' is a builtin with type 'long double (long double, int)'\r\n      _configtest.c:31:5: warning: incompatible redeclaration of library function 'exp2l' [-Wincompatible-library-redeclaration]\r\n      int exp2l (void);\r\n          ^\r\n      _configtest.c:31:5: note: 'exp2l' is a builtin with type 'long double (long double)'\r\n      _configtest.c:32:5: warning: incompatible redeclaration of library function 'log2l' [-Wincompatible-library-redeclaration]\r\n      int log2l (void);\r\n          ^\r\n      _configtest.c:32:5: note: 'log2l' is a builtin with type 'long double (long double)'\r\n      _configtest.c:33:5: warning: incompatible redeclaration of library function 'copysignl' [-Wincompatible-library-redeclaration]\r\n      int copysignl (void);\r\n          ^\r\n      _configtest.c:33:5: note: 'copysignl' is a builtin with type 'long double (long double, long double)'\r\n      _configtest.c:34:5: warning: incompatible redeclaration of library function 'nextafterl' [-Wincompatible-library-redeclaration]\r\n      int nextafterl (void);\r\n          ^\r\n      _configtest.c:34:5: note: 'nextafterl' is a builtin with type 'long double (long double, long double)'\r\n      _configtest.c:35:5: warning: incompatible redeclaration of library function 'cbrtl' [-Wincompatible-library-redeclaration]\r\n      int cbrtl (void);\r\n          ^\r\n      _configtest.c:35:5: note: 'cbrtl' is a builtin with type 'long double (long double)'\r\n      35 warnings generated.\r\n      clang _configtest.o -o _configtest\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d _configtest\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      _configtest.c:8:12: error: use of undeclared identifier 'HAVE_DECL_SIGNBIT'\r\n          (void) HAVE_DECL_SIGNBIT;\r\n                 ^\r\n      1 error generated.\r\n      failure.\r\n      removing: _configtest.c _configtest.o\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      _configtest.c:1:5: warning: incompatible redeclaration of library function 'cabs' [-Wincompatible-library-redeclaration]\r\n      int cabs (void);\r\n          ^\r\n      _configtest.c:1:5: note: 'cabs' is a builtin with type 'double (_Complex double)'\r\n      _configtest.c:2:5: warning: incompatible redeclaration of library function 'cacos' [-Wincompatible-library-redeclaration]\r\n      int cacos (void);\r\n          ^\r\n      _configtest.c:2:5: note: 'cacos' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:3:5: warning: incompatible redeclaration of library function 'cacosh' [-Wincompatible-library-redeclaration]\r\n      int cacosh (void);\r\n          ^\r\n      _configtest.c:3:5: note: 'cacosh' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:4:5: warning: incompatible redeclaration of library function 'carg' [-Wincompatible-library-redeclaration]\r\n      int carg (void);\r\n          ^\r\n      _configtest.c:4:5: note: 'carg' is a builtin with type 'double (_Complex double)'\r\n      _configtest.c:5:5: warning: incompatible redeclaration of library function 'casin' [-Wincompatible-library-redeclaration]\r\n      int casin (void);\r\n          ^\r\n      _configtest.c:5:5: note: 'casin' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:6:5: warning: incompatible redeclaration of library function 'casinh' [-Wincompatible-library-redeclaration]\r\n      int casinh (void);\r\n          ^\r\n      _configtest.c:6:5: note: 'casinh' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:7:5: warning: incompatible redeclaration of library function 'catan' [-Wincompatible-library-redeclaration]\r\n      int catan (void);\r\n          ^\r\n      _configtest.c:7:5: note: 'catan' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:8:5: warning: incompatible redeclaration of library function 'catanh' [-Wincompatible-library-redeclaration]\r\n      int catanh (void);\r\n          ^\r\n      _configtest.c:8:5: note: 'catanh' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:9:5: warning: incompatible redeclaration of library function 'ccos' [-Wincompatible-library-redeclaration]\r\n      int ccos (void);\r\n          ^\r\n      _configtest.c:9:5: note: 'ccos' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:10:5: warning: incompatible redeclaration of library function 'ccosh' [-Wincompatible-library-redeclaration]\r\n      int ccosh (void);\r\n          ^\r\n      _configtest.c:10:5: note: 'ccosh' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:11:5: warning: incompatible redeclaration of library function 'cexp' [-Wincompatible-library-redeclaration]\r\n      int cexp (void);\r\n          ^\r\n      _configtest.c:11:5: note: 'cexp' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:12:5: warning: incompatible redeclaration of library function 'cimag' [-Wincompatible-library-redeclaration]\r\n      int cimag (void);\r\n          ^\r\n      _configtest.c:12:5: note: 'cimag' is a builtin with type 'double (_Complex double)'\r\n      _configtest.c:13:5: warning: incompatible redeclaration of library function 'clog' [-Wincompatible-library-redeclaration]\r\n      int clog (void);\r\n          ^\r\n      _configtest.c:13:5: note: 'clog' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:14:5: warning: incompatible redeclaration of library function 'conj' [-Wincompatible-library-redeclaration]\r\n      int conj (void);\r\n          ^\r\n      _configtest.c:14:5: note: 'conj' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:15:5: warning: incompatible redeclaration of library function 'cpow' [-Wincompatible-library-redeclaration]\r\n      int cpow (void);\r\n          ^\r\n      _configtest.c:15:5: note: 'cpow' is a builtin with type '_Complex double (_Complex double, _Complex double)'\r\n      _configtest.c:16:5: warning: incompatible redeclaration of library function 'cproj' [-Wincompatible-library-redeclaration]\r\n      int cproj (void);\r\n          ^\r\n      _configtest.c:16:5: note: 'cproj' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:17:5: warning: incompatible redeclaration of library function 'creal' [-Wincompatible-library-redeclaration]\r\n      int creal (void);\r\n          ^\r\n      _configtest.c:17:5: note: 'creal' is a builtin with type 'double (_Complex double)'\r\n      _configtest.c:18:5: warning: incompatible redeclaration of library function 'csin' [-Wincompatible-library-redeclaration]\r\n      int csin (void);\r\n          ^\r\n      _configtest.c:18:5: note: 'csin' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:19:5: warning: incompatible redeclaration of library function 'csinh' [-Wincompatible-library-redeclaration]\r\n      int csinh (void);\r\n          ^\r\n      _configtest.c:19:5: note: 'csinh' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:20:5: warning: incompatible redeclaration of library function 'csqrt' [-Wincompatible-library-redeclaration]\r\n      int csqrt (void);\r\n          ^\r\n      _configtest.c:20:5: note: 'csqrt' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:21:5: warning: incompatible redeclaration of library function 'ctan' [-Wincompatible-library-redeclaration]\r\n      int ctan (void);\r\n          ^\r\n      _configtest.c:21:5: note: 'ctan' is a builtin with type '_Complex double (_Complex double)'\r\n      _configtest.c:22:5: warning: incompatible redeclaration of library function 'ctanh' [-Wincompatible-library-redeclaration]\r\n      int ctanh (void);\r\n          ^\r\n      _configtest.c:22:5: note: 'ctanh' is a builtin with type '_Complex double (_Complex double)'\r\n      22 warnings generated.\r\n      clang _configtest.o -o _configtest\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d _configtest\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      _configtest.c:1:5: warning: incompatible redeclaration of library function 'cabsf' [-Wincompatible-library-redeclaration]\r\n      int cabsf (void);\r\n          ^\r\n      _configtest.c:1:5: note: 'cabsf' is a builtin with type 'float (_Complex float)'\r\n      _configtest.c:2:5: warning: incompatible redeclaration of library function 'cacosf' [-Wincompatible-library-redeclaration]\r\n      int cacosf (void);\r\n          ^\r\n      _configtest.c:2:5: note: 'cacosf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:3:5: warning: incompatible redeclaration of library function 'cacoshf' [-Wincompatible-library-redeclaration]\r\n      int cacoshf (void);\r\n          ^\r\n      _configtest.c:3:5: note: 'cacoshf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:4:5: warning: incompatible redeclaration of library function 'cargf' [-Wincompatible-library-redeclaration]\r\n      int cargf (void);\r\n          ^\r\n      _configtest.c:4:5: note: 'cargf' is a builtin with type 'float (_Complex float)'\r\n      _configtest.c:5:5: warning: incompatible redeclaration of library function 'casinf' [-Wincompatible-library-redeclaration]\r\n      int casinf (void);\r\n          ^\r\n      _configtest.c:5:5: note: 'casinf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:6:5: warning: incompatible redeclaration of library function 'casinhf' [-Wincompatible-library-redeclaration]\r\n      int casinhf (void);\r\n          ^\r\n      _configtest.c:6:5: note: 'casinhf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:7:5: warning: incompatible redeclaration of library function 'catanf' [-Wincompatible-library-redeclaration]\r\n      int catanf (void);\r\n          ^\r\n      _configtest.c:7:5: note: 'catanf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:8:5: warning: incompatible redeclaration of library function 'catanhf' [-Wincompatible-library-redeclaration]\r\n      int catanhf (void);\r\n          ^\r\n      _configtest.c:8:5: note: 'catanhf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:9:5: warning: incompatible redeclaration of library function 'ccosf' [-Wincompatible-library-redeclaration]\r\n      int ccosf (void);\r\n          ^\r\n      _configtest.c:9:5: note: 'ccosf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:10:5: warning: incompatible redeclaration of library function 'ccoshf' [-Wincompatible-library-redeclaration]\r\n      int ccoshf (void);\r\n          ^\r\n      _configtest.c:10:5: note: 'ccoshf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:11:5: warning: incompatible redeclaration of library function 'cexpf' [-Wincompatible-library-redeclaration]\r\n      int cexpf (void);\r\n          ^\r\n      _configtest.c:11:5: note: 'cexpf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:12:5: warning: incompatible redeclaration of library function 'cimagf' [-Wincompatible-library-redeclaration]\r\n      int cimagf (void);\r\n          ^\r\n      _configtest.c:12:5: note: 'cimagf' is a builtin with type 'float (_Complex float)'\r\n      _configtest.c:13:5: warning: incompatible redeclaration of library function 'clogf' [-Wincompatible-library-redeclaration]\r\n      int clogf (void);\r\n          ^\r\n      _configtest.c:13:5: note: 'clogf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:14:5: warning: incompatible redeclaration of library function 'conjf' [-Wincompatible-library-redeclaration]\r\n      int conjf (void);\r\n          ^\r\n      _configtest.c:14:5: note: 'conjf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:15:5: warning: incompatible redeclaration of library function 'cpowf' [-Wincompatible-library-redeclaration]\r\n      int cpowf (void);\r\n          ^\r\n      _configtest.c:15:5: note: 'cpowf' is a builtin with type '_Complex float (_Complex float, _Complex float)'\r\n      _configtest.c:16:5: warning: incompatible redeclaration of library function 'cprojf' [-Wincompatible-library-redeclaration]\r\n      int cprojf (void);\r\n          ^\r\n      _configtest.c:16:5: note: 'cprojf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:17:5: warning: incompatible redeclaration of library function 'crealf' [-Wincompatible-library-redeclaration]\r\n      int crealf (void);\r\n          ^\r\n      _configtest.c:17:5: note: 'crealf' is a builtin with type 'float (_Complex float)'\r\n      _configtest.c:18:5: warning: incompatible redeclaration of library function 'csinf' [-Wincompatible-library-redeclaration]\r\n      int csinf (void);\r\n          ^\r\n      _configtest.c:18:5: note: 'csinf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:19:5: warning: incompatible redeclaration of library function 'csinhf' [-Wincompatible-library-redeclaration]\r\n      int csinhf (void);\r\n          ^\r\n      _configtest.c:19:5: note: 'csinhf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:20:5: warning: incompatible redeclaration of library function 'csqrtf' [-Wincompatible-library-redeclaration]\r\n      int csqrtf (void);\r\n          ^\r\n      _configtest.c:20:5: note: 'csqrtf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:21:5: warning: incompatible redeclaration of library function 'ctanf' [-Wincompatible-library-redeclaration]\r\n      int ctanf (void);\r\n          ^\r\n      _configtest.c:21:5: note: 'ctanf' is a builtin with type '_Complex float (_Complex float)'\r\n      _configtest.c:22:5: warning: incompatible redeclaration of library function 'ctanhf' [-Wincompatible-library-redeclaration]\r\n      int ctanhf (void);\r\n          ^\r\n      _configtest.c:22:5: note: 'ctanhf' is a builtin with type '_Complex float (_Complex float)'\r\n      22 warnings generated.\r\n      clang _configtest.o -o _configtest\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d _configtest\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      _configtest.c:1:5: warning: incompatible redeclaration of library function 'cabsl' [-Wincompatible-library-redeclaration]\r\n      int cabsl (void);\r\n          ^\r\n      _configtest.c:1:5: note: 'cabsl' is a builtin with type 'long double (_Complex long double)'\r\n      _configtest.c:2:5: warning: incompatible redeclaration of library function 'cacosl' [-Wincompatible-library-redeclaration]\r\n      int cacosl (void);\r\n          ^\r\n      _configtest.c:2:5: note: 'cacosl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:3:5: warning: incompatible redeclaration of library function 'cacoshl' [-Wincompatible-library-redeclaration]\r\n      int cacoshl (void);\r\n          ^\r\n      _configtest.c:3:5: note: 'cacoshl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:4:5: warning: incompatible redeclaration of library function 'cargl' [-Wincompatible-library-redeclaration]\r\n      int cargl (void);\r\n          ^\r\n      _configtest.c:4:5: note: 'cargl' is a builtin with type 'long double (_Complex long double)'\r\n      _configtest.c:5:5: warning: incompatible redeclaration of library function 'casinl' [-Wincompatible-library-redeclaration]\r\n      int casinl (void);\r\n          ^\r\n      _configtest.c:5:5: note: 'casinl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:6:5: warning: incompatible redeclaration of library function 'casinhl' [-Wincompatible-library-redeclaration]\r\n      int casinhl (void);\r\n          ^\r\n      _configtest.c:6:5: note: 'casinhl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:7:5: warning: incompatible redeclaration of library function 'catanl' [-Wincompatible-library-redeclaration]\r\n      int catanl (void);\r\n          ^\r\n      _configtest.c:7:5: note: 'catanl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:8:5: warning: incompatible redeclaration of library function 'catanhl' [-Wincompatible-library-redeclaration]\r\n      int catanhl (void);\r\n          ^\r\n      _configtest.c:8:5: note: 'catanhl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:9:5: warning: incompatible redeclaration of library function 'ccosl' [-Wincompatible-library-redeclaration]\r\n      int ccosl (void);\r\n          ^\r\n      _configtest.c:9:5: note: 'ccosl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:10:5: warning: incompatible redeclaration of library function 'ccoshl' [-Wincompatible-library-redeclaration]\r\n      int ccoshl (void);\r\n          ^\r\n      _configtest.c:10:5: note: 'ccoshl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:11:5: warning: incompatible redeclaration of library function 'cexpl' [-Wincompatible-library-redeclaration]\r\n      int cexpl (void);\r\n          ^\r\n      _configtest.c:11:5: note: 'cexpl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:12:5: warning: incompatible redeclaration of library function 'cimagl' [-Wincompatible-library-redeclaration]\r\n      int cimagl (void);\r\n          ^\r\n      _configtest.c:12:5: note: 'cimagl' is a builtin with type 'long double (_Complex long double)'\r\n      _configtest.c:13:5: warning: incompatible redeclaration of library function 'clogl' [-Wincompatible-library-redeclaration]\r\n      int clogl (void);\r\n          ^\r\n      _configtest.c:13:5: note: 'clogl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:14:5: warning: incompatible redeclaration of library function 'conjl' [-Wincompatible-library-redeclaration]\r\n      int conjl (void);\r\n          ^\r\n      _configtest.c:14:5: note: 'conjl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:15:5: warning: incompatible redeclaration of library function 'cpowl' [-Wincompatible-library-redeclaration]\r\n      int cpowl (void);\r\n          ^\r\n      _configtest.c:15:5: note: 'cpowl' is a builtin with type '_Complex long double (_Complex long double, _Complex long double)'\r\n      _configtest.c:16:5: warning: incompatible redeclaration of library function 'cprojl' [-Wincompatible-library-redeclaration]\r\n      int cprojl (void);\r\n          ^\r\n      _configtest.c:16:5: note: 'cprojl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:17:5: warning: incompatible redeclaration of library function 'creall' [-Wincompatible-library-redeclaration]\r\n      int creall (void);\r\n          ^\r\n      _configtest.c:17:5: note: 'creall' is a builtin with type 'long double (_Complex long double)'\r\n      _configtest.c:18:5: warning: incompatible redeclaration of library function 'csinl' [-Wincompatible-library-redeclaration]\r\n      int csinl (void);\r\n          ^\r\n      _configtest.c:18:5: note: 'csinl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:19:5: warning: incompatible redeclaration of library function 'csinhl' [-Wincompatible-library-redeclaration]\r\n      int csinhl (void);\r\n          ^\r\n      _configtest.c:19:5: note: 'csinhl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:20:5: warning: incompatible redeclaration of library function 'csqrtl' [-Wincompatible-library-redeclaration]\r\n      int csqrtl (void);\r\n          ^\r\n      _configtest.c:20:5: note: 'csqrtl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:21:5: warning: incompatible redeclaration of library function 'ctanl' [-Wincompatible-library-redeclaration]\r\n      int ctanl (void);\r\n          ^\r\n      _configtest.c:21:5: note: 'ctanl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      _configtest.c:22:5: warning: incompatible redeclaration of library function 'ctanhl' [-Wincompatible-library-redeclaration]\r\n      int ctanhl (void);\r\n          ^\r\n      _configtest.c:22:5: note: 'ctanhl' is a builtin with type '_Complex long double (_Complex long double)'\r\n      22 warnings generated.\r\n      clang _configtest.o -o _configtest\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d _configtest\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      _configtest.c:2:12: warning: unused function 'static_func' [-Wunused-function]\r\n      static int static_func (char * restrict a)\r\n                 ^\r\n      1 warning generated.\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      _configtest.c:3:19: warning: unused function 'static_func' [-Wunused-function]\r\n      static inline int static_func (void)\r\n                        ^\r\n      1 warning generated.\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      File: build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/config.h\r\n      #define SIZEOF_PY_INTPTR_T 8\r\n      #define SIZEOF_OFF_T 8\r\n      #define SIZEOF_PY_LONG_LONG 8\r\n      #define MATHLIB\r\n      #define HAVE_SIN 1\r\n      #define HAVE_COS 1\r\n      #define HAVE_TAN 1\r\n      #define HAVE_SINH 1\r\n      #define HAVE_COSH 1\r\n      #define HAVE_TANH 1\r\n      #define HAVE_FABS 1\r\n      #define HAVE_FLOOR 1\r\n      #define HAVE_CEIL 1\r\n      #define HAVE_SQRT 1\r\n      #define HAVE_LOG10 1\r\n      #define HAVE_LOG 1\r\n      #define HAVE_EXP 1\r\n      #define HAVE_ASIN 1\r\n      #define HAVE_ACOS 1\r\n      #define HAVE_ATAN 1\r\n      #define HAVE_FMOD 1\r\n      #define HAVE_MODF 1\r\n      #define HAVE_FREXP 1\r\n      #define HAVE_LDEXP 1\r\n      #define HAVE_RINT 1\r\n      #define HAVE_TRUNC 1\r\n      #define HAVE_EXP2 1\r\n      #define HAVE_LOG2 1\r\n      #define HAVE_ATAN2 1\r\n      #define HAVE_POW 1\r\n      #define HAVE_NEXTAFTER 1\r\n      #define HAVE_STRTOLL 1\r\n      #define HAVE_STRTOULL 1\r\n      #define HAVE_CBRT 1\r\n      #define HAVE_STRTOLD_L 1\r\n      #define HAVE_BACKTRACE 1\r\n      #define HAVE_MADVISE 1\r\n      #define HAVE_XMMINTRIN_H 1\r\n      #define HAVE_EMMINTRIN_H 1\r\n      #define HAVE_XLOCALE_H 1\r\n      #define HAVE_DLFCN_H 1\r\n      #define HAVE_SYS_MMAN_H 1\r\n      #define HAVE___BUILTIN_ISNAN 1\r\n      #define HAVE___BUILTIN_ISINF 1\r\n      #define HAVE___BUILTIN_ISFINITE 1\r\n      #define HAVE___BUILTIN_BSWAP32 1\r\n      #define HAVE___BUILTIN_BSWAP64 1\r\n      #define HAVE___BUILTIN_EXPECT 1\r\n      #define HAVE___BUILTIN_MUL_OVERFLOW 1\r\n      #define HAVE___BUILTIN_CPU_SUPPORTS 1\r\n      #define HAVE__M_FROM_INT64 1\r\n      #define HAVE__MM_LOAD_PS 1\r\n      #define HAVE__MM_PREFETCH 1\r\n      #define HAVE__MM_LOAD_PD 1\r\n      #define HAVE___BUILTIN_PREFETCH 1\r\n      #define HAVE_LINK_AVX 1\r\n      #define HAVE_LINK_AVX2 1\r\n      #define HAVE_XGETBV 1\r\n      #define HAVE_ATTRIBUTE_NONNULL 1\r\n      #define HAVE_ATTRIBUTE_TARGET_AVX 1\r\n      #define HAVE_ATTRIBUTE_TARGET_AVX2 1\r\n      #define HAVE___THREAD 1\r\n      #define HAVE_SINF 1\r\n      #define HAVE_COSF 1\r\n      #define HAVE_TANF 1\r\n      #define HAVE_SINHF 1\r\n      #define HAVE_COSHF 1\r\n      #define HAVE_TANHF 1\r\n      #define HAVE_FABSF 1\r\n      #define HAVE_FLOORF 1\r\n      #define HAVE_CEILF 1\r\n      #define HAVE_RINTF 1\r\n      #define HAVE_TRUNCF 1\r\n      #define HAVE_SQRTF 1\r\n      #define HAVE_LOG10F 1\r\n      #define HAVE_LOGF 1\r\n      #define HAVE_LOG1PF 1\r\n      #define HAVE_EXPF 1\r\n      #define HAVE_EXPM1F 1\r\n      #define HAVE_ASINF 1\r\n      #define HAVE_ACOSF 1\r\n      #define HAVE_ATANF 1\r\n      #define HAVE_ASINHF 1\r\n      #define HAVE_ACOSHF 1\r\n      #define HAVE_ATANHF 1\r\n      #define HAVE_HYPOTF 1\r\n      #define HAVE_ATAN2F 1\r\n      #define HAVE_POWF 1\r\n      #define HAVE_FMODF 1\r\n      #define HAVE_MODFF 1\r\n      #define HAVE_FREXPF 1\r\n      #define HAVE_LDEXPF 1\r\n      #define HAVE_EXP2F 1\r\n      #define HAVE_LOG2F 1\r\n      #define HAVE_COPYSIGNF 1\r\n      #define HAVE_NEXTAFTERF 1\r\n      #define HAVE_CBRTF 1\r\n      #define HAVE_SINL 1\r\n      #define HAVE_COSL 1\r\n      #define HAVE_TANL 1\r\n      #define HAVE_SINHL 1\r\n      #define HAVE_COSHL 1\r\n      #define HAVE_TANHL 1\r\n      #define HAVE_FABSL 1\r\n      #define HAVE_FLOORL 1\r\n      #define HAVE_CEILL 1\r\n      #define HAVE_RINTL 1\r\n      #define HAVE_TRUNCL 1\r\n      #define HAVE_SQRTL 1\r\n      #define HAVE_LOG10L 1\r\n      #define HAVE_LOGL 1\r\n      #define HAVE_LOG1PL 1\r\n      #define HAVE_EXPL 1\r\n      #define HAVE_EXPM1L 1\r\n      #define HAVE_ASINL 1\r\n      #define HAVE_ACOSL 1\r\n      #define HAVE_ATANL 1\r\n      #define HAVE_ASINHL 1\r\n      #define HAVE_ACOSHL 1\r\n      #define HAVE_ATANHL 1\r\n      #define HAVE_HYPOTL 1\r\n      #define HAVE_ATAN2L 1\r\n      #define HAVE_POWL 1\r\n      #define HAVE_FMODL 1\r\n      #define HAVE_MODFL 1\r\n      #define HAVE_FREXPL 1\r\n      #define HAVE_LDEXPL 1\r\n      #define HAVE_EXP2L 1\r\n      #define HAVE_LOG2L 1\r\n      #define HAVE_COPYSIGNL 1\r\n      #define HAVE_NEXTAFTERL 1\r\n      #define HAVE_CBRTL 1\r\n      #define HAVE_DECL_SIGNBIT\r\n      #define HAVE_COMPLEX_H 1\r\n      #define HAVE_CABS 1\r\n      #define HAVE_CACOS 1\r\n      #define HAVE_CACOSH 1\r\n      #define HAVE_CARG 1\r\n      #define HAVE_CASIN 1\r\n      #define HAVE_CASINH 1\r\n      #define HAVE_CATAN 1\r\n      #define HAVE_CATANH 1\r\n      #define HAVE_CCOS 1\r\n      #define HAVE_CCOSH 1\r\n      #define HAVE_CEXP 1\r\n      #define HAVE_CIMAG 1\r\n      #define HAVE_CLOG 1\r\n      #define HAVE_CONJ 1\r\n      #define HAVE_CPOW 1\r\n      #define HAVE_CPROJ 1\r\n      #define HAVE_CREAL 1\r\n      #define HAVE_CSIN 1\r\n      #define HAVE_CSINH 1\r\n      #define HAVE_CSQRT 1\r\n      #define HAVE_CTAN 1\r\n      #define HAVE_CTANH 1\r\n      #define HAVE_CABSF 1\r\n      #define HAVE_CACOSF 1\r\n      #define HAVE_CACOSHF 1\r\n      #define HAVE_CARGF 1\r\n      #define HAVE_CASINF 1\r\n      #define HAVE_CASINHF 1\r\n      #define HAVE_CATANF 1\r\n      #define HAVE_CATANHF 1\r\n      #define HAVE_CCOSF 1\r\n      #define HAVE_CCOSHF 1\r\n      #define HAVE_CEXPF 1\r\n      #define HAVE_CIMAGF 1\r\n      #define HAVE_CLOGF 1\r\n      #define HAVE_CONJF 1\r\n      #define HAVE_CPOWF 1\r\n      #define HAVE_CPROJF 1\r\n      #define HAVE_CREALF 1\r\n      #define HAVE_CSINF 1\r\n      #define HAVE_CSINHF 1\r\n      #define HAVE_CSQRTF 1\r\n      #define HAVE_CTANF 1\r\n      #define HAVE_CTANHF 1\r\n      #define HAVE_CABSL 1\r\n      #define HAVE_CACOSL 1\r\n      #define HAVE_CACOSHL 1\r\n      #define HAVE_CARGL 1\r\n      #define HAVE_CASINL 1\r\n      #define HAVE_CASINHL 1\r\n      #define HAVE_CATANL 1\r\n      #define HAVE_CATANHL 1\r\n      #define HAVE_CCOSL 1\r\n      #define HAVE_CCOSHL 1\r\n      #define HAVE_CEXPL 1\r\n      #define HAVE_CIMAGL 1\r\n      #define HAVE_CLOGL 1\r\n      #define HAVE_CONJL 1\r\n      #define HAVE_CPOWL 1\r\n      #define HAVE_CPROJL 1\r\n      #define HAVE_CREALL 1\r\n      #define HAVE_CSINL 1\r\n      #define HAVE_CSINHL 1\r\n      #define HAVE_CSQRTL 1\r\n      #define HAVE_CTANL 1\r\n      #define HAVE_CTANHL 1\r\n      #define NPY_RESTRICT restrict\r\n      #define NPY_RELAXED_STRIDES_CHECKING 1\r\n      #define HAVE_LDOUBLE_INTEL_EXTENDED_16_BYTES_LE 1\r\n      #define NPY_PY3K 1\r\n      #ifndef __cplusplus\r\n      /* #undef inline */\r\n      #endif\r\n  \r\n      #ifndef _NPY_NPY_CONFIG_H_\r\n      #error config.h should never be included directly, include npy_config.h instead\r\n      #endif\r\n  \r\n      EOF\r\n        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/config.h' to sources.\r\n      Generating build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/_numpyconfig.h\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      _configtest.c:1:5: warning: incompatible redeclaration of library function 'exp' [-Wincompatible-library-redeclaration]\r\n      int exp (void);\r\n          ^\r\n      _configtest.c:1:5: note: 'exp' is a builtin with type 'double (double)'\r\n      1 warning generated.\r\n      clang _configtest.o -o _configtest\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d _configtest\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'\r\n      clang: _configtest.c\r\n      success!\r\n      removing: _configtest.c _configtest.o _configtest.o.d\r\n      File: build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/_numpyconfig.h\r\n      #define NPY_SIZEOF_SHORT SIZEOF_SHORT\r\n      #define NPY_SIZEOF_INT SIZEOF_INT\r\n      #define NPY_SIZEOF_LONG SIZEOF_LONG\r\n      #define NPY_SIZEOF_FLOAT 4\r\n      #define NPY_SIZEOF_COMPLEX_FLOAT 8\r\n      #define NPY_SIZEOF_DOUBLE 8\r\n      #define NPY_SIZEOF_COMPLEX_DOUBLE 16\r\n      #define NPY_SIZEOF_LONGDOUBLE 16\r\n      #define NPY_SIZEOF_COMPLEX_LONGDOUBLE 32\r\n      #define NPY_SIZEOF_PY_INTPTR_T 8\r\n      #define NPY_SIZEOF_OFF_T 8\r\n      #define NPY_SIZEOF_PY_LONG_LONG 8\r\n      #define NPY_SIZEOF_LONGLONG 8\r\n      #define NPY_NO_SMP 0\r\n      #define NPY_HAVE_DECL_ISNAN\r\n      #define NPY_HAVE_DECL_ISINF\r\n      #define NPY_HAVE_DECL_ISFINITE\r\n      #define NPY_HAVE_DECL_SIGNBIT\r\n      #define NPY_USE_C99_COMPLEX 1\r\n      #define NPY_HAVE_COMPLEX_DOUBLE 1\r\n      #define NPY_HAVE_COMPLEX_FLOAT 1\r\n      #define NPY_HAVE_COMPLEX_LONG_DOUBLE 1\r\n      #define NPY_RELAXED_STRIDES_CHECKING 1\r\n      #define NPY_USE_C99_FORMATS 1\r\n      #define NPY_VISIBILITY_HIDDEN __attribute__((visibility(\"hidden\")))\r\n      #define NPY_ABI_VERSION 0x01000009\r\n      #define NPY_API_VERSION 0x0000000D\r\n  \r\n      #ifndef __STDC_FORMAT_MACROS\r\n      #define __STDC_FORMAT_MACROS 1\r\n      #endif\r\n  \r\n      EOF\r\n        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/_numpyconfig.h' to sources.\r\n      executing numpy/core/code_generators/generate_numpy_api.py\r\n        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__multiarray_api.h' to sources.\r\n      numpy.core - nothing done with h_files = ['build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/config.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/_numpyconfig.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__multiarray_api.h']\r\n      building extension \"numpy.core._multiarray_tests\" sources\r\n      creating build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/_multiarray_tests.c\r\n      building extension \"numpy.core._multiarray_umath\" sources\r\n        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/config.h' to sources.\r\n        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/_numpyconfig.h' to sources.\r\n      executing numpy/core/code_generators/generate_numpy_api.py\r\n        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__multiarray_api.h' to sources.\r\n      executing numpy/core/code_generators/generate_ufunc_api.py\r\n        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__ufunc_api.h' to sources.\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/arraytypes.c\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/einsum.c\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/lowlevel_strided_loops.c\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/nditer_templ.c\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/scalartypes.c\r\n      creating build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/funcs.inc\r\n        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath' to include_dirs.\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/simd.inc\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/loops.h\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/loops.c\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/matmul.h\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/matmul.c\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/scalarmath.c\r\n        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath' to include_dirs.\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/common/templ_common.h\r\n        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/common' to include_dirs.\r\n      numpy.core - nothing done with h_files = ['build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/funcs.inc', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/simd.inc', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/loops.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/matmul.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/npy_math_internal.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/common/templ_common.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/config.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/_numpyconfig.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__multiarray_api.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__ufunc_api.h']\r\n      building extension \"numpy.core._umath_tests\" sources\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_umath_tests.c\r\n      building extension \"numpy.core._rational_tests\" sources\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_rational_tests.c\r\n      building extension \"numpy.core._struct_ufunc_tests\" sources\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_struct_ufunc_tests.c\r\n      building extension \"numpy.core._operand_flag_tests\" sources\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_operand_flag_tests.c\r\n      building extension \"numpy.fft.fftpack_lite\" sources\r\n      building extension \"numpy.linalg.lapack_lite\" sources\r\n      creating build/src.macosx-10.15-x86_64-3.9/numpy/linalg\r\n        adding 'numpy/linalg/lapack_lite/python_xerbla.c' to sources.\r\n      building extension \"numpy.linalg._umath_linalg\" sources\r\n        adding 'numpy/linalg/lapack_lite/python_xerbla.c' to sources.\r\n      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/linalg/umath_linalg.c\r\n      building extension \"numpy.random.mtrand\" sources\r\n      creating build/src.macosx-10.15-x86_64-3.9/numpy/random\r\n      building data_files sources\r\n      build_src: building npy-pkg config files\r\n      running build_py\r\n      creating build/lib.macosx-10.15-x86_64-3.9\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy\r\n      copying numpy/conftest.py -> build/lib.macosx-10.15-x86_64-3.9/numpy\r\n      copying numpy/version.py -> build/lib.macosx-10.15-x86_64-3.9/numpy\r\n      copying numpy/_globals.py -> build/lib.macosx-10.15-x86_64-3.9/numpy\r\n      copying numpy/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy\r\n      copying numpy/dual.py -> build/lib.macosx-10.15-x86_64-3.9/numpy\r\n      copying numpy/_distributor_init.py -> build/lib.macosx-10.15-x86_64-3.9/numpy\r\n      copying numpy/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy\r\n      copying numpy/ctypeslib.py -> build/lib.macosx-10.15-x86_64-3.9/numpy\r\n      copying numpy/matlib.py -> build/lib.macosx-10.15-x86_64-3.9/numpy\r\n      copying numpy/_pytesttester.py -> build/lib.macosx-10.15-x86_64-3.9/numpy\r\n      copying build/src.macosx-10.15-x86_64-3.9/numpy/__config__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/compat\r\n      copying numpy/compat/py3k.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/compat\r\n      copying numpy/compat/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/compat\r\n      copying numpy/compat/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/compat\r\n      copying numpy/compat/_inspect.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/compat\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/umath.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/fromnumeric.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/_dtype.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/_add_newdocs.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/_methods.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/_internal.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/_string_helpers.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/multiarray.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/records.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/setup_common.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/_aliased_types.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/memmap.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/overrides.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/getlimits.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/_dtype_ctypes.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/defchararray.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/shape_base.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/machar.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/numeric.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/function_base.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/einsumfunc.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/umath_tests.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/numerictypes.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/_type_aliases.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/cversions.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/arrayprint.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      copying numpy/core/code_generators/generate_numpy_api.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/unixccompiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/numpy_distribution.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/conv_template.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/cpuinfo.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/ccompiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/msvc9compiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/npy_pkg_config.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/compat.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/misc_util.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/log.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/line_endings.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/lib2def.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/pathccompiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/system_info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/core.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/__version__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/exec_command.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/from_template.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/mingw32ccompiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/extension.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/msvccompiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/intelccompiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying numpy/distutils/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      copying build/src.macosx-10.15-x86_64-3.9/numpy/distutils/__config__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/build.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/config_compiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/build_ext.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/config.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/install_headers.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/build_py.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/build_src.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/sdist.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/build_scripts.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/bdist_rpm.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/install_clib.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/build_clib.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/autodist.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/egg_info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/install.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/develop.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      copying numpy/distutils/command/install_data.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/gnu.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/compaq.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/intel.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/none.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/nag.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/pg.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/ibm.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/sun.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/lahey.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/g95.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/mips.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/hpux.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/environment.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/pathf95.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/absoft.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      copying numpy/distutils/fcompiler/vast.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      copying numpy/doc/misc.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      copying numpy/doc/internals.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      copying numpy/doc/creation.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      copying numpy/doc/constants.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      copying numpy/doc/ufuncs.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      copying numpy/doc/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      copying numpy/doc/broadcasting.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      copying numpy/doc/basics.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      copying numpy/doc/subclassing.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      copying numpy/doc/indexing.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      copying numpy/doc/byteswapping.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      copying numpy/doc/structured_arrays.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      copying numpy/doc/glossary.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/cfuncs.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/common_rules.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/crackfortran.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/cb_rules.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/rules.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/f2py2e.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/func2subr.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/__version__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/diagnose.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/capi_maps.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/f90mod_rules.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/f2py_testing.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/use_rules.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/auxfuncs.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      copying numpy/f2py/__main__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/fft\r\n      copying numpy/fft/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/fft\r\n      copying numpy/fft/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/fft\r\n      copying numpy/fft/helper.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/fft\r\n      copying numpy/fft/fftpack.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/fft\r\n      copying numpy/fft/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/fft\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/_iotools.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/mixins.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/nanfunctions.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/recfunctions.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/histograms.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/scimath.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/_version.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/user_array.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/format.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/twodim_base.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/financial.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/index_tricks.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/npyio.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/shape_base.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/stride_tricks.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/utils.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/arrayterator.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/function_base.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/arraysetops.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/arraypad.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/type_check.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/polynomial.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/_datasource.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      copying numpy/lib/ufunclike.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/linalg\r\n      copying numpy/linalg/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/linalg\r\n      copying numpy/linalg/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/linalg\r\n      copying numpy/linalg/linalg.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/linalg\r\n      copying numpy/linalg/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/linalg\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/ma\r\n      copying numpy/ma/extras.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma\r\n      copying numpy/ma/version.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma\r\n      copying numpy/ma/testutils.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma\r\n      copying numpy/ma/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma\r\n      copying numpy/ma/core.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma\r\n      copying numpy/ma/bench.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma\r\n      copying numpy/ma/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma\r\n      copying numpy/ma/timer_comparison.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma\r\n      copying numpy/ma/mrecords.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/matrixlib\r\n      copying numpy/matrixlib/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/matrixlib\r\n      copying numpy/matrixlib/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/matrixlib\r\n      copying numpy/matrixlib/defmatrix.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/matrixlib\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial\r\n      copying numpy/polynomial/laguerre.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial\r\n      copying numpy/polynomial/_polybase.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial\r\n      copying numpy/polynomial/polyutils.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial\r\n      copying numpy/polynomial/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial\r\n      copying numpy/polynomial/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial\r\n      copying numpy/polynomial/hermite_e.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial\r\n      copying numpy/polynomial/chebyshev.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial\r\n      copying numpy/polynomial/polynomial.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial\r\n      copying numpy/polynomial/legendre.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial\r\n      copying numpy/polynomial/hermite.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/random\r\n      copying numpy/random/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/random\r\n      copying numpy/random/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/random\r\n      copying numpy/random/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/random\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/testing\r\n      copying numpy/testing/nosetester.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing\r\n      copying numpy/testing/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing\r\n      copying numpy/testing/noseclasses.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing\r\n      copying numpy/testing/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing\r\n      copying numpy/testing/utils.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing\r\n      copying numpy/testing/print_coercion_tables.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing\r\n      copying numpy/testing/decorators.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing\r\n      creating build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private\r\n      copying numpy/testing/_private/nosetester.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private\r\n      copying numpy/testing/_private/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private\r\n      copying numpy/testing/_private/noseclasses.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private\r\n      copying numpy/testing/_private/utils.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private\r\n      copying numpy/testing/_private/parameterized.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private\r\n      copying numpy/testing/_private/decorators.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private\r\n      running build_clib\r\n      customize UnixCCompiler\r\n      customize UnixCCompiler using build_clib\r\n      building 'npymath' library\r\n      compiling C sources\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      creating build/temp.macosx-10.15-x86_64-3.9\r\n      creating build/temp.macosx-10.15-x86_64-3.9/numpy\r\n      creating build/temp.macosx-10.15-x86_64-3.9/numpy/core\r\n      creating build/temp.macosx-10.15-x86_64-3.9/numpy/core/src\r\n      creating build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/npymath\r\n      creating build/temp.macosx-10.15-x86_64-3.9/build\r\n      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9\r\n      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy\r\n      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core\r\n      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src\r\n      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath\r\n      compile options: '-Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'\r\n      clang: numpy/core/src/npymath/npy_math.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/npy_math_complex.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/ieee754.c\r\n      clang: numpy/core/src/npymath/halffloat.c\r\n      numpy/core/src/npymath/npy_math_complex.c.src:48:33: warning: unused variable 'tiny' [-Wunused-const-variable]\r\n      static const volatile npy_float tiny = 3.9443045e-31f;\r\n                                      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:67:25: warning: unused variable 'c_halff' [-Wunused-const-variable]\r\n      static const npy_cfloat c_halff = {0.5F, 0.0};\r\n                              ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:68:25: warning: unused variable 'c_if' [-Wunused-const-variable]\r\n      static const npy_cfloat c_if = {0.0, 1.0F};\r\n                              ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:69:25: warning: unused variable 'c_ihalff' [-Wunused-const-variable]\r\n      static const npy_cfloat c_ihalff = {0.0, 0.5F};\r\n                              ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:79:1: warning: unused function 'caddf' [-Wunused-function]\r\n      caddf(npy_cfloat a, npy_cfloat b)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:87:1: warning: unused function 'csubf' [-Wunused-function]\r\n      csubf(npy_cfloat a, npy_cfloat b)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:137:1: warning: unused function 'cnegf' [-Wunused-function]\r\n      cnegf(npy_cfloat a)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:144:1: warning: unused function 'cmulif' [-Wunused-function]\r\n      cmulif(npy_cfloat a)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:67:26: warning: unused variable 'c_half' [-Wunused-const-variable]\r\n      static const npy_cdouble c_half = {0.5, 0.0};\r\n                               ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:68:26: warning: unused variable 'c_i' [-Wunused-const-variable]\r\n      static const npy_cdouble c_i = {0.0, 1.0};\r\n                               ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:69:26: warning: unused variable 'c_ihalf' [-Wunused-const-variable]\r\n      static const npy_cdouble c_ihalf = {0.0, 0.5};\r\n                               ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:79:1: warning: unused function 'cadd' [-Wunused-function]\r\n      cadd(npy_cdouble a, npy_cdouble b)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:87:1: warning: unused function 'csub' [-Wunused-function]\r\n      csub(npy_cdouble a, npy_cdouble b)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:137:1: warning: unused function 'cneg' [-Wunused-function]\r\n      cneg(npy_cdouble a)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:144:1: warning: unused function 'cmuli' [-Wunused-function]\r\n      cmuli(npy_cdouble a)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:67:30: warning: unused variable 'c_halfl' [-Wunused-const-variable]\r\n      static const npy_clongdouble c_halfl = {0.5L, 0.0};\r\n                                   ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:68:30: warning: unused variable 'c_il' [-Wunused-const-variable]\r\n      static const npy_clongdouble c_il = {0.0, 1.0L};\r\n                                   ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:69:30: warning: unused variable 'c_ihalfl' [-Wunused-const-variable]\r\n      static const npy_clongdouble c_ihalfl = {0.0, 0.5L};\r\n                                   ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:79:1: warning: unused function 'caddl' [-Wunused-function]\r\n      caddl(npy_clongdouble a, npy_clongdouble b)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:87:1: warning: unused function 'csubl' [-Wunused-function]\r\n      csubl(npy_clongdouble a, npy_clongdouble b)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:137:1: warning: unused function 'cnegl' [-Wunused-function]\r\n      cnegl(npy_clongdouble a)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:144:1: warning: unused function 'cmulil' [-Wunused-function]\r\n      cmulil(npy_clongdouble a)\r\n      ^\r\n      22 warnings generated.\r\n      ar: adding 4 object files to build/temp.macosx-10.15-x86_64-3.9/libnpymath.a\r\n      ranlib:@ build/temp.macosx-10.15-x86_64-3.9/libnpymath.a\r\n      building 'npysort' library\r\n      compiling C sources\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npysort\r\n      compile options: '-Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npysort/quicksort.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npysort/mergesort.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npysort/heapsort.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npysort/selection.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npysort/binsearch.c\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp k;\r\n              ^~~~~~~~~~~\r\n      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead\r\n          else if (0 && kth == num - 1) {\r\n                   ^\r\n                   /* DISABLES CODE */ ( )\r\n      22 warnings generated.\r\n      ar: adding 5 object files to build/temp.macosx-10.15-x86_64-3.9/libnpysort.a\r\n      ranlib:@ build/temp.macosx-10.15-x86_64-3.9/libnpysort.a\r\n      running build_ext\r\n      customize UnixCCompiler\r\n      customize UnixCCompiler using build_ext\r\n      building 'numpy.core._dummy' extension\r\n      compiling C sources\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'\r\n      clang: numpy/core/src/dummymodule.c\r\n      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/dummymodule.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_dummy.cpython-39-darwin.so\r\n      building 'numpy.core._multiarray_tests' extension\r\n      compiling C sources\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray\r\n      creating build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common\r\n      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/_multiarray_tests.c\r\n      clang: numpy/core/src/common/mem_overlap.c\r\n      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/_multiarray_tests.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/mem_overlap.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -lnpymath -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_multiarray_tests.cpython-39-darwin.so\r\n      building 'numpy.core._multiarray_umath' extension\r\n      compiling C sources\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      creating build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray\r\n      creating build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath\r\n      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath\r\n      creating build/temp.macosx-10.15-x86_64-3.9/private\r\n      creating build/temp.macosx-10.15-x86_64-3.9/private/var\r\n      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders\r\n      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz\r\n      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn\r\n      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T\r\n      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l\r\n      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e\r\n      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e/numpy\r\n      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e/numpy/_build_utils\r\n      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e/numpy/_build_utils/src\r\n      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'\r\n      extra options: '-msse3 -I/System/Library/Frameworks/vecLib.framework/Headers'\r\n      clang: numpy/core/src/multiarray/alloc.c\r\n      clang: numpy/core/src/multiarray/calculation.cclang: numpy/core/src/multiarray/array_assign_scalar.c\r\n      clang: numpy/core/src/multiarray/convert.c\r\n  \r\n      clang: numpy/core/src/multiarray/ctors.c\r\n      clang: numpy/core/src/multiarray/datetime_busday.c\r\n      clang: numpy/core/src/multiarray/dragon4.cclang: numpy/core/src/multiarray/flagsobject.c\r\n  \r\n      numpy/core/src/multiarray/ctors.c:2261:36: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n          if (!(PyUString_Check(name) && PyUString_GET_SIZE(name) == 0)) {\r\n                                         ^\r\n      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'\r\n      #define PyUString_GET_SIZE PyUnicode_GET_SIZE\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/ctors.c:2261:36: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n          if (!(PyUString_Check(name) && PyUString_GET_SIZE(name) == 0)) {\r\n                                         ^\r\n      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'\r\n      #define PyUString_GET_SIZE PyUnicode_GET_SIZE\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/ctors.c:2261:36: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n          if (!(PyUString_Check(name) && PyUString_GET_SIZE(name) == 0)) {\r\n                                         ^\r\n      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'\r\n      #define PyUString_GET_SIZE PyUnicode_GET_SIZE\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      clang: numpy/core/src/multiarray/arrayobject.c\r\n      clang: numpy/core/src/multiarray/array_assign_array.c\r\n      clang: numpy/core/src/multiarray/convert_datatype.c\r\n      clang: numpy/core/src/multiarray/getset.c\r\n      clang: numpy/core/src/multiarray/datetime_busdaycal.c\r\n      clang: numpy/core/src/multiarray/buffer.c\r\n      clang: numpy/core/src/multiarray/compiled_base.c\r\n      clang: numpy/core/src/multiarray/hashdescr.c\r\n      clang: numpy/core/src/multiarray/descriptor.c\r\n      numpy/core/src/multiarray/descriptor.c:453:13: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n              if (PyUString_GET_SIZE(name) == 0) {\r\n                  ^\r\n      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'\r\n      #define PyUString_GET_SIZE PyUnicode_GET_SIZE\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/descriptor.c:453:13: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n              if (PyUString_GET_SIZE(name) == 0) {\r\n                  ^\r\n      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'\r\n      #define PyUString_GET_SIZE PyUnicode_GET_SIZE\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/descriptor.c:453:13: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n              if (PyUString_GET_SIZE(name) == 0) {\r\n                  ^\r\n      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'\r\n      #define PyUString_GET_SIZE PyUnicode_GET_SIZE\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/descriptor.c:460:48: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                  else if (PyUString_Check(title) && PyUString_GET_SIZE(title) > 0) {\r\n                                                     ^\r\n      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'\r\n      #define PyUString_GET_SIZE PyUnicode_GET_SIZE\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/descriptor.c:460:48: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n                  else if (PyUString_Check(title) && PyUString_GET_SIZE(title) > 0) {\r\n                                                     ^\r\n      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'\r\n      #define PyUString_GET_SIZE PyUnicode_GET_SIZE\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/descriptor.c:460:48: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                  else if (PyUString_Check(title) && PyUString_GET_SIZE(title) > 0) {\r\n                                                     ^\r\n      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'\r\n      #define PyUString_GET_SIZE PyUnicode_GET_SIZE\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      clang: numpy/core/src/multiarray/conversion_utils.c\r\n      clang: numpy/core/src/multiarray/item_selection.c\r\n      clang: numpy/core/src/multiarray/dtype_transfer.c\r\n      clang: numpy/core/src/multiarray/mapping.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/arraytypes.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/nditer_templ.c\r\n      3 warnings generated.\r\n      clang: numpy/core/src/multiarray/datetime.c\r\n      numpy/core/src/multiarray/arraytypes.c.src:477:11: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n          ptr = PyUnicode_AS_UNICODE(temp);\r\n                ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:279:7: note: expanded from macro 'PyUnicode_AS_UNICODE'\r\n            PyUnicode_AsUnicode(_PyObject_CAST(op)))\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/arraytypes.c.src:482:15: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n          datalen = PyUnicode_GET_DATA_SIZE(temp);\r\n                    ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/arraytypes.c.src:482:15: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n          datalen = PyUnicode_GET_DATA_SIZE(temp);\r\n                    ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/arraytypes.c.src:482:15: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n          datalen = PyUnicode_GET_DATA_SIZE(temp);\r\n                    ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      clang: numpy/core/src/multiarray/common.c\r\n      numpy/core/src/multiarray/common.c:187:28: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                      itemsize = PyUnicode_GET_DATA_SIZE(temp);\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/common.c:187:28: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n                      itemsize = PyUnicode_GET_DATA_SIZE(temp);\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/common.c:187:28: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                      itemsize = PyUnicode_GET_DATA_SIZE(temp);\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/common.c:239:28: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                      itemsize = PyUnicode_GET_DATA_SIZE(temp);\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/common.c:239:28: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n                      itemsize = PyUnicode_GET_DATA_SIZE(temp);\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/common.c:239:28: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                      itemsize = PyUnicode_GET_DATA_SIZE(temp);\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/common.c:282:24: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n              int itemsize = PyUnicode_GET_DATA_SIZE(obj);\r\n                             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/common.c:282:24: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n              int itemsize = PyUnicode_GET_DATA_SIZE(obj);\r\n                             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/common.c:282:24: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n              int itemsize = PyUnicode_GET_DATA_SIZE(obj);\r\n                             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      6 warnings generated.\r\n      clang: numpy/core/src/multiarray/nditer_pywrap.c\r\n      9 warnings generated.\r\n      clang: numpy/core/src/multiarray/sequence.c\r\n      clang: numpy/core/src/multiarray/shape.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/einsum.c\r\n      clang: numpy/core/src/multiarray/methods.c\r\n      clang: numpy/core/src/multiarray/iterators.c\r\n      clang: numpy/core/src/multiarray/datetime_strings.c\r\n      clang: numpy/core/src/multiarray/number.c\r\n      clang: numpy/core/src/multiarray/scalarapi.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/scalartypes.c\r\n      numpy/core/src/multiarray/scalarapi.c:74:28: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n                  return (void *)PyUnicode_AS_DATA(scalar);\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:283:21: note: expanded from macro 'PyUnicode_AS_DATA'\r\n          ((const char *)(PyUnicode_AS_UNICODE(op)))\r\n                          ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:279:7: note: expanded from macro 'PyUnicode_AS_UNICODE'\r\n            PyUnicode_AsUnicode(_PyObject_CAST(op)))\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalarapi.c:135:28: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n                  return (void *)PyUnicode_AS_DATA(scalar);\r\n                                 ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:283:21: note: expanded from macro 'PyUnicode_AS_DATA'\r\n          ((const char *)(PyUnicode_AS_UNICODE(op)))\r\n                          ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:279:7: note: expanded from macro 'PyUnicode_AS_UNICODE'\r\n            PyUnicode_AsUnicode(_PyObject_CAST(op)))\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalarapi.c:568:29: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                  descr->elsize = PyUnicode_GET_DATA_SIZE(sc);\r\n                                  ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalarapi.c:568:29: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n                  descr->elsize = PyUnicode_GET_DATA_SIZE(sc);\r\n                                  ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalarapi.c:568:29: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                  descr->elsize = PyUnicode_GET_DATA_SIZE(sc);\r\n                                  ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:475:17: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n          ip = dptr = PyUnicode_AS_UNICODE(self);\r\n                      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:279:7: note: expanded from macro 'PyUnicode_AS_UNICODE'\r\n            PyUnicode_AsUnicode(_PyObject_CAST(op)))\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:476:11: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n          len = PyUnicode_GET_SIZE(self);\r\n                ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:476:11: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n          len = PyUnicode_GET_SIZE(self);\r\n                ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:476:11: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n          len = PyUnicode_GET_SIZE(self);\r\n                ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:481:11: warning: 'PyUnicode_FromUnicode' is deprecated [-Wdeprecated-declarations]\r\n          new = PyUnicode_FromUnicode(ip, len);\r\n                ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:551:1: note: 'PyUnicode_FromUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(PyObject*) PyUnicode_FromUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:475:17: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n          ip = dptr = PyUnicode_AS_UNICODE(self);\r\n                      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:279:7: note: expanded from macro 'PyUnicode_AS_UNICODE'\r\n            PyUnicode_AsUnicode(_PyObject_CAST(op)))\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:476:11: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n          len = PyUnicode_GET_SIZE(self);\r\n                ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:476:11: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n          len = PyUnicode_GET_SIZE(self);\r\n                ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:476:11: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n          len = PyUnicode_GET_SIZE(self);\r\n                ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:481:11: warning: 'PyUnicode_FromUnicode' is deprecated [-Wdeprecated-declarations]\r\n          new = PyUnicode_FromUnicode(ip, len);\r\n                ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:551:1: note: 'PyUnicode_FromUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(PyObject*) PyUnicode_FromUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:1849:18: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n              buffer = PyUnicode_AS_DATA(self);\r\n                       ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:283:21: note: expanded from macro 'PyUnicode_AS_DATA'\r\n          ((const char *)(PyUnicode_AS_UNICODE(op)))\r\n                          ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:279:7: note: expanded from macro 'PyUnicode_AS_UNICODE'\r\n            PyUnicode_AsUnicode(_PyObject_CAST(op)))\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:1850:18: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n              buflen = PyUnicode_GET_DATA_SIZE(self);\r\n                       ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:1850:18: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n              buflen = PyUnicode_GET_DATA_SIZE(self);\r\n                       ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/scalartypes.c.src:1850:18: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n              buflen = PyUnicode_GET_DATA_SIZE(self);\r\n                       ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'\r\n          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)\r\n           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      5 warnings generated.\r\n      clang: numpy/core/src/multiarray/typeinfo.c\r\n      clang: numpy/core/src/multiarray/refcount.c\r\n      clang: numpy/core/src/multiarray/usertypes.c\r\n      clang: numpy/core/src/multiarray/multiarraymodule.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/lowlevel_strided_loops.c\r\n      clang: numpy/core/src/multiarray/vdot.c\r\n      clang: numpy/core/src/umath/umathmodule.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/matmul.c\r\n      clang: numpy/core/src/umath/reduction.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/loops.c\r\n      clang: numpy/core/src/multiarray/nditer_api.c\r\n      14 warnings generated.\r\n      clang: numpy/core/src/multiarray/strfuncs.c\r\n      numpy/core/src/umath/loops.c.src:655:18: warning: 'PyEval_CallObjectWithKeywords' is deprecated [-Wdeprecated-declarations]\r\n              result = PyEval_CallObject(tocall, arglist);\r\n                       ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/ceval.h:24:5: note: expanded from macro 'PyEval_CallObject'\r\n          PyEval_CallObjectWithKeywords(callable, arg, (PyObject *)NULL)\r\n          ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/ceval.h:17:1: note: 'PyEval_CallObjectWithKeywords' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.9) PyAPI_FUNC(PyObject *) PyEval_CallObjectWithKeywords(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/strfuncs.c:178:13: warning: 'PyEval_CallObjectWithKeywords' is deprecated [-Wdeprecated-declarations]\r\n              s = PyEval_CallObject(PyArray_ReprFunction, arglist);\r\n                  ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/ceval.h:24:5: note: expanded from macro 'PyEval_CallObject'\r\n          PyEval_CallObjectWithKeywords(callable, arg, (PyObject *)NULL)\r\n          ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/ceval.h:17:1: note: 'PyEval_CallObjectWithKeywords' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.9) PyAPI_FUNC(PyObject *) PyEval_CallObjectWithKeywords(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/core/src/multiarray/strfuncs.c:195:13: warning: 'PyEval_CallObjectWithKeywords' is deprecated [-Wdeprecated-declarations]\r\n              s = PyEval_CallObject(PyArray_StrFunction, arglist);\r\n                  ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/ceval.h:24:5: note: expanded from macro 'PyEval_CallObject'\r\n          PyEval_CallObjectWithKeywords(callable, arg, (PyObject *)NULL)\r\n          ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/ceval.h:17:1: note: 'PyEval_CallObjectWithKeywords' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.9) PyAPI_FUNC(PyObject *) PyEval_CallObjectWithKeywords(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      2 warnings generated.\r\n      clang: numpy/core/src/multiarray/temp_elide.c\r\n      clang: numpy/core/src/umath/cpuid.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/scalarmath.c\r\n      clang: numpy/core/src/umath/ufunc_object.c\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'byte_long' [-Wunused-function]\r\n      byte_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'ubyte_long' [-Wunused-function]\r\n      ubyte_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'short_long' [-Wunused-function]\r\n      short_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'ushort_long' [-Wunused-function]\r\n      ushort_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'int_long' [-Wunused-function]\r\n      int_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'uint_long' [-Wunused-function]\r\n      uint_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'long_long' [-Wunused-function]\r\n      long_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'ulong_long' [-Wunused-function]\r\n      ulong_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'longlong_long' [-Wunused-function]\r\n      longlong_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'ulonglong_long' [-Wunused-function]\r\n      ulonglong_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'half_long' [-Wunused-function]\r\n      half_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'float_long' [-Wunused-function]\r\n      float_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'double_long' [-Wunused-function]\r\n      double_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'longdouble_long' [-Wunused-function]\r\n      longdouble_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'cfloat_long' [-Wunused-function]\r\n      cfloat_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'cdouble_long' [-Wunused-function]\r\n      cdouble_long(PyObject *obj)\r\n      ^\r\n      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'clongdouble_long' [-Wunused-function]\r\n      clongdouble_long(PyObject *obj)\r\n      ^\r\n      clang: numpy/core/src/multiarray/nditer_constr.c\r\n      numpy/core/src/umath/ufunc_object.c:657:19: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]\r\n          for (i = 0; i < len; i++) {\r\n                      ~ ^ ~~~\r\n      clang: numpy/core/src/umath/override.c\r\n      clang: numpy/core/src/npymath/npy_math.c\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/ieee754.c\r\n      numpy/core/src/umath/loops.c.src:2527:22: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp n = dimensions[0];\r\n                           ^~~~~~~~~~\r\n      numpy/core/src/umath/loops.c.src:2526:29: note: silence by adding parentheses to mark code as explicitly dead\r\n          if (IS_BINARY_REDUCE && 0) {\r\n                                  ^\r\n                                  /* DISABLES CODE */ ( )\r\n      numpy/core/src/umath/loops.c.src:2527:22: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp n = dimensions[0];\r\n                           ^~~~~~~~~~\r\n      numpy/core/src/umath/loops.c.src:2526:29: note: silence by adding parentheses to mark code as explicitly dead\r\n          if (IS_BINARY_REDUCE && 0) {\r\n                                  ^\r\n                                  /* DISABLES CODE */ ( )\r\n      numpy/core/src/umath/loops.c.src:2527:22: warning: code will never be executed [-Wunreachable-code]\r\n              npy_intp n = dimensions[0];\r\n                           ^~~~~~~~~~\r\n      numpy/core/src/umath/loops.c.src:2526:29: note: silence by adding parentheses to mark code as explicitly dead\r\n          if (IS_BINARY_REDUCE && 0) {\r\n                                  ^\r\n                                  /* DISABLES CODE */ ( )\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/npy_math_complex.c\r\n      numpy/core/src/npymath/npy_math_complex.c.src:48:33: warning: unused variable 'tiny' [-Wunused-const-variable]\r\n      static const volatile npy_float tiny = 3.9443045e-31f;\r\n                                      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:67:25: warning: unused variable 'c_halff' [-Wunused-const-variable]\r\n      static const npy_cfloat c_halff = {0.5F, 0.0};\r\n                              ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:68:25: warning: unused variable 'c_if' [-Wunused-const-variable]\r\n      static const npy_cfloat c_if = {0.0, 1.0F};\r\n                              ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:69:25: warning: unused variable 'c_ihalff' [-Wunused-const-variable]\r\n      static const npy_cfloat c_ihalff = {0.0, 0.5F};\r\n                              ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:79:1: warning: unused function 'caddf' [-Wunused-function]\r\n      caddf(npy_cfloat a, npy_cfloat b)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:87:1: warning: unused function 'csubf' [-Wunused-function]\r\n      csubf(npy_cfloat a, npy_cfloat b)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:137:1: warning: unused function 'cnegf' [-Wunused-function]\r\n      cnegf(npy_cfloat a)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:144:1: warning: unused function 'cmulif' [-Wunused-function]\r\n      cmulif(npy_cfloat a)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:67:26: warning: unused variable 'c_half' [-Wunused-const-variable]\r\n      static const npy_cdouble c_half = {0.5, 0.0};\r\n                               ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:68:26: warning: unused variable 'c_i' [-Wunused-const-variable]\r\n      static const npy_cdouble c_i = {0.0, 1.0};\r\n                               ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:69:26: warning: unused variable 'c_ihalf' [-Wunused-const-variable]\r\n      static const npy_cdouble c_ihalf = {0.0, 0.5};\r\n                               ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:79:1: warning: unused function 'cadd' [-Wunused-function]\r\n      cadd(npy_cdouble a, npy_cdouble b)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:87:1: warning: unused function 'csub' [-Wunused-function]\r\n      csub(npy_cdouble a, npy_cdouble b)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:137:1: warning: unused function 'cneg' [-Wunused-function]\r\n      cneg(npy_cdouble a)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:144:1: warning: unused function 'cmuli' [-Wunused-function]\r\n      cmuli(npy_cdouble a)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:67:30: warning: unused variable 'c_halfl' [-Wunused-const-variable]\r\n      static const npy_clongdouble c_halfl = {0.5L, 0.0};\r\n                                   ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:68:30: warning: unused variable 'c_il' [-Wunused-const-variable]\r\n      static const npy_clongdouble c_il = {0.0, 1.0L};\r\n                                   ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:69:30: warning: unused variable 'c_ihalfl' [-Wunused-const-variable]\r\n      static const npy_clongdouble c_ihalfl = {0.0, 0.5L};\r\n                                   ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:79:1: warning: unused function 'caddl' [-Wunused-function]\r\n      caddl(npy_clongdouble a, npy_clongdouble b)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:87:1: warning: unused function 'csubl' [-Wunused-function]\r\n      csubl(npy_clongdouble a, npy_clongdouble b)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:137:1: warning: unused function 'cnegl' [-Wunused-function]\r\n      cnegl(npy_clongdouble a)\r\n      ^\r\n      numpy/core/src/npymath/npy_math_complex.c.src:144:1: warning: unused function 'cmulil' [-Wunused-function]\r\n      cmulil(npy_clongdouble a)\r\n      ^\r\n      22 warnings generated.\r\n      clang: numpy/core/src/common/mem_overlap.c\r\n      clang: numpy/core/src/npymath/halffloat.c\r\n      clang: numpy/core/src/common/array_assign.c\r\n      clang: numpy/core/src/common/ufunc_override.c\r\n      clang: numpy/core/src/common/npy_longdouble.c\r\n      clang: numpy/core/src/common/numpyos.c\r\n      clang: numpy/core/src/common/ucsnarrow.c\r\n      1 warning generated.\r\n      clang: numpy/core/src/umath/extobj.c\r\n      numpy/core/src/common/ucsnarrow.c:139:34: warning: 'PyUnicode_FromUnicode' is deprecated [-Wdeprecated-declarations]\r\n              ret = (PyUnicodeObject *)PyUnicode_FromUnicode((Py_UNICODE*)buf,\r\n                                       ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:551:1: note: 'PyUnicode_FromUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(PyObject*) PyUnicode_FromUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      1 warning generated.\r\n      clang: numpy/core/src/common/python_xerbla.c\r\n      clang: numpy/core/src/common/cblasfuncs.c\r\n      clang: /private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e/numpy/_build_utils/src/apple_sgemv_fix.c\r\n      In file included from /private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e/numpy/_build_utils/src/apple_sgemv_fix.c:26:\r\n      In file included from numpy/core/include/numpy/arrayobject.h:4:\r\n      In file included from numpy/core/include/numpy/ndarrayobject.h:21:\r\n      build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__multiarray_api.h:1463:1: warning: unused function '_import_array' [-Wunused-function]\r\n      _import_array(void)\r\n      ^\r\n      1 warning generated.\r\n      17 warnings generated.\r\n      clang: numpy/core/src/umath/ufunc_type_resolution.c\r\n      4 warnings generated.\r\n      4 warnings generated.\r\n      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/alloc.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/arrayobject.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/arraytypes.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/array_assign_scalar.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/array_assign_array.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/buffer.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/calculation.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/compiled_base.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/common.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/convert.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/convert_datatype.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/conversion_utils.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/ctors.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/datetime.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/datetime_strings.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/datetime_busday.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/datetime_busdaycal.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/descriptor.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/dragon4.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/dtype_transfer.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/einsum.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/flagsobject.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/getset.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/hashdescr.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/item_selection.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/iterators.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/lowlevel_strided_loops.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/mapping.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/methods.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/multiarraymodule.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/nditer_templ.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/nditer_api.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/nditer_constr.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/nditer_pywrap.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/number.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/refcount.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/sequence.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/shape.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/scalarapi.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/scalartypes.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/strfuncs.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/temp_elide.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/typeinfo.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/usertypes.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/vdot.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/umathmodule.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/reduction.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/loops.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/matmul.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/ufunc_object.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/extobj.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/cpuid.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/scalarmath.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/ufunc_type_resolution.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/override.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/npy_math.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/ieee754.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/npy_math_complex.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/halffloat.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/array_assign.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/mem_overlap.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/npy_longdouble.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/ucsnarrow.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/ufunc_override.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/numpyos.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/cblasfuncs.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/python_xerbla.o build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e/numpy/_build_utils/src/apple_sgemv_fix.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -lnpymath -lnpysort -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_multiarray_umath.cpython-39-darwin.so -Wl,-framework -Wl,Accelerate\r\n      building 'numpy.core._umath_tests' extension\r\n      compiling C sources\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_umath_tests.c\r\n      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_umath_tests.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_umath_tests.cpython-39-darwin.so\r\n      building 'numpy.core._rational_tests' extension\r\n      compiling C sources\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_rational_tests.c\r\n      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_rational_tests.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_rational_tests.cpython-39-darwin.so\r\n      building 'numpy.core._struct_ufunc_tests' extension\r\n      compiling C sources\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_struct_ufunc_tests.c\r\n      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_struct_ufunc_tests.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_struct_ufunc_tests.cpython-39-darwin.so\r\n      building 'numpy.core._operand_flag_tests' extension\r\n      compiling C sources\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_operand_flag_tests.c\r\n      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_operand_flag_tests.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_operand_flag_tests.cpython-39-darwin.so\r\n      building 'numpy.fft.fftpack_lite' extension\r\n      compiling C sources\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      creating build/temp.macosx-10.15-x86_64-3.9/numpy/fft\r\n      compile options: '-Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'\r\n      clang: numpy/fft/fftpack_litemodule.c\r\n      clang: numpy/fft/fftpack.c\r\n      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/numpy/fft/fftpack_litemodule.o build/temp.macosx-10.15-x86_64-3.9/numpy/fft/fftpack.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/fft/fftpack_lite.cpython-39-darwin.so\r\n      building 'numpy.linalg.lapack_lite' extension\r\n      compiling C sources\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      creating build/temp.macosx-10.15-x86_64-3.9/numpy/linalg\r\n      creating build/temp.macosx-10.15-x86_64-3.9/numpy/linalg/lapack_lite\r\n      compile options: '-DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'\r\n      extra options: '-msse3 -I/System/Library/Frameworks/vecLib.framework/Headers'\r\n      clang: numpy/linalg/lapack_litemodule.c\r\n      clang: numpy/linalg/lapack_lite/python_xerbla.c\r\n      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/numpy/linalg/lapack_litemodule.o build/temp.macosx-10.15-x86_64-3.9/numpy/linalg/lapack_lite/python_xerbla.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/linalg/lapack_lite.cpython-39-darwin.so -Wl,-framework -Wl,Accelerate\r\n      building 'numpy.linalg._umath_linalg' extension\r\n      compiling C sources\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/linalg\r\n      compile options: '-DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'\r\n      extra options: '-msse3 -I/System/Library/Frameworks/vecLib.framework/Headers'\r\n      clang: build/src.macosx-10.15-x86_64-3.9/numpy/linalg/umath_linalg.c\r\n      numpy/linalg/umath_linalg.c.src:735:32: warning: unknown warning group '-Wmaybe-uninitialized', ignored [-Wunknown-warning-option]\r\n      #pragma GCC diagnostic ignored \"-Wmaybe-uninitialized\"\r\n                                     ^\r\n      numpy/linalg/umath_linalg.c.src:541:1: warning: unused function 'dump_ufunc_object' [-Wunused-function]\r\n      dump_ufunc_object(PyUFuncObject* ufunc)\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:566:1: warning: unused function 'dump_linearize_data' [-Wunused-function]\r\n      dump_linearize_data(const char* name, const LINEARIZE_DATA_t* params)\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:602:1: warning: unused function 'dump_FLOAT_matrix' [-Wunused-function]\r\n      dump_FLOAT_matrix(const char* name,\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:602:1: warning: unused function 'dump_DOUBLE_matrix' [-Wunused-function]\r\n      dump_DOUBLE_matrix(const char* name,\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:602:1: warning: unused function 'dump_CFLOAT_matrix' [-Wunused-function]\r\n      dump_CFLOAT_matrix(const char* name,\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:602:1: warning: unused function 'dump_CDOUBLE_matrix' [-Wunused-function]\r\n      dump_CDOUBLE_matrix(const char* name,\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:865:1: warning: unused function 'zero_FLOAT_matrix' [-Wunused-function]\r\n      zero_FLOAT_matrix(void *dst_in, const LINEARIZE_DATA_t* data)\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:865:1: warning: unused function 'zero_DOUBLE_matrix' [-Wunused-function]\r\n      zero_DOUBLE_matrix(void *dst_in, const LINEARIZE_DATA_t* data)\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:865:1: warning: unused function 'zero_CFLOAT_matrix' [-Wunused-function]\r\n      zero_CFLOAT_matrix(void *dst_in, const LINEARIZE_DATA_t* data)\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:865:1: warning: unused function 'zero_CDOUBLE_matrix' [-Wunused-function]\r\n      zero_CDOUBLE_matrix(void *dst_in, const LINEARIZE_DATA_t* data)\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:1862:1: warning: unused function 'dump_geev_params' [-Wunused-function]\r\n      dump_geev_params(const char *name, GEEV_PARAMS_t* params)\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:2132:1: warning: unused function 'init_cgeev' [-Wunused-function]\r\n      init_cgeev(GEEV_PARAMS_t* params,\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:2213:1: warning: unused function 'process_cgeev_results' [-Wunused-function]\r\n      process_cgeev_results(GEEV_PARAMS_t *NPY_UNUSED(params))\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:2376:1: warning: unused function 'dump_gesdd_params' [-Wunused-function]\r\n      dump_gesdd_params(const char *name,\r\n      ^\r\n      numpy/linalg/umath_linalg.c.src:2864:1: warning: unused function 'dump_gelsd_params' [-Wunused-function]\r\n      dump_gelsd_params(const char *name,\r\n      ^\r\n      16 warnings generated.\r\n      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/linalg/umath_linalg.o build/temp.macosx-10.15-x86_64-3.9/numpy/linalg/lapack_lite/python_xerbla.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -lnpymath -o build/lib.macosx-10.15-x86_64-3.9/numpy/linalg/_umath_linalg.cpython-39-darwin.so -Wl,-framework -Wl,Accelerate\r\n      building 'numpy.random.mtrand' extension\r\n      compiling C sources\r\n      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers\r\n  \r\n      creating build/temp.macosx-10.15-x86_64-3.9/numpy/random\r\n      creating build/temp.macosx-10.15-x86_64-3.9/numpy/random/mtrand\r\n      compile options: '-D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'\r\n      clang: numpy/random/mtrand/mtrand.c\r\n      clang: numpy/random/mtrand/initarray.cclang: numpy/random/mtrand/randomkit.c\r\n  \r\n      clang: numpy/random/mtrand/distributions.c\r\n      numpy/random/mtrand/mtrand.c:40400:34: error: no member named 'tp_print' in 'struct _typeobject'\r\n        __pyx_type_6mtrand_RandomState.tp_print = 0;\r\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^\r\n      numpy/random/mtrand/mtrand.c:42673:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                          (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :\r\n                           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/random/mtrand/mtrand.c:42673:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n                          (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :\r\n                           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/random/mtrand/mtrand.c:42673:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                          (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :\r\n                           ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/random/mtrand/mtrand.c:42673:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                          (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :\r\n                                                         ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/random/mtrand/mtrand.c:42673:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n                          (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :\r\n                                                         ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/random/mtrand/mtrand.c:42673:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                          (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :\r\n                                                         ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/random/mtrand/mtrand.c:42689:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                              (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :\r\n                               ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/random/mtrand/mtrand.c:42689:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n                              (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :\r\n                               ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/random/mtrand/mtrand.c:42689:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                              (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :\r\n                               ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/random/mtrand/mtrand.c:42689:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                              (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :\r\n                                                                ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            PyUnicode_WSTR_LENGTH(op) :                    \\\r\n            ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/random/mtrand/mtrand.c:42689:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]\r\n                              (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :\r\n                                                                ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\\\r\n                   ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      numpy/random/mtrand/mtrand.c:42689:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]\r\n                              (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :\r\n                                                                ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'\r\n             PyUnicode_WSTR_LENGTH(op)))\r\n             ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'\r\n      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)\r\n                                        ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here\r\n      Py_DEPRECATED(3.3)\r\n      ^\r\n      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'\r\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\r\n                                                           ^\r\n      12 warnings and 1 error generated.\r\n      error: Command \"clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c numpy/random/mtrand/mtrand.c -o build/temp.macosx-10.15-x86_64-3.9/numpy/random/mtrand/mtrand.o -MMD -MF build/temp.macosx-10.15-x86_64-3.9/numpy/random/mtrand/mtrand.o.d\" failed with exit status 1",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 780971987,
    "title": "fix ner_tag bugs in thainer",
    "dateCreated": "2021-01-07T02:12:33Z",
    "dateModified": "2021-01-07T02:12:33Z",
    "description": "fix bug that results in `ner_tag` always equal to 'O'.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 780429080,
    "title": "Add OSCAR",
    "dateCreated": "2021-01-06T10:21:08Z",
    "dateModified": "2021-01-06T10:21:08Z",
    "description": "Continuation of #348 \r\nThe files have been moved to S3 and only the unshuffled version is available.\r\nBoth original and deduplicated versions of each language are available.\r\n\r\nExample of usage:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\noscar_dedup_en = load_dataset(\"oscar\", \"unshuffled_deduplicated_en\", split=\"train\")\r\noscar_orig_fr = load_dataset(\"oscar\", \"unshuffled_original_fr\", split=\"train\")\r\n```\r\n\r\ncc @pjox @jonatasgrosman \r\n\r\n-------------\r\n\r\nTo make the metadata generation work in parallel I did a few changes in the `datasets-cli test` command to add the `num_proc` and `proc_rank` arguments. This way you can run multiple processes for the metadata computation.\r\n\r\n```\r\ndatasets-cli test ./datasets/oscar --save_infos --all_configs --num_proc 4 --proc_rank 0 --clear_cache --cache_dir tmp0\r\n```\r\n\r\n-------------\r\n\r\nToDo: add the dummy_data",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 780268595,
    "title": "Fix reuters metadata parsing errors",
    "dateCreated": "2021-01-06T08:26:03Z",
    "dateModified": "2021-01-06T08:26:03Z",
    "description": "Was missing the last entry in each metadata category",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 779882271,
    "title": "Updated HuggingFace Datasets README (fix typos)",
    "dateCreated": "2021-01-06T02:14:38Z",
    "dateModified": "2021-01-06T02:14:38Z",
    "description": "Awesome work on \ud83e\udd17 Datasets. I found a couple of small typos in the README. Hope this helps.\r\n\r\n\r\n\r\n![](https://emojipedia-us.s3.dualstack.us-west-1.amazonaws.com/thumbs/160/google/56/hugging-face_1f917.png)\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 779441631,
    "title": "Fast start up",
    "dateCreated": "2021-01-05T19:07:53Z",
    "dateModified": "2021-01-05T19:07:53Z",
    "description": "Currently if optional dependencies such as tensorflow, torch, apache_beam, faiss and elasticsearch are installed, then it takes a long time to do `import datasets` since it imports all of these heavy dependencies.\r\n\r\nTo make a fast start up for `datasets` I changed that so that they are not imported when `datasets` is being imported. On my side it changed the import time of `datasets` from 5sec to 0.5sec, which is enjoyable.\r\n\r\nTo be able to check if optional dependencies are available without importing them I'm using `importlib_metadata`, which is part of the standard lib in python>=3.8 and was backported. The difference with `importlib` is that it also enables to get the versions of the libraries without importing them.\r\nI added this dependency in `setup.py`.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 779107313,
    "title": "Fix ade_corpus_v2 config names",
    "dateCreated": "2021-01-05T14:33:28Z",
    "dateModified": "2021-01-05T14:33:28Z",
    "description": "There are currently some typos in the config names of the `ade_corpus_v2` dataset, I fixed them:\r\n\r\n- Ade_corpos_v2_classificaion -> Ade_corpus_v2_classification\r\n- Ade_corpos_v2_drug_ade_relation -> Ade_corpus_v2_drug_ade_relation\r\n- Ade_corpos_v2_drug_dosage_relation -> Ade_corpus_v2_drug_dosage_relation",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 779029685,
    "title": "Fix DaNE last example",
    "dateCreated": "2021-01-05T13:29:37Z",
    "dateModified": "2021-01-05T13:29:37Z",
    "description": "The last example from the DaNE dataset is empty.\r\n\r\nFix #1686 ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 779004894,
    "title": "Question: Shouldn't .info be a part of DatasetDict?",
    "dateCreated": "2021-01-05T13:08:41Z",
    "dateModified": "2021-01-05T13:08:41Z",
    "description": "Currently, only `Dataset` contains the .info or .features, but as many datasets contains standard splits (train, test) and thus the underlying information is the same (or at least should be) across the datasets. \r\n\r\nFor instance:\r\n```\r\n>>> ds = datasets.load_dataset(\"conll2002\", \"es\")\r\n>>> ds.info\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'DatasetDict' object has no attribute 'info'\r\n```\r\n\r\nI could imagine that this wouldn't work for datasets dicts which hold entirely different datasets (multimodal datasets), but it seems odd that splits of the same dataset is treated the same as what is essentially different datasets. \r\n\r\nIntuitively it would also make sense that if a dataset is supplied via. the load_dataset that is have a common .info which covers the entire dataset.\r\n\r\nIt is entirely possible that I am missing another perspective",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 778921684,
    "title": "Dataset Error: DaNE contains empty samples at the end",
    "dateCreated": "2021-01-05T11:54:26Z",
    "dateModified": "2021-01-05T11:54:26Z",
    "description": "The dataset DaNE, contains empty samples at the end. It is naturally easy to remove using a filter but should probably not be there, to begin with as it can cause errors.\r\n\r\n```python\r\n>>> import datasets\r\n[...]\r\n>>> dataset = datasets.load_dataset(\"dane\")\r\n[...]\r\n>>> dataset[\"test\"][-1]\r\n{'dep_ids': [], 'dep_labels': [], 'lemmas': [], 'morph_tags': [], 'ner_tags': [], 'pos_tags': [], 'sent_id': '', 'text': '', 'tok_ids': [], 'tokens': []}\r\n>>> dataset[\"train\"][-1]\r\n{'dep_ids': [], 'dep_labels': [], 'lemmas': [], 'morph_tags': [], 'ner_tags': [], 'pos_tags': [], 'sent_id': '', 'text': '', 'tok_ids': [], 'tokens': []}\r\n```\r\n\r\nBest,\r\nKenneth",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 778914431,
    "title": "Update README.md of covid-tweets-japanese",
    "dateCreated": "2021-01-05T11:47:27Z",
    "dateModified": "2021-01-05T11:47:27Z",
    "description": "Update README.md of covid-tweets-japanese added by PR https://github.com/huggingface/datasets/pull/1367 and https://github.com/huggingface/datasets/pull/1402.\r\n\r\n- Update \"Data Splits\" to be more precise that no information is provided for now.\r\n  - old: [More Information Needed]\r\n  - new: No information about data splits is provided for now.\r\n\r\n- The automatic generation of links seemed not working properly, so I added a space before and after the URL to make the links work correctly.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 778356196,
    "title": "Add CANER Corpus",
    "dateCreated": "2021-01-04T20:49:11Z",
    "dateModified": "2021-01-04T20:49:11Z",
    "description": "What does this PR do?\r\n\r\nAdds the following dataset:\r\n\r\nhttps://github.com/RamziSalah/Classical-Arabic-Named-Entity-Recognition-Corpus\r\n\r\nWho can review?\r\n\r\n@lhoestq",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 778287612,
    "title": "`ArrowInvalid` occurs while running `Dataset.map()` function for DPRContext",
    "dateCreated": "2021-01-04T18:47:53Z",
    "dateModified": "2021-01-04T18:47:53Z",
    "description": "It seems to fail the final batch ):\r\n\r\nsteps to reproduce:\r\n```\r\nfrom datasets import load_dataset\r\nfrom elasticsearch import Elasticsearch\r\nimport torch\r\nfrom transformers import file_utils, set_seed\r\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizerFast\r\nMAX_SEQ_LENGTH = 256\r\nctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", cache_dir=\"../datasets/\")\r\nctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(\r\n    \"facebook/dpr-ctx_encoder-single-nq-base\", \r\n    cache_dir=\"..datasets/\"\r\n)\r\n\r\ndataset = load_dataset('text', \r\n                data_files='data/raw/ARC_Corpus.txt',\r\n                cache_dir='../datasets')\r\n\r\ntorch.set_grad_enabled(False)\r\nds_with_embeddings = dataset.map(\r\n    lambda example: {\r\n        'embeddings': ctx_encoder(\r\n            **ctx_tokenizer(\r\n                example[\"text\"], \r\n                padding='max_length', \r\n                truncation=True, \r\n                max_length=MAX_SEQ_LENGTH,\r\n                return_tensors=\"pt\"\r\n            )\r\n        )[0][0].numpy(),\r\n    },\r\n    batched=True,\r\n    load_from_cache_file=False,\r\n    batch_size=1000\r\n)\r\n```\r\nARC Corpus can be obtained from [here](https://ai2-datasets.s3-us-west-2.amazonaws.com/arc/ARC-V1-Feb2018.zip)\r\n\r\nAnd then the error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\n<ipython-input-13-67d139bb2ed3> in <module>\r\n     14     batched=True,\r\n     15     load_from_cache_file=False,\r\n---> 16     batch_size=1000\r\n     17 )\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/dataset_dict.py in map(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc)\r\n    301                     num_proc=num_proc,\r\n    302                 )\r\n--> 303                 for k, dataset in self.items()\r\n    304             }\r\n    305         )\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/dataset_dict.py in <dictcomp>(.0)\r\n    301                     num_proc=num_proc,\r\n    302                 )\r\n--> 303                 for k, dataset in self.items()\r\n    304             }\r\n    305         )\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\r\n   1257                 fn_kwargs=fn_kwargs,\r\n   1258                 new_fingerprint=new_fingerprint,\r\n-> 1259                 update_data=update_data,\r\n   1260             )\r\n   1261         else:\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    155         }\r\n    156         # apply actual function\r\n--> 157         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    158         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    159         # re-apply format to the output\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    161             # Call actual function\r\n    162 \r\n--> 163             out = func(self, *args, **kwargs)\r\n    164 \r\n    165             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/arrow_dataset.py in _map_single(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, update_data)\r\n   1526                     if update_data:\r\n   1527                         batch = cast_to_python_objects(batch)\r\n-> 1528                         writer.write_batch(batch)\r\n   1529             if update_data:\r\n   1530                 writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/arrow_writer.py in write_batch(self, batch_examples, writer_batch_size)\r\n    276             typed_sequence = TypedSequence(batch_examples[col], type=col_type, try_type=col_try_type)\r\n    277             typed_sequence_examples[col] = typed_sequence\r\n--> 278         pa_table = pa.Table.from_pydict(typed_sequence_examples)\r\n    279         self.write_table(pa_table)\r\n    280 \r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_pydict()\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_arrays()\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.validate()\r\n\r\n~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Column 1 named text expected length 768 but got length 1000\r\n```",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 778268156,
    "title": "Don't use xlrd for xlsx files",
    "dateCreated": "2021-01-04T18:11:50Z",
    "dateModified": "2021-01-04T18:11:50Z",
    "description": "Since the latest release of `xlrd` (2.0), the support for xlsx files stopped.\r\nTherefore we needed to use something else.\r\nA good alternative is `openpyxl` which has also an integration with pandas si we can still call `pd.read_excel`.\r\n\r\nI left the unused import of `openpyxl` in the dataset scripts to show users that this is a required dependency to use the scripts.\r\n\r\nI tested the different datasets using `datasets-cli test` and the tests are successful (no missing examples).",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 777644163,
    "title": "Dataset \"dane\" missing",
    "dateCreated": "2021-01-03T14:03:03Z",
    "dateModified": "2021-01-03T14:03:03Z",
    "description": "the `dane` dataset appear to be missing in the latest version (1.1.3).\r\n\r\n```python\r\n>>> import datasets\r\n>>> datasets.__version__\r\n'1.1.3'\r\n>>> \"dane\" in datasets.list_datasets()\r\nTrue\r\n```\r\n\r\nAs we can see it should be present, but doesn't seem to be findable when using `load_dataset`.\r\n\r\n```python\r\n>>> datasets.load_dataset(\"dane\")\r\nTraceback (most recent call last):\r\n  File \"/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/load.py\", line 267, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 300, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 486, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/dane/dane.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/load.py\", line 278, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 300, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 486, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/dane/dane.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/load.py\", line 588, in load_dataset\r\n    module_path, hash = prepare_module(\r\n  File \"/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/load.py\", line 280, in prepare_module\r\n    raise FileNotFoundError(\r\nFileNotFoundError: Couldn't find file locally at dane/dane.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/dane/dane.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/dane/dane.py\r\n```\r\n\r\nThis issue might be relevant to @ophelielacroix from the Alexandra Institut whom created the data.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 777623053,
    "title": "added TurkishProductReviews dataset",
    "dateCreated": "2021-01-03T11:52:59Z",
    "dateModified": "2021-01-03T11:52:59Z",
    "description": "This PR added **Turkish Product Reviews Dataset contains 235.165 product reviews collected online. There are 220.284 positive, 14881 negative reviews**.\r\n\r\n- **Repository:** [turkish-text-data](https://github.com/fthbrmnby/turkish-text-data)\r\n- **Point of Contact:** Fatih Barmanbay - @fthbrmnby",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 777587792,
    "title": "Can't import cc100 dataset",
    "dateCreated": "2021-01-03T07:12:56Z",
    "dateModified": "2021-01-03T07:12:56Z",
    "description": "There is some issue to import cc100 dataset.\r\n\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"cc100\")\r\n```\r\n\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/cc100/cc100.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nFileNotFoundError                         Traceback (most recent call last)\r\nFileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/cc100/cc100.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nFileNotFoundError                         Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, **download_kwargs)\r\n    280                 raise FileNotFoundError(\r\n    281                     \"Couldn't find file locally at {}, or remotely at {} or {}\".format(\r\n--> 282                         combined_path, github_file_path, file_path\r\n    283                     )\r\n    284                 )\r\n\r\nFileNotFoundError: Couldn't find file locally at cc100/cc100.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/cc100/cc100.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/cc100/cc100.py",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 777567920,
    "title": "Switchboard Dialog Act Corpus added under `datasets/swda`",
    "dateCreated": "2021-01-03T03:53:41Z",
    "dateModified": "2021-01-03T03:53:41Z",
    "description": "Switchboard Dialog Act Corpus\r\n\r\nIntro:\r\nThe Switchboard Dialog Act Corpus (SwDA) extends the Switchboard-1 Telephone Speech Corpus, Release 2,\r\nwith turn/utterance-level dialog-act tags. The tags summarize syntactic, semantic, and pragmatic information\r\nabout the associated turn. The SwDA project was undertaken at UC Boulder in the late 1990s.\r\n\r\nDetails:\r\n[homepage](http://compprag.christopherpotts.net/swda.html)\r\n[repo](https://github.com/NathanDuran/Switchboard-Corpus/raw/master/swda_data/)\r\n\r\nI believe this is an important dataset to have since there is no dataset related to dialogue act added.\r\n\r\nI didn't find any formatting for pull request. I hope all this information is enough.\r\n\r\nFor any support please contact me. ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 777553383,
    "title": "Switchboard Dialog Act Corpus added under `datasets/swda`",
    "dateCreated": "2021-01-03T01:16:42Z",
    "dateModified": "2021-01-03T01:16:42Z",
    "description": "Pleased to announced that I added my first dataset **Switchboard Dialog Act Corpus**.\r\n\r\n\r\nI think this is an important datasets to be added since it is the only one related to dialogue act classification. \r\n\r\nHope the pull request is ok. Wasn't able to see any special formatting for the pull request form.\r\n\r\n\r\nThe Switchboard Dialog Act Corpus (SwDA) extends the Switchboard-1 Telephone Speech Corpus, Release 2,\r\nwith turn/utterance-level dialog-act tags. The tags summarize syntactic, semantic, and pragmatic information\r\nabout the associated turn. The SwDA project was undertaken at UC Boulder in the late 1990s.\r\n\r\n\r\n[webpage](http://compprag.christopherpotts.net/swda.html)\r\n\r\n[repo](https://github.com/NathanDuran/Switchboard-Corpus/raw/master/swda_data/)\r\n\r\nPlease contact me for any support!\r\n\r\nAll tests passed and followed all steps in the contribution guide!\r\n\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 777477645,
    "title": "new version of Ted Talks IWSLT (WIT3)",
    "dateCreated": "2021-01-02T15:30:03Z",
    "dateModified": "2021-01-02T15:30:03Z",
    "description": "In the previous iteration #1608 I had used language pairs. Which created 21,582 configs (109*108) !!! \r\n\r\nNow, TED talks in _each language_ is a separate config. So it's more cleaner with _just 109 configs_  (one for each language). Dummy files were created manually. \r\n\r\nLocally  I was able to clear the `python datasets-cli test datasets/......` . Which created the `dataset_info.json`  file . The test for the dummy files was also cleared. However couldn't figure out how to specify the local data folder for the real dataset\r\n\r\n\r\n**Note: that this requires manual download of the dataset.** \r\n**Note2: The high number of _Files changed (112)_ is because of the large number of dummy files/configs!**",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 777367320,
    "title": "Add the 800GB Pile dataset?",
    "dateCreated": "2021-01-01T22:58:12Z",
    "dateModified": "2021-01-01T22:58:12Z",
    "description": "## Adding a Dataset\r\n- **Name:** The Pile\r\n- **Description:** The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together. See [here](https://twitter.com/nabla_theta/status/1345130408170541056?s=20) for the Twitter announcement\r\n- **Paper:** https://pile.eleuther.ai/paper.pdf\r\n- **Data:** https://pile.eleuther.ai/\r\n- **Motivation:** Enables hardcore  (GPT-3 scale!) language modelling\r\n\r\n## Remarks\r\nGiven the extreme size of this dataset, I'm not sure how feasible this will be to include in `datasets` \ud83e\udd2f  . I'm also unsure how many `datasets` users are pretraining LMs, so the usage of this dataset may not warrant the effort to integrate it.\r\n",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 777321840,
    "title": "dutch_social can't be loaded",
    "dateCreated": "2021-01-01T17:37:08Z",
    "dateModified": "2021-01-01T17:37:08Z",
    "description": "Hi all,\r\n\r\nI'm trying to import the `dutch_social` dataset described [here](https://huggingface.co/datasets/dutch_social).\r\n\r\nHowever, the code that should load the data doesn't seem to be working, in particular because the corresponding files can't be found at the provided links.\r\n\r\n```\r\n(base) Koens-MacBook-Pro:~ koenvandenberge$ python\r\nPython 3.7.4 (default, Aug 13 2019, 15:17:50) \r\n[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from datasets import load_dataset\r\ndataset = load_dataset(\r\n   'dutch_social')\r\n>>> dataset = load_dataset(\r\n...    'dutch_social')\r\nTraceback (most recent call last):\r\n  File \"/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py\", line 267, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 486, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/dutch_social/dutch_social.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py\", line 278, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 486, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/dutch_social/dutch_social.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py\", line 589, in load_dataset\r\n    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n  File \"/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py\", line 282, in prepare_module\r\n    combined_path, github_file_path, file_path\r\nFileNotFoundError: Couldn't find file locally at dutch_social/dutch_social.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/dutch_social/dutch_social.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/dutch_social/dutch_social.py\r\n```",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 777263651,
    "title": "Unable to Download Hindi Wikipedia Dataset",
    "dateCreated": "2021-01-01T10:52:53Z",
    "dateModified": "2021-01-01T10:52:53Z",
    "description": "I used the Dataset Library in Python to load the wikipedia dataset with the Hindi Config 20200501.hi along with something called beam_runner='DirectRunner' and it keeps giving me the error that the file is not found. I have attached the screenshot of the error and the code both. Please help me to understand how to resolve this issue.\r\n\r\n![Code](https://user-images.githubusercontent.com/30871963/103437466-1f3a3300-4c4e-11eb-9d54-fc9601abfeec.png)\r\n\r\n![Error](https://user-images.githubusercontent.com/30871963/103437407-7ee40e80-4c4d-11eb-8151-a86eb664e6be.png)\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 777258941,
    "title": "load_dataset hang on file_lock",
    "dateCreated": "2021-01-01T10:25:07Z",
    "dateModified": "2021-01-01T10:25:07Z",
    "description": "I am trying to load the squad dataset. Fails on Windows 10 but succeeds in Colab.\r\nTransformers:  3.3.1\r\nDatasets:  1.0.2\r\nWindows 10 (also tested in WSL)\r\n\r\n```\r\ndatasets.logging.set_verbosity_debug()\r\ndatasets.\r\ntrain_dataset = load_dataset('squad', split='train')\r\nvalid_dataset = load_dataset('squad', split='validation')\r\n\r\ntrain_dataset.features\r\n```\r\n\r\n```\r\nhttps://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/squad.py not found in cache or force_download set to True, downloading to C:\\Users\\simpl\\.cache\\huggingface\\datasets\\tmpzj_o_6u7\r\nDownloading:\r\n5.24k/? [00:00<00:00, 134kB/s]\r\nstoring https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/squad.py in cache at C:\\Users\\simpl\\.cache\\huggingface\\datasets\\f6877c8d2e01e8fcb60dc101be28b54a7522feac756deb9ac5c39c6d8ebef1ce.85f43de978b9b25921cb78d7a2f2b350c04acdbaedb9ecb5f7101cd7c0950e68.py\r\ncreating metadata file for C:\\Users\\simpl\\.cache\\huggingface\\datasets\\f6877c8d2e01e8fcb60dc101be28b54a7522feac756deb9ac5c39c6d8ebef1ce.85f43de978b9b25921cb78d7a2f2b350c04acdbaedb9ecb5f7101cd7c0950e68.py\r\n\r\nChecking C:\\Users\\simpl\\.cache\\huggingface\\datasets\\f6877c8d2e01e8fcb60dc101be28b54a7522feac756deb9ac5c39c6d8ebef1ce.85f43de978b9b25921cb78d7a2f2b350c04acdbaedb9ecb5f7101cd7c0950e68.py for additional imports.\r\nFound main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/squad.py at C:\\Users\\simpl\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\squad\r\nFound specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/squad.py at C:\\Users\\simpl\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\squad\\1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41\r\nFound script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/squad.py to C:\\Users\\simpl\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\squad\\1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41\\squad.py\r\nCouldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad\\dataset_infos.json\r\nFound metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/squad.py at C:\\Users\\simpl\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\squad\\1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41\\squad.json\r\nNo config specified, defaulting to first: squad/plain_text\r\n```\r\n\r\nInterrupting the jupyter kernel we are in a file lock.\r\n\r\nIn Google Colab the download is ok. In contrast to a local run in colab dataset_infos.json is downloaded\r\n```\r\nhttps://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/tmptl9ha_ad\r\n\r\nDownloading:\r\n2.19k/? [00:00<00:00, 26.2kB/s]\r\n```",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 776652193,
    "title": "connection issue ",
    "dateCreated": "2020-12-30T21:56:20Z",
    "dateModified": "2020-12-30T21:56:20Z",
    "description": "Hi\r\nI am getting this connection issue, resulting in large failure on cloud, @lhoestq  I appreciate your help on this.\r\n\r\nIf I want to keep the codes the same, so not using save_to_disk, load_from_disk, but save the datastes in the way load_dataset reads from and copy the files in the same folder the datasets library reads from, could you assist me how this can be done, thanks\r\n\r\nI tried to do read the data, save it to a path and then set HF_HOME, which does not work and this is still not reading from the old set path, could you assist me how to save the datasets in a path, and let dataset library read from this path to avoid connection issue. thanks\r\n\r\n```\r\nimdb = datasets.load_dataset(\"imdb\")\r\nimdb.save_to_disk(\"/idiap/temp/rkarimi/hf_datasets/imdb\")\r\n>>> os.environ[\"HF_HOME\"]=\"/idiap/temp/rkarimi/hf_datasets/\"\r\n>>> imdb = datasets.load_dataset(\"imdb\")\r\nReusing dataset imdb (/idiap/temp/rkarimi/cache_home_2/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\r\n```\r\n\r\nI tried afterwards to set HF_HOME in bash, this makes it read from it, but it cannot let dataset library load from the saved path and still  downloading data. could you tell me how to fix this issue @lhoestq  thanks \r\n\r\nAlso this is on cloud, so I save them in a path, copy it to \"another machine\" to load the data\r\n\r\n### Error stack\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./finetune_t5_trainer.py\", line 344, in <module>\r\n    main()\r\n  File \"./finetune_t5_trainer.py\", line 232, in main\r\n    for task in data_args.eval_tasks} if training_args.do_test else None\r\n  File \"./finetune_t5_trainer.py\", line 232, in <dictcomp>\r\n    for task in data_args.eval_tasks} if training_args.do_test else None\r\n  File \"/workdir/seq2seq/data/tasks.py\", line 136, in get_dataset\r\n    split = self.get_sampled_split(split, n_obs)\r\n  File \"/workdir/seq2seq/data/tasks.py\", line 64, in get_sampled_split\r\n    dataset = self.load_dataset(split)\r\n  File \"/workdir/seq2seq/data/tasks.py\", line 454, in load_dataset\r\n    split=split, script_version=\"master\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/load.py\", line 589, in load_dataset\r\n    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/load.py\", line 263, in prepare_module\r\n    head_hf_s3(path, filename=name, dataset=dataset)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py\", line 200, in head_hf_s3\r\n    return http_head(hf_bucket_url(identifier=identifier, filename=filename, use_cdn=use_cdn, dataset=dataset))\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py\", line 403, in http_head\r\n    url, proxies=proxies, headers=headers, cookies=cookies, allow_redirects=allow_redirects, timeout=timeout\r\n  File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 104, in head\r\n    return request('head', url, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 61, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 542, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 655, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 504, in send\r\n    raise ConnectTimeout(e, request=request)\r\nrequests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/glue/glue.py (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7ff6d6c60a20>, 'Connection to s3.amazonaws.com timed out. (connect timeout=10)'))\r\n```\r\n",
    "status": "open",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 776608579,
    "title": "wiki_dpr pre-processing performance",
    "dateCreated": "2020-12-30T19:41:43Z",
    "dateModified": "2020-12-30T19:41:43Z",
    "description": "I've been working with wiki_dpr and noticed that the dataset processing is seriously impaired in performance [1]. It takes about 12h to process the entire dataset. Most of this time is simply loading and processing the data, but the actual indexing is also quite slow (3h).\r\n\r\nI won't repeat the concerns around multiprocessing as they are addressed in other issues (#786), but this is the first obvious thing to do. Using cython to speed up the text manipulation may be also help. Loading and processing a dataset of this size in under 15 minutes does not seem unreasonable on a modern multi-core machine. I have hit such targets myself on similar tasks. Would love to see this improve.\r\n\r\nThe other issue is that it takes 3h to construct the FAISS index. If only we could use GPUs with HNSW, but we can't. My sharded GPU indexing code can build an IVF + PQ index in 10 minutes on 20 million vectors. Still, 3h seems slow even for the CPU.\r\n\r\nIt looks like HF is adding only 1000 vectors at a time by default [2], whereas the faiss benchmarks adds 1 million vectors at a time (effectively) [3]. It's possible the runtime could be reduced with a larger batch. Also, it looks like project dependencies ultimately use OpenBLAS, but this is known to have issues when combined with OpenMP, which HNSW does [3]. A workaround is to set the environment variable `OMP_WAIT_POLICY=PASSIVE` via `os.environ` or similar.\r\n\r\nReferences:\r\n[1] https://github.com/huggingface/datasets/blob/master/datasets/wiki_dpr/wiki_dpr.py\r\n[2] https://github.com/huggingface/datasets/blob/master/src/datasets/search.py\r\n[3] https://github.com/facebookresearch/faiss/blob/master/benchs/bench_hnsw.py\r\n[4] https://github.com/facebookresearch/faiss/issues/422",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 776608386,
    "title": "wiki_dpr dataset pre-processesing performance",
    "dateCreated": "2020-12-30T19:41:09Z",
    "dateModified": "2020-12-30T19:41:09Z",
    "description": "I've been working with wiki_dpr and noticed that the dataset processing is seriously impaired in performance [1]. It takes about 12h to process the entire dataset. Most of this time is simply loading and processing the data, but the actual indexing is also quite slow (3h).\r\n\r\nI won't repeat the concerns around multiprocessing as they are addressed in other issues (#786), but this is the first obvious thing to do. Using cython to speed up the text manipulation may be also help. Loading and processing a dataset of this size in under 15 minutes does not seem unreasonable on a modern multi-core machine. I have hit such targets myself on similar tasks. Would love to see this improve.\r\n\r\nThe other issue is that it takes 3h to construct the FAISS index. If only we could use GPUs with HNSW, but we can't. My sharded GPU indexing code can build an IVF + PQ index in 10 minutes on 20 million vectors. Still, 3h seems slow even for the CPU.\r\n\r\nIt looks like HF is adding only 1000 vectors at a time by default [2], whereas the faiss benchmarks adds 1 million vectors at a time (effectively) [3]. It's possible the runtime could be reduced with a larger batch. Also, it looks like project dependencies ultimately use OpenBLAS, but this is known to have issues when combined with OpenMP, which HNSW does [3]. A workaround is to set the environment variable `OMP_WAIT_POLICY=PASSIVE` via `os.environ` or similar.\r\n\r\nReferences:\r\n[1] https://github.com/huggingface/datasets/blob/master/datasets/wiki_dpr/wiki_dpr.py\r\n[2] https://github.com/huggingface/datasets/blob/master/src/datasets/search.py\r\n[3] https://github.com/facebookresearch/faiss/blob/master/benchs/bench_hnsw.py\r\n[4] https://github.com/facebookresearch/faiss/issues/422",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 776552854,
    "title": "xed_en_fi dataset Cleanup",
    "dateCreated": "2020-12-30T17:11:18Z",
    "dateModified": "2020-12-30T17:11:18Z",
    "description": "Fix ClassLabel feature type and minor mistakes in the dataset card",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 776446658,
    "title": "Fix NER metric example in Overview notebook",
    "dateCreated": "2020-12-30T13:05:19Z",
    "dateModified": "2020-12-30T13:05:19Z",
    "description": "Fix errors in `NER metric example` section in `Overview.ipynb`.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-37-ee559b166e25> in <module>()\r\n----> 1 ner_metric = load_metric('seqeval')\r\n      2 references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\r\n      3 predictions =  [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\r\n      4 ner_metric.compute(predictions, references)\r\n\r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, **download_kwargs)\r\n    340     if needs_to_be_installed:\r\n    341         raise ImportError(\r\n--> 342             f\"To be able to use this {module_type}, you need to install the following dependencies\"\r\n    343             f\"{[lib_name for lib_name, lib_path in needs_to_be_installed]} using 'pip install \"\r\n    344             f\"{' '.join([lib_path for lib_name, lib_path in needs_to_be_installed])}' for instance'\"\r\n\r\nImportError: To be able to use this metric, you need to install the following dependencies['seqeval'] using 'pip install seqeval' for instance'\r\n```\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-39-ee559b166e25> in <module>()\r\n      2 references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\r\n      3 predictions =  [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\r\n----> 4 ner_metric.compute(predictions, references)\r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/metric.py in compute(self, *args, **kwargs)\r\n    378         \"\"\"\r\n    379         if args:\r\n--> 380             raise ValueError(\"Please call `compute` using keyword arguments.\")\r\n    381 \r\n    382         predictions = kwargs.pop(\"predictions\", None)\r\n\r\nValueError: Please call `compute` using keyword arguments.\r\n```",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 776432006,
    "title": "Add language to dataset card for Makhzan dataset.",
    "dateCreated": "2020-12-30T12:25:52Z",
    "dateModified": "2020-12-30T12:25:52Z",
    "description": "Add language to dataset card.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 776431087,
    "title": "Add language to dataset card for Counter dataset.",
    "dateCreated": "2020-12-30T12:23:20Z",
    "dateModified": "2020-12-30T12:23:20Z",
    "description": "Add language.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 775956441,
    "title": "removed \\n in labels",
    "dateCreated": "2020-12-29T15:41:43Z",
    "dateModified": "2020-12-29T15:41:43Z",
    "description": "updated social_i_qa labels as per #1633 ",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 775914320,
    "title": "update saving and loading methods for faiss index so to accept path l\u2026",
    "dateCreated": "2020-12-29T14:15:37Z",
    "dateModified": "2020-12-29T14:15:37Z",
    "description": "- Update saving and loading methods for faiss index so to accept path like objects from pathlib\r\n\r\nThe current code only supports using a string type to save and load a faiss index. This change makes it possible to use a string type OR a Path from [pathlib](https://docs.python.org/3/library/pathlib.html). The codes becomes a more intuitive this way I think.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 775890154,
    "title": "Arrow file is too large when saving vector data",
    "dateCreated": "2020-12-29T13:23:12Z",
    "dateModified": "2020-12-29T13:23:12Z",
    "description": "I computed the sentence embedding of each sentence of bookcorpus data using bert base and saved them to disk. I used 20M sentences and the obtained arrow file is about 59GB while the original text file is only about 1.3GB. Are there any ways to reduce the size of the arrow file?",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 775840801,
    "title": "updated dataset cards",
    "dateCreated": "2020-12-29T11:20:40Z",
    "dateModified": "2020-12-29T11:20:40Z",
    "description": "added dataset instance in the card.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 775831423,
    "title": "add dataset info",
    "dateCreated": "2020-12-29T10:58:19Z",
    "dateModified": "2020-12-29T10:58:19Z",
    "description": "",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 775831288,
    "title": "update dataset info",
    "dateCreated": "2020-12-29T10:58:01Z",
    "dateModified": "2020-12-29T10:58:01Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 775651085,
    "title": "brwac dataset: add instances and data splits info",
    "dateCreated": "2020-12-29T01:24:45Z",
    "dateModified": "2020-12-29T01:24:45Z",
    "description": "",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 775647000,
    "title": "mac_morpho dataset: add data splits info",
    "dateCreated": "2020-12-29T01:05:21Z",
    "dateModified": "2020-12-29T01:05:21Z",
    "description": "",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 775645356,
    "title": "assin 2 dataset: add instances and data splits info",
    "dateCreated": "2020-12-29T00:57:51Z",
    "dateModified": "2020-12-29T00:57:51Z",
    "description": "",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 775643418,
    "title": "assin dataset: add instances and data splits info",
    "dateCreated": "2020-12-29T00:47:56Z",
    "dateModified": "2020-12-29T00:47:56Z",
    "description": "",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 775640729,
    "title": "lener_br dataset: add instances and data splits info",
    "dateCreated": "2020-12-29T00:35:12Z",
    "dateModified": "2020-12-29T00:35:12Z",
    "description": "",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 775632945,
    "title": "harem dataset: add data splits info",
    "dateCreated": "2020-12-28T23:58:20Z",
    "dateModified": "2020-12-28T23:58:20Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 775571813,
    "title": "Update dataset cards from previous sprint",
    "dateCreated": "2020-12-28T20:20:47Z",
    "dateModified": "2020-12-28T20:20:47Z",
    "description": "This PR updates the dataset cards/readmes for the 4 approved PRs I submitted in the previous sprint.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 775554319,
    "title": "Add twi wordsim353",
    "dateCreated": "2020-12-28T19:31:55Z",
    "dateModified": "2020-12-28T19:31:55Z",
    "description": "Added the citation information to the README file",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 775545912,
    "title": "Update README.md",
    "dateCreated": "2020-12-28T19:09:05Z",
    "dateModified": "2020-12-28T19:09:05Z",
    "description": "added dataset summary",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 775544487,
    "title": "Update README.md",
    "dateCreated": "2020-12-28T19:05:00Z",
    "dateModified": "2020-12-28T19:05:00Z",
    "description": "Added information in the dataset card",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 775542360,
    "title": "Update README.md",
    "dateCreated": "2020-12-28T18:59:06Z",
    "dateModified": "2020-12-28T18:59:06Z",
    "description": "added dataset summary",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 775525799,
    "title": "NarrativeQA fails to load with `load_dataset`",
    "dateCreated": "2020-12-28T18:16:09Z",
    "dateModified": "2020-12-28T18:16:09Z",
    "description": "When loading the NarrativeQA dataset with `load_dataset('narrativeqa')` as given in the documentation [here](https://huggingface.co/datasets/narrativeqa), I receive a cascade of exceptions, ending with\r\n\r\n    FileNotFoundError: Couldn't find file locally at narrativeqa/narrativeqa.py, or remotely at \r\n        https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/narrativeqa/narrativeqa.py or \r\n        https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/narrativeqa/narrativeqa.py\r\n\r\nWorkaround: manually copy the `narrativeqa.py` builder into my local directory with \r\n\r\n    curl https://raw.githubusercontent.com/huggingface/datasets/master/datasets/narrativeqa/narrativeqa.py -o narrativeqa.py\r\n\r\nand load the dataset as `load_dataset('narrativeqa.py')` everything works fine. I'm on datasets v1.1.3 using Python 3.6.10.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 775499344,
    "title": "Add missing homepage in some dataset cards",
    "dateCreated": "2020-12-28T17:09:48Z",
    "dateModified": "2020-12-28T17:09:48Z",
    "description": "In some dataset cards the homepage field in the `Dataset Description` section was missing/empty",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 775473106,
    "title": "Rename \"part-of-speech-tagging\" tag in some dataset cards",
    "dateCreated": "2020-12-28T16:09:09Z",
    "dateModified": "2020-12-28T16:09:09Z",
    "description": "`part-of-speech-tagging` was not part of the tagging taxonomy under `structure-prediction`",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 775375880,
    "title": "HoVeR dataset fails to load",
    "dateCreated": "2020-12-28T12:27:07Z",
    "dateModified": "2020-12-28T12:27:07Z",
    "description": "Hi! I'm getting an error when trying to load **HoVeR** dataset. Another one (**SQuAD**) does work for me. I'm using the latest (1.1.3) version of the library.\r\n\r\nSteps to reproduce the error:\r\n\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"hover\")\r\nTraceback (most recent call last):\r\n  File \"/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/load.py\", line 267, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 486, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/hover/hover.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/load.py\", line 278, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 486, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/hover/hover.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/load.py\", line 589, in load_dataset\r\n    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n  File \"/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/load.py\", line 282, in prepare_module\r\n    combined_path, github_file_path, file_path\r\nFileNotFoundError: Couldn't find file locally at hover/hover.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/hover/hover.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/hover/hover.py\r\n```",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 775280046,
    "title": "Dataset social_bias_frames 404",
    "dateCreated": "2020-12-28T08:35:34Z",
    "dateModified": "2020-12-28T08:35:34Z",
    "description": "```\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"social_bias_frames\")\r\n...\r\nDownloading and preparing dataset social_bias_frames/default\r\n...\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag)\r\n    484             )\r\n    485         elif response is not None and response.status_code == 404:\r\n--> 486             raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\n    487         raise ConnectionError(\"Couldn't reach {}\".format(url))\r\n    488 \r\n\r\nFileNotFoundError: Couldn't find file at https://homes.cs.washington.edu/~msap/social-bias-frames/SocialBiasFrames_v2.tgz\r\n```\r\n[Here](https://homes.cs.washington.edu/~msap/social-bias-frames/) we find button `Download data` with the correct URL for the data: https://homes.cs.washington.edu/~msap/social-bias-frames/SBIC.v2.tgz",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 775159568,
    "title": "Ollie dataset",
    "dateCreated": "2020-12-28T02:43:37Z",
    "dateModified": "2020-12-28T02:43:37Z",
    "description": "This is the dataset used to train the Ollie open information extraction algorithm. It has over 21M sentences. See http://knowitall.github.io/ollie/ for more details.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 775110872,
    "title": "muchocine dataset cannot be dowloaded",
    "dateCreated": "2020-12-27T21:26:28Z",
    "dateModified": "2020-12-27T21:26:28Z",
    "description": "```python\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, **download_kwargs)\r\n    267         try:\r\n--> 268             local_path = cached_path(file_path, download_config=download_config)\r\n    269         except FileNotFoundError:\r\n\r\n7 frames\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/muchocine/muchocine.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nFileNotFoundError                         Traceback (most recent call last)\r\nFileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/muchocine/muchocine.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nFileNotFoundError                         Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, **download_kwargs)\r\n    281                 raise FileNotFoundError(\r\n    282                     \"Couldn't find file locally at {}, or remotely at {} or {}\".format(\r\n--> 283                         combined_path, github_file_path, file_path\r\n    284                     )\r\n    285                 )\r\n\r\nFileNotFoundError: Couldn't find file locally at muchocine/muchocine.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/muchocine/muchocine.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/muchocine/muchocine.py\r\n```",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 774921836,
    "title": "Fix \"'BertTokenizerFast' object has no attribute 'max_len'\"",
    "dateCreated": "2020-12-26T19:25:41Z",
    "dateModified": "2020-12-26T19:25:41Z",
    "description": "Tensorflow 2.3.0 gives:\r\n FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\r\n\r\nTensorflow 2.4.0 gives:\r\nAttributeError 'BertTokenizerFast' object has no attribute 'max_len'",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 774903472,
    "title": "bug with sst2 in glue ",
    "dateCreated": "2020-12-26T16:57:23Z",
    "dateModified": "2020-12-26T16:57:23Z",
    "description": "Hi\r\nI am getting very low accuracy on SST2 I investigate this and observe that for this dataset sentences are tokenized, while this is correct for the other datasets in GLUE, please see below.\r\nIs there any alternatives I could get untokenized sentences? I am unfortunately under time pressure to report some results on this dataset. thank you for your help. @lhoestq \r\n \r\n```\r\n>>> a =  datasets.load_dataset('glue', 'sst2', split=\"validation\", script_version=\"master\")\r\nReusing dataset glue (/julia/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\r\n>>> a[:10]\r\n{'idx': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'label': [1, 0, 1, 1, 0, 1, 0, 0, 1, 0], 'sentence': [\"it 's a charming and often affecting journey . \", 'unflinchingly bleak and desperate ', 'allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . ', \"the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \", \"it 's slow -- very , very slow . \", 'although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women . ', 'a sometimes tedious film . ', \"or doing last year 's taxes with your ex-wife . \", \"you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance . \", \"in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 sank from quirky to jerky to utter turkey . \"]}\r\n\r\n```",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 774869184,
    "title": "Add id_puisi dataset",
    "dateCreated": "2020-12-26T12:41:55Z",
    "dateModified": "2020-12-26T12:41:55Z",
    "description": "Puisi (poem) is an Indonesian poetic form. The dataset contains 7223 Indonesian puisi with its title and author. :)",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 774710014,
    "title": "Added `pn_summary` dataset",
    "dateCreated": "2020-12-25T11:01:24Z",
    "dateModified": "2020-12-25T11:01:24Z",
    "description": "#1635 \r\n\r\nYou did a great job with the fluent procedure regarding adding a dataset. I took the chance to add the dataset on my own. Thank you for your awesome job, and I hope this dataset found the researchers happy, specifically those interested in Persian Language (Farsi)!",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 774574378,
    "title": "winogrande cannot be dowloaded ",
    "dateCreated": "2020-12-24T22:28:22Z",
    "dateModified": "2020-12-24T22:28:22Z",
    "description": "Hi,\r\nI am getting this error when trying to run the codes on the cloud.  Thank you for any suggestion and help on this @lhoestq \r\n\r\n```\r\n File \"./finetune_trainer.py\", line 318, in <module>\r\n    main()\r\n  File \"./finetune_trainer.py\", line 148, in main\r\n    for task in data_args.tasks]\r\n  File \"./finetune_trainer.py\", line 148, in <listcomp>\r\n    for task in data_args.tasks]\r\n  File \"/workdir/seq2seq/data/tasks.py\", line 65, in get_dataset\r\n    dataset = self.load_dataset(split=split)\r\n  File \"/workdir/seq2seq/data/tasks.py\", line 466, in load_dataset\r\n    return datasets.load_dataset('winogrande', 'winogrande_l', split=split)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/load.py\", line 589, in load_dataset\r\n    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/load.py\", line 267, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py\", line 487, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/winogrande/winogrande.py\r\nyo/0 I1224 14:17:46.419031 31226 main shadow.py:122 > Traceback (most recent call last):\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/distributed/launch.py\", line 260, in <module>\r\n    main()\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/distributed/launch.py\", line 256, in main\r\n    cmd=cmd)\r\n```",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 774524492,
    "title": "Persian Abstractive/Extractive Text Summarization",
    "dateCreated": "2020-12-24T17:47:12Z",
    "dateModified": "2020-12-24T17:47:12Z",
    "description": "Assembling datasets tailored to different tasks and languages is a precious target. This would be great to have this dataset included.\r\n\r\n## Adding a Dataset\r\n- **Name:** *pn-summary*\r\n- **Description:** *A well-structured summarization dataset for the Persian language consists of 93,207 records. It is prepared for Abstractive/Extractive tasks (like cnn_dailymail for English). It can also be used in other scopes like Text Generation, Title Generation, and News Category Classification.*\r\n- **Paper:** *https://arxiv.org/abs/2012.11204*\r\n- **Data:** *https://github.com/hooshvare/pn-summary/#download*\r\n- **Motivation:** *It is the first Persian abstractive/extractive Text summarization dataset (like cnn_dailymail for English)!*\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 774487934,
    "title": "Inspecting datasets per category",
    "dateCreated": "2020-12-24T15:26:34Z",
    "dateModified": "2020-12-24T15:26:34Z",
    "description": "Hi\r\nIs there a way I could get all NLI datasets/all QA datasets to get some understanding of available datasets per category? this is hard for me to inspect the datasets one by one in the webpage, thanks for the suggestions @lhoestq ",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 774422603,
    "title": "social_i_qa wrong format of labels",
    "dateCreated": "2020-12-24T13:11:54Z",
    "dateModified": "2020-12-24T13:11:54Z",
    "description": "Hi,\r\nthere is extra \"\\n\" in labels of social_i_qa datasets, no big deal, but I was wondering if you could remove it to make it consistent.\r\nso label is 'label': '1\\n', not '1'\r\nthanks\r\n\r\n```\r\n>>> import datasets \r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\r\n...    'social_i_qa')\r\ncahce dir  /julia/cache/datasets\r\nDownloading: 4.72kB [00:00, 3.52MB/s]                                                                                                  \r\ncahce dir /julia/cache/datasets\r\nDownloading: 2.19kB [00:00, 1.81MB/s]                                                                                                  \r\nUsing custom data configuration default\r\nReusing dataset social_i_qa (/julia/datasets/social_i_qa/default/0.1.0/4a4190cc2d2482d43416c2167c0c5dccdd769d4482e84893614bd069e5c3ba06)\r\n>>> dataset['train'][0]\r\n{'answerA': 'like attending', 'answerB': 'like staying home', 'answerC': 'a good friend to have', 'context': 'Cameron decided to have a barbecue and gathered her friends together.', 'label': '1\\n', 'question': 'How would Others feel as a result?'}\r\n\r\n```\r\n\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 774388625,
    "title": "SICK dataset ",
    "dateCreated": "2020-12-24T12:40:14Z",
    "dateModified": "2020-12-24T12:40:14Z",
    "description": "Hi, this would be great to have this dataset included. I might be missing something, but I could not find it in the list of already included datasets. Thank you. \r\n\r\n## Adding a Dataset\r\n- **Name:** SICK\r\n- **Description:**  SICK consists of about 10,000 English sentence pairs that include many examples of the lexical, syntactic, and semantic phenomena. \r\n- **Paper:** https://www.aclweb.org/anthology/L14-1314/\r\n- **Data:** http://marcobaroni.org/composes/sick.html\r\n- **Motivation:** This dataset is well-known in the NLP community used for recognizing entailment between sentences.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 774349222,
    "title": "Update README.md",
    "dateCreated": "2020-12-24T11:45:52Z",
    "dateModified": "2020-12-24T11:45:52Z",
    "description": "I made small change for citation",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 774332129,
    "title": "Adding UKP Argument Aspect Similarity Corpus",
    "dateCreated": "2020-12-24T11:01:31Z",
    "dateModified": "2020-12-24T11:01:31Z",
    "description": "Hi, this would be great to have this dataset included.\r\n\r\n## Adding a Dataset\r\n- **Name:** UKP Argument Aspect Similarity Corpus\r\n- **Description:** The UKP Argument Aspect Similarity Corpus (UKP ASPECT) includes 3,595 sentence pairs over 28 controversial topics. Each sentence pair was annotated via crowdsourcing as either \u201chigh similarity\u201d, \u201csome similarity\u201d, \u201cno similarity\u201d or \u201cnot related\u201d with respect to the topic.\r\n- **Paper:** https://www.aclweb.org/anthology/P19-1054/\r\n- **Data:** https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/1998\r\n- **Motivation:** this is one of the datasets currently used frequently in recent adapter papers like https://arxiv.org/pdf/2005.00247.pdf \r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n\r\nThank you",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 774255716,
    "title": "add wongnai_reviews test set labels",
    "dateCreated": "2020-12-24T08:02:31Z",
    "dateModified": "2020-12-24T08:02:31Z",
    "description": "- add test set labels provided by @ekapolc\r\n- refactor `star_rating` to a `datasets.features.ClassLabel` field",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 774091411,
    "title": "made suggested changes to hate-speech-and-offensive-language",
    "dateCreated": "2020-12-23T23:25:32Z",
    "dateModified": "2020-12-23T23:25:32Z",
    "description": "",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 773960255,
    "title": "`Dataset.map` disable progress bar",
    "dateCreated": "2020-12-23T17:53:42Z",
    "dateModified": "2020-12-23T17:53:42Z",
    "description": "I can't find anything to turn off the `tqdm` progress bars while running a preprocessing function using `Dataset.map`. I want to do akin to `disable_tqdm=True` in the case of `transformers`. Is there something like that?",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 773840368,
    "title": "Fix dataset_dict.shuffle with single seed",
    "dateCreated": "2020-12-23T14:33:36Z",
    "dateModified": "2020-12-23T14:33:36Z",
    "description": "Fix #1610 \r\n\r\nI added support for single integer used in `DatasetDict.shuffle`. Previously only a dictionary of seed was allowed.\r\nMoreover I added the missing `seed` parameter. Previously only `seeds` was allowed.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 773771596,
    "title": "Fixed bug in the shape property",
    "dateCreated": "2020-12-23T13:33:21Z",
    "dateModified": "2020-12-23T13:33:21Z",
    "description": "Fix to the bug reported in issue #1622. Just replaced `return tuple(self._indices.num_rows, self._data.num_columns)` by `return (self._indices.num_rows, self._data.num_columns)`.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 773669700,
    "title": "Cannot download ade_corpus_v2",
    "dateCreated": "2020-12-23T10:58:14Z",
    "dateModified": "2020-12-23T10:58:14Z",
    "description": "I tried this to get the dataset following this url : https://huggingface.co/datasets/ade_corpus_v2\r\n\r\nbut received this error : \r\n\r\n`Traceback (most recent call last):\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py\", line 267, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 486, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/ade_corpus_v2/ade_corpus_v2.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py\", line 278, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 486, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/ade_corpus_v2/ade_corpus_v2.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py\", line 589, in load_dataset\r\n    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py\", line 282, in prepare_module\r\n    combined_path, github_file_path, file_path\r\nFileNotFoundError: Couldn't find file locally at ade_corpus_v2/ade_corpus_v2.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/ade_corpus_v2/ade_corpus_v2.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/ade_corpus_v2/ade_corpus_v2.py`\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 772950710,
    "title": "Add CLIMATE-FEVER dataset",
    "dateCreated": "2020-12-22T13:34:05Z",
    "dateModified": "2020-12-22T13:34:05Z",
    "description": "As suggested by @SBrandeis , fresh PR that adds CLIMATE-FEVER. Replaces PR #1579.\r\n\r\n---\r\n\r\nA dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change collected on the internet. Each claim is accompanied by five manually annotated evidence sentences retrieved from the English Wikipedia that support, refute or do not give enough information to validate the claim totalling in 7,675 claim-evidence pairs. The dataset features challenging claims that relate multiple facets and disputed cases of claims where both supporting and refuting evidence are present.\r\n\r\nMore information can be found at:\r\n\r\n* Homepage: http://climatefever.ai\r\n* Paper: https://arxiv.org/abs/2012.00614",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 772940768,
    "title": "Can't call shape on the output of select()",
    "dateCreated": "2020-12-22T13:18:40Z",
    "dateModified": "2020-12-22T13:18:40Z",
    "description": "I get the error `TypeError: tuple expected at most 1 argument, got 2` when calling `shape` on the output of `select()`.\r\nIt's line 531 in shape in arrow_dataset.py that causes the problem:\r\n``return tuple(self._indices.num_rows, self._data.num_columns)``\r\nThis makes sense, since `tuple(num1, num2)` is not a valid call.\r\n \r\nFull code to reproduce:\r\n\r\n```python\r\ndataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\r\ntrain_set = dataset[\"train\"]\r\nt = train_set.select(range(10))\r\nprint(t.shape)",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 772940417,
    "title": "updated dutch_social.py for loading jsonl (lines instead of list) files",
    "dateCreated": "2020-12-22T13:18:11Z",
    "dateModified": "2020-12-22T13:18:11Z",
    "description": "the data_loader  is modified to load files on the fly. Earlier it was reading the entire file and then processing the records\r\n\r\nPls refer to previous PR #1321 ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 772620056,
    "title": "Adding myPOS2017 dataset",
    "dateCreated": "2020-12-22T04:04:55Z",
    "dateModified": "2020-12-22T04:04:55Z",
    "description": "myPOS Corpus (Myanmar Part-of-Speech Corpus) for Myanmar language NLP Research and Developments",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 772508558,
    "title": "data loader for reading comprehension task",
    "dateCreated": "2020-12-21T22:40:34Z",
    "dateModified": "2020-12-21T22:40:34Z",
    "description": "added doc2dial data loader and dummy data for reading comprehension task.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 772248730,
    "title": "Can't filter language:EN on https://huggingface.co/datasets",
    "dateCreated": "2020-12-21T15:23:23Z",
    "dateModified": "2020-12-21T15:23:23Z",
    "description": "When visiting https://huggingface.co/datasets, I don't see an obvious way to filter only English datasets. This is unexpected for me, am I missing something? I'd expect English to be selectable in the language widget. This problem reproduced on Mozilla Firefox and MS Edge:\r\n\r\n![screenshot](https://user-images.githubusercontent.com/4547987/102792244-892e1f00-43a8-11eb-9e89-4826ca201a87.png)\r\n",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 772084764,
    "title": "cifar10 initial commit",
    "dateCreated": "2020-12-21T11:18:50Z",
    "dateModified": "2020-12-21T11:18:50Z",
    "description": "CIFAR-10 dataset. Didn't add the tagging since there are no vision related tags.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 772074229,
    "title": "added TurkishMovieSentiment dataset",
    "dateCreated": "2020-12-21T11:03:16Z",
    "dateModified": "2020-12-21T11:03:16Z",
    "description": "This PR adds the **TurkishMovieSentiment: This dataset contains turkish movie reviews.**\r\n\r\n- **Homepage:** [https://www.kaggle.com/mustfkeskin/turkish-movie-sentiment-analysis-dataset/tasks](https://www.kaggle.com/mustfkeskin/turkish-movie-sentiment-analysis-dataset/tasks)\r\n- **Point of Contact:** [Mustafa Keskin](https://www.linkedin.com/in/mustfkeskin/)",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 771641088,
    "title": "Bug: Can't download TriviaQA with `load_dataset` - custom `cache_dir`",
    "dateCreated": "2020-12-20T17:27:38Z",
    "dateModified": "2020-12-20T17:27:38Z",
    "description": "Hello,\r\nI'm having issue downloading TriviaQA dataset with `load_dataset`.\r\n\r\n## Environment info\r\n- `datasets` version: 1.1.3\r\n- Platform: Linux-4.19.129-aufs-1-x86_64-with-debian-10.1\r\n- Python version: 3.7.3\r\n\r\n## The code I'm running:\r\n```python\r\nimport datasets\r\ndataset = datasets.load_dataset(\"trivia_qa\", \"rc\", cache_dir = \"./datasets\")\r\n```\r\n\r\n## The output:\r\n1. Download begins:\r\n```\r\nDownloading and preparing dataset trivia_qa/rc (download: 2.48 GiB, generated: 14.92 GiB, post-processed: Unknown size, total: 17.40 GiB) to /cs/labs/gabis/sapirweissbuch/tr\r\nivia_qa/rc/1.1.0/e734e28133f4d9a353af322aa52b9f266f6f27cbf2f072690a1694e577546b0d...                                                                                         \r\nDownloading:  17%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                   | 446M/2.67G [00:37<04:45, 7.77MB/s]\r\n```\r\n2. 100% is reached\r\n3. It got stuck here for about an hour, and added additional 30G of data to \"./datasets\" directory. I killed the process eventually.\r\n\r\nA similar issue can be observed in Google Colab:\r\n\r\nhttps://colab.research.google.com/drive/1nn1Lw02GhfGFylzbS2j6yksGjPo7kkN-?usp=sharing\r\n\r\n## Expected behaviour:\r\nThe dataset \"TriviaQA\" should be successfully downloaded.\r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 771577050,
    "title": "Add id_clickbait",
    "dateCreated": "2020-12-20T12:24:49Z",
    "dateModified": "2020-12-20T12:24:49Z",
    "description": "This is the CLICK-ID dataset, a collection of annotated clickbait Indonesian news headlines that was collected from 12 local online news ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 771558160,
    "title": "Adding wiki asp dataset as new PR",
    "dateCreated": "2020-12-20T10:25:08Z",
    "dateModified": "2020-12-20T10:25:08Z",
    "description": "Hi @lhoestq, Adding wiki asp as new branch because #1539 has other commits. This version has dummy data for each domain <20/30KB.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 771486456,
    "title": "shuffle with torch generator ",
    "dateCreated": "2020-12-20T00:57:14Z",
    "dateModified": "2020-12-20T00:57:14Z",
    "description": "Hi\r\nI need to shuffle mutliple large datasets with `generator = torch.Generator()` for a distributed sampler which needs to make sure datasets are consistent across different cores, for this, this is really necessary for me to use  torch generator, based on documentation this generator is not supported with datasets, I really need to make shuffle work with this generator and I was wondering what I can do about this issue, thanks for your help \r\n\r\n@lhoestq ",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 771453599,
    "title": "shuffle does not accept seed ",
    "dateCreated": "2020-12-19T20:59:39Z",
    "dateModified": "2020-12-19T20:59:39Z",
    "description": "Hi\r\nI need to shuffle the dataset, but this needs to be based on epoch+seed to be consistent across the cores, when I pass seed to shuffle, this does not accept seed, could you assist me with this? thanks  @lhoestq\r\n ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 771421881,
    "title": "Not able to use 'jigsaw_toxicity_pred' dataset",
    "dateCreated": "2020-12-19T17:35:48Z",
    "dateModified": "2020-12-19T17:35:48Z",
    "description": " When trying to use jigsaw_toxicity_pred dataset, like this in a [colab](https://colab.research.google.com/drive/1LwO2A5M2X5dvhkAFYE4D2CUT3WUdWnkn?usp=sharing):\r\n```\r\nfrom datasets import list_datasets, list_metrics, load_dataset, load_metric\r\n\r\nds = load_dataset(\"jigsaw_toxicity_pred\")\r\n```\r\n \r\nI see below error:\r\n\r\n> FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/jigsaw_toxicity_pred/jigsaw_toxicity_pred.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nFileNotFoundError                         Traceback (most recent call last)\r\nFileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/jigsaw_toxicity_pred/jigsaw_toxicity_pred.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nFileNotFoundError                         Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, **download_kwargs)\r\n    280                 raise FileNotFoundError(\r\n    281                     \"Couldn't find file locally at {}, or remotely at {} or {}\".format(\r\n--> 282                         combined_path, github_file_path, file_path\r\n    283                     )\r\n    284                 )\r\n\r\nFileNotFoundError: Couldn't find file locally at jigsaw_toxicity_pred/jigsaw_toxicity_pred.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/jigsaw_toxicity_pred/jigsaw_toxicity_pred.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/jigsaw_toxicity_pred/jigsaw_toxicity_pred.py",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 771329434,
    "title": "adding  ted_talks_iwslt",
    "dateCreated": "2020-12-19T07:36:41Z",
    "dateModified": "2020-12-19T07:36:41Z",
    "description": "UPDATE2: (2nd Jan) Wrote a long writeup on the slack channel. I don't think this approach is correct. Basically this created language pairs (109*108) \r\nRunning the `pytest `went for more than 40+ hours and it was still running!  \r\nSo working on a different approach, such that the number of configs = number of languages. Will make a new pull request with that. \r\n\r\nUPDATE: This requires manual download dataset\r\n\r\nThis is a draft version ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 771325852,
    "title": "modified tweets hate speech detection",
    "dateCreated": "2020-12-19T07:13:40Z",
    "dateModified": "2020-12-19T07:13:40Z",
    "description": "",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 771116455,
    "title": "added Semantic Scholar Open Research Corpus",
    "dateCreated": "2020-12-18T19:21:24Z",
    "dateModified": "2020-12-18T19:21:24Z",
    "description": "I picked up this dataset [Semantic Scholar Open Research Corpus](https://allenai.org/data/s2orc) but it contains 6000 files to be downloaded. I tried the current code with 100 files and it worked fine (took ~15GB space). For 6000 files it would occupy ~900GB space which I don\u2019t have. Can someone from the HF team with that much of disk space help me with generate dataset_infos and dummy_data?",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 770979620,
    "title": "Navigation version breaking",
    "dateCreated": "2020-12-18T15:36:24Z",
    "dateModified": "2020-12-18T15:36:24Z",
    "description": "Hi, \r\n\r\nwhen navigating docs (Chrome, Ubuntu) (e.g. on this page: https://huggingface.co/docs/datasets/loading_metrics.html#using-a-custom-metric-script) the version control dropdown has the wrong string displayed as the current version: \r\n\r\n![image](https://user-images.githubusercontent.com/3007947/102632187-02cad080-414f-11eb-813b-28f3c8d80def.png)\r\n\r\n**Edit:** this actually happens _only_ if you open a link to a concrete subsection.\r\n\r\nIMO, the best way to fix this without getting too deep into the intricacies of retrieving version numbers from the URL would be to change [this](https://github.com/huggingface/datasets/blob/master/docs/source/_static/js/custom.js#L112) line to:\r\n```\r\nlet label = (version in versionMapping) ? version : stableVersion\r\n```\r\nwhich delegates the check to the (already maintained) keys of the version mapping dictionary & should be more robust. There's a similar ternary expression [here](https://github.com/huggingface/datasets/blob/master/docs/source/_static/js/custom.js#L97) which should also fail in this case.\r\n\r\nI'd also suggest swapping this [block](https://github.com/huggingface/datasets/blob/master/docs/source/_static/js/custom.js#L80-L90) to `string.contains(version) for version in versionMapping` which might be more robust. I'd add a PR myself but I'm by no means competent in JS :)\r\n\r\nI also have a side question wrt. docs versioning: I'm trying to make docs for a project which are versioned alike to your dropdown versioning. I was wondering how do you handle storage of multiple doc versions on your server? Do you update what `https://huggingface.co/docs/datasets` points to for every stable release & manually create new folders for each released version?\r\nSo far I'm building & publishing (scping) the docs to the server with a github action which works well for a single version, but would ideally need to reorder the public files triggered on a new release.",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 770862112,
    "title": "Add tests for the download functions ?",
    "dateCreated": "2020-12-18T12:49:25Z",
    "dateModified": "2020-12-18T12:49:25Z",
    "description": "AFAIK the download functions in `DownloadManager` are not tested yet. It could be good to add some to ensure behavior is as expected.",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 770857221,
    "title": "Add retries to HTTP requests",
    "dateCreated": "2020-12-18T12:41:31Z",
    "dateModified": "2020-12-18T12:41:31Z",
    "description": "## What does this PR do ?\r\n\r\nAdding retries to HTTP GET & HEAD requests, when they fail with a `ConnectTimeout` exception.\r\n\r\nThe \"canonical\" way to do this is to use [urllib's Retry class](https://urllib3.readthedocs.io/en/latest/reference/urllib3.util.html#urllib3.util.Retry) and wrap it in a [HttpAdapter](https://requests.readthedocs.io/en/master/api/#requests.adapters.HTTPAdapter). Seems a bit overkill to me, plus it forces us to use the `requests.Session` object. I prefer this simpler implementation. I'm open to remarks and suggestions @lhoestq @yjernite \r\n\r\nFixes #1102 ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 770841810,
    "title": "second update of id_newspapers_2018",
    "dateCreated": "2020-12-18T12:16:37Z",
    "dateModified": "2020-12-18T12:16:37Z",
    "description": "The feature \"url\" is currently set wrongly to data[\"date\"], this PR fix it to data[\"url\"].\r\nI added also an additional POC.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 770758914,
    "title": "second update of the id_newspapers_2018",
    "dateCreated": "2020-12-18T10:10:20Z",
    "dateModified": "2020-12-18T10:10:20Z",
    "description": "The feature \"url\" is currently set wrongly to data[\"date\"], this PR fix it to data[\"url\"].\r\nI added also an additional POC.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 770582960,
    "title": "AttributeError: 'DatasetDict' object has no attribute 'train_test_split'",
    "dateCreated": "2020-12-18T05:37:10Z",
    "dateModified": "2020-12-18T05:37:10Z",
    "description": "The following code fails with \"'DatasetDict' object has no attribute 'train_test_split'\" - am I doing something wrong?\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('csv', data_files='data.txt')\r\ndataset = dataset.train_test_split(test_size=0.1)\r\n```\r\n\r\n> AttributeError: 'DatasetDict' object has no attribute 'train_test_split'",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 770431389,
    "title": "add Korean Sarcasm Dataset",
    "dateCreated": "2020-12-17T22:49:56Z",
    "dateModified": "2020-12-17T22:49:56Z",
    "description": "",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 770332440,
    "title": "made suggested changes in fake-news-english",
    "dateCreated": "2020-12-17T20:06:29Z",
    "dateModified": "2020-12-17T20:06:29Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 770276140,
    "title": "adding hate-speech-and-offensive-language",
    "dateCreated": "2020-12-17T18:35:15Z",
    "dateModified": "2020-12-17T18:35:15Z",
    "description": "",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 770260531,
    "title": "made suggested changes to hate-speech-and-offensive-language",
    "dateCreated": "2020-12-17T18:09:26Z",
    "dateModified": "2020-12-17T18:09:26Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 770153693,
    "title": "Logiqa en",
    "dateCreated": "2020-12-17T15:42:00Z",
    "dateModified": "2020-12-17T15:42:00Z",
    "description": "logiqa in english.",
    "status": "open",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 769747767,
    "title": "connection error ",
    "dateCreated": "2020-12-17T09:18:34Z",
    "dateModified": "2020-12-17T09:18:34Z",
    "description": "Hi\r\nI am hitting to this error, thanks \r\n\r\n```\r\n> Traceback (most recent call last):\r\n  File \"finetune_t5_trainer.py\", line 379, in <module>\r\n    main()\r\n  File \"finetune_t5_trainer.py\", line 208, in main\r\n    if training_args.do_eval or training_args.evaluation_strategy != EvaluationStrategy.NO\r\n  File \"finetune_t5_trainer.py\", line 207, in <dictcomp>\r\n    for task in data_args.eval_tasks}\r\n  File \"/workdir/seq2seq/data/tasks.py\", line 70, in get_dataset\r\n    dataset = self.load_dataset(split=split)\r\n  File \"/workdir/seq2seq/data/tasks.py\", line 66, in load_dataset\r\n    return datasets.load_dataset(self.task.name, split=split, script_version=\"master\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/load.py\", line 589, in load_dataset\r\n    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/load.py\", line 267, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py\", line 487, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/master/datasets/boolq/boolq.py\r\nel/0 I1217 01:11:33.898849 354161 main shadow.py:210 Current job status: FINISHED\r\n```",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 769611386,
    "title": "Access to key in DatasetDict map",
    "dateCreated": "2020-12-17T07:02:20Z",
    "dateModified": "2020-12-17T07:02:20Z",
    "description": "It is possible that we want to do different things in the `map` function (and possibly other functions too) of a `DatasetDict`, depending on the key. I understand that `DatasetDict.map` is a really thin wrapper of `Dataset.map`, so it is easy to directly implement this functionality in the client code. Still, it'd be nice if there can be a flag, similar to `with_indices`, that allows the callable to know the key inside `DatasetDict`.",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 769529421,
    "title": "Using datasets.Metric with Trainer()",
    "dateCreated": "2020-12-17T05:17:04Z",
    "dateModified": "2020-12-17T05:17:04Z",
    "description": "## Using datasets.Metric with Trainer()\r\n\r\nHi team, I was quite surprised in the [Metric documentation](https://huggingface.co/docs/datasets/using_metrics.html) I don't see how it can be used with `Trainer()`. That would be the most intuitive use case instead of having to iterate the batches and add predictions and references to the metric, then compute the metric manually. Ideally, any pre-built metrics can be added to `compute_metrics` argument of `Trainer()` and they will be calculated at an interval specified by `TrainingArguments.evaluation_strategy`. \r\n\r\nIs this option available but just not mentioned in the documentation or it's not possible at the moment? I notice in the [Transformer | Training and fine-tuning](https://huggingface.co/transformers/training.html) tutorial, you are using custom scripts to calculate the accuracy, P/R/F, which are already in the pre-built metrics.",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 769383714,
    "title": "IWSLT-17 Link Broken",
    "dateCreated": "2020-12-17T00:46:42Z",
    "dateModified": "2020-12-17T00:46:42Z",
    "description": "```\r\nFileNotFoundError: Couldn't find file at https://wit3.fbk.eu/archive/2017-01-trnmted//texts/DeEnItNlRo/DeEnItNlRo/DeEnItNlRo-DeEnItNlRo.tgz\r\n```",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 769242858,
    "title": "Add helper to resolve namespace collision",
    "dateCreated": "2020-12-16T20:17:24Z",
    "dateModified": "2020-12-16T20:17:24Z",
    "description": "Many projects use a module called `datasets`, however this is incompatible with huggingface datasets. It would be great if there if there was some helper or similar function to resolve such a common conflict. ",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 769187141,
    "title": "Update doc2dial.py",
    "dateCreated": "2020-12-16T18:50:56Z",
    "dateModified": "2020-12-16T18:50:56Z",
    "description": "Added data loader for machine reading comprehension tasks proposed in the Doc2Dial EMNLP 2020 paper.",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 769068227,
    "title": "Modified hind encorp",
    "dateCreated": "2020-12-16T16:28:14Z",
    "dateModified": "2020-12-16T16:28:14Z",
    "description": "description added, unnecessary comments removed from .py and readme.md reformated \r\n@lhoestq  for #1584",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 768929877,
    "title": "Add nq_open question answering dataset ",
    "dateCreated": "2020-12-16T14:22:08Z",
    "dateModified": "2020-12-16T14:22:08Z",
    "description": "this is pr is a copy of #1506 due to messed up git history in that pr.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 768864502,
    "title": "added irc disentangle dataset",
    "dateCreated": "2020-12-16T13:25:58Z",
    "dateModified": "2020-12-16T13:25:58Z",
    "description": "added irc disentanglement dataset",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 768831171,
    "title": "FileNotFoundError for `amazon_polarity`",
    "dateCreated": "2020-12-16T12:51:05Z",
    "dateModified": "2020-12-16T12:51:05Z",
    "description": "Version: `datasets==v1.1.3`\r\n\r\n### Reproduction\r\n```python\r\nfrom datasets import load_dataset\r\ndata = load_dataset(\"amazon_polarity\")\r\n```\r\ncrashes with\r\n```bash\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/amazon_polarity/amazon_polarity.py\r\n```\r\nand \r\n```bash\r\nFileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/amazon_polarity/amazon_polarity.py\r\n```\r\nand\r\n```bash\r\nFileNotFoundError: Couldn't find file locally at amazon_polarity/amazon_polarity.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/amazon_polarity/amazon_polarity.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/amazon_polarity/amazon_polarity.py\r\n```",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 768820406,
    "title": "Load hind encorp",
    "dateCreated": "2020-12-16T12:38:38Z",
    "dateModified": "2020-12-16T12:38:38Z",
    "description": "reformated well documented, yaml tags added, code",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 768795986,
    "title": "Update metrics docstrings.",
    "dateCreated": "2020-12-16T12:14:18Z",
    "dateModified": "2020-12-16T12:14:18Z",
    "description": "#1478 Correcting the argument descriptions for metrics.\r\n\r\nLet me know if there's any issues.\r\n",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 768776617,
    "title": "Adding wiki lingua dataset as new branch",
    "dateCreated": "2020-12-16T11:53:07Z",
    "dateModified": "2020-12-16T11:53:07Z",
    "description": "Adding the dataset as new branch as advised here: #1470\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 768320594,
    "title": "Installing datasets and transformers in a tensorflow docker image throws Permission Error on 'import transformers'",
    "dateCreated": "2020-12-16T00:02:21Z",
    "dateModified": "2020-12-16T00:02:21Z",
    "description": "I am using a docker container, based on latest tensorflow-gpu image, to run transformers and datasets (4.0.1 and 1.1.3 respectively - Dockerfile attached below). Importing transformers throws a Permission Error to access `/.cache`:\r\n\r\n```\r\n$ docker run --gpus=all --rm -it -u $(id -u):$(id -g) -v $(pwd)/data:/root/data -v $(pwd):/root -v $(pwd)/models/:/root/models -v $(pwd)/saved_models/:/root/saved_models -e \"HOST_HOSTNAME=$(hostname)\" hf-error:latest /bin/bash\r\n\r\n________                               _______________                \r\n___  __/__________________________________  ____/__  /________      __\r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /\r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / \r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/\r\n\r\n\r\nYou are running this container as user with ID 1000 and group 1000,\r\nwhich should map to the ID and group for your user on the Docker host. Great!\r\n\r\ntf-docker /root > python\r\nPython 3.6.9 (default, Oct  8 2020, 12:12:24) \r\n[GCC 8.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import transformers\r\n2020-12-15 23:53:21.165827: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/transformers/__init__.py\", line 22, in <module>\r\n    from .integrations import (  # isort:skip\r\n  File \"/usr/local/lib/python3.6/dist-packages/transformers/integrations.py\", line 5, in <module>\r\n    from .trainer_utils import EvaluationStrategy\r\n  File \"/usr/local/lib/python3.6/dist-packages/transformers/trainer_utils.py\", line 25, in <module>\r\n    from .file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\r\n  File \"/usr/local/lib/python3.6/dist-packages/transformers/file_utils.py\", line 88, in <module>\r\n    import datasets  # noqa: F401\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/__init__.py\", line 26, in <module>\r\n    from .arrow_dataset import Dataset, concatenate_datasets\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py\", line 40, in <module>\r\n    from .arrow_reader import ArrowReader\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/arrow_reader.py\", line 31, in <module>\r\n    from .utils import cached_path, logging\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/__init__.py\", line 20, in <module>\r\n    from .download_manager import DownloadManager, GenerateMode\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/download_manager.py\", line 25, in <module>\r\n    from .file_utils import HF_DATASETS_CACHE, cached_path, get_from_cache, hash_url_to_filename\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py\", line 118, in <module>\r\n    os.makedirs(HF_MODULES_CACHE, exist_ok=True)\r\n  File \"/usr/lib/python3.6/os.py\", line 210, in makedirs\r\n    makedirs(head, mode, exist_ok)\r\n  File \"/usr/lib/python3.6/os.py\", line 210, in makedirs\r\n    makedirs(head, mode, exist_ok)\r\n  File \"/usr/lib/python3.6/os.py\", line 220, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '/.cache'\r\n```\r\nI've pinned the problem to `RUN pip install datasets`, and by commenting it you can actually import transformers correctly. Another workaround I've found is creating the directory and giving permissions to it directly on the Dockerfile.\r\n\r\n```\r\nFROM tensorflow/tensorflow:latest-gpu-jupyter\r\nWORKDIR /root\r\n\r\nEXPOSE 80\r\nEXPOSE 8888\r\nEXPOSE 6006\r\n\r\nENV SHELL /bin/bash\r\nENV PATH=\"/root/.local/bin:${PATH}\"\r\n\r\nENV CUDA_CACHE_PATH=\"/root/cache/cuda\"\r\nENV CUDA_CACHE_MAXSIZE=\"4294967296\"\r\n\r\nENV TFHUB_CACHE_DIR=\"/root/cache/tfhub\"\r\n\r\nRUN pip install --upgrade pip\r\n\r\nRUN apt update -y && apt upgrade -y\r\n\r\nRUN pip install transformers\r\n\r\n#Installing datasets will throw the error, try commenting and rebuilding\r\nRUN pip install datasets\r\n\r\n#Another workaround is creating the directory and give permissions explicitly\r\n#RUN mkdir /.cache\r\n#RUN chmod 777 /.cache\r\n```\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 768111377,
    "title": "made suggested changes in diplomacy_detection.py",
    "dateCreated": "2020-12-15T19:52:00Z",
    "dateModified": "2020-12-15T19:52:00Z",
    "description": "",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 767808465,
    "title": "Adding CLIMATE-FEVER dataset",
    "dateCreated": "2020-12-15T16:49:22Z",
    "dateModified": "2020-12-15T16:49:22Z",
    "description": "This PR request the addition of the CLIMATE-FEVER dataset:\r\nA dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change collected on the internet. Each claim is accompanied by five manually annotated evidence sentences retrieved from the English Wikipedia that support, refute or do not give enough information to validate the claim totalling in 7,675 claim-evidence pairs. The dataset features challenging claims that relate multiple facets and disputed cases of claims where both supporting and refuting evidence are present.\r\n\r\nMore information can be found at:\r\n- Homepage: <http://climatefever.ai>\r\n- Paper: <https://arxiv.org/abs/2012.00614>\r\n\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 767760513,
    "title": "update multiwozv22 checksums",
    "dateCreated": "2020-12-15T16:13:52Z",
    "dateModified": "2020-12-15T16:13:52Z",
    "description": "a file was updated on the GitHub repo for the dataset",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 767342432,
    "title": "Add comet metric",
    "dateCreated": "2020-12-15T08:56:00Z",
    "dateModified": "2020-12-15T08:56:00Z",
    "description": "Hey! I decided to add our new Crosslingual Optimized Metric for Evaluation of Translation (COMET) to the list of the available metrics.\r\n\r\nCOMET was [presented at EMNLP20](https://www.aclweb.org/anthology/2020.emnlp-main.213/) and it is the highest performing metric, so far, on the WMT19 benchmark.\r\n\r\nWe also participated in the [WMT20 Metrics shared task ](http://www.statmt.org/wmt20/pdf/2020.wmt-1.101.pdf) where once again COMET was validated as a top-performing metric. \r\n\r\n\r\nI hope that this metric will help researcher's and industry workers to better validate their MT systems in the future \ud83e\udd17 !\r\n\r\nCheers,\r\nRicardo\r\n\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 767080645,
    "title": "Remove the contributors section",
    "dateCreated": "2020-12-15T01:47:15Z",
    "dateModified": "2020-12-15T01:47:15Z",
    "description": "sourcerer is down",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 767076374,
    "title": "Hind_Encorp all done",
    "dateCreated": "2020-12-15T01:36:02Z",
    "dateModified": "2020-12-15T01:36:02Z",
    "description": "",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 767015317,
    "title": "Diplomacy detection 3",
    "dateCreated": "2020-12-14T23:28:51Z",
    "dateModified": "2020-12-14T23:28:51Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 767011938,
    "title": "adding dataset for diplomacy detection-2",
    "dateCreated": "2020-12-14T23:21:37Z",
    "dateModified": "2020-12-14T23:21:37Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 767008470,
    "title": "add Gnad10 dataset ",
    "dateCreated": "2020-12-14T23:15:02Z",
    "dateModified": "2020-12-14T23:15:02Z",
    "description": "reference [PR#1317](https://github.com/huggingface/datasets/pull/1317)",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 766981721,
    "title": "Fixing the KILT tasks to match our current standards",
    "dateCreated": "2020-12-14T22:26:12Z",
    "dateModified": "2020-12-14T22:26:12Z",
    "description": "This introduces a few changes to the Knowledge Intensive Learning task benchmark to bring it more in line with our current datasets, including adding the (minimal) dataset card and having one config per sub-task",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 766830545,
    "title": "Documentation for loading CSV datasets misleads the user",
    "dateCreated": "2020-12-14T19:04:37Z",
    "dateModified": "2020-12-14T19:04:37Z",
    "description": "Documentation for loading CSV datasets misleads the user into thinking setting `quote_char' to False will disable quoting.\r\n\r\nThere are two problems here:\r\n  i) `quote_char' is misspelled, must be `quotechar'\r\n  ii) the documentation should mention `quoting'",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 766758895,
    "title": "added un_ga dataset",
    "dateCreated": "2020-12-14T17:42:04Z",
    "dateModified": "2020-12-14T17:42:04Z",
    "description": "Hi :hugs:, This is a PR for [United nations general assembly resolutions: A six-language parallel corpus](http://opus.nlpl.eu/UN.php) dataset.\r\nWith suggested changes in #1330 ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 766722994,
    "title": "Added the dataset clickbait_news_bg",
    "dateCreated": "2020-12-14T17:03:00Z",
    "dateModified": "2020-12-14T17:03:00Z",
    "description": "There was a problem with my [previous PR 1445](https://github.com/huggingface/datasets/pull/1445) after rebasing, so I'm copying the dataset code into a new branch and submitting a new PR.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 766382609,
    "title": "[wording] Update Readme.md",
    "dateCreated": "2020-12-14T12:34:52Z",
    "dateModified": "2020-12-14T12:34:52Z",
    "description": "Make the features of the library clearer.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 766354236,
    "title": "Add Microsoft Research Sequential Question Answering (SQA) Dataset",
    "dateCreated": "2020-12-14T12:02:30Z",
    "dateModified": "2020-12-14T12:02:30Z",
    "description": "For more information: https://msropendata.com/datasets/b25190ed-0f59-47b1-9211-5962858142c2",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 766333940,
    "title": "Create README.md",
    "dateCreated": "2020-12-14T11:40:23Z",
    "dateModified": "2020-12-14T11:40:23Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 766266609,
    "title": "added saudinewsnet",
    "dateCreated": "2020-12-14T10:35:09Z",
    "dateModified": "2020-12-14T10:35:09Z",
    "description": "I'm having issues in creating the dummy data. I'm still investigating how to fix it. I'll close the PR if I couldn't find a solution",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 766211931,
    "title": "adding tmu-gfm-dataset",
    "dateCreated": "2020-12-14T09:45:30Z",
    "dateModified": "2020-12-14T09:45:30Z",
    "description": "Adding TMU-GFM-Dataset for Grammatical Error Correction.\r\n\r\nhttps://github.com/tmu-nlp/TMU-GFM-Dataset\r\n\r\nA dataset for GEC metrics with manual evaluations of grammaticality, fluency, and meaning preservation for system outputs.\r\nMore detail about the creation of the dataset can be found in [Yoshimura et al. (2020)](https://www.aclweb.org/anthology/2020.coling-main.573.pdf).",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 765981749,
    "title": "Add dataset COrpus of Urdu News TExt Reuse (COUNTER).",
    "dateCreated": "2020-12-14T06:32:48Z",
    "dateModified": "2020-12-14T06:32:48Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 765831436,
    "title": "Lama",
    "dateCreated": "2020-12-14T03:27:10Z",
    "dateModified": "2020-12-14T03:27:10Z",
    "description": "This the LAMA dataset for probing facts and common sense from language models. \r\n\r\nSee https://github.com/facebookresearch/LAMA for more details.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 765814964,
    "title": "Adding the BrWaC dataset",
    "dateCreated": "2020-12-14T03:03:56Z",
    "dateModified": "2020-12-14T03:03:56Z",
    "description": "Adding the BrWaC dataset, a large corpus of Portuguese language texts",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 765714183,
    "title": "adding dataset card information to CONTRIBUTING.md",
    "dateCreated": "2020-12-14T00:08:43Z",
    "dateModified": "2020-12-14T00:08:43Z",
    "description": "Added a documentation line and link to the full sprint guide in the \"How to add a dataset\" section, and a section on how to contribute to the dataset card of an existing dataset.\r\n\r\nAnd a thank you note at the end :hugs: ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 765707907,
    "title": "Adding Igbo NER data ",
    "dateCreated": "2020-12-13T23:52:11Z",
    "dateModified": "2020-12-13T23:52:11Z",
    "description": "This PR adds the Igbo NER dataset.\r\nData: https://github.com/IgnatiusEzeani/IGBONLP/tree/master/ig_ner ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 765693927,
    "title": "HindEncorp again commited",
    "dateCreated": "2020-12-13T23:09:02Z",
    "dateModified": "2020-12-13T23:09:02Z",
    "description": "",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 765689730,
    "title": "add bswac",
    "dateCreated": "2020-12-13T22:55:35Z",
    "dateModified": "2020-12-13T22:55:35Z",
    "description": "",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 765681607,
    "title": "Added Opus TedTalks",
    "dateCreated": "2020-12-13T22:29:33Z",
    "dateModified": "2020-12-13T22:29:33Z",
    "description": "Dataset : http://opus.nlpl.eu/TedTalks.php",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 765675148,
    "title": "Opus CAPES added",
    "dateCreated": "2020-12-13T22:11:34Z",
    "dateModified": "2020-12-13T22:11:34Z",
    "description": "Dataset : http://opus.nlpl.eu/CAPES.php",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 765670083,
    "title": "added air_dialogue",
    "dateCreated": "2020-12-13T21:59:02Z",
    "dateModified": "2020-12-13T21:59:02Z",
    "description": "UPDATE2 (3797ce5): Updated for multi-configs \r\n\r\nUPDATE (7018082): manually created the dummy_datasets. All tests were cleared locally. Pushed it to origin/master\r\n\r\nDRAFT VERSION (57fdb20):  (_no longer draft_)\r\nUploaded the air_dialogue database. \r\ndummy_data creation was failing in local, since the original downloaded file has some nested folders. Pushing it since the tests with real data was cleared. Will re-check & update via manually creating some dummy_data",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 765664411,
    "title": "Added OPUS ParaCrawl",
    "dateCreated": "2020-12-13T21:44:29Z",
    "dateModified": "2020-12-13T21:44:29Z",
    "description": "Dataset : http://opus.nlpl.eu/ParaCrawl.php",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 765621879,
    "title": "Monero",
    "dateCreated": "2020-12-13T19:56:48Z",
    "dateModified": "2020-12-13T19:56:48Z",
    "description": "Biomedical Romanian dataset :)",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 765620925,
    "title": "Add offensive langauge dravidian dataset",
    "dateCreated": "2020-12-13T19:54:19Z",
    "dateModified": "2020-12-13T19:54:19Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 765612905,
    "title": "Generics kb new branch",
    "dateCreated": "2020-12-13T19:33:10Z",
    "dateModified": "2020-12-13T19:33:10Z",
    "description": "Datasets need manual downloads. Have thus created dummy data as well. But pytest on real and dummy data are failing.\r\nI have completed the readme , tags and other required things. I need to create the metadata json once tests get successful.\r\nOpening a PR while working with Yacine Jernite to resolve my pytest issues.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 765592336,
    "title": "Fix `\ud83e\udd17Datasets` - `tfds` differences link + a few aesthetics",
    "dateCreated": "2020-12-13T18:48:21Z",
    "dateModified": "2020-12-13T18:48:21Z",
    "description": "",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 765562792,
    "title": "Adding PolEval2019 Machine Translation Task dataset",
    "dateCreated": "2020-12-13T17:50:03Z",
    "dateModified": "2020-12-13T17:50:03Z",
    "description": "Facing an error with pytest in training. Dummy data is passing.\r\nREADME has to be updated.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 765559923,
    "title": "Add persian ner dataset",
    "dateCreated": "2020-12-13T17:45:48Z",
    "dateModified": "2020-12-13T17:45:48Z",
    "description": "Adding the following dataset:\r\n\r\nhttps://github.com/HaniehP/PersianNER\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 765550283,
    "title": "add hrwac",
    "dateCreated": "2020-12-13T17:31:54Z",
    "dateModified": "2020-12-13T17:31:54Z",
    "description": "",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 765514828,
    "title": "Added Wiki Summary Dataset",
    "dateCreated": "2020-12-13T16:33:46Z",
    "dateModified": "2020-12-13T16:33:46Z",
    "description": "Wiki Summary: Dataset extracted from Persian Wikipedia into the form of articles and highlights.\r\nLink: https://github.com/m3hrdadfi/wiki-summary",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 765476196,
    "title": "adding HindEncorp",
    "dateCreated": "2020-12-13T15:39:07Z",
    "dateModified": "2020-12-13T15:39:07Z",
    "description": "adding Hindi Wikipedia corpus",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 765439746,
    "title": "fix typo readme",
    "dateCreated": "2020-12-13T14:41:22Z",
    "dateModified": "2020-12-13T14:41:22Z",
    "description": "",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 765430586,
    "title": "connection issue while downloading data",
    "dateCreated": "2020-12-13T14:27:00Z",
    "dateModified": "2020-12-13T14:27:00Z",
    "description": "Hi\r\nI am running my codes on google cloud, and I am getting this error resulting in the failure of the codes when trying to download the data, could you assist me to solve this? also as a temporary solution, could you tell me how I can increase the number of retries and timeout to at least let the models run for now. thanks \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"finetune_t5_trainer.py\", line 361, in <module>\r\n    main()\r\n  File \"finetune_t5_trainer.py\", line 269, in main\r\n    add_prefix=False if training_args.train_adapters else True)\r\n  File \"/workdir/seq2seq/data/tasks.py\", line 70, in get_dataset\r\n    dataset = self.load_dataset(split=split)\r\n  File \"/workdir/seq2seq/data/tasks.py\", line 306, in load_dataset\r\n    return datasets.load_dataset('glue', 'cola', split=split)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/load.py\", line 589, in load_dataset\r\n    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/load.py\", line 263, in prepare_module\r\n    head_hf_s3(path, filename=name, dataset=dataset)\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py\", line 200, in head_hf_s3\r\n    return http_head(hf_bucket_url(identifier=identifier, filename=filename, use_cdn=use_cdn, dataset=dataset))\r\n  File \"/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py\", line 403, in http_head\r\n    url, proxies=proxies, headers=headers, cookies=cookies, allow_redirects=allow_redirects, timeout=timeout\r\n  File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 104, in head\r\n    return request('head', url, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 61, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 542, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 655, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 504, in send\r\n    raise ConnectTimeout(e, request=request)\r\nrequests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/glue/glue.py (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f47db511e80>, 'Connection to s3.amazonaws.com timed out. (connect timeout=10)'))\r\n```",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 765357702,
    "title": "added TTC4900: A Benchmark Data for Turkish Text Categorization dataset",
    "dateCreated": "2020-12-13T12:43:33Z",
    "dateModified": "2020-12-13T12:43:33Z",
    "description": "This PR adds the TTC4900 dataset which is a Turkish Text Categorization dataset by me and @basakbuluz. \r\n\r\nHomepage: [https://www.kaggle.com/savasy/ttc4900](https://www.kaggle.com/savasy/ttc4900)\r\nPoint of Contact: [Sava\u015f Y\u0131ld\u0131r\u0131m](mailto:savasy@gmail.com) / @savasy\r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 765338910,
    "title": "Added Wiki Asp dataset",
    "dateCreated": "2020-12-13T12:18:34Z",
    "dateModified": "2020-12-13T12:18:34Z",
    "description": "Hello,\r\n\r\nI have added Wiki Asp dataset. Please review the PR.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 765139739,
    "title": "tweets_hate_speech_detection",
    "dateCreated": "2020-12-13T07:37:53Z",
    "dateModified": "2020-12-13T07:37:53Z",
    "description": "",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 765095210,
    "title": "added ohsumed ",
    "dateCreated": "2020-12-13T06:58:23Z",
    "dateModified": "2020-12-13T06:58:23Z",
    "description": "UPDATE2: PR passed all tests. Now waiting for review.\r\n\r\nUPDATE: pushed  a new version. cross fingers that it should complete all the tests! :) \r\n               If it passes all tests then it's not a draft version. \r\n\r\nThis is a draft version ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 765043121,
    "title": "Add Hippocorpus Dataset",
    "dateCreated": "2020-12-13T06:13:02Z",
    "dateModified": "2020-12-13T06:13:02Z",
    "description": "",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 764977542,
    "title": "Adding Igbo monolingual dataset",
    "dateCreated": "2020-12-13T05:16:37Z",
    "dateModified": "2020-12-13T05:16:37Z",
    "description": "This PR adds the Igbo Monolingual dataset.\r\nData: https://github.com/IgnatiusEzeani/IGBONLP/tree/master/ig_monoling\r\nPaper: https://arxiv.org/abs/2004.00648  ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 764934681,
    "title": "adding dataset for diplomacy detection",
    "dateCreated": "2020-12-13T04:38:43Z",
    "dateModified": "2020-12-13T04:38:43Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 764835913,
    "title": "add id_panl_bppt, a parallel corpus for en-id",
    "dateCreated": "2020-12-13T03:11:27Z",
    "dateModified": "2020-12-13T03:11:27Z",
    "description": "Parallel Text Corpora for English - Indonesian",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 764772184,
    "title": "adding hate-speech-and-offensive-language",
    "dateCreated": "2020-12-13T02:16:31Z",
    "dateModified": "2020-12-13T02:16:31Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 764752882,
    "title": "adding hate-speech-and-offensive-language",
    "dateCreated": "2020-12-13T01:59:07Z",
    "dateModified": "2020-12-13T01:59:07Z",
    "description": "",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 764749507,
    "title": "add indonlu benchmark datasets",
    "dateCreated": "2020-12-13T01:56:09Z",
    "dateModified": "2020-12-13T01:56:09Z",
    "description": "The IndoNLU benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems for the Indonesian language. There are 12 datasets in IndoNLU.\r\n\r\nThis is a new clean PR from [#1322](https://github.com/huggingface/datasets/pull/1322)",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 764748410,
    "title": "Ro sent",
    "dateCreated": "2020-12-13T01:55:02Z",
    "dateModified": "2020-12-13T01:55:02Z",
    "description": "Movies reviews dataset for Romanian language.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 764724035,
    "title": "initial commit for Common Crawl Domain Names",
    "dateCreated": "2020-12-13T01:32:49Z",
    "dateModified": "2020-12-13T01:32:49Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 764638504,
    "title": "Add : Conv AI 2 (Messed up original PR)",
    "dateCreated": "2020-12-13T00:21:14Z",
    "dateModified": "2020-12-13T00:21:14Z",
    "description": "@lhoestq Sorry I messed up the previous 2 PR's -> https://github.com/huggingface/datasets/pull/1462 -> https://github.com/huggingface/datasets/pull/1383. So created a new one. Also, everything is fixed in this PR. Can you please review it ?\r\nThanks in advance. ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 764591243,
    "title": "added Hebrew thisworld corpus",
    "dateCreated": "2020-12-12T23:42:52Z",
    "dateModified": "2020-12-12T23:42:52Z",
    "description": "added corpus from https://thisworld.online/ , https://github.com/thisworld1/thisworld.online",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 764530582,
    "title": "Adding a second branch for Atomic to fix git errors",
    "dateCreated": "2020-12-12T22:54:50Z",
    "dateModified": "2020-12-12T22:54:50Z",
    "description": "Adding the Atomic common sense dataset.\r\nSee https://homes.cs.washington.edu/~msap/atomic/",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 764521672,
    "title": "ADD: swahili dataset for language modeling",
    "dateCreated": "2020-12-12T22:47:18Z",
    "dateModified": "2020-12-12T22:47:18Z",
    "description": "Add a corpus for Swahili language modelling. All tests passed locally. README updated with all information available.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 764359524,
    "title": "Add eHealth Knowledge Discovery dataset",
    "dateCreated": "2020-12-12T20:44:18Z",
    "dateModified": "2020-12-12T20:44:18Z",
    "description": "This Spanish dataset can be used to mine knowledge from unstructured health texts. \r\n\r\nIn particular, for:\r\n- Entity recognition\r\n- Relation extraction\r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 764341594,
    "title": "Add semeval 2020 task 11",
    "dateCreated": "2020-12-12T20:32:14Z",
    "dateModified": "2020-12-12T20:32:14Z",
    "description": "Adding in propaganda detection task (task 11) from Sem Eval 2020",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 764320841,
    "title": "Atomic",
    "dateCreated": "2020-12-12T20:18:08Z",
    "dateModified": "2020-12-12T20:18:08Z",
    "description": "This is the ATOMIC common sense dataset. More info can be found here:\r\n\r\n* README.md still to be created.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 764140938,
    "title": "ru_reviews dataset adding",
    "dateCreated": "2020-12-12T18:13:06Z",
    "dateModified": "2020-12-12T18:13:06Z",
    "description": "RuReviews: An Automatically Annotated Sentiment Analysis Dataset for Product Reviews in Russian",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 764107360,
    "title": "Initial commit for AQuaMuSe",
    "dateCreated": "2020-12-12T17:46:16Z",
    "dateModified": "2020-12-12T17:46:16Z",
    "description": "There is an issue in generation of dummy data. Tests on real data have passed locally.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 764045722,
    "title": "Add twi text",
    "dateCreated": "2020-12-12T16:52:02Z",
    "dateModified": "2020-12-12T16:52:02Z",
    "description": "Add Twi texts",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 764045214,
    "title": "Kd conv smangrul",
    "dateCreated": "2020-12-12T16:51:30Z",
    "dateModified": "2020-12-12T16:51:30Z",
    "description": "",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 764032327,
    "title": "adding wrbsc",
    "dateCreated": "2020-12-12T16:38:40Z",
    "dateModified": "2020-12-12T16:38:40Z",
    "description": "",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 764022753,
    "title": "Add yoruba text",
    "dateCreated": "2020-12-12T16:29:30Z",
    "dateModified": "2020-12-12T16:29:30Z",
    "description": "Adding Yoruba text C3",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 764017148,
    "title": "how to get all the options of a property in datasets ",
    "dateCreated": "2020-12-12T16:24:08Z",
    "dateModified": "2020-12-12T16:24:08Z",
    "description": "Hi\r\ncould you tell me how I can get all unique options of a property of dataset?\r\nfor instance in case of boolq, if the user wants to know which unique labels it has, is there a way to access unique labels without getting all training data lables and then forming a set i mean? thanks",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 764016850,
    "title": "app_reviews_by_users",
    "dateCreated": "2020-12-12T16:23:49Z",
    "dateModified": "2020-12-12T16:23:49Z",
    "description": "Software Applications User Reviews ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 764010722,
    "title": "Add Hippocorpus Dataset",
    "dateCreated": "2020-12-12T16:17:53Z",
    "dateModified": "2020-12-12T16:17:53Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 764006477,
    "title": "poleval cyberbullying",
    "dateCreated": "2020-12-12T16:13:44Z",
    "dateModified": "2020-12-12T16:13:44Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 763980369,
    "title": "Add Dataset for (qa_srl)Question-Answer Driven Semantic Role Labeling",
    "dateCreated": "2020-12-12T15:48:11Z",
    "dateModified": "2020-12-12T15:48:11Z",
    "description": "- Added tags, Readme file\r\n- Added code changes",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 763964857,
    "title": "Added dataset Makhzan",
    "dateCreated": "2020-12-12T15:34:07Z",
    "dateModified": "2020-12-12T15:34:07Z",
    "description": "Need help with the dummy data.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 763908724,
    "title": "Fix namedsplit docs",
    "dateCreated": "2020-12-12T14:43:38Z",
    "dateModified": "2020-12-12T14:43:38Z",
    "description": "Fixes a broken link and `DatasetInfoMixin.split`'s docstring.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 763857872,
    "title": "Add SelQA Dataset",
    "dateCreated": "2020-12-12T13:58:07Z",
    "dateModified": "2020-12-12T13:58:07Z",
    "description": "Add the SelQA Dataset, a new benchmark for selection-based question answering tasks\r\nRepo: https://github.com/emorynlp/selqa/\r\nPaper: https://arxiv.org/pdf/1606.08513.pdf",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 763846074,
    "title": "Add nq_open question answering dataset",
    "dateCreated": "2020-12-12T13:46:48Z",
    "dateModified": "2020-12-12T13:46:48Z",
    "description": "Added nq_open Open-domain question answering dataset.\r\n\r\nThe NQ-Open task is currently being used to evaluate submissions to the EfficientQA competition, which is part of the NeurIPS 2020 competition track.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 763750773,
    "title": "add ilist dataset",
    "dateCreated": "2020-12-12T12:44:12Z",
    "dateModified": "2020-12-12T12:44:12Z",
    "description": "This PR will add Indo-Aryan Language Identification Shared Task Dataset.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 763697231,
    "title": "Add SentiWS dataset for pos-tagging and sentiment-scoring (German)",
    "dateCreated": "2020-12-12T12:17:53Z",
    "dateModified": "2020-12-12T12:17:53Z",
    "description": "",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 763667489,
    "title": "Adding COVID QA dataset in Chinese and English from UC SanDiego",
    "dateCreated": "2020-12-12T12:02:48Z",
    "dateModified": "2020-12-12T12:02:48Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 763658208,
    "title": "Add Senti_Lex Dataset",
    "dateCreated": "2020-12-12T11:55:29Z",
    "dateModified": "2020-12-12T11:55:29Z",
    "description": "TODO:\r\nFix feature format issue\r\nCreate dataset_info.json file\r\nRun pytests\r\nMake Style",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 763517647,
    "title": "Adds XED dataset",
    "dateCreated": "2020-12-12T09:47:00Z",
    "dateModified": "2020-12-12T09:47:00Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 763479305,
    "title": "adding polsum",
    "dateCreated": "2020-12-12T09:05:29Z",
    "dateModified": "2020-12-12T09:05:29Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 763464693,
    "title": "update the dataset id_newspapers_2018",
    "dateCreated": "2020-12-12T08:47:12Z",
    "dateModified": "2020-12-12T08:47:12Z",
    "description": "Hi, I need to update the link to the dataset. The link in the previous PR was to a small test dataset. Thanks",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 763303606,
    "title": "add stereoset",
    "dateCreated": "2020-12-12T05:04:37Z",
    "dateModified": "2020-12-12T05:04:37Z",
    "description": "StereoSet is a dataset that measures stereotype bias in language models. StereoSet consists of 17,000 sentences that measures model preferences across gender, race, religion, and profession.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 763180824,
    "title": "adding fake-news-english-5",
    "dateCreated": "2020-12-12T02:13:11Z",
    "dateModified": "2020-12-12T02:13:11Z",
    "description": "",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 763091663,
    "title": "Add Multi-Dimensional Gender Bias classification data",
    "dateCreated": "2020-12-12T00:17:37Z",
    "dateModified": "2020-12-12T00:17:37Z",
    "description": "https://parl.ai/projects/md_gender/\r\n\r\nMostly has the ABOUT dimension since the others are inferred from other datasets in most cases.\r\n\r\nI tried to keep the dummy data small but one of the configs has 140 splits ( > 56KB data)",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 763025562,
    "title": "Opus DGT added",
    "dateCreated": "2020-12-11T23:05:09Z",
    "dateModified": "2020-12-11T23:05:09Z",
    "description": "Dataset : http://opus.nlpl.eu/DGT.php",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 762992601,
    "title": "Added Opus Wikipedia",
    "dateCreated": "2020-12-11T22:28:03Z",
    "dateModified": "2020-12-11T22:28:03Z",
    "description": "Dataset : http://opus.nlpl.eu/Wikipedia.php",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 762979415,
    "title": "Added RONEC dataset.",
    "dateCreated": "2020-12-11T22:14:50Z",
    "dateModified": "2020-12-11T22:14:50Z",
    "description": "",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 762965239,
    "title": "OPUS UBUNTU dataset",
    "dateCreated": "2020-12-11T22:01:37Z",
    "dateModified": "2020-12-11T22:01:37Z",
    "description": "Dataset : http://opus.nlpl.eu/Ubuntu.php",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 762920920,
    "title": "added opus GNOME data",
    "dateCreated": "2020-12-11T21:21:51Z",
    "dateModified": "2020-12-11T21:21:51Z",
    "description": "Dataset : http://opus.nlpl.eu/GNOME.php",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 762915346,
    "title": "ADD: opus_rf dataset for translation",
    "dateCreated": "2020-12-11T21:16:43Z",
    "dateModified": "2020-12-11T21:16:43Z",
    "description": "Passed all local tests. Hopefully passes all Circle CI tests too. Tried to keep the commit history clean.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 762908763,
    "title": "Fake news english 4",
    "dateCreated": "2020-12-11T21:10:35Z",
    "dateModified": "2020-12-11T21:10:35Z",
    "description": "",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 762860679,
    "title": "Adding NELL",
    "dateCreated": "2020-12-11T20:25:25Z",
    "dateModified": "2020-12-11T20:25:25Z",
    "description": "NELL is a knowledge base and knowledge graph along with sentences used to create the KB. See http://rtw.ml.cmu.edu/rtw/ for more details.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 762794921,
    "title": " added conv_ai_3 dataset",
    "dateCreated": "2020-12-11T19:26:26Z",
    "dateModified": "2020-12-11T19:26:26Z",
    "description": "Dataset : https://github.com/aliannejadi/ClariQ/\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 762790102,
    "title": "hate speech 18 dataset",
    "dateCreated": "2020-12-11T19:22:14Z",
    "dateModified": "2020-12-11T19:22:14Z",
    "description": "This is again a PR instead of #1339, because something went wrong there. ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 762774822,
    "title": "Re-added wiki_movies dataset due to previous PR having changes from m\u2026",
    "dateCreated": "2020-12-11T19:07:48Z",
    "dateModified": "2020-12-11T19:07:48Z",
    "description": "\u2026any other unassociated files.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 762747096,
    "title": "Add peer-read dataset",
    "dateCreated": "2020-12-11T18:43:44Z",
    "dateModified": "2020-12-11T18:43:44Z",
    "description": "",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 762712337,
    "title": "Added Times of India News Headlines Dataset",
    "dateCreated": "2020-12-11T18:12:38Z",
    "dateModified": "2020-12-11T18:12:38Z",
    "description": "Dataset name: Times of India News Headlines\r\nlink: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DPQMQH",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 762686820,
    "title": "Adding medical database chinese and english",
    "dateCreated": "2020-12-11T17:50:39Z",
    "dateModified": "2020-12-11T17:50:39Z",
    "description": "Error in creating dummy dataset",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 762579658,
    "title": "Fix ADD_NEW_DATASET to avoid rebasing once pushed",
    "dateCreated": "2020-12-11T16:27:49Z",
    "dateModified": "2020-12-11T16:27:49Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 762530805,
    "title": "Adding the Mac-Morpho dataset",
    "dateCreated": "2020-12-11T16:01:38Z",
    "dateModified": "2020-12-11T16:01:38Z",
    "description": "Adding the Mac-Morpho dataset, a Portuguese language dataset for Part-of-speech tagging tasks",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 762320736,
    "title": "Add narrativeQA",
    "dateCreated": "2020-12-11T12:58:31Z",
    "dateModified": "2020-12-11T12:58:31Z",
    "description": "Redo of #1368 #309 #499\r\n\r\nIn redoing the dummy data a few times, I ended up adding a load of files to git. Hopefully this should work.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 762293076,
    "title": "Inconsistent argument names.",
    "dateCreated": "2020-12-11T12:19:38Z",
    "dateModified": "2020-12-11T12:19:38Z",
    "description": "Just find it a wee bit odd that in the transformers library `predictions` are those made by the model:\r\nhttps://github.com/huggingface/transformers/blob/master/src/transformers/trainer_utils.py#L51-L61\r\n\r\nWhile in many datasets metrics they are the ground truth labels:\r\nhttps://github.com/huggingface/datasets/blob/c3f53792a744ede18d748a1133b6597fdd2d8d18/metrics/accuracy/accuracy.py#L31-L40\r\n\r\nDo you think predictions & references should be swapped? I'd be willing to do some refactoring here if you agree.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 762288811,
    "title": "Jigsaw toxicity pred",
    "dateCreated": "2020-12-11T12:13:20Z",
    "dateModified": "2020-12-11T12:13:20Z",
    "description": "Managed to mess up my original pull request, opening a fresh one incorporating the changes suggested by @lhoestq.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 762256048,
    "title": "Add Spanish Billion Words Corpus",
    "dateCreated": "2020-12-11T11:24:58Z",
    "dateModified": "2020-12-11T11:24:58Z",
    "description": "Add an unannotated Spanish corpus of nearly 1.5 billion words, compiled from different resources from the web.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 762187000,
    "title": "Fix XML iterparse in opus_dogc dataset",
    "dateCreated": "2020-12-11T10:08:18Z",
    "dateModified": "2020-12-11T10:08:18Z",
    "description": "I forgot to add `elem.clear()` to clear the element from memory.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 762083706,
    "title": "Create JSON dummy data without loading all dataset in memory",
    "dateCreated": "2020-12-11T08:44:23Z",
    "dateModified": "2020-12-11T08:44:23Z",
    "description": "See #1442.\r\n\r\nThe statement `json.load()` loads **all the file content in memory**.\r\n\r\nIn order to avoid this, file content should be parsed **iteratively**, by using the library `ijson` e.g.\r\n\r\nI have refactorized the code into a function `_create_json_dummy_data` and I have added some tests.",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 762055694,
    "title": "add srwac",
    "dateCreated": "2020-12-11T08:20:29Z",
    "dateModified": "2020-12-11T08:20:29Z",
    "description": "",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 762037907,
    "title": "add Srwac",
    "dateCreated": "2020-12-11T08:04:57Z",
    "dateModified": "2020-12-11T08:04:57Z",
    "description": "",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 761842512,
    "title": "Adding the HAREM dataset",
    "dateCreated": "2020-12-11T03:21:10Z",
    "dateModified": "2020-12-11T03:21:10Z",
    "description": "Adding the HAREM dataset, a Portuguese language dataset for NER tasks",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 761791065,
    "title": "Add wiki lingua dataset",
    "dateCreated": "2020-12-11T02:04:18Z",
    "dateModified": "2020-12-11T02:04:18Z",
    "description": "Hello @lhoestq ,\r\n\r\nI am opening a fresh pull request as advised in my original PR https://github.com/huggingface/datasets/pull/1308\r\n\r\nThanks",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 761611315,
    "title": "ADD: Wino_bias dataset",
    "dateCreated": "2020-12-10T20:59:45Z",
    "dateModified": "2020-12-10T20:59:45Z",
    "description": "Updated PR to counter messed up history of previous one (https://github.com/huggingface/datasets/pull/1235) due to rebase.\r\nRemoved manual downloading of dataset.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 761607531,
    "title": "add Indonesian newspapers (id_newspapers_2018)",
    "dateCreated": "2020-12-10T20:54:12Z",
    "dateModified": "2020-12-10T20:54:12Z",
    "description": "The dataset contains around 500K articles (136M of words) from 7 Indonesian newspapers. The size of uncompressed 500K json files (newspapers-json.tgz) is around 2.2GB.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 761557290,
    "title": "adding snow_simplified_japanese_corpus",
    "dateCreated": "2020-12-10T19:45:03Z",
    "dateModified": "2020-12-10T19:45:03Z",
    "description": "Adding simplified Japanese corpus \"SNOW T15\" and \"SNOW T23\".\r\nThey contain original Japanese, simplified Japanese, and original English (the original text is gotten from en-ja translation corpus). Hence, it can be used not only for Japanese simplification but also for en-ja translation.\r\n\r\n- http://www.jnlp.org/SNOW/T15\r\n- http://www.jnlp.org/SNOW/T23",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 761554357,
    "title": "Add Turkish News Category Dataset (270K).Updates were made for review\u2026",
    "dateCreated": "2020-12-10T19:41:12Z",
    "dateModified": "2020-12-10T19:41:12Z",
    "description": "This PR adds the **Turkish News Categories Dataset (270K)** dataset which is a text classification dataset by me and @yavuzKomecoglu. Turkish news dataset consisting of **273601 news in 17 categories**, compiled from printed media and news websites between 2010 and 2017 by the [Interpress](https://www.interpress.com/) media monitoring company.\r\n\r\n**Note**: Resubmitted as a clean version of the previous Pull Request(#1419).  @SBrandeis @lhoestq ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 761538931,
    "title": "Add clean menyo20k data",
    "dateCreated": "2020-12-10T19:22:00Z",
    "dateModified": "2020-12-10T19:22:00Z",
    "description": "New Clean PR for menyo20k_mt",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 761533566,
    "title": "Reddit jokes",
    "dateCreated": "2020-12-10T19:15:19Z",
    "dateModified": "2020-12-10T19:15:19Z",
    "description": "196k Reddit Jokes dataset\r\nDataset link- https://raw.githubusercontent.com/taivop/joke-dataset/master/reddit_jokes.json",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 761510908,
    "title": "Adding enriched_web_nlg features + handling xml bugs",
    "dateCreated": "2020-12-10T18:48:19Z",
    "dateModified": "2020-12-10T18:48:19Z",
    "description": "This PR adds features of the enriched_web_nlg dataset that were not present yet (most notably sorted rdf triplet sets), and deals with some xml issues that led to returning no data in cases where surgery could be performed to salvage it.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 761489274,
    "title": "Added conv ai 2 (Again)",
    "dateCreated": "2020-12-10T18:21:55Z",
    "dateModified": "2020-12-10T18:21:55Z",
    "description": "The original PR -> https://github.com/huggingface/datasets/pull/1383\r\n\r\nReason for creating again - \r\n\r\nThe reason I had to create the PR again was due to the master rebasing issue. After rebasing the changes, all the previous commits got added to the branch. ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 761415420,
    "title": "Adding NewsQA dataset",
    "dateCreated": "2020-12-10T17:01:10Z",
    "dateModified": "2020-12-10T17:01:10Z",
    "description": "Since the dataset has legal restrictions to circulate the original data. It has to be manually downloaded by the user and loaded to the library. ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 761349149,
    "title": "add Bengali Hate Speech dataset",
    "dateCreated": "2020-12-10T15:40:55Z",
    "dateModified": "2020-12-10T15:40:55Z",
    "description": "",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 761258395,
    "title": "Add Google Conceptual Captions Dataset (manual download)",
    "dateCreated": "2020-12-10T13:50:33Z",
    "dateModified": "2020-12-10T13:50:33Z",
    "description": "",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 761235962,
    "title": "Add id_nergrit_corpus",
    "dateCreated": "2020-12-10T13:20:34Z",
    "dateModified": "2020-12-10T13:20:34Z",
    "description": "Nergrit Corpus is a dataset collection of Indonesian Named Entity Recognition, Statement Extraction, and Sentiment Analysis. \r\nRecently my PR for id_nergrit_ner has been accepted and merged to the main branch. The id_nergrit_ner has only one dataset (NER), and this new PR renamed the dataset from id_nergrit_ner to id_nergrit_corpus and added 2 other remaining datasets (Statement Extraction, and Sentiment Analysis.)",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 761232610,
    "title": "add hrenwac_para",
    "dateCreated": "2020-12-10T13:16:20Z",
    "dateModified": "2020-12-10T13:16:20Z",
    "description": "",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 761231296,
    "title": "Add CC100 Dataset",
    "dateCreated": "2020-12-10T13:14:37Z",
    "dateModified": "2020-12-10T13:14:37Z",
    "description": "Closes #773 ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 761205073,
    "title": "Add HEAD-QA: A Healthcare Dataset for Complex Reasoning",
    "dateCreated": "2020-12-10T12:36:56Z",
    "dateModified": "2020-12-10T12:36:56Z",
    "description": "HEAD-QA is a multi-choice HEAlthcare Dataset, the questions come from exams to access a specialized position in the\r\nSpanish healthcare system.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 761199862,
    "title": "Add kinnews_kirnews",
    "dateCreated": "2020-12-10T12:29:08Z",
    "dateModified": "2020-12-10T12:29:08Z",
    "description": "Add kinnews and kirnews",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 761188657,
    "title": "Adding ethos dataset clean",
    "dateCreated": "2020-12-10T12:13:21Z",
    "dateModified": "2020-12-10T12:13:21Z",
    "description": "I addressed the comments on the PR1318",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 761104924,
    "title": "SNLI dataset contains labels with value -1",
    "dateCreated": "2020-12-10T10:16:55Z",
    "dateModified": "2020-12-10T10:16:55Z",
    "description": "```\r\nimport datasets\r\nnli_data = datasets.load_dataset(\"snli\")\r\ntrain_data = nli_data['train']\r\ntrain_labels = train_data['label']\r\nlabel_set = set(train_labels)\r\nprint(label_set)\r\n```\r\n\r\n**Output:**\r\n`{0, 1, 2, -1}`",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 761102770,
    "title": "Add European Center for Disease Control and Preventions's (ECDC) Translation Memory dataset",
    "dateCreated": "2020-12-10T10:14:20Z",
    "dateModified": "2020-12-10T10:14:20Z",
    "description": "ECDC-TM homepage: https://ec.europa.eu/jrc/en/language-technologies/ecdc-translation-memory",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 761102429,
    "title": "Fix version in bible_para",
    "dateCreated": "2020-12-10T10:13:55Z",
    "dateModified": "2020-12-10T10:13:55Z",
    "description": "",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 761083210,
    "title": "add W&I + LOCNESS dataset (BEA-2019 workshop shared task on GEC) [PROPER]",
    "dateCreated": "2020-12-10T09:51:08Z",
    "dateModified": "2020-12-10T09:51:08Z",
    "description": "- **Name:** W&I + LOCNESS dataset (from the BEA-2019 workshop shared task on GEC)\r\n- **Description:** https://www.cl.cam.ac.uk/research/nl/bea2019st/#data\r\n- **Paper:** https://www.aclweb.org/anthology/W19-4406/\r\n- **Motivation:** This is a recent dataset (actually two in one) for grammatical error correction and is used for benchmarking in this field of NLP.\r\n\r\n### Checkbox\r\n\r\n- [x] Create the dataset script `/datasets/my_dataset/my_dataset.py` using the template\r\n- [x] Fill the `_DESCRIPTION` and `_CITATION` variables\r\n- [x] Implement `_infos()`, `_split_generators()` and `_generate_examples()`\r\n- [x] Make sure that the `BUILDER_CONFIGS` class attribute is filled with the different configurations of the dataset and that the `BUILDER_CONFIG_CLASS` is specified if there is a custom config class.\r\n- [x] Generate the metadata file `dataset_infos.json` for all configurations\r\n- [x] Generate the dummy data `dummy_data.zip` files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card `README.md` using the template : fill the tags and the various paragraphs\r\n- [x] Both tests for the real data and the dummy data pass.\r\n",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 761080776,
    "title": "add thai_toxicity_tweet",
    "dateCreated": "2020-12-10T09:48:02Z",
    "dateModified": "2020-12-10T09:48:02Z",
    "description": "Thai Toxicity Tweet Corpus contains 3,300 tweets (506 tweets with texts missing) annotated by humans with guidelines including a 44-word dictionary. The author obtained 2,027 and 1,273 toxic and non-toxic tweets, respectively; these were labeled by three annotators. The result of corpus analysis indicates that tweets that include toxic words are not always toxic. Further, it is more likely that a tweet is toxic, if it contains toxic words indicating their original meaning. Moreover, disagreements in annotation are primarily because of sarcasm, unclear existing target, and word sense ambiguity.\r\n\r\nNotes from data cleaner: The data is included into [huggingface/datasets](https://www.github.com/huggingface/datasets) in Dec 2020. By this time, 506 of the tweets are not available publicly anymore. We denote these by `TWEET_NOT_FOUND` in `tweet_text`.\r\nProcessing can be found at [this PR](https://github.com/tmu-nlp/ThaiToxicityTweetCorpus/pull/1).",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 761067955,
    "title": "Update step-by-step guide for windows",
    "dateCreated": "2020-12-10T09:30:59Z",
    "dateModified": "2020-12-10T09:30:59Z",
    "description": "Update step-by-step guide for windows to give an alternative to `make style`.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 761060323,
    "title": "Add Bing Coronavirus Query Set",
    "dateCreated": "2020-12-10T09:20:46Z",
    "dateModified": "2020-12-10T09:20:46Z",
    "description": "",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 761057851,
    "title": "Added dataset clickbait_news_bg",
    "dateCreated": "2020-12-10T09:17:28Z",
    "dateModified": "2020-12-10T09:17:28Z",
    "description": "",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 761055651,
    "title": "FileNotFound remotly, can't load a dataset",
    "dateCreated": "2020-12-10T09:14:47Z",
    "dateModified": "2020-12-10T09:14:47Z",
    "description": "```py\r\n!pip install datasets\r\nimport datasets as ds\r\n\r\ncorpus = ds.load_dataset('large_spanish_corpus')\r\n```\r\ngives the error\r\n\r\n> FileNotFoundError: Couldn't find file locally at large_spanish_corpus/large_spanish_corpus.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/large_spanish_corpus/large_spanish_corpus.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/large_spanish_corpus/large_spanish_corpus.py\r\n\r\nnot just `large_spanish_corpus`,  `zest` too, but `squad` is available. \r\n\r\nthis was using colab and localy ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 761033061,
    "title": "Add OPUS Wikimedia Translations Dataset",
    "dateCreated": "2020-12-10T08:43:02Z",
    "dateModified": "2020-12-10T08:43:02Z",
    "description": "",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 761026069,
    "title": "Create XML dummy data without loading all dataset in memory",
    "dateCreated": "2020-12-10T08:32:07Z",
    "dateModified": "2020-12-10T08:32:07Z",
    "description": "While I was adding one XML dataset, I noticed that all the dataset was loaded in memory during the dummy data generation process (using nearly all my laptop RAM).\r\n\r\nLooking at the code, I have found that the origin is the use of `ET.parse()`. This method loads **all the file content in memory**.\r\n\r\nIn order to fix this, I have refactorized the code and use `ET.iterparse()` instead, which **parses the file content incrementally**.\r\n\r\nI have also implemented a test.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 761021823,
    "title": "Add Igbo-English Machine Translation Dataset",
    "dateCreated": "2020-12-10T08:25:34Z",
    "dateModified": "2020-12-10T08:25:34Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 760973057,
    "title": "Adding english plaintext jokes dataset",
    "dateCreated": "2020-12-10T07:04:17Z",
    "dateModified": "2020-12-10T07:04:17Z",
    "description": "This PR adds a dataset of 200k English plaintext Jokes from three sources: Reddit, Stupidstuff, and Wocka.\r\nLink: https://github.com/taivop/joke-dataset \r\n\r\nThis is my second PR. \r\nFirst was: [#1269 ](https://github.com/huggingface/datasets/pull/1269)",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 760968410,
    "title": "Update README.md",
    "dateCreated": "2020-12-10T06:57:01Z",
    "dateModified": "2020-12-10T06:57:01Z",
    "description": "1k-10k -> 1k-1M\r\n\r\n3 separate configs are available with min. 1K and max. 211.3k examples",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 760962193,
    "title": "A descriptive name for my changes",
    "dateCreated": "2020-12-10T06:47:24Z",
    "dateModified": "2020-12-10T06:47:24Z",
    "description": "hind encorp resubmited",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 760891879,
    "title": "Add Indosum dataset",
    "dateCreated": "2020-12-10T05:02:00Z",
    "dateModified": "2020-12-10T05:02:00Z",
    "description": "",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 760873132,
    "title": "add ALT",
    "dateCreated": "2020-12-10T04:17:21Z",
    "dateModified": "2020-12-10T04:17:21Z",
    "description": "ALT dataset -- https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 760867325,
    "title": "Add FreebaseQA dataset",
    "dateCreated": "2020-12-10T04:03:27Z",
    "dateModified": "2020-12-10T04:03:27Z",
    "description": "This PR adds the FreebaseQA dataset: A Trivia-type QA Data Set over the Freebase Knowledge Graph\r\n\r\nRepo: https://github.com/kelvin-jiang/FreebaseQA\r\n\r\nPaper: https://www.aclweb.org/anthology/N19-1028.pdf\r\n\r\n\r\n## TODO: create dummy data\r\n\r\nError encountered when running `python datasets-cli dummy_data datasets/freebase_qa --auto_generate`\r\n```\r\n    f\"Couldn't parse columns {list(json_data.keys())}. \"\r\nValueError: Couldn't parse columns ['Dataset', 'Version', 'Questions']. Maybe specify which json field must be used to read the data with --json_field <my_field>.\r\n```\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 760821474,
    "title": "add_sofc_materials_articles",
    "dateCreated": "2020-12-10T02:15:02Z",
    "dateModified": "2020-12-10T02:15:02Z",
    "description": "adding [SOFC-Exp Corpus](https://arxiv.org/abs/2006.03039)",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 760813539,
    "title": "Adding the ASSIN 2 dataset",
    "dateCreated": "2020-12-10T01:57:02Z",
    "dateModified": "2020-12-10T01:57:02Z",
    "description": "Adding the ASSIN 2 dataset, a Portuguese language dataset for Natural Language Inference and Semantic Similarity Scoring",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 760808449,
    "title": "Adding journalists questions dataset",
    "dateCreated": "2020-12-10T01:44:47Z",
    "dateModified": "2020-12-10T01:44:47Z",
    "description": "This is my first dataset to be added to HF. ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 760791019,
    "title": "Ar cov19",
    "dateCreated": "2020-12-10T00:59:34Z",
    "dateModified": "2020-12-10T00:59:34Z",
    "description": "Adding ArCOV-19 dataset. ArCOV-19 is an Arabic COVID-19 Twitter dataset that covers the period from 27th of January till 30th of April 2020. ArCOV-19 is the first publicly-available Arabic Twitter dataset covering COVID-19 pandemic that includes over 1M tweets alongside the propagation networks of the most-popular subset of them (i.e., most-retweeted and-liked). The propagation networks include both retweets and conversational threads (i.e., threads of replies). ArCOV-19 is designed to enable research under several domains including natural language processing, information retrieval, and social computing, among others. ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 760779666,
    "title": "Add 1.5 billion words Arabic corpus ",
    "dateCreated": "2020-12-10T00:32:18Z",
    "dateModified": "2020-12-10T00:32:18Z",
    "description": "Needs https://github.com/huggingface/datasets/pull/1429 to work. ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 760737818,
    "title": "extract rar files",
    "dateCreated": "2020-12-09T23:01:10Z",
    "dateModified": "2020-12-09T23:01:10Z",
    "description": "Unfortunately, I didn't find any native python libraries for extracting rar files. The user has to manually install `sudo apt-get install unrar`. Discussion with @yjernite is in the slack channel. ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 760736726,
    "title": "Add twi wordsim353",
    "dateCreated": "2020-12-09T22:59:19Z",
    "dateModified": "2020-12-09T22:59:19Z",
    "description": "Add twi WordSim 353",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 760736703,
    "title": "Hebrew project BenYehuda",
    "dateCreated": "2020-12-09T22:59:17Z",
    "dateModified": "2020-12-09T22:59:17Z",
    "description": "Added Hebrew corpus from https://github.com/projectbenyehuda/public_domain_dump",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 760735763,
    "title": "init commit for MultiReQA for third PR with all issues fixed",
    "dateCreated": "2020-12-09T22:57:41Z",
    "dateModified": "2020-12-09T22:57:41Z",
    "description": "3rd PR w.r.t. PR #1349 with all the issues fixed. As #1349 had uploaded other files along with the multi_re_qa dataset",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 760733638,
    "title": "Add german common crawl dataset",
    "dateCreated": "2020-12-09T22:54:12Z",
    "dateModified": "2020-12-09T22:54:12Z",
    "description": "Adding a subpart of the Common Crawl which was extracted with this repo https://github.com/facebookresearch/cc_net and additionally filtered for duplicates ",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 760724914,
    "title": "Add yoruba wordsim353",
    "dateCreated": "2020-12-09T22:37:42Z",
    "dateModified": "2020-12-09T22:37:42Z",
    "description": "Added WordSim-353 evaluation dataset for Yoruba",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 760712421,
    "title": "Imppres",
    "dateCreated": "2020-12-09T22:14:12Z",
    "dateModified": "2020-12-09T22:14:12Z",
    "description": "2nd PR ever! Hopefully I'm starting to get the hang of this. This is for the IMPPRES dataset. Please let me know of any corrections or changes that need to be made.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 760707113,
    "title": "Can't map dataset (loaded from csv)",
    "dateCreated": "2020-12-09T22:05:42Z",
    "dateModified": "2020-12-09T22:05:42Z",
    "description": "Hello! I am trying to load single csv file with two columns: ('label': str, 'text' str), where is label is str of two possible classes.\r\n\r\nBelow steps are similar with [this notebook](https://colab.research.google.com/drive/1-JIJlao4dI-Ilww_NnTc0rxtp-ymgDgM?usp=sharing), where bert model and tokenizer are used to classify lmdb loaded dataset. Only one difference it is the dataset loaded from .csv file.\r\nHere is how I load it:\r\n\r\n```python\r\ndata_path = 'data.csv'\r\ndata = pd.read_csv(data_path)\r\n\r\n# process class name to indices\r\nclasses = ['neg', 'pos']\r\nclass_to_idx = { cl: i for i, cl in enumerate(classes) }\r\n\r\n# now data is like {'label': int, 'text' str}\r\ndata['label'] = data['label'].apply(lambda x: class_to_idx[x])\r\n\r\n# load dataset and map it with defined `tokenize` function\r\nfeatures = Features({\r\n  target: ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None),\r\n  feature: Value(dtype='string', id=None),\r\n})\r\ndataset = Dataset.from_pandas(data, features=features)\r\ndataset.map(tokenize, batched=True, batch_size=len(dataset))\r\n```\r\n\r\nIt ruins on the last line with following error:\r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-112-32b6275ce418> in <module>()\r\n      9 })\r\n     10 dataset = Dataset.from_pandas(data, features=features)\r\n---> 11 dataset.map(tokenizer, batched=True, batch_size=len(dataset))\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\r\n   1237         test_inputs = self[:2] if batched else self[0]\r\n   1238         test_indices = [0, 1] if batched else 0\r\n-> 1239         update_data = does_function_return_dict(test_inputs, test_indices)\r\n   1240         logger.info(\"Testing finished, running the mapping function on the dataset\")\r\n   1241 \r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py in does_function_return_dict(inputs, indices)\r\n   1208             fn_args = [inputs] if input_columns is None else [inputs[col] for col in input_columns]\r\n   1209             processed_inputs = (\r\n-> 1210                 function(*fn_args, indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n   1211             )\r\n   1212             does_return_dict = isinstance(processed_inputs, Mapping)\r\n\r\n/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py in __call__(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\r\n   2281             )\r\n   2282         ), (\r\n-> 2283             \"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\r\n   2284             \"or `List[List[str]]` (batch of pretokenized examples).\"\r\n   2285         )\r\n\r\nAssertionError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\r\n```\r\n\r\nwhich I think is not expected. I also tried the same steps using `Dataset.from_csv` which resulted in the same error.\r\n\r\nFor reproducing this, I used [this dataset from kaggle](https://www.kaggle.com/team-ai/spam-text-message-classification)",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 760706851,
    "title": "adding fake-news-english-2",
    "dateCreated": "2020-12-09T22:05:13Z",
    "dateModified": "2020-12-09T22:05:13Z",
    "description": "",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 760700388,
    "title": "Add dataset yoruba_wordsim353",
    "dateCreated": "2020-12-09T21:54:29Z",
    "dateModified": "2020-12-09T21:54:29Z",
    "description": "Contains loading script as well as dataset card including YAML tags.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 760673716,
    "title": "Add Turkish News Category Dataset (270K)",
    "dateCreated": "2020-12-09T21:08:33Z",
    "dateModified": "2020-12-09T21:08:33Z",
    "description": "This PR adds the Turkish News Categories Dataset (270K) dataset which is a text classification dataset by me and @yavuzKomecoglu.  Turkish news dataset consisting of **273601 news** in **17 categories**, compiled from printed media and news websites between 2010 and 2017 by the [Interpress](https://www.interpress.com/) media monitoring company.\r\n\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 760672320,
    "title": "Add arabic dialects",
    "dateCreated": "2020-12-09T21:06:07Z",
    "dateModified": "2020-12-09T21:06:07Z",
    "description": "Data loading script and dataset card for Dialectal Arabic Resources dataset. \r\nFixed git issues from PR  #976",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 760660918,
    "title": "WIP: Vinay/add peer read dataset",
    "dateCreated": "2020-12-09T20:49:52Z",
    "dateModified": "2020-12-09T20:49:52Z",
    "description": "",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 760653971,
    "title": "Add Shrinked Turkish NER from Kaggle.",
    "dateCreated": "2020-12-09T20:38:35Z",
    "dateModified": "2020-12-09T20:38:35Z",
    "description": "Add Shrinked Turkish NER from [Kaggle](https://www.kaggle.com/behcetsenturk/shrinked-twnertc-turkish-ner-data-by-kuzgunlar).",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 760642786,
    "title": "Add Hate Speech and Offensive Language Detection dataset",
    "dateCreated": "2020-12-09T20:22:12Z",
    "dateModified": "2020-12-09T20:22:12Z",
    "description": "Add [Hate Speech and Offensive Language Detection dataset](https://github.com/t-davidson/hate-speech-and-offensive-language) from [this paper](https://arxiv.org/abs/1703.04009).",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 760622133,
    "title": "Adding BioCreative II Gene Mention corpus",
    "dateCreated": "2020-12-09T19:49:28Z",
    "dateModified": "2020-12-09T19:49:28Z",
    "description": "Adding BioCreative II Gene Mention corpus",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 760615090,
    "title": "Add OffComBR",
    "dateCreated": "2020-12-09T19:38:08Z",
    "dateModified": "2020-12-09T19:38:08Z",
    "description": "Add [OffComBR](https://github.com/rogersdepelle/OffComBR) from [Offensive Comments in the Brazilian Web: a dataset and baseline results](https://sol.sbc.org.br/index.php/brasnam/article/view/3260/3222) paper.\r\n\r\nBut I'm having a hard time generating dummy data since the original dataset extion is `.arff` and the [_create_dummy_data function](https://github.com/huggingface/datasets/blob/a4aeaf911240057286a01bff1b1d75a89aedd57b/src/datasets/commands/dummy_data.py#L185) doesn't allow it.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 760607959,
    "title": "Adding the ASSIN dataset",
    "dateCreated": "2020-12-09T19:27:06Z",
    "dateModified": "2020-12-09T19:27:06Z",
    "description": "Adding the ASSIN dataset, a Portuguese language dataset for Natural Language Inference and Semantic Similarity Scoring",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 760606290,
    "title": "2 typos",
    "dateCreated": "2020-12-09T19:24:34Z",
    "dateModified": "2020-12-09T19:24:34Z",
    "description": "Corrected 2 typos",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 760597092,
    "title": "Add penn treebank dataset",
    "dateCreated": "2020-12-09T19:11:33Z",
    "dateModified": "2020-12-09T19:11:33Z",
    "description": "",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 760593932,
    "title": "Adding the ASSIN dataset",
    "dateCreated": "2020-12-09T19:07:00Z",
    "dateModified": "2020-12-09T19:07:00Z",
    "description": "Adding the ASSIN dataset, a Portuguese language dataset for Natural Language Inference and Semantic Similarity Scoring",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 760590589,
    "title": "adding fake-news-english",
    "dateCreated": "2020-12-09T19:02:07Z",
    "dateModified": "2020-12-09T19:02:07Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 760581756,
    "title": "Add Tweet Eval Dataset",
    "dateCreated": "2020-12-09T18:48:57Z",
    "dateModified": "2020-12-09T18:48:57Z",
    "description": "",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 760581330,
    "title": "Add Portuguese Hate Speech dataset",
    "dateCreated": "2020-12-09T18:48:16Z",
    "dateModified": "2020-12-09T18:48:16Z",
    "description": "Binary Portuguese Hate Speech dataset from [this paper](https://www.aclweb.org/anthology/W19-3510/).",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 760578035,
    "title": "Adding TaPaCo Dataset with README.md",
    "dateCreated": "2020-12-09T18:42:58Z",
    "dateModified": "2020-12-09T18:42:58Z",
    "description": "",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 760575473,
    "title": "Add Acronym Identification Dataset",
    "dateCreated": "2020-12-09T18:38:54Z",
    "dateModified": "2020-12-09T18:38:54Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 760571419,
    "title": "Add dataset clickbait_news_bg",
    "dateCreated": "2020-12-09T18:32:12Z",
    "dateModified": "2020-12-09T18:32:12Z",
    "description": "Adding a new dataset - clickbait_news_bg",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 760538325,
    "title": "adding covid-tweets-japanese (again)",
    "dateCreated": "2020-12-09T17:46:46Z",
    "dateModified": "2020-12-09T17:46:46Z",
    "description": "I had mistaken use git rebase, I was so hurried to fix it. However, I didn't fully consider the use of git reset , so I unintendedly stopped PR (#1367) altogether. Sorry about that.\r\nI'll make a new PR.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 760525949,
    "title": "Add reasoning_bg",
    "dateCreated": "2020-12-09T17:30:49Z",
    "dateModified": "2020-12-09T17:30:49Z",
    "description": "Adding reading comprehension dataset for Bulgarian language",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 760514215,
    "title": "Add European Union Education and Culture Translation Memory (EAC-TM) dataset",
    "dateCreated": "2020-12-09T17:14:52Z",
    "dateModified": "2020-12-09T17:14:52Z",
    "description": "Adding the EAC Translation Memory dataset : https://ec.europa.eu/jrc/en/language-technologies/eac-translation-memory",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 760499576,
    "title": "Add HoVer Dataset",
    "dateCreated": "2020-12-09T16:55:39Z",
    "dateModified": "2020-12-09T16:55:39Z",
    "description": "HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification\r\nhttps://arxiv.org/abs/2011.03088 ",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 760497024,
    "title": "Add Neural Code Search Dataset",
    "dateCreated": "2020-12-09T16:52:16Z",
    "dateModified": "2020-12-09T16:52:16Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 760467501,
    "title": "datasets card-creator link added",
    "dateCreated": "2020-12-09T16:15:18Z",
    "dateModified": "2020-12-09T16:15:18Z",
    "description": "dataset card creator link has been added \r\nlink: https://huggingface.co/datasets/card-creator/",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 760455295,
    "title": "initial commit for MultiReQA for second PR",
    "dateCreated": "2020-12-09T16:00:35Z",
    "dateModified": "2020-12-09T16:00:35Z",
    "description": "Since last PR #1349 had some issues passing the tests. So, a new PR is generated.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 760448255,
    "title": "Add WikiSource Dataset",
    "dateCreated": "2020-12-09T15:52:06Z",
    "dateModified": "2020-12-09T15:52:06Z",
    "description": "",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 760436365,
    "title": "Add OfisPublik Dataset",
    "dateCreated": "2020-12-09T15:37:45Z",
    "dateModified": "2020-12-09T15:37:45Z",
    "description": "",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 760436267,
    "title": "Add script_version suggestion when dataset/metric not found",
    "dateCreated": "2020-12-09T15:37:38Z",
    "dateModified": "2020-12-09T15:37:38Z",
    "description": "Adds a helpful prompt to the error message when a dataset/metric is not found, suggesting the user might need to pass `script_version=\"master\"` if the dataset was added recently. The whole error looks like:\r\n\r\n> Couldn't find file locally at blah/blah.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1/metrics/blah/blah.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/met\r\nrics/blah/blah.py.\r\nIf the dataset was added recently, you may need to to pass script_version=\"master\" to find the loading script on the master branch.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 760432261,
    "title": "Add KDE4 Dataset",
    "dateCreated": "2020-12-09T15:32:58Z",
    "dateModified": "2020-12-09T15:32:58Z",
    "description": "",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 760432041,
    "title": "Add MultiParaCrawl Dataset",
    "dateCreated": "2020-12-09T15:32:46Z",
    "dateModified": "2020-12-09T15:32:46Z",
    "description": "",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 760431051,
    "title": "Add SPC Dataset",
    "dateCreated": "2020-12-09T15:31:51Z",
    "dateModified": "2020-12-09T15:31:51Z",
    "description": "",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 760402224,
    "title": "add amazon polarity dataset",
    "dateCreated": "2020-12-09T14:58:21Z",
    "dateModified": "2020-12-09T14:58:21Z",
    "description": "This corresponds to the amazon (binary dataset) requested in https://github.com/huggingface/datasets/issues/353",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 760373136,
    "title": "hind_encorp",
    "dateCreated": "2020-12-09T14:22:59Z",
    "dateModified": "2020-12-09T14:22:59Z",
    "description": "resubmit of hind_encorp file changes",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 760368355,
    "title": "Add LIAR dataset",
    "dateCreated": "2020-12-09T14:16:55Z",
    "dateModified": "2020-12-09T14:16:55Z",
    "description": "Add LIAR dataset from [\u201cLiar, Liar Pants on Fire\u201d: A New Benchmark Dataset for Fake News Detection](https://www.aclweb.org/anthology/P17-2067/).",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 760365505,
    "title": "Add RecipeNLG Dataset (manual download)",
    "dateCreated": "2020-12-09T14:13:19Z",
    "dateModified": "2020-12-09T14:13:19Z",
    "description": "",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 760351405,
    "title": "add best2009",
    "dateCreated": "2020-12-09T13:56:09Z",
    "dateModified": "2020-12-09T13:56:09Z",
    "description": "`best2009` is a Thai word-tokenization dataset from encyclopedia, novels, news and articles by [NECTEC](https://www.nectec.or.th/) (148,995/2,252 lines of train/test). It was created for [BEST 2010: Word Tokenization Competition](https://thailang.nectec.or.th/archive/indexa290.html?q=node/10). The test set answers are not provided publicly.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 760331767,
    "title": "Add News Commentary Dataset",
    "dateCreated": "2020-12-09T13:30:36Z",
    "dateModified": "2020-12-09T13:30:36Z",
    "description": "",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 760331480,
    "title": "added conv ai 2",
    "dateCreated": "2020-12-09T13:30:12Z",
    "dateModified": "2020-12-09T13:30:12Z",
    "description": "Dataset : https://github.com/DeepPavlov/convai/tree/master/2018",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 760325077,
    "title": "adding UNPC",
    "dateCreated": "2020-12-09T13:21:41Z",
    "dateModified": "2020-12-09T13:21:41Z",
    "description": "Adding United Nations Parallel Corpus\r\nhttp://opus.nlpl.eu/UNPC.php",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 760320960,
    "title": "Add twi text c3",
    "dateCreated": "2020-12-09T13:16:38Z",
    "dateModified": "2020-12-09T13:16:38Z",
    "description": "Added Twi texts for training embeddings and language models based on the paper https://www.aclweb.org/anthology/2020.lrec-1.335/",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 760320494,
    "title": "Add Tatoeba Dataset",
    "dateCreated": "2020-12-09T13:16:04Z",
    "dateModified": "2020-12-09T13:16:04Z",
    "description": "",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 760320487,
    "title": "Add yoruba text c3",
    "dateCreated": "2020-12-09T13:16:03Z",
    "dateModified": "2020-12-09T13:16:03Z",
    "description": "Added Yoruba texts for training embeddings and language models based on the paper https://www.aclweb.org/anthology/2020.lrec-1.335/",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 760313108,
    "title": "Add FACTCK.BR dataset",
    "dateCreated": "2020-12-09T13:06:22Z",
    "dateModified": "2020-12-09T13:06:22Z",
    "description": "This PR adds [FACTCK.BR](https://github.com/jghm-f/FACTCK.BR) dataset from [FACTCK.BR: a new dataset to study fake news](https://dl.acm.org/doi/10.1145/3323503.3361698).",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 760309435,
    "title": "adding marathi-wiki dataset",
    "dateCreated": "2020-12-09T13:01:20Z",
    "dateModified": "2020-12-09T13:01:20Z",
    "description": "Adding marathi-wiki-articles dataset. ",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 760309300,
    "title": "Add SETimes Dataset",
    "dateCreated": "2020-12-09T13:01:08Z",
    "dateModified": "2020-12-09T13:01:08Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 760294931,
    "title": "Add OPUS EMEA Dataset",
    "dateCreated": "2020-12-09T12:39:44Z",
    "dateModified": "2020-12-09T12:39:44Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 760288291,
    "title": "Add OPUS Tilde Model Dataset",
    "dateCreated": "2020-12-09T12:29:23Z",
    "dateModified": "2020-12-09T12:29:23Z",
    "description": "",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 760280869,
    "title": "Add OPUS ECB Dataset",
    "dateCreated": "2020-12-09T12:18:22Z",
    "dateModified": "2020-12-09T12:18:22Z",
    "description": "",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 760274046,
    "title": "Add OPUS Books Dataset",
    "dateCreated": "2020-12-09T12:08:49Z",
    "dateModified": "2020-12-09T12:08:49Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 760270116,
    "title": "Adding Scielo",
    "dateCreated": "2020-12-09T12:02:48Z",
    "dateModified": "2020-12-09T12:02:48Z",
    "description": "Adding Scielo: Parallel corpus of full-text articles in Portuguese, English and Spanish from SciELO\r\nhttps://sites.google.com/view/felipe-soares/datasets#h.p_92uSCyAjWSRB",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 760264132,
    "title": "Add OPUS PHP Dataset",
    "dateCreated": "2020-12-09T11:53:30Z",
    "dateModified": "2020-12-09T11:53:30Z",
    "description": "",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 760227776,
    "title": "Use passed --cache_dir for modules cache",
    "dateCreated": "2020-12-09T10:59:59Z",
    "dateModified": "2020-12-09T10:59:59Z",
    "description": "When passed `--cache_dir` arg:\r\n```shell\r\npython datasets-cli test datasets/<my-dataset-folder> --save_infos --all_configs --cache_dir <my-cache-dir>\r\n```\r\nit is not used for caching the modules, which are cached in the default location at `.cache/huggingface/modules`.\r\n\r\nWith this fix, the modules will be cached at `<my-cache-dir>/modules`.",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 760222616,
    "title": "Re-adding narrativeqa dataset",
    "dateCreated": "2020-12-09T10:53:09Z",
    "dateModified": "2020-12-09T10:53:09Z",
    "description": "An update of #309. ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 760208191,
    "title": "adding covid-tweets-japanese",
    "dateCreated": "2020-12-09T10:34:01Z",
    "dateModified": "2020-12-09T10:34:01Z",
    "description": "Adding COVID-19 Japanese Tweets Dataset as part of the sprint.\r\n\r\nTesting with dummy data is not working (the file is said to not exist). Sorry for the incomplete PR.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 760205506,
    "title": "Adding Hope EDI dataset",
    "dateCreated": "2020-12-09T10:30:23Z",
    "dateModified": "2020-12-09T10:30:23Z",
    "description": "",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 760188457,
    "title": "Add Mkqa dataset",
    "dateCreated": "2020-12-09T10:06:33Z",
    "dateModified": "2020-12-09T10:06:33Z",
    "description": "# MKQA: Multilingual Knowledge Questions & Answers Dataset\r\nAdding the [MKQA](https://github.com/apple/ml-mkqa) dataset as part of the sprint \ud83c\udf89\r\n\r\nThere is no official data splits so I added just a `train` split.\r\n \r\ndifferently from the original:\r\n- answer:type field is a ClassLabel (I thought it might be possible to train on this as a label for categorizing questions)\r\n- answer:entity field has a default value of empty string '' (since this key is not available for all in original)\r\n- answer:alias has default value of []\r\n\r\n- [x] All tests passed\r\n- [x] Added dummy data\r\n- [x] Added data card (as much as I could)\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 760164558,
    "title": "Narrative QA (Manual Download Stories) Dataset",
    "dateCreated": "2020-12-09T09:33:59Z",
    "dateModified": "2020-12-09T09:33:59Z",
    "description": "Narrative QA with manual download for stories. ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 760160944,
    "title": "Adding OPUS MultiUN",
    "dateCreated": "2020-12-09T09:29:01Z",
    "dateModified": "2020-12-09T09:29:01Z",
    "description": "Adding UnMulti\r\nhttp://www.euromatrixplus.net/multi-un/",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 760138233,
    "title": "adding opus_infopankki",
    "dateCreated": "2020-12-09T08:57:10Z",
    "dateModified": "2020-12-09T08:57:10Z",
    "description": "Adding opus_infopankki\r\nhttp://opus.nlpl.eu/infopankki-v1.php",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 760101728,
    "title": "adding bprec",
    "dateCreated": "2020-12-09T08:02:45Z",
    "dateModified": "2020-12-09T08:02:45Z",
    "description": "Brand-Product Relation Extraction Corpora in Polish",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 760088419,
    "title": "add wisesight1000",
    "dateCreated": "2020-12-09T07:41:30Z",
    "dateModified": "2020-12-09T07:41:30Z",
    "description": "`wisesight1000` contains Thai social media texts randomly drawn from the full `wisesight-sentiment`, tokenized by human annotators. Out of the labels `neg` (negative), `neu` (neutral), `pos` (positive), `q` (question), 250 samples each. Some texts are removed because they look like spam.Because these samples are representative of real world content, we believe having these annotaed samples will allow the community to robustly evaluate tokenization algorithms.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 760055969,
    "title": "Add JNLPBA",
    "dateCreated": "2020-12-09T06:48:51Z",
    "dateModified": "2020-12-09T06:48:51Z",
    "description": "",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 760031131,
    "title": "Add spider dataset",
    "dateCreated": "2020-12-09T06:06:18Z",
    "dateModified": "2020-12-09T06:06:18Z",
    "description": "This PR adds the Spider dataset, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\r\n\r\nDataset website: https://yale-lily.github.io/spider\r\nPaper link: https://www.aclweb.org/anthology/D18-1425/",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 760023525,
    "title": "Youtube caption corrections",
    "dateCreated": "2020-12-09T05:52:34Z",
    "dateModified": "2020-12-09T05:52:34Z",
    "description": "This PR adds a new dataset of YouTube captions, error and corrections. This dataset was created in just the last week, as inspired by this sprint!",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 759994457,
    "title": "Add StackOverflow StackSample dataset",
    "dateCreated": "2020-12-09T04:59:51Z",
    "dateModified": "2020-12-09T04:59:51Z",
    "description": "This PR adds the StackOverflow StackSample dataset from Kaggle: https://www.kaggle.com/stackoverflow/stacksample\r\n\r\nRan through all of the steps. However, since my dataset requires manually downloading the data, I was unable to run the pytest on the real dataset (the dummy data pytest passed).",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 759994208,
    "title": "Addition of py_ast dataset",
    "dateCreated": "2020-12-09T04:59:17Z",
    "dateModified": "2020-12-09T04:59:17Z",
    "description": "@lhoestq as discussed in PR #1195 ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 759987763,
    "title": "Add TweetQA dataset",
    "dateCreated": "2020-12-09T04:44:01Z",
    "dateModified": "2020-12-09T04:44:01Z",
    "description": "This PR adds the  TweetQA dataset, the first dataset for QA on social media data by leveraging news media and crowdsourcing.\r\n\r\nPaper: https://arxiv.org/abs/1907.06292\r\nRepository: https://tweetqa.github.io/",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 759980004,
    "title": "New instruction for how to generate dataset_infos.json",
    "dateCreated": "2020-12-09T04:24:40Z",
    "dateModified": "2020-12-09T04:24:40Z",
    "description": "Add additional instructions for how to generate dataset_infos.json for manual download datasets. Information courtesy of `Taimur Ibrahim` from the slack channel",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 759978543,
    "title": "change url for prachathai67k to internet archive",
    "dateCreated": "2020-12-09T04:20:37Z",
    "dateModified": "2020-12-09T04:20:37Z",
    "description": "`prachathai67k` is currently downloaded from git-lfs of PyThaiNLP github. Since the size is quite large (~250MB), I moved the URL to archive.org in order to prevent rate limit issues.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 759902770,
    "title": "added craigslist_bargians",
    "dateCreated": "2020-12-09T01:02:31Z",
    "dateModified": "2020-12-09T01:02:31Z",
    "description": "`craigslist_bargains` data set from [here](https://worksheets.codalab.org/worksheets/0x453913e76b65495d8b9730d41c7e0a0c/)\r\n\r\n(Cleaned up version of #1278)",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 759879789,
    "title": "add LeNER-Br dataset",
    "dateCreated": "2020-12-09T00:06:38Z",
    "dateModified": "2020-12-09T00:06:38Z",
    "description": "Adding the LeNER-Br dataset, a Portuguese language dataset for named entity recognition ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 759870664,
    "title": "initial commit for MultiReQA ",
    "dateCreated": "2020-12-08T23:44:34Z",
    "dateModified": "2020-12-08T23:44:34Z",
    "description": "Added MultiReQA, which is a dataset containing the sentence boundary annotation from eight publicly available QA datasets including SearchQA, TriviaQA, HotpotQA, NaturalQuestions, SQuAD, BioASQ, RelationExtraction, and TextbookQA. ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 759869849,
    "title": "add Yoruba NER dataset",
    "dateCreated": "2020-12-08T23:42:35Z",
    "dateModified": "2020-12-08T23:42:35Z",
    "description": "Added Yoruba GV dataset based on this paper",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 759845231,
    "title": "Add spanish billion words corpus",
    "dateCreated": "2020-12-08T22:51:38Z",
    "dateModified": "2020-12-08T22:51:38Z",
    "description": "Add an unannotated Spanish corpus of nearly 1.5 billion words, compiled from different resources from the web.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 759844137,
    "title": "Add MultiBooked dataset",
    "dateCreated": "2020-12-08T22:49:36Z",
    "dateModified": "2020-12-08T22:49:36Z",
    "description": "Add dataset.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 759835486,
    "title": "First commit of NarrativeQA Dataset",
    "dateCreated": "2020-12-08T22:31:59Z",
    "dateModified": "2020-12-08T22:31:59Z",
    "description": "Added NarrativeQA dataset and included a manual downloading option to download scripts from the original scripts provided by the authors. ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 759831925,
    "title": "Add hausa ner corpus",
    "dateCreated": "2020-12-08T22:25:04Z",
    "dateModified": "2020-12-08T22:25:04Z",
    "description": "Added Hausa VOA NER data ",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 759809999,
    "title": "Add LiveQA",
    "dateCreated": "2020-12-08T21:52:36Z",
    "dateModified": "2020-12-08T21:52:36Z",
    "description": "This PR adds LiveQA, the Chinese real-time/timeline-based QA task by [Liu et al., 2020](https://arxiv.org/pdf/2010.00526.pdf). ",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 759794121,
    "title": "[yaml] Fix metadata according to pre-specified scheme",
    "dateCreated": "2020-12-08T21:26:34Z",
    "dateModified": "2020-12-08T21:26:34Z",
    "description": "@lhoestq @yjernite ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 759784557,
    "title": "added references to only data card creator to all guides",
    "dateCreated": "2020-12-08T21:11:11Z",
    "dateModified": "2020-12-08T21:11:11Z",
    "description": "We can now use the wonderful online form for dataset cards created by @evrardts ",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 759765408,
    "title": ":fist: \u00a1Viva la Independencia!",
    "dateCreated": "2020-12-08T20:43:43Z",
    "dateModified": "2020-12-08T20:43:43Z",
    "description": "Adds the Catalonia Independence Corpus for stance-detection of Tweets.\r\n\r\nReady for review!",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 759744088,
    "title": "hate_speech_18 initial commit",
    "dateCreated": "2020-12-08T20:10:08Z",
    "dateModified": "2020-12-08T20:10:08Z",
    "description": "",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 759725770,
    "title": "Add GigaFren Dataset",
    "dateCreated": "2020-12-08T19:42:04Z",
    "dateModified": "2020-12-08T19:42:04Z",
    "description": "",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 759710482,
    "title": "Add spanish billion words",
    "dateCreated": "2020-12-08T19:18:02Z",
    "dateModified": "2020-12-08T19:18:02Z",
    "description": "Add an unannotated corpus of the Spanish language of nearly 1.5 billion words, compiled from different resources from the web.\r\n\r\nThe dataset needs 10 GB (download: 1.89 GiB, generated: 8.34 GiB, post-processed: Unknown size, total: 10.22 GiB), the test using dummy data pass but my laptop isn't able to run it on the real data (I left it running for over 8 hours and it didn't finish).",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 759706932,
    "title": "Add dataset Yoruba BBC Topic Classification",
    "dateCreated": "2020-12-08T19:12:18Z",
    "dateModified": "2020-12-08T19:12:18Z",
    "description": "Added new dataset Yoruba BBC Topic Classification\r\n\r\nContains loading script as well as dataset card including YAML tags.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 759705835,
    "title": "Added Bianet dataset",
    "dateCreated": "2020-12-08T19:10:32Z",
    "dateModified": "2020-12-08T19:10:32Z",
    "description": "Hi :hugs:, This is a PR for [Bianet: A parallel news corpus in Turkish, Kurdish and English; Source](http://opus.nlpl.eu/Bianet.php) dataset",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 759699993,
    "title": "Add QED Amara Dataset",
    "dateCreated": "2020-12-08T19:01:13Z",
    "dateModified": "2020-12-08T19:01:13Z",
    "description": "",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 759687836,
    "title": "Add Tanzil Dataset",
    "dateCreated": "2020-12-08T18:45:15Z",
    "dateModified": "2020-12-08T18:45:15Z",
    "description": "",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 759679135,
    "title": "Add Open Subtitles Dataset",
    "dateCreated": "2020-12-08T18:31:45Z",
    "dateModified": "2020-12-08T18:31:45Z",
    "description": "",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 759677189,
    "title": "First version of the new dataset hausa_voa_topics",
    "dateCreated": "2020-12-08T18:28:52Z",
    "dateModified": "2020-12-08T18:28:52Z",
    "description": "Contains loading script as well as dataset card including YAML tags.\r\n\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 759657324,
    "title": "added un_ga dataset",
    "dateCreated": "2020-12-08T17:58:38Z",
    "dateModified": "2020-12-08T17:58:38Z",
    "description": "Hi :hugs:, This is a PR for [United nations general assembly resolutions: A six-language parallel corpus](http://opus.nlpl.eu/UN.php) dataset",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 759654174,
    "title": "Add yoruba ner corpus",
    "dateCreated": "2020-12-08T17:54:00Z",
    "dateModified": "2020-12-08T17:54:00Z",
    "description": "",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 759634907,
    "title": "Added the NewsPH Raw dataset and corresponding dataset card",
    "dateCreated": "2020-12-08T17:25:45Z",
    "dateModified": "2020-12-08T17:25:45Z",
    "description": "This PR adds the original NewsPH dataset which is used to autogenerate the NewsPH-NLI dataset. Reopened a new PR as the previous one had problems.\r\n\r\nPaper: https://arxiv.org/abs/2010.11574\r\nRepo: https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 759629321,
    "title": "Add msr_genomics_kbcomp dataset",
    "dateCreated": "2020-12-08T17:18:20Z",
    "dateModified": "2020-12-08T17:18:20Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 759611784,
    "title": "TEP: Tehran English-Persian parallel corpus",
    "dateCreated": "2020-12-08T16:56:53Z",
    "dateModified": "2020-12-08T16:56:53Z",
    "description": "TEP: Tehran English-Persian parallel corpus\r\nmore info : http://opus.nlpl.eu/TEP.php",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 759595556,
    "title": "Add humicroedit dataset",
    "dateCreated": "2020-12-08T16:35:46Z",
    "dateModified": "2020-12-08T16:35:46Z",
    "description": "Pull request for adding humicroedit dataset",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 759587864,
    "title": "\u2753 Sharing ElasticSearch indexed dataset ",
    "dateCreated": "2020-12-08T16:25:58Z",
    "dateModified": "2020-12-08T16:25:58Z",
    "description": "Hi there,\r\n\r\nFirst of all, thank you very much for this amazing library. Datasets have become my preferred data structure for basically everything I am currently doing.\r\n\r\n**Question:** I'm working with a dataset and I have an elasticsearch container running at localhost:9200. I added an elasticsearch index and I was wondering\r\n\r\n- how can I know where it has been saved?\r\n\r\n- how can I share the indexed dataset with others?\r\n\r\nI tried to dig into the docs, but could not find anything about that.\r\n\r\nThank you very much for your help.\r\n\r\nBest,\r\nPietro\r\n\r\nEdit: apologies for the wrong label",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 759581919,
    "title": "Add CC-News dataset of English language articles",
    "dateCreated": "2020-12-08T16:18:15Z",
    "dateModified": "2020-12-08T16:18:15Z",
    "description": "Adds [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/) dataset. It contains 708241 English language news articles. Although each article has a language field these tags are not reliable. I've used Spacy language detection [pipeline](https://spacy.io/universe/project/spacy-langdetect) to confirm that the article language is indeed English. \r\n\r\nThe prepared dataset is temporarily hosted on my private Google Storage [bucket](https://storage.googleapis.com/hf_datasets/cc_news.tar.gz). We can move it to HF storage and update this PR before merging. ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 759576003,
    "title": "add indonlu benchmark datasets",
    "dateCreated": "2020-12-08T16:10:58Z",
    "dateModified": "2020-12-08T16:10:58Z",
    "description": "The IndoNLU benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems for the Indonesian language. There are 12 datasets in IndoNLU.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 759573610,
    "title": "added dutch_social",
    "dateCreated": "2020-12-08T16:07:54Z",
    "dateModified": "2020-12-08T16:07:54Z",
    "description": "The Dutch social media tweets dataset. Which has a total of more than 210k tweets in dutch language. These tweets have been machine annotated with sentiment scores (`label` feature) and `industry` and `hisco_codes`\r\n\r\nIt can be used for sentiment analysis, multi-label classification and entity tagging",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 759566148,
    "title": "Added the WikiText-TL39 dataset and corresponding card",
    "dateCreated": "2020-12-08T16:00:26Z",
    "dateModified": "2020-12-08T16:00:26Z",
    "description": "This PR adds the WikiText-TL-39 Filipino Language Modeling dataset. Restarted a new pull request since there were problems with the earlier one.\r\n\r\nPaper: https://arxiv.org/abs/1907.00409\r\nRepo: https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 759565923,
    "title": "adding wili-2018 language identification dataset",
    "dateCreated": "2020-12-08T16:00:09Z",
    "dateModified": "2020-12-08T16:00:09Z",
    "description": "",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 759565629,
    "title": "ethos first commit",
    "dateCreated": "2020-12-08T15:59:47Z",
    "dateModified": "2020-12-08T15:59:47Z",
    "description": "Ethos passed all the tests except from this one: \r\nRUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_dataset_all_configs_<your-dataset-name>\r\n\r\nwith this error: \r\nE               OSError: Cannot find data file. \r\nE               Original error:\r\nE               [Errno 2] No such file or directory: ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 759553495,
    "title": "add 10k German News Article Dataset",
    "dateCreated": "2020-12-08T15:44:25Z",
    "dateModified": "2020-12-08T15:44:25Z",
    "description": "",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 759549601,
    "title": "Allow GitHub releases as dataset source",
    "dateCreated": "2020-12-08T15:39:35Z",
    "dateModified": "2020-12-08T15:39:35Z",
    "description": "# Summary\r\n\r\nProviding a GitHub release URL to `DownloadManager.download()` currently throws a `ConnectionError: Couldn't reach [DOWNLOAD_URL]`. This PR fixes this problem by adding an exception for GitHub releases in `datasets.utils.file_utils.get_from_cache()`.\r\n\r\n# Reproduce\r\n\r\n```\r\nimport datasets\r\nurl = 'http://github.com/benjaminvdb/DBRD/releases/download/v3.0/DBRD_v3.tgz'\r\nresult = datasets.utils.file_utils.get_from_cache(url)\r\n\r\n# Returns: ConnectionError: Couldn't reach http://github.com/benjaminvdb/DBRD/releases/download/v3.0/DBRD_v3.tgz\r\n```\r\n\r\n# Cause\r\n\r\nGitHub releases returns a HTTP status 403 (FOUND), indicating that the request is being redirected (to AWS S3, in this case). `get_from_cache()` checks whether the status is 200 (OK) or if it is part of two exceptions (Google Drive or Firebase), otherwise the mentioned error is thrown.\r\n\r\n# Solution\r\n\r\nJust like the exceptions for Google Drive and Firebase, add a condition for GitHub releases URLs that return the HTTP status 403. If this is the case, continue normally.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 759548706,
    "title": "add yelp_review_full",
    "dateCreated": "2020-12-08T15:38:27Z",
    "dateModified": "2020-12-08T15:38:27Z",
    "description": "This corresponds to the Yelp-5 requested in https://github.com/huggingface/datasets/issues/353\r\nI included the dataset card. ",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 759541937,
    "title": "Add snips built in intents 2016 12",
    "dateCreated": "2020-12-08T15:30:19Z",
    "dateModified": "2020-12-08T15:30:19Z",
    "description": "This PR proposes to add the Snips.ai built in intents dataset. The first configuration added is for the intent labels only, but the dataset includes entity slots that may in future be added as alternate configurations.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 759536512,
    "title": "Add HateSpeech Corpus for Polish",
    "dateCreated": "2020-12-08T15:23:53Z",
    "dateModified": "2020-12-08T15:23:53Z",
    "description": "This PR adds a HateSpeech Corpus for Polish, containing offensive language examples.\r\n\r\n- **Homepage:** http://zil.ipipan.waw.pl/HateSpeech\r\n- **Paper:** http://www.qualitativesociologyreview.org/PL/Volume38/PSJ_13_2_Troszynski_Wawer.pdf",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 759532626,
    "title": "Jigsaw toxicity pred",
    "dateCreated": "2020-12-08T15:19:14Z",
    "dateModified": "2020-12-08T15:19:14Z",
    "description": "Requires manually downloading data from Kaggle.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 759514819,
    "title": "Add OPUS Bible Corpus (102 Languages)",
    "dateCreated": "2020-12-08T14:57:08Z",
    "dateModified": "2020-12-08T14:57:08Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 759508921,
    "title": "Add OffensEval-TR 2020 Dataset",
    "dateCreated": "2020-12-08T14:49:51Z",
    "dateModified": "2020-12-08T14:49:51Z",
    "description": "This PR adds the OffensEval-TR 2020 dataset which is a Turkish offensive language corpus by me and @basakbuluz. The corpus consist of randomly sampled tweets and annotated in a similar way to [OffensEval](https://sites.google.com/site/offensevalsharedtask/) and [GermEval](https://projects.fzai.h-da.de/iggsa/).\r\n\r\n- **Homepage:** [offensive-turkish](https://coltekin.github.io/offensive-turkish/)\r\n- **Paper:** [A Corpus of Turkish Offensive Language on Social Media](https://coltekin.github.io/offensive-turkish/troff.pdf)\r\n- **Point of Contact:** [\u00c7a\u011fr\u0131 \u00c7\u00f6ltekin](ccoltekin@sfs.uni-tuebingen.de)",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 759501370,
    "title": "Add SAMSum Corpus dataset",
    "dateCreated": "2020-12-08T14:40:56Z",
    "dateModified": "2020-12-08T14:40:56Z",
    "description": "Did not spent much time writing README, might update later.\r\n\r\nCopied description and some stuff from tensorflow_datasets\r\nhttps://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/summarization/samsum.py",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 759492953,
    "title": "Add Wiki Lingua Dataset",
    "dateCreated": "2020-12-08T14:30:13Z",
    "dateModified": "2020-12-08T14:30:13Z",
    "description": "Hello,\r\n\r\nThis is my first PR. \r\n\r\nI have added Wiki Lingua Dataset along with dataset card to the best of my knowledge.\r\nThere was one hiccup though. I was unable to create dummy data because the data is in pkl format.\r\nFrom the document, I see that:\r\n```At the moment it supports data files in the following format: txt, csv, tsv, jsonl, json, xml```\r\n\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 759458835,
    "title": "adding capes",
    "dateCreated": "2020-12-08T13:46:13Z",
    "dateModified": "2020-12-08T13:46:13Z",
    "description": "Adding Parallel corpus of theses and dissertation abstracts in Portuguese and English from CAPES\r\nhttps://sites.google.com/view/felipe-soares/datasets#h.p_kxOR6EhHm2a6",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 759448427,
    "title": "add W&I + LOCNESS dataset (BEA-2019 workshop shared task on GEC)",
    "dateCreated": "2020-12-08T13:31:34Z",
    "dateModified": "2020-12-08T13:31:34Z",
    "description": "- **Name:** W&I + LOCNESS dataset (from the BEA-2019 workshop shared task on GEC)\r\n- **Description:** https://www.cl.cam.ac.uk/research/nl/bea2019st/#data\r\n- **Paper:** https://www.aclweb.org/anthology/W19-4406/\r\n- **Motivation:** This is a recent dataset (actually two in one) for grammatical error correction and is used for benchmarking in this field of NLP.\r\n\r\n### Checkbox\r\n\r\n- [x] Create the dataset script `/datasets/my_dataset/my_dataset.py` using the template\r\n- [x] Fill the `_DESCRIPTION` and `_CITATION` variables\r\n- [x] Implement `_infos()`, `_split_generators()` and `_generate_examples()`\r\n- [x] Make sure that the `BUILDER_CONFIGS` class attribute is filled with the different configurations of the dataset and that the `BUILDER_CONFIG_CLASS` is specified if there is a custom config class.\r\n- [x] Generate the metadata file `dataset_infos.json` for all configurations\r\n- [x] Generate the dummy data `dummy_data.zip` files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card `README.md` using the template : fill the tags and the various paragraphs\r\n- [x] Both tests for the real data and the dummy data pass.\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 759446665,
    "title": "[README] Added Windows command to enable slow tests",
    "dateCreated": "2020-12-08T13:29:04Z",
    "dateModified": "2020-12-08T13:29:04Z",
    "description": "The Windows command to run slow tests has caused issues, so this adds a functional Windows command.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 759440841,
    "title": "adding eitb_parcc",
    "dateCreated": "2020-12-08T13:20:54Z",
    "dateModified": "2020-12-08T13:20:54Z",
    "description": "Adding EiTB-ParCC: Parallel Corpus of Comparable News\r\nhttp://opus.nlpl.eu/EiTB-ParCC.php",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 759440484,
    "title": "adding opus_openoffice",
    "dateCreated": "2020-12-08T13:20:21Z",
    "dateModified": "2020-12-08T13:20:21Z",
    "description": "Adding Opus OpenOffice: http://opus.nlpl.eu/OpenOffice.php\r\n8 languages, 28 bitexts",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 759435740,
    "title": "Add Danish NER dataset",
    "dateCreated": "2020-12-08T13:13:54Z",
    "dateModified": "2020-12-08T13:13:54Z",
    "description": "",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 759419945,
    "title": "arxiv dataset added",
    "dateCreated": "2020-12-08T12:50:51Z",
    "dateModified": "2020-12-08T12:50:51Z",
    "description": "**adding arXiv dataset**: arXiv dataset and metadata of 1.7M+ scholarly papers across STEM\r\ndataset link: https://www.kaggle.com/Cornell-University/arxiv",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 759418122,
    "title": "added dutch_social",
    "dateCreated": "2020-12-08T12:47:50Z",
    "dateModified": "2020-12-08T12:47:50Z",
    "description": "WIP \r\nAs some tests did not clear! \ud83d\udc4e\ud83c\udffc ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 759414566,
    "title": "can't load \"german_legal_entity_recognition\" dataset",
    "dateCreated": "2020-12-08T12:42:01Z",
    "dateModified": "2020-12-08T12:42:01Z",
    "description": "FileNotFoundError: Couldn't find file locally at german_legal_entity_recognition/german_legal_entity_recognition.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/german_legal_entity_recognition/german_legal_entity_recognition.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/german_legal_entity_recognition/german_legal_entity_recognition.py\r\n",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 759412451,
    "title": "Add OPUS Ted Talks 2013",
    "dateCreated": "2020-12-08T12:38:38Z",
    "dateModified": "2020-12-08T12:38:38Z",
    "description": "",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 759404103,
    "title": "OPUS Ted Talks 2013",
    "dateCreated": "2020-12-08T12:25:39Z",
    "dateModified": "2020-12-08T12:25:39Z",
    "description": "",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 759375292,
    "title": "The Snips Built In Intents 2016 dataset.",
    "dateCreated": "2020-12-08T11:40:10Z",
    "dateModified": "2020-12-08T11:40:10Z",
    "description": "This PR proposes to add the Snips.ai built in intents dataset. The first configuration added is for the intent labels only, but the dataset includes entity slots that may in future be added as alternate configurations.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 759375251,
    "title": "add hrenwac_para",
    "dateCreated": "2020-12-08T11:40:06Z",
    "dateModified": "2020-12-08T11:40:06Z",
    "description": "",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 759365246,
    "title": "adding opus_euconst",
    "dateCreated": "2020-12-08T11:24:16Z",
    "dateModified": "2020-12-08T11:24:16Z",
    "description": "Adding EUconst, a parallel corpus collected from the European Constitution.\r\n21 languages, 210 bitexts",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 759360113,
    "title": "add hrenwac_para",
    "dateCreated": "2020-12-08T11:16:41Z",
    "dateModified": "2020-12-08T11:16:41Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 759354627,
    "title": "arXiv dataset added",
    "dateCreated": "2020-12-08T11:08:28Z",
    "dateModified": "2020-12-08T11:08:28Z",
    "description": "",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 759352810,
    "title": "adding pubmed_qa dataset",
    "dateCreated": "2020-12-08T11:05:44Z",
    "dateModified": "2020-12-08T11:05:44Z",
    "description": "Pubmed QA dataset:\r\nPQA-L(abeled) 1k\r\nPQA-U(labeled) 61.2k\r\nPQA-A(rtifical labeled) 211.3k",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 759339989,
    "title": "imdb dataset cannot be downloaded",
    "dateCreated": "2020-12-08T10:47:36Z",
    "dateModified": "2020-12-08T10:47:36Z",
    "description": "hi\r\nplease find error below getting imdb train spli:\r\nthanks\r\n\r\n`\r\ndatasets.load_dataset>>> datasets.load_dataset(\"imdb\", split=\"train\")`\r\n\r\n\r\nerrors\r\n\r\n\r\n```\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\nDownloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.06 MiB, post-processed: Unknown size, total: 207.28 MiB) to /idiap/temp/rkarimi/cache_home_1/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3...\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets/downloads\r\nTraceback (most recent call last):        \r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 558, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/info_utils.py\", line 73, in verify_splits\r\n    raise NonMatchingSplitsSizesError(str(bad_splits))\r\ndatasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='unsupervised', num_bytes=67125548, num_examples=50000, dataset_name='imdb'), 'recorded': SplitInfo(name='unsupervised', num_bytes=7486451, num_examples=5628, dataset_name='imdb')}]\r\n\r\n\r\n```",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 759333684,
    "title": "Jigsaw toxicity classification dataset added",
    "dateCreated": "2020-12-08T10:38:51Z",
    "dateModified": "2020-12-08T10:38:51Z",
    "description": "The dataset requires manually downloading data from Kaggle.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 759309457,
    "title": "Add CodeSearchNet corpus dataset",
    "dateCreated": "2020-12-08T10:07:50Z",
    "dateModified": "2020-12-08T10:07:50Z",
    "description": "This PR adds the CodeSearchNet corpus proxy dataset for semantic code search: https://github.com/github/CodeSearchNet\r\nI have had a few issues, mentioned below. Would appreciate some help on how to solve them.\r\n\r\n## Issues generating dataset card\r\nIs there something wrong with my declaration of the dataset features ?\r\n```\r\nfeatures=datasets.Features(\r\n    {\r\n        \"repository_name\": datasets.Value(\"string\"),\r\n        \"func_path_in_repository\": datasets.Value(\"string\"),\r\n        \"func_name\": datasets.Value(\"string\"),\r\n        \"whole_func_string\": datasets.Value(\"string\"),\r\n        \"language\": datasets.Value(\"string\"),\r\n        \"func_code_string\": datasets.Value(\"string\"),\r\n        \"func_code_tokens\": datasets.Sequence(datasets.Value(\"string\")),\r\n        \"func_documentation_string\": datasets.Value(\"string\"),\r\n        \"func_documentation_tokens\": datasets.Sequence(datasets.Value(\"string\")),\r\n        \"split_name\": datasets.Value(\"string\"),\r\n        \"func_code_url\": datasets.Value(\"string\"),\r\n        # TODO - add licensing info in the examples\r\n    }\r\n),\r\n\r\n```\r\nWhen running the streamlite app for tagging the dataset on my machine, I get the following error :\r\n![image](https://user-images.githubusercontent.com/33657802/101469132-9ed12c80-3944-11eb-94ff-2d9c1d0ea080.png)\r\n\r\n\r\n## Issues with dummy data\r\nDue to the unusual structure of the data, I have been unable to generate dummy data automatically.\r\nI tried to generate it manually, but pytests fail when using the manually-generated dummy data ! Pytests work fine when using the real data.\r\n```\r\n============================================================================================== test session starts ==============================================================================================\r\nplatform linux -- Python 3.7.9, pytest-6.1.2, py-1.9.0, pluggy-0.13.1\r\nplugins: xdist-2.1.0, forked-1.3.0\r\ncollected 1 item\r\n\r\ntests/test_dataset_common.py F                                                                                                                                                                            [100%]\r\n\r\n=================================================================================================== FAILURES ====================================================================================================\r\n________________________________________________________________________ LocalDatasetTest.test_load_dataset_all_configs_code_search_net _________________________________________________________________________\r\nself = <tests.test_dataset_common.LocalDatasetTest testMethod=test_load_dataset_all_configs_code_search_net>, dataset_name = 'code_search_net'\r\n\r\n    @slow\r\n    def test_load_dataset_all_configs(self, dataset_name):\r\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)\r\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True)\r\n\r\ntests/test_dataset_common.py:237:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\ntests/test_dataset_common.py:198: in check_load_dataset\r\n    self.parent.assertTrue(len(dataset[split]) > 0)\r\nE   AssertionError: False is not true\r\n--------------------------------------------------------------------------------------------- Captured stdout call ----------------------------------------------------------------------------------------------\r\nDownloading and preparing dataset code_search_net/all (download: 1.00 MiB, generated: 1.00 MiB, post-processed: Unknown size, total: 2.00 MiB) to /tmp/tmppx78sj24/code_search_net/all/1.0.0...\r\nDataset code_search_net downloaded and prepared to /tmp/tmppx78sj24/code_search_net/all/1.0.0. Subsequent calls will reuse this data.\r\n--------------------------------------------------------------------------------------------- Captured stderr call ----------------------------------------------------------------------------------------------\r\n... (irrelevant info - Deprecation warnings)\r\n============================================================================================ short test summary info ============================================================================================\r\nFAILED tests/test_dataset_common.py::LocalDatasetTest::test_load_dataset_all_configs_code_search_net - AssertionError: False is not true\r\n========================================================================================= 1 failed, 4 warnings in 3.00s ========================================================================================\r\n```\r\n\r\n\r\n## Note : Data structure in S3\r\nThe data is stored on S3, and organized by programming languages.\r\nIt is stored in the following repository structure:\r\n```\r\n.\r\n\u251c\u2500\u2500 <language_name>  # e.g. python\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 final\r\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 jsonl\r\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 test\r\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 <language_name>_test_0.jsonl.gz\r\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 train\r\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 <language_name>_train_0.jsonl.gz\r\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 <language_name>_train_1.jsonl.gz\r\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\r\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 <language_name>_train_n.jsonl.gz\r\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 valid\r\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 <language_name>_valid_0.jsonl.gz\r\n\u251c\u2500\u2500 <language_name>_dedupe_definitions_v2.pkl\r\n\u2514\u2500\u2500 <language_name>_licenses.pkl\r\n```",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 759300992,
    "title": "'iwslt2017-ro-nl', cannot be downloaded ",
    "dateCreated": "2020-12-08T09:56:55Z",
    "dateModified": "2020-12-08T09:56:55Z",
    "description": "Hi\r\nI am trying \r\n`>>> datasets.load_dataset(\"iwslt2017\", 'iwslt2017-ro-nl', split=\"train\")`\r\n\r\ngetting this error thank you for your help\r\n```\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\nDownloading and preparing dataset iwsl_t217/iwslt2017-ro-nl (download: 314.07 MiB, generated: 39.92 MiB, post-processed: Unknown size, total: 354.00 MiB) to /idiap/temp/rkarimi/cache_home_1/datasets/iwsl_t217/iwslt2017-ro-nl/1.0.0/cca6935a0851a8ceac1202a62c958738bdfa23c57a51bc52ac1c5ebd2aa172cd...\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets/downloads\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 531, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \" /idiap/home/rkarimi/.cache/huggingface/modules/datasets_modules/datasets/iwslt2017/cca6935a0851a8ceac1202a62c958738bdfa23c57a51bc52ac1c5ebd2aa172cd/iwslt2017.py\", line 118, in _split_generators\r\n    dl_dir = dl_manager.download_and_extract(MULTI_URL)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 254, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 179, in download\r\n    num_proc=download_config.num_proc,\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 216, in map_nested\r\n    return function(data_struct)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 477, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https://wit3.fbk.eu/archive/2017-01-trnmted//texts/DeEnItNlRo/DeEnItNlRo/DeEnItNlRo-DeEnItNlRo.tgz\r\n\r\n```",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 759291509,
    "title": "[libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0):  terminate called after throwing an instance of 'google::protobuf::FatalException'   what():  CHECK failed: (index) >= (0):  Aborted",
    "dateCreated": "2020-12-08T09:44:15Z",
    "dateModified": "2020-12-08T09:44:15Z",
    "description": "Hi\r\nI am getting this error when evaluating on wmt16-ro-en using finetune_trainer.py of huggingface repo. thank for your help\r\n\r\n{'epoch': 20.0}                                                                                                                                             \r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:16<00:00,  1.22it/s]\r\n12/08/2020 10:41:19 - INFO - seq2seq.trainers.trainer -   Saving model checkpoint to outputs/experiment/joint/finetune/lr-2e-5\r\n12/08/2020 10:41:24 - INFO - __main__ -   {'wmt16-en-ro': Dataset(features: {'src_texts': Value(dtype='string', id=None), 'task': Value(dtype='string', id=None), 'tgt_texts': Value(dtype='string', id=None)}, num_rows: 1998), 'qnli': Dataset(features: {'src_texts': Value(dtype='string', id=None), 'task': Value(dtype='string', id=None), 'tgt_texts': Value(dtype='string', id=None)}, num_rows: 5462), 'scitail': Dataset(features: {'src_texts': Value(dtype='string', id=None), 'task': Value(dtype='string', id=None), 'tgt_texts': Value(dtype='string', id=None)}, num_rows: 1303)}\r\n12/08/2020 10:41:24 - INFO - __main__ -   *** Evaluate ***\r\n12/08/2020 10:41:24 - INFO - seq2seq.utils.utils -   using task specific params for wmt16-en-ro: {'max_length': 300, 'num_beams': 4}\r\n12/08/2020 10:41:24 - INFO - seq2seq.trainers.trainer -   ***** Running Evaluation *****\r\n12/08/2020 10:41:24 - INFO - seq2seq.trainers.trainer -     Num examples = 1998\r\n12/08/2020 10:41:24 - INFO - seq2seq.trainers.trainer -     Batch size = 64\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [00:37<00:00,  1.19s/it][libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0): \r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: (index) >= (0): \r\nAborted\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 759278758,
    "title": "boolq does not work ",
    "dateCreated": "2020-12-08T09:28:47Z",
    "dateModified": "2020-12-08T09:28:47Z",
    "description": "Hi\r\nI am getting this error when trying to load boolq, thanks for your help\r\n\r\nts_boolq_default_0.1.0_2987db1f15deaa19500ae24de560eabeaf1f8ef51df88c0470beeec72943bf11.lock\r\nTraceback (most recent call last):\r\n  File \"finetune_t5_trainer.py\", line 274, in <module>\r\n    main()\r\n  File \"finetune_t5_trainer.py\", line 147, in main\r\n    for task in data_args.tasks]\r\n  File \"finetune_t5_trainer.py\", line 147, in <listcomp>\r\n    for task in data_args.tasks]\r\n  File \"/remote/idiap.svm/user.active/rkarimi/dev/ruse/seq2seq/tasks/tasks.py\", line 58, in get_dataset\r\n    dataset = self.load_dataset(split=split)\r\n  File \"/remote/idiap.svm/user.active/rkarimi/dev/ruse/seq2seq/tasks/tasks.py\", line 54, in load_dataset\r\n    return datasets.load_dataset(self.task.name, split=split)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 531, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \" /idiap/home/rkarimi/.cache/huggingface/modules/datasets_modules/datasets/boolq/2987db1f15deaa19500ae24de560eabeaf1f8ef51df88c0470beeec72943bf11/boolq.py\", line 74, in _split_generators\r\n    downloaded_files = dl_manager.download_custom(urls_to_download, tf.io.gfile.copy)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 149, in download_custom\r\n    custom_download(url, path)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 516, in copy_v2\r\n    compat.path_to_bytes(src), compat.path_to_bytes(dst), overwrite)\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: file already exists\r\n\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 759269920,
    "title": "Update coqa dataset url",
    "dateCreated": "2020-12-08T09:16:38Z",
    "dateModified": "2020-12-08T09:16:38Z",
    "description": "`datasets.stanford.edu` is invalid.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 759251457,
    "title": "Add dutch book review dataset",
    "dateCreated": "2020-12-08T08:50:48Z",
    "dateModified": "2020-12-08T08:50:48Z",
    "description": "- Name: Dutch Book Review Dataset (DBRD)\r\n- Description: The DBRD (pronounced dee-bird) dataset contains over 110k book reviews along with associated binary sentiment polarity labels and is intended as a benchmark for sentiment classification in Dutch.\r\n- Paper: https://arxiv.org/abs/1910.00896\r\n- Data: https://github.com/benjaminvdb/DBRD\r\n- Motivation: A large (real-life) dataset of Dutch book reviews and sentiment polarity (positive/negative), based on the associated rating.\r\n\r\nChecks\r\n- [x] Create the dataset script /datasets/dbrd/dbrd.py using the template\r\n- [x] Fill the _DESCRIPTION and _CITATION variables\r\n- [x] Implement _info(), _split_generators() and _generate_examples()\r\n- [x] Make sure that the BUILDER_CONFIGS class attribute is filled with the different configurations of the dataset and that the BUILDER_CONFIG_CLASS is specified if there is a custom config class.\r\n- [x] Generate the metadata file dataset_infos.json for all configurations\r\n- [x] Generate the dummy data dummy_data.zip files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card README.md using the template : fill the tags and the various paragraphs\r\n- [x] Both tests for the real data and the dummy data pass.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 759208335,
    "title": "add thaiqa_squad",
    "dateCreated": "2020-12-08T08:14:38Z",
    "dateModified": "2020-12-08T08:14:38Z",
    "description": "Example format is a little different from SQuAD since `thaiqa` always have one answer per question so I added a check to convert answers to lists if they are not already one to future-proof additional questions that might have multiple answers.\r\n\r\n`thaiqa_squad` is an open-domain, extractive question answering dataset (4,000 questions in `train` and 74 questions in `dev`) in [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) format, originally created by [NECTEC](https://www.nectec.or.th/en/) from Wikipedia articles and adapted to [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) format by [PyThaiNLP](https://github.com/PyThaiNLP/).",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 759203317,
    "title": "adding hybrid_qa",
    "dateCreated": "2020-12-08T08:10:19Z",
    "dateModified": "2020-12-08T08:10:19Z",
    "description": "Adding  HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data\r\nhttps://github.com/wenhuchen/HybridQA",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 759151028,
    "title": "disaster response messages dataset",
    "dateCreated": "2020-12-08T07:27:16Z",
    "dateModified": "2020-12-08T07:27:16Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 759108726,
    "title": "added para_pat",
    "dateCreated": "2020-12-08T06:28:47Z",
    "dateModified": "2020-12-08T06:28:47Z",
    "description": "Dataset link : https://figshare.com/articles/ParaPat_The_Multi-Million_Sentences_Parallel_Corpus_of_Patents_Abstracts/12627632\r\nWorking on README.md currently",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 758988465,
    "title": "Craigslist bargains",
    "dateCreated": "2020-12-08T01:45:55Z",
    "dateModified": "2020-12-08T01:45:55Z",
    "description": "`craigslist_bargains` dataset from [here](https://worksheets.codalab.org/worksheets/0x453913e76b65495d8b9730d41c7e0a0c/)",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 758965936,
    "title": "add One Million Posts Corpus",
    "dateCreated": "2020-12-08T00:50:08Z",
    "dateModified": "2020-12-08T00:50:08Z",
    "description": "- **Name:** One Million Posts Corpus\r\n- **Description:** The \u201cOne Million Posts\u201d corpus is an annotated data set consisting of user comments posted to an Austrian newspaper website (in German language).\r\n- **Paper:** https://dl.acm.org/doi/10.1145/3077136.3080711\r\n- **Data:** https://github.com/OFAI/million-post-corpus\r\n- **Motivation:** Big German (real-life) dataset containing different annotations around forum moderation with expert annotations.\r\n\r\n### Checkbox\r\n\r\n- [X] Create the dataset script `/datasets/my_dataset/my_dataset.py` using the template\r\n- [X] Fill the `_DESCRIPTION` and `_CITATION` variables\r\n- [X] Implement `_infos()`, `_split_generators()` and `_generate_examples()`\r\n- [X] Make sure that the `BUILDER_CONFIGS` class attribute is filled with the different configurations of the dataset and that the `BUILDER_CONFIG_CLASS` is specified if there is a custom config class.\r\n- [X] Generate the metadata file `dataset_infos.json` for all configurations\r\n- [X] Generate the dummy data `dummy_data.zip` files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [X] Add the dataset card `README.md` using the template : fill the tags and the various paragraphs\r\n- [X] Both tests for the real data and the dummy data pass.\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 758958066,
    "title": "Yoruba GV NER added",
    "dateCreated": "2020-12-08T00:31:38Z",
    "dateModified": "2020-12-08T00:31:38Z",
    "description": "I just added Yoruba GV NER dataset from this paper https://www.aclweb.org/anthology/2020.lrec-1.335/",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 758943174,
    "title": "oclar-dataset",
    "dateCreated": "2020-12-07T23:56:45Z",
    "dateModified": "2020-12-07T23:56:45Z",
    "description": "Opinion Corpus for Lebanese Arabic Reviews (OCLAR) corpus is utilizable for Arabic sentiment classification on reviews, including hotels, restaurants, shops, and others. : [homepage](http://archive.ics.uci.edu/ml/datasets/Opinion+Corpus+for+Lebanese+Arabic+Reviews+%28OCLAR%29#)",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 758935768,
    "title": "Created wiki_movies dataset.",
    "dateCreated": "2020-12-07T23:38:54Z",
    "dateModified": "2020-12-07T23:38:54Z",
    "description": "First PR (ever). Hopefully this movies dataset is useful to others!",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 758924960,
    "title": "Psc",
    "dateCreated": "2020-12-07T23:19:36Z",
    "dateModified": "2020-12-07T23:19:36Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 758924203,
    "title": "SMS Spam Dataset",
    "dateCreated": "2020-12-07T23:18:06Z",
    "dateModified": "2020-12-07T23:18:06Z",
    "description": "Hi :) I added this [SMS Spam Dataset](http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 758917216,
    "title": "add DFKI SmartData Corpus",
    "dateCreated": "2020-12-07T23:03:48Z",
    "dateModified": "2020-12-07T23:03:48Z",
    "description": "- **Name:** DFKI SmartData Corpus\r\n- **Description:** DFKI SmartData Corpus is a dataset of 2598 German-language documents which has been annotated with fine-grained geo-entities, such as streets, stops and routes, as well as standard named entity types.\r\n- **Paper:** https://www.dfki.de/fileadmin/user_upload/import/9427_lrec_smartdata_corpus.pdf\r\n- **Data:** https://github.com/DFKI-NLP/smartdata-corpus\r\n- **Motivation:** Contains fine-grained NER labels for German.\r\n\r\n### Checkbox\r\n\r\n- [X] Create the dataset script `/datasets/my_dataset/my_dataset.py` using the template\r\n- [X] Fill the `_DESCRIPTION` and `_CITATION` variables\r\n- [X] Implement `_infos()`, `_split_generators()` and `_generate_examples()`\r\n- [X] Make sure that the `BUILDER_CONFIGS` class attribute is filled with the different configurations of the dataset and that the `BUILDER_CONFIG_CLASS` is specified if there is a custom config class.\r\n- [X] Generate the metadata file `dataset_infos.json` for all configurations\r\n- [X] Generate the dummy data `dummy_data.zip` files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [X] Add the dataset card `README.md` using the template : fill the tags and the various paragraphs\r\n- [X] Both tests for the real data and the dummy data pass.\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 758886174,
    "title": "Adding OneStopEnglish corpus dataset",
    "dateCreated": "2020-12-07T22:05:11Z",
    "dateModified": "2020-12-07T22:05:11Z",
    "description": "This PR adds OneStopEnglish Corpus containing texts classified into reading levels (elementary, intermediate, advance) for automatic readability assessment and text simplification. \r\n\r\nLink to the paper: https://www.aclweb.org/anthology/W18-0535.pdf ",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 758871252,
    "title": "new pr for Turkish NER",
    "dateCreated": "2020-12-07T21:40:26Z",
    "dateModified": "2020-12-07T21:40:26Z",
    "description": "",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 758826568,
    "title": "Has part",
    "dateCreated": "2020-12-07T20:32:03Z",
    "dateModified": "2020-12-07T20:32:03Z",
    "description": "",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 758704178,
    "title": "removing unzipped hansards dummy data",
    "dateCreated": "2020-12-07T17:31:16Z",
    "dateModified": "2020-12-07T17:31:16Z",
    "description": "which were added by mistake",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 758687223,
    "title": "Add CovidQA dataset",
    "dateCreated": "2020-12-07T17:06:51Z",
    "dateModified": "2020-12-07T17:06:51Z",
    "description": "This PR adds CovidQA, a question answering dataset specifically designed for COVID-19, built by hand from knowledge gathered from Kaggle\u2019s COVID-19 Open Research Dataset Challenge.\r\n\r\nLink to the paper: https://arxiv.org/pdf/2004.11339.pdf\r\nLink to the homepage: https://covidqa.ai",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 758686474,
    "title": "enriched webnlg dataset rebase",
    "dateCreated": "2020-12-07T17:05:45Z",
    "dateModified": "2020-12-07T17:05:45Z",
    "description": "Rebase of #1206 !",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 758663787,
    "title": "Added kannada news headlines classification dataset. ",
    "dateCreated": "2020-12-07T16:35:37Z",
    "dateModified": "2020-12-07T16:35:37Z",
    "description": "Manual Download of a kaggle dataset. Mostly followed process as ms_terms.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 758637124,
    "title": "Adding msr_genomics_kbcomp dataset",
    "dateCreated": "2020-12-07T16:01:30Z",
    "dateModified": "2020-12-07T16:01:30Z",
    "description": "",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 758626112,
    "title": "Add Google Sentence Compression dataset",
    "dateCreated": "2020-12-07T15:47:43Z",
    "dateModified": "2020-12-07T15:47:43Z",
    "description": "For more information: https://www.aclweb.org/anthology/D13-1155.pdf",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 758601828,
    "title": "Added NewsPH Raw Dataset",
    "dateCreated": "2020-12-07T15:17:53Z",
    "dateModified": "2020-12-07T15:17:53Z",
    "description": "Added the raw version of the NewsPH dataset, which was used to automatically generate the NewsPH-NLI corpus. Dataset of news articles in Filipino from mainstream Philippine news sites on the internet. Can be used as a language modeling dataset or to reproduce the NewsPH-NLI dataset.\r\n\r\nPaper: https://arxiv.org/abs/2010.11574\r\nRepo: https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 758565320,
    "title": "Add KorQPair dataset",
    "dateCreated": "2020-12-07T14:33:57Z",
    "dateModified": "2020-12-07T14:33:57Z",
    "description": "This PR adds a [Korean paired question dataset](https://github.com/songys/Question_pair) containing labels indicating whether two questions in a given pair are semantically identical. This dataset was used to evaluate the performance of [KoGPT2](https://github.com/SKT-AI/KoGPT2#subtask-evaluations) on a phrase detection downstream task. ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 758557169,
    "title": "arXiv dataset added",
    "dateCreated": "2020-12-07T14:23:33Z",
    "dateModified": "2020-12-07T14:23:33Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 758550490,
    "title": "Add Swahili news classification dataset",
    "dateCreated": "2020-12-07T14:15:13Z",
    "dateModified": "2020-12-07T14:15:13Z",
    "description": "Add Swahili news classification dataset",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 758531980,
    "title": "adding LiMiT dataset",
    "dateCreated": "2020-12-07T14:00:41Z",
    "dateModified": "2020-12-07T14:00:41Z",
    "description": "Adding LiMiT: The Literal Motion in Text Dataset\r\nhttps://github.com/ilmgut/limit_dataset",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 758530243,
    "title": "[doc] nlp/viewer \u27a1\ufe0fdatasets/viewer",
    "dateCreated": "2020-12-07T13:58:41Z",
    "dateModified": "2020-12-07T13:58:41Z",
    "description": "cc @srush",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 758518774,
    "title": "Added WikiText-TL-39",
    "dateCreated": "2020-12-07T13:43:48Z",
    "dateModified": "2020-12-07T13:43:48Z",
    "description": "This PR adds the WikiText-TL-39 Filipino Language Modeling dataset.\r\n\r\nPaper: https://arxiv.org/abs/1907.00409\r\nRepo: https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 758517391,
    "title": "add thainer",
    "dateCreated": "2020-12-07T13:41:54Z",
    "dateModified": "2020-12-07T13:41:54Z",
    "description": "ThaiNER (v1.3) is a 6,456-sentence named entity recognition dataset created from expanding the 2,258-sentence\r\n[unnamed dataset](http://pioneer.chula.ac.th/~awirote/Data-Nutcha.zip) by\r\n[Tirasaroj and Aroonmanakun (2012)](http://pioneer.chula.ac.th/~awirote/publications/).\r\nIt is used to train NER taggers in [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp).\r\nThe NER tags are annotated by [Tirasaroj and Aroonmanakun (2012)]((http://pioneer.chula.ac.th/~awirote/publications/))\r\nfor 2,258 sentences and the rest by [@wannaphong](https://github.com/wannaphong/).\r\nThe POS tags are done by [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp)'s `perceptron` engine trained on `orchid_ud`.\r\n[@wannaphong](https://github.com/wannaphong/) is now the only maintainer of this dataset.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 758511388,
    "title": "Add Naver sentiment movie corpus",
    "dateCreated": "2020-12-07T13:33:45Z",
    "dateModified": "2020-12-07T13:33:45Z",
    "description": "Supersedes #1168 \r\n\r\n> This PR adds the [Naver sentiment movie corpus](https://github.com/e9t/nsmc), a dataset containing Korean movie reviews from Naver, the most commonly used search engine in Korea. This dataset is often used to benchmark models on Korean NLP tasks, as seen in [this paper](https://www.aclweb.org/anthology/2020.lrec-1.199.pdf). ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 758503689,
    "title": "Add Wiki Atomic Edits Dataset (43M edits)",
    "dateCreated": "2020-12-07T13:23:08Z",
    "dateModified": "2020-12-07T13:23:08Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 758491704,
    "title": "added Nergrit dataset",
    "dateCreated": "2020-12-07T13:06:12Z",
    "dateModified": "2020-12-07T13:06:12Z",
    "description": "Nergrit Corpus is a dataset collection for Indonesian Named Entity Recognition, Statement Extraction, and Sentiment Analysis. This PR is only for the Named Entity Recognition.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 758472863,
    "title": "Add doc2dial dataset",
    "dateCreated": "2020-12-07T12:39:09Z",
    "dateModified": "2020-12-07T12:39:09Z",
    "description": "### Doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset v0.9\r\n\r\nOnce complete this will add the [Doc2dial](https://doc2dial.github.io/data.html) dataset from the generic data sets list.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 758454438,
    "title": "Update step-by-step guide about the dataset cards",
    "dateCreated": "2020-12-07T12:12:12Z",
    "dateModified": "2020-12-07T12:12:12Z",
    "description": "Small update in the step-by-step guide about the dataset cards to indicate it can be created and completing while exploring the dataset.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 758431640,
    "title": "Adding indonlu dataset",
    "dateCreated": "2020-12-07T11:38:45Z",
    "dateModified": "2020-12-07T11:38:45Z",
    "description": "IndoNLU benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems for Bahasa Indonesia. It contains 12 datasets.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 758418652,
    "title": "arXiv dataset added",
    "dateCreated": "2020-12-07T11:20:23Z",
    "dateModified": "2020-12-07T11:20:23Z",
    "description": "",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 758411233,
    "title": "Add Google Turkish Treebank Dataset",
    "dateCreated": "2020-12-07T11:09:17Z",
    "dateModified": "2020-12-07T11:09:17Z",
    "description": "",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 758384417,
    "title": "arxiv dataset added",
    "dateCreated": "2020-12-07T10:32:54Z",
    "dateModified": "2020-12-07T10:32:54Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 758378904,
    "title": "Add Google Noun Verb Dataset",
    "dateCreated": "2020-12-07T10:26:05Z",
    "dateModified": "2020-12-07T10:26:05Z",
    "description": "",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 758370579,
    "title": "adding bprec",
    "dateCreated": "2020-12-07T10:15:49Z",
    "dateModified": "2020-12-07T10:15:49Z",
    "description": "",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 758360643,
    "title": "Opus elhuyar dataset for MT task having languages pair in Spanish to Basque",
    "dateCreated": "2020-12-07T10:03:34Z",
    "dateModified": "2020-12-07T10:03:34Z",
    "description": "Opus elhuyar dataset for MT task having languages pair in Spanish to Basque\r\nMore info : http://opus.nlpl.eu/Elhuyar.php",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 758355523,
    "title": "Multi Domain Sentiment Analysis Dataset (MDSA)",
    "dateCreated": "2020-12-07T09:57:15Z",
    "dateModified": "2020-12-07T09:57:15Z",
    "description": "",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 758339593,
    "title": "add yelp_review_full dataset",
    "dateCreated": "2020-12-07T09:35:36Z",
    "dateModified": "2020-12-07T09:35:36Z",
    "description": "This corresponds to the Yelp-5 requested in https://github.com/huggingface/datasets/issues/353 ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 758321688,
    "title": "adding poem_sentiment",
    "dateCreated": "2020-12-07T09:11:52Z",
    "dateModified": "2020-12-07T09:11:52Z",
    "description": "Adding poem_sentiment dataset.\r\nhttps://github.com/google-research-datasets/poem-sentiment",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 758318353,
    "title": "Add AmbigQA dataset",
    "dateCreated": "2020-12-07T09:07:19Z",
    "dateModified": "2020-12-07T09:07:19Z",
    "description": "# AmbigQA: Answering Ambiguous Open-domain Questions Dataset\r\nAdding the [AmbigQA](https://nlp.cs.washington.edu/ambigqa/) dataset as part of the sprint \ud83c\udf89 (from Open dataset list for Dataset sprint)\r\n\r\nAdded both the light and full versions (as seen on the dataset homepage)\r\nThe json format changes based on the value of one 'type' field, so I set the unavailable field to an empty list. This is explained in the README -> Data Fields\r\n\r\n```py\r\ntrain_light_dataset = load_dataset('./datasets/ambig_qa',\"light\",split=\"train\")\r\nval_light_dataset = load_dataset('./datasets/ambig_qa',\"light\",split=\"validation\")\r\ntrain_full_dataset = load_dataset('./datasets/ambig_qa',\"full\",split=\"train\")\r\nval_full_dataset = load_dataset('./datasets/ambig_qa',\"full\",split=\"validation\")\r\n\r\n\r\nfor example in train_light_dataset:\r\n    for i,t in enumerate(example['annotations']['type']):\r\n        if t =='singleAnswer':\r\n            # use the example['annotations']['answer'][i]\r\n            # example['annotations']['qaPairs'][i] - > is []\r\n            print(example['annotations']['answer'][i])\r\n        else:\r\n            # use the example['annotations']['qaPairs'][i]\r\n            # example['annotations']['answer'][i] - > is []\r\n            print(example['annotations']['qaPairs'][i])\r\n\r\n```\r\n\r\n- [x] All tests passed\r\n- [x] Added dummy data\r\n- [x] Added data card (as much as I could)\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 758263012,
    "title": "Opus finlex dataset of language pair  Finnish and Swedish",
    "dateCreated": "2020-12-07T07:53:57Z",
    "dateModified": "2020-12-07T07:53:57Z",
    "description": "Added Opus_finlex dataset of language pair  Finnish and Swedish\r\nMore info : http://opus.nlpl.eu/Finlex.php",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 758234511,
    "title": "Wino bias",
    "dateCreated": "2020-12-07T07:12:42Z",
    "dateModified": "2020-12-07T07:12:42Z",
    "description": "The PR will fail circleCi tests because of the requirement of manual loading of data. Fresh PR because of messed up history of the previous one. ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 758229304,
    "title": "Added ade_corpus_v2, with 3 configs for relation extraction and classification task",
    "dateCreated": "2020-12-07T07:05:14Z",
    "dateModified": "2020-12-07T07:05:14Z",
    "description": "Adverse Drug Reaction Data: ADE-Corpus-V2 dataset added configs for different tasks with given data",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 758188699,
    "title": "Add Curiosity Dialogs Dataset",
    "dateCreated": "2020-12-07T06:01:00Z",
    "dateModified": "2020-12-07T06:01:00Z",
    "description": "Add Facebook [Curiosity Dialogs](https://github.com/facebookresearch/curiosity) Dataset.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 758180669,
    "title": "Add Grail QA dataset",
    "dateCreated": "2020-12-07T05:46:45Z",
    "dateModified": "2020-12-07T05:46:45Z",
    "description": "For more information: https://dki-lab.github.io/GrailQA/",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 758121398,
    "title": "Add Urdu Sentiment Corpus (USC)",
    "dateCreated": "2020-12-07T03:25:20Z",
    "dateModified": "2020-12-07T03:25:20Z",
    "description": "@lhoestq opened a clean PR containing only relevant files.\r\n\r\nold PR #1140",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 758119342,
    "title": "Add Urdu fake news dataset",
    "dateCreated": "2020-12-07T03:19:50Z",
    "dateModified": "2020-12-07T03:19:50Z",
    "description": "@lhoestq opened a clean PR containing only relevant files.\r\n\r\nold PR #1125 ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 758100707,
    "title": "Muchocine - Spanish movie reviews dataset",
    "dateCreated": "2020-12-07T02:23:29Z",
    "dateModified": "2020-12-07T02:23:29Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 758049068,
    "title": "add opus_100 dataset",
    "dateCreated": "2020-12-06T23:17:24Z",
    "dateModified": "2020-12-06T23:17:24Z",
    "description": "This PR will add [opus100 dataset](http://opus.nlpl.eu/opus-100.php).",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 758049060,
    "title": "readme: remove link to Google's responsible AI practices",
    "dateCreated": "2020-12-06T23:17:22Z",
    "dateModified": "2020-12-06T23:17:22Z",
    "description": "...maybe we'll find a company that reallly stands behind responsible AI practices ;)",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 758036979,
    "title": "Add menyo_20k_mt dataset",
    "dateCreated": "2020-12-06T22:16:15Z",
    "dateModified": "2020-12-06T22:16:15Z",
    "description": "Add menyo_20k_mt dataset",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 758035501,
    "title": "Add Winobias dataset",
    "dateCreated": "2020-12-06T22:08:20Z",
    "dateModified": "2020-12-06T22:08:20Z",
    "description": "Pardon me for different commits with same message. There were conflicts after I rebased master while simultaneously pushing my changes to local repo, hence the duplicate entries.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 758022998,
    "title": "adding conceptnet5",
    "dateCreated": "2020-12-06T21:06:53Z",
    "dateModified": "2020-12-06T21:06:53Z",
    "description": "Adding the conceptnet5 and omcs txt files used to create the conceptnet5 dataset. Conceptne5 is a common sense dataset. More info can be found here: https://github.com/commonsense/conceptnet5/wiki",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 758022208,
    "title": "\ud83c\uddf8\ud83c\uddea  Added Swedish Reviews dataset for sentiment classification in Sw\u2026",
    "dateCreated": "2020-12-06T21:02:54Z",
    "dateModified": "2020-12-06T21:02:54Z",
    "description": "perhaps: @lhoestq \ud83e\udd17 ",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 758018953,
    "title": "Add numeric fused head dataset",
    "dateCreated": "2020-12-06T20:46:53Z",
    "dateModified": "2020-12-06T20:46:53Z",
    "description": "Adding the [NFH: Numeric Fused Head](https://nlp.biu.ac.il/~lazary/fh/) dataset.\r\n\r\nEverything looks sensible and I've included both the identification and resolution tasks. I haven't personally used this dataset in my research so am unable to specify what the default configuration / supervised keys should be.\r\n\r\nI've filled out the basic info on the model card to the best of my knowledge but it's a little tricky to understand exactly what the fields represent.\r\n\r\nDataset author: @yanaiela ",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 758016032,
    "title": "Add HKCanCor",
    "dateCreated": "2020-12-06T20:32:07Z",
    "dateModified": "2020-12-06T20:32:07Z",
    "description": "This PR adds the [Hong Kong Cantonese Corpus](http://compling.hss.ntu.edu.sg/hkcancor/), by [Luke and Wong 2015](http://compling.hss.ntu.edu.sg/hkcancor/data/LukeWong_Hong-Kong-Cantonese-Corpus.pdf). \r\n\r\nThe dummy data included here was manually created, as the original dataset uses a xml-like format (see a copy hosted [here](https://github.com/fcbond/hkcancor/blob/master/sample/d1_v.txt) for example) that requires a few processing steps. ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 758015894,
    "title": "add Korean HateSpeech dataset",
    "dateCreated": "2020-12-06T20:31:29Z",
    "dateModified": "2020-12-06T20:31:29Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 758013368,
    "title": "Add Korean NER dataset",
    "dateCreated": "2020-12-06T20:19:06Z",
    "dateModified": "2020-12-06T20:19:06Z",
    "description": "Supersedes #1177 \r\n\r\n> This PR adds the [Korean named entity recognition dataset](https://github.com/kmounlp/NER). This dataset has been used in many downstream tasks, such as training [KoBERT](https://github.com/SKTBrain/KoBERT) for NER, as seen in this [KoBERT-CRF implementation](https://github.com/eagle705/pytorch-bert-crf-ner).",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 758009113,
    "title": "Add WMT20 MLQE 3 shared tasks",
    "dateCreated": "2020-12-06T19:59:12Z",
    "dateModified": "2020-12-06T19:59:12Z",
    "description": "3 tasks for the WMT 20 MLQE shared tasks -> 3 different datasets\r\n\r\n(I re-created #1137 because it was too messy).\r\n\r\nNote that in L199 `task3.py`, I used `logging.warning` to print some missing data in the train set.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 758008321,
    "title": "adding DataCommons fact checking",
    "dateCreated": "2020-12-06T19:56:12Z",
    "dateModified": "2020-12-06T19:56:12Z",
    "description": "Adding the data from: https://datacommons.org/factcheck/\r\n\r\nHad to cheat a bit with the dummy data as the test doesn't recognize `.txt.gz`: had to rename uncompressed files with the `.gz` extension manually without actually compressing",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 758005982,
    "title": "Add limit",
    "dateCreated": "2020-12-06T19:46:18Z",
    "dateModified": "2020-12-06T19:46:18Z",
    "description": "This PR adds [LiMiT](https://github.com/ilmgut/limit_dataset), a dataset for literal motion classification/extraction by [Manotas et al., 2020](https://www.aclweb.org/anthology/2020.findings-emnlp.88.pdf).",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 758002885,
    "title": "Add irc disentanglement",
    "dateCreated": "2020-12-06T19:30:46Z",
    "dateModified": "2020-12-06T19:30:46Z",
    "description": "added files for irc disentanglement dataset\r\nwas unable to test dummy data as a result of vpn/proxy issues",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 758002786,
    "title": "adding medical-questions-pairs dataset",
    "dateCreated": "2020-12-06T19:30:12Z",
    "dateModified": "2020-12-06T19:30:12Z",
    "description": "This dataset consists of 3048 similar and dissimilar medical question pairs hand-generated and labeled by Curai's doctors.\r\nDataset : https://github.com/curai/medical-question-pair-dataset\r\nPaper : https://drive.google.com/file/d/1CHPGBXkvZuZc8hpr46HeHU6U6jnVze-s/view",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 757983884,
    "title": "add taskmaster3",
    "dateCreated": "2020-12-06T17:56:03Z",
    "dateModified": "2020-12-06T17:56:03Z",
    "description": "Adding Taskmaster-3 dataset\r\nhttps://github.com/google-research-datasets/Taskmaster/tree/master/TM-3-2020.\r\n\r\nThe dataset structure almost same as original dataset with these two changes\r\n\r\n1. In original dataset, each `apis` has a `args` filed which is a `dict`  with variable keys, which represent the name and value of the args. Here converted that to a `list` of `dict` with keys `arg_name` and `arg_value`. For ex.\r\n\r\n```python\r\nargs = {\"name.movie\": \"Mulan\", \"name.theater\": \": \"Mountain AMC 16\"}\r\n```\r\nbecomes \r\n```python\r\n[\r\n  {\r\n    \"arg_name\": \"name.movie\",\r\n    \"arg_value\": \"Mulan\"\r\n  },\r\n  {\r\n    \"arg_name\": \"name.theater\",\r\n    \"arg_value\": \"Mountain AMC 16\"\r\n  }\r\n]\r\n```\r\n\r\n2. Each `apis` has a `response` which is also a `dict` with variable keys representing response name/type and it's value. As above converted it to `list` of `dict` with keys `response_name` and `response_value`.\r\n\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 757978795,
    "title": "Add Sanskrit Classic texts in datasets",
    "dateCreated": "2020-12-06T17:31:31Z",
    "dateModified": "2020-12-06T17:31:31Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 757973719,
    "title": "Add large spanish corpus",
    "dateCreated": "2020-12-06T17:06:50Z",
    "dateModified": "2020-12-06T17:06:50Z",
    "description": "Adds a collection of Spanish corpora that can be useful for pretraining language models. \r\n\r\nFollowing a nice suggestion from @yjernite we provide the user with three main ways to preprocess / load either \r\n\r\n* the whole corpus (17GB!)\r\n* one specific sub-corpus\r\n* the whole corpus, but return a single split. this is useful if you want to cache the whole preprocessing step once and interact with individual sub-corpora\r\n\r\nSee the dataset card for more details.\r\n\r\nReady for review!",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 757966959,
    "title": "Add XSUM Hallucination Annotations Dataset",
    "dateCreated": "2020-12-06T16:40:19Z",
    "dateModified": "2020-12-06T16:40:19Z",
    "description": "Adding Google [XSum Hallucination Annotations](https://github.com/google-research-datasets/xsum_hallucination_annotations) dataset.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 757965934,
    "title": "[AfriBooms] Dataset exists already",
    "dateCreated": "2020-12-06T16:35:13Z",
    "dateModified": "2020-12-06T16:35:13Z",
    "description": "When trying to add \"AfriBooms\": https://docs.google.com/spreadsheets/d/12ShVow0M6RavnzbBEabm5j5dv12zBaf0y-niwEPPlo4/edit#gid=1386399609 I noticed that the dataset exists already as a config of Universal Dependencies (universal_dependencies.py). I checked and the data exactly matches so that the new data link does not give any new data.\r\n\r\nThis PR improves the config's description a bit by linking to the paper.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 757961368,
    "title": "Add HKCanCor",
    "dateCreated": "2020-12-06T16:14:43Z",
    "dateModified": "2020-12-06T16:14:43Z",
    "description": "(Apologies, didn't manage the branches properly and the PR got too messy. Going to open a new PR with everything in order)",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 757953830,
    "title": "Add msr_genomics_kbcomp Dataset",
    "dateCreated": "2020-12-06T15:40:05Z",
    "dateModified": "2020-12-06T15:40:05Z",
    "description": "",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 757952992,
    "title": "Adding Enriched WebNLG dataset",
    "dateCreated": "2020-12-06T15:36:20Z",
    "dateModified": "2020-12-06T15:36:20Z",
    "description": "This pull requests adds the `en` and `de` versions of the [Enriched WebNLG](https://github.com/ThiagoCF05/webnlg) dataset",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 757942403,
    "title": "add lst20 with manual download",
    "dateCreated": "2020-12-06T14:49:10Z",
    "dateModified": "2020-12-06T14:49:10Z",
    "description": "passed on local:\r\n```\r\nRUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_dataset_all_configs_lst20\r\n```\r\nNot sure how to test:\r\n```\r\nRUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_real_dataset_lst20\r\n```\r\n\r\n```\r\nLST20 Corpus is a dataset for Thai language processing developed by National Electronics and Computer Technology Center (NECTEC), Thailand.\r\nIt offers five layers of linguistic annotation: word boundaries, POS tagging, named entities, clause boundaries, and sentence boundaries.\r\nAt a large scale, it consists of 3,164,002 words, 288,020 named entities, 248,181 clauses, and 74,180 sentences, while it is annotated with\r\n16 distinct POS tags. All 3,745 documents are also annotated with one of 15 news genres. Regarding its sheer size, this dataset is\r\nconsidered large enough for developing joint neural models for NLP.\r\nManually download at https://aiforthai.in.th/corpus.php\r\n```",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 757939475,
    "title": "adding meta_woz dataset",
    "dateCreated": "2020-12-06T14:34:13Z",
    "dateModified": "2020-12-06T14:34:13Z",
    "description": "",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 757935170,
    "title": "Add Neural Code Search Dataset",
    "dateCreated": "2020-12-06T14:12:39Z",
    "dateModified": "2020-12-06T14:12:39Z",
    "description": "",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 757934408,
    "title": "Medical question pairs",
    "dateCreated": "2020-12-06T14:09:07Z",
    "dateModified": "2020-12-06T14:09:07Z",
    "description": "This dataset consists of 3048 similar and dissimilar medical question pairs hand-generated and labeled by Curai's doctors.\r\nDataset : https://github.com/curai/medical-question-pair-dataset\r\nPaper : https://drive.google.com/file/d/1CHPGBXkvZuZc8hpr46HeHU6U6jnVze-s/view\r\n**No splits added**",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 757927941,
    "title": "adding medical-questions-pairs",
    "dateCreated": "2020-12-06T13:36:52Z",
    "dateModified": "2020-12-06T13:36:52Z",
    "description": "",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 757926823,
    "title": "Update ADD_NEW_DATASET.md",
    "dateCreated": "2020-12-06T13:31:32Z",
    "dateModified": "2020-12-06T13:31:32Z",
    "description": "Windows needs special treatment again: unfortunately adding `torch` to the requirements does not work well (crashing the installation). Users should first install torch manually and then continue with the other commands.\r\n\r\nThis issue arises all the time when adding torch as a dependency, but because so many novice users seem to participate in adding datasets, it may be useful to add an explicit note for Windows users to ensure that they do not run into issues.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 757909237,
    "title": "Turkish NER dataset, script works fine, couldn't generate dummy data",
    "dateCreated": "2020-12-06T12:00:03Z",
    "dateModified": "2020-12-06T12:00:03Z",
    "description": "I've written the script (Turkish_NER.py) that includes dataset. The dataset is a zip inside another zip, and it's extracted as .DUMP file. However, after preprocessing I only get .arrow file. After I ran the script with no error messages, I get .arrow file of dataset, LICENSE and dataset_info.json. ",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 757903453,
    "title": "Add ALT",
    "dateCreated": "2020-12-06T11:25:30Z",
    "dateModified": "2020-12-06T11:25:30Z",
    "description": "ALT dataset -- https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 757900160,
    "title": "add taskmaster-2",
    "dateCreated": "2020-12-06T11:05:18Z",
    "dateModified": "2020-12-06T11:05:18Z",
    "description": "Adding taskmaster-2 dataset.\r\nhttps://github.com/google-research-datasets/Taskmaster/tree/master/TM-2-2020",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 757894920,
    "title": "Add IWSLT'15 English-Vietnamese machine translation Data",
    "dateCreated": "2020-12-06T10:36:31Z",
    "dateModified": "2020-12-06T10:36:31Z",
    "description": "Preprocessed Dataset from IWSLT'15 English-Vietnamese machine translation: English-Vietnamese.\r\n\r\nfrom https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 757889045,
    "title": "addition of py_ast",
    "dateCreated": "2020-12-06T10:00:52Z",
    "dateModified": "2020-12-06T10:00:52Z",
    "description": "The dataset consists of parsed Parsed ASTs that were used to train and evaluate the DeepSyn tool. \r\nThe Python programs are collected from GitHub repositories\r\nby removing duplicate files, removing project forks (copy of another existing repository)\r\n,keeping only programs that parse and have at most 30'000 nodes in the AST and \r\nwe aim to remove obfuscated files",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 757880647,
    "title": "Add msr_text_compression",
    "dateCreated": "2020-12-06T09:06:11Z",
    "dateModified": "2020-12-06T09:06:11Z",
    "description": "Add [MSR Abstractive Text Compression Dataset](https://msropendata.com/datasets/f8ce2ec9-7fbd-48f7-a8bb-2d2279373563)",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 757840830,
    "title": "add taskmaster-1",
    "dateCreated": "2020-12-06T04:09:57Z",
    "dateModified": "2020-12-06T04:09:57Z",
    "description": "Adding Taskmaster-1 dataset\r\nhttps://github.com/google-research-datasets/Taskmaster/tree/master/TM-1-2019",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 757839671,
    "title": "Add NewsPH_NLI dataset",
    "dateCreated": "2020-12-06T04:00:31Z",
    "dateModified": "2020-12-06T04:00:31Z",
    "description": "This PR adds the NewsPH-NLI Dataset, the first benchmark dataset for sentence entailment in the low-resource Filipino language. Constructed through exploting the structure of news articles. Contains 600,000 premise-hypothesis pairs, in 70-15-15 split for training, validation, and testing.\r\n\r\nLink to the paper: https://arxiv.org/pdf/2010.11574.pdf\r\n\r\nLink to the dataset/repo: https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 757836654,
    "title": "Added Translator Human Parity Data For a Chinese-English news transla\u2026",
    "dateCreated": "2020-12-06T03:34:13Z",
    "dateModified": "2020-12-06T03:34:13Z",
    "description": "\u2026tion system from Open dataset list for Dataset sprint, Microsoft Datasets tab.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 757833698,
    "title": "Add Fake News Detection in Filipino dataset",
    "dateCreated": "2020-12-06T03:12:15Z",
    "dateModified": "2020-12-06T03:12:15Z",
    "description": "This PR adds the Fake News Filipino Dataset, a low-resource fake news detection corpora in Filipino. Contains 3,206 expertly-labeled news samples, half of which are real and half of which are fake.\r\n\r\nLink to the paper: http://www.lrec-conf.org/proceedings/lrec2020/index.html\r\n\r\nLink to the dataset/repo: https://github.com/jcblaisecruz02/Tagalog-fake-news",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 757831035,
    "title": "Add Dengue dataset in Filipino",
    "dateCreated": "2020-12-06T02:50:47Z",
    "dateModified": "2020-12-06T02:50:47Z",
    "description": "This PR adds the Dengue Dataset, a benchmark dataset for low-resource multiclass classification, with 4,015 training, 500 testing, and 500 validation examples, each labeled as part of five classes. Each sample can be a part of multiple classes. Collected as tweets.\r\n\r\nLink to the paper: https://ieeexplore.ieee.org/document/8459963\r\n\r\nLink to the dataset/repo: https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 757827407,
    "title": "adding hind_encorp dataset",
    "dateCreated": "2020-12-06T02:18:45Z",
    "dateModified": "2020-12-06T02:18:45Z",
    "description": "adding Hindi_Encorp05 dataset",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 757826707,
    "title": "Added AQUA-RAT (Algebra Question Answering with Rationales) Dataset",
    "dateCreated": "2020-12-06T02:12:52Z",
    "dateModified": "2020-12-06T02:12:52Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 757826660,
    "title": "all test passed ",
    "dateCreated": "2020-12-06T02:12:32Z",
    "dateModified": "2020-12-06T02:12:32Z",
    "description": "need help creating dummy data",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 757825413,
    "title": "Add Hate Speech Dataset in Filipino",
    "dateCreated": "2020-12-06T02:01:56Z",
    "dateModified": "2020-12-06T02:01:56Z",
    "description": "This PR adds the Hate Speech Dataset, a text classification dataset in Filipino, consisting 10k tweets (training set) that are labeled as hate speech or non-hate speech. Released with 4,232 validation and 4,232 testing samples. Collected during the 2016 Philippine Presidential Elections.\r\n\r\nLink to the paper: https://pcj.csp.org.ph/index.php/pcj/issue/download/29/PCJ%20V14%20N1%20pp1-14%202019\r\n\r\nLink to the dataset/repo: https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 757807583,
    "title": "Add Adversarial SQuAD dataset",
    "dateCreated": "2020-12-05T23:51:57Z",
    "dateModified": "2020-12-05T23:51:57Z",
    "description": "# Adversarial SQuAD\r\n\r\nAdding the Adversarial [SQuAD](https://github.com/robinjia/adversarial-squad) dataset as part of the sprint \ud83c\udf89 \r\nThis dataset adds adversarial sentences to a subset of the SQuAD dataset's dev examples. How to get the original squad example id is explained in readme->Data Instances. The whole data is intended for use in evaluation. (Which could of course be also used for training if one wants). So there is no classical train/val/test split, but a split based on the number of adversaries added.\r\n\r\nThere are 2 splits of this dataset:\r\n\r\n- AddSent: Has up to five candidate adversarial sentences that don't answer the question, but have a lot of words in common with the question. This adversary is does not query the model in any way.\r\n- AddOneSent: Similar to AddSent, but just one candidate sentences was picked at random. This adversary is does not query the model in any way.\r\n\r\n(The AddAny and AddCommon datasets mentioned in the paper are dynamically generated based on model's output distribution thus are not included here)\r\n\r\nThe failing test look like some unrelated timeout thing, will probably clear if rerun.\r\n- [x] All tests passed\r\n- [x] Added dummy data\r\n- [x] Added data card (as much as I could)",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 757806570,
    "title": "add mkb dataset",
    "dateCreated": "2020-12-05T23:44:33Z",
    "dateModified": "2020-12-05T23:44:33Z",
    "description": "This PR will add Mann Ki Baat dataset (parallel data for Indian languages).",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 757804877,
    "title": "ADD COVID-QA dataset",
    "dateCreated": "2020-12-05T23:31:56Z",
    "dateModified": "2020-12-05T23:31:56Z",
    "description": "This PR adds the COVID-QA dataset, a question answering dataset consisting of 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19\r\n\r\nLink to the paper: https://openreview.net/forum?id=JENSKEEzsoU\r\nLink to the dataset/repo: https://github.com/deepset-ai/COVID-QA",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 757791992,
    "title": "added emotions detection in arabic dataset",
    "dateCreated": "2020-12-05T22:08:46Z",
    "dateModified": "2020-12-05T22:08:46Z",
    "description": "Dataset for Emotions detection in Arabic text\r\n\r\nmore info: https://github.com/AmrMehasseb/Emotional-Tone",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 757784612,
    "title": "Add  KorQuAD v2 Dataset",
    "dateCreated": "2020-12-05T21:33:34Z",
    "dateModified": "2020-12-05T21:33:34Z",
    "description": "# The Korean Question Answering Dataset v2\r\nAdding the [KorQuAD](https://korquad.github.io/) v2 dataset as part of the sprint \ud83c\udf89 \r\nThis dataset is very similar to SQuAD and is an extension of [squad_kor_v1](https://github.com/huggingface/datasets/pull/1178) which is why I added it as `squad_kor_v2`. \r\n\r\n- Crowd generated questions and answer (1-answer per question) for Wikipedia articles. Differently from V1 it includes the html structure and markup, which makes it a different enough dataset. (doesn't share ids between v1 and v2 either)\r\n\r\n- [x] All tests passed\r\n- [x] Added dummy data\r\n- [x] Added data card (as much as I could)\r\n\r\nEdit: \ud83e\udd26 looks like squad_kor_v1 commit sneaked in here too",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 757784074,
    "title": "Small update to the doc: add flatten_indices in doc",
    "dateCreated": "2020-12-05T21:30:10Z",
    "dateModified": "2020-12-05T21:30:10Z",
    "description": "Small update to the doc: add flatten_indices in doc",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 757783435,
    "title": "Add  KorQuAD v1 Dataset",
    "dateCreated": "2020-12-05T21:25:46Z",
    "dateModified": "2020-12-05T21:25:46Z",
    "description": "# The Korean Question Answering Dataset\r\nAdding the [KorQuAD](https://korquad.github.io/KorQuad%201.0/) v1 dataset as part of the sprint \ud83c\udf89 \r\nThis dataset is very similar to SQuAD which is why I added it as `squad_kor_v1`. There is also a v2 which I added [here](https://github.com/huggingface/datasets/pull/1180).\r\n\r\n- Crowd generated questions and answer (1-answer per question) for Wikipedia articles.\r\n\r\n- [x] All tests passed\r\n- [x] Added dummy data\r\n- [x] Added data card (as much as I could)\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 757778684,
    "title": "Add Korean NER dataset",
    "dateCreated": "2020-12-05T20:56:00Z",
    "dateModified": "2020-12-05T20:56:00Z",
    "description": "This PR adds the [Korean named entity recognition dataset](https://github.com/kmounlp/NER). This dataset has been used in many downstream tasks, such as training [KoBERT](https://github.com/SKTBrain/KoBERT) for NER, as seen in this [KoBERT-CRF implementation](https://github.com/eagle705/pytorch-bert-crf-ner).",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 757778365,
    "title": "Add OpenPI Dataset",
    "dateCreated": "2020-12-05T20:54:06Z",
    "dateModified": "2020-12-05T20:54:06Z",
    "description": "Add the OpenPI Dataset by AI2 (AllenAI)",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 757770077,
    "title": "added ReDial dataset",
    "dateCreated": "2020-12-05T20:04:18Z",
    "dateModified": "2020-12-05T20:04:18Z",
    "description": "Updating README\r\nDataset link: https://redialdata.github.io/website/datasheet",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 757768474,
    "title": "Add Universal Morphologies",
    "dateCreated": "2020-12-05T19:54:43Z",
    "dateModified": "2020-12-05T19:54:43Z",
    "description": "Adding unimorph universal morphology annotations for 110 languages, pfew!!!\r\n\r\none lemma per row with all possible forms and annotations\r\n\r\nhttps://unimorph.github.io/",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 757761967,
    "title": "add wikipedia biography dataset",
    "dateCreated": "2020-12-05T19:14:50Z",
    "dateModified": "2020-12-05T19:14:50Z",
    "description": "My first PR containing the Wikipedia biographies dataset. I have followed all the steps in the [guide](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md). It passes all the tests.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 757758532,
    "title": "Add proto_qa dataset",
    "dateCreated": "2020-12-05T18:55:04Z",
    "dateModified": "2020-12-05T18:55:04Z",
    "description": "Added dataset tags as required.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 757757000,
    "title": "Add imdb Urdu Reviews dataset.",
    "dateCreated": "2020-12-05T18:46:05Z",
    "dateModified": "2020-12-05T18:46:05Z",
    "description": "Added the imdb Urdu reviews dataset. More info about the dataset over <a href=\"https://github.com/mirfan899/Urdu\">here</a>.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 757754378,
    "title": "Fix path handling for Windows",
    "dateCreated": "2020-12-05T18:31:54Z",
    "dateModified": "2020-12-05T18:31:54Z",
    "description": "",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 757747997,
    "title": "Add Opus fiskmo dataset for Finnish and Swedish for MT task",
    "dateCreated": "2020-12-05T17:56:55Z",
    "dateModified": "2020-12-05T17:56:55Z",
    "description": "Adding fiskmo, a massive parallel corpus for Finnish and Swedish.\r\nfor more info : http://opus.nlpl.eu/fiskmo.php",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 757740780,
    "title": "Add Naver sentiment movie corpus",
    "dateCreated": "2020-12-05T17:25:23Z",
    "dateModified": "2020-12-05T17:25:23Z",
    "description": "This PR adds the [Naver sentiment movie corpus](https://github.com/e9t/nsmc), a dataset containing Korean movie reviews from Naver, the most commonly used search engine in Korea. This dataset is often used to benchmark models on Korean NLP tasks, as seen in [this paper](https://www.aclweb.org/anthology/2020.lrec-1.199.pdf). ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 757722921,
    "title": "\u2753 On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders",
    "dateCreated": "2020-12-05T17:02:56Z",
    "dateModified": "2020-12-05T17:02:56Z",
    "description": "Hi there,\r\n\r\nI have a question regarding \"on-the-fly\" tokenization. This question was elicited by reading the \"How to train a new language model from scratch using Transformers and Tokenizers\" [here](https://huggingface.co/blog/how-to-train). Towards the end there is this sentence: \"If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step\". I've tried coming up with a solution that would combine both `datasets` and `tokenizers`, but did not manage to find a good pattern.\r\n\r\nI guess the solution would entail wrapping a dataset into a Pytorch dataset.\r\n\r\nAs a concrete example from the [docs](https://huggingface.co/transformers/custom_datasets.html)\r\n\r\n```python\r\nimport torch\r\n\r\nclass SquadDataset(torch.utils.data.Dataset):\r\n    def __init__(self, encodings):\r\n        # instead of doing this beforehand, I'd like to do tokenization on the fly\r\n        self.encodings = encodings \r\n\r\n    def __getitem__(self, idx):\r\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\r\n\r\n    def __len__(self):\r\n        return len(self.encodings.input_ids)\r\n\r\ntrain_dataset = SquadDataset(train_encodings)\r\n```\r\n\r\nHow would one implement this with \"on-the-fly\" tokenization exploiting the vectorized capabilities of tokenizers?\r\n\r\n\r\n----\r\n\r\nEdit: I have come up with this solution. It does what I want, but I feel it's not very elegant\r\n\r\n```python\r\nclass CustomPytorchDataset(Dataset):\r\n    def __init__(self):\r\n        self.dataset = some_hf_dataset(...)\r\n        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\r\n\r\n    def __getitem__(self, batch_idx):\r\n        instance = self.dataset[text_col][batch_idx]\r\n        tokenized_text = self.tokenizer(instance, truncation=True, padding=True)\r\n        return tokenized_text\r\n\r\n    def __len__(self):\r\n        return len(self.dataset)\r\n\r\n    @staticmethod\r\n    def collate_fn(batch):\r\n        # batch is a list, however it will always contain 1 item because we should not use the\r\n        # batch_size argument as batch_size is controlled by the sampler\r\n        return {k: torch.tensor(v) for k, v in batch[0].items()}\r\n\r\ntorch_ds = CustomPytorchDataset()\r\n\r\n# NOTE: batch_sampler returns list of integers and since here we have SequentialSampler\r\n# it returns: [1, 2, 3], [4, 5, 6], etc. - check calling `list(batch_sampler)`\r\nbatch_sampler = BatchSampler(SequentialSampler(torch_ds), batch_size=3, drop_last=True)\r\n\r\n# NOTE: no `batch_size` as now the it is controlled by the sampler!\r\ndl = DataLoader(dataset=torch_ds, sampler=batch_sampler, collate_fn=torch_ds.collate_fn)\r\n```",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 757721208,
    "title": "Opus montenegrinsubs",
    "dateCreated": "2020-12-05T17:00:44Z",
    "dateModified": "2020-12-05T17:00:44Z",
    "description": "Opus montenegrinsubs - language pair en-me\r\nmore info : http://opus.nlpl.eu/MontenegrinSubs.php",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 757720226,
    "title": "Add ar rest reviews",
    "dateCreated": "2020-12-05T16:56:42Z",
    "dateModified": "2020-12-05T16:56:42Z",
    "description": "added restaurants reviews in Arabic for sentiment analysis tasks",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 757716575,
    "title": "Add DaNe dataset",
    "dateCreated": "2020-12-05T16:36:50Z",
    "dateModified": "2020-12-05T16:36:50Z",
    "description": "",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 757711340,
    "title": "Added  memat : Xhosa-English parallel corpora",
    "dateCreated": "2020-12-05T16:08:50Z",
    "dateModified": "2020-12-05T16:08:50Z",
    "description": "Added  memat : Xhosa-English parallel corpora\r\nfor more info : http://opus.nlpl.eu/memat.php",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 757707085,
    "title": "Add Mocha dataset",
    "dateCreated": "2020-12-05T15:45:14Z",
    "dateModified": "2020-12-05T15:45:14Z",
    "description": "More information: https://allennlp.org/mocha",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 757705286,
    "title": "Linguisticprobing",
    "dateCreated": "2020-12-05T15:35:18Z",
    "dateModified": "2020-12-05T15:35:18Z",
    "description": "Adding Linguistic probing datasets from\r\nWhat you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties\r\n https://www.aclweb.org/anthology/P18-1198/",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 757677188,
    "title": "adding TabFact dataset",
    "dateCreated": "2020-12-05T13:05:52Z",
    "dateModified": "2020-12-05T13:05:52Z",
    "description": "Adding TabFact: A Large-scale Dataset for Table-based Fact Verification.\r\n\r\nhttps://github.com/wenhuchen/Table-Fact-Checking\r\n\r\n- The tables are stored as individual csv files, so need to download 16,573 \ud83e\udd2f  csv files. As a result the `datasets_infos.json` file is huge (6.62 MB).\r\n- Original dataset has nested structure where, where table is one example and each table has multiple statements,\r\nflattening the structure here so that each statement is one example.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 757661128,
    "title": "Add Roman Urdu dataset",
    "dateCreated": "2020-12-05T11:36:43Z",
    "dateModified": "2020-12-05T11:36:43Z",
    "description": "This PR adds the [Roman Urdu dataset](https://archive.ics.uci.edu/ml/datasets/Roman+Urdu+Data+Set#). ",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 757658926,
    "title": "Add BBC Hindi NLI Dataset ",
    "dateCreated": "2020-12-05T11:25:34Z",
    "dateModified": "2020-12-05T11:25:34Z",
    "description": "# Dataset Card for BBC Hindi NLI Dataset\r\n\r\n## Table of Contents\r\n- [Dataset Description](#dataset-description)\r\n  - [Dataset Summary](#dataset-summary)\r\n  - [Supported Tasks](#supported-tasks-and-leaderboards)\r\n  - [Languages](#languages)\r\n- [Dataset Structure](#dataset-structure)\r\n  - [Data Instances](#data-instances)\r\n  - [Data Fields](#data-fields)\r\n  - [Data Splits](#data-splits)\r\n- [Dataset Creation](#dataset-creation)\r\n  - [Curation Rationale](#curation-rationale)\r\n  - [Source Data](#source-data)\r\n  - [Annotations](#annotations)\r\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\r\n- [Considerations for Using the Data](#considerations-for-using-the-data)\r\n  - [Social Impact of Dataset](#social-impact-of-dataset)\r\n  - [Discussion of Biases](#discussion-of-biases)\r\n  - [Other Known Limitations](#other-known-limitations)\r\n- [Additional Information](#additional-information)\r\n  - [Dataset Curators](#dataset-curators)\r\n  - [Licensing Information](#licensing-information)\r\n  - [Citation Information](#citation-information)\r\n\r\n## Dataset Description\r\n\r\n- HomePage : https://github.com/midas-research/hindi-nli-data\r\n- Paper : \"https://www.aclweb.org/anthology/2020.aacl-main.71\"\r\n- Point of Contact : https://github.com/midas-research/hindi-nli-data\r\n\r\n### Dataset Summary\r\n\r\n- Dataset for Natural Language Inference in Hindi Language. BBC Hindi Dataset consists of textual-entailment pairs.\r\n- Each row of the Datasets if made up of 4 columns - Premise, Hypothesis, Label and Topic.\r\n- Context and Hypothesis is written in Hindi while Entailment_Label is in English.\r\n- Entailment_label is of 2 types - entailed and not-entailed.\r\n- Dataset can be used to train models for Natural Language Inference tasks in Hindi Language.\r\n[More Information Needed]\r\n\r\n### Supported Tasks and Leaderboards\r\n\r\n- Natural Language Inference for Hindi\r\n\r\n### Languages\r\n\r\nDataset is in Hindi\r\n\r\n## Dataset Structure\r\n\r\n- Data is structured in TSV format. \r\n- Train and Test files are in seperate files\r\n\r\n\r\n### Dataset Instances\r\n\r\nAn example of 'train' looks as follows.\r\n\r\n```\r\n{'hypothesis': '\u092f\u0939 \u0916\u092c\u0930 \u0915\u0940 \u0938\u0942\u091a\u0928\u093e \u0939\u0948|', 'label': 'entailed', 'premise': '\u0917\u094b\u092a\u0928\u0940\u092f\u0924\u093e \u0915\u0940 \u0928\u0940\u0924\u093f', 'topic': '1'}\r\n\r\n```\r\n### Data Fields\r\n\r\n- Each row contatins 4 columns - Premise, Hypothesis, Label and Topic.\r\n\r\n### Data Splits\r\n\r\n- Train : 15553\r\n- Valid : 2581\r\n- Test : 2593\r\n\r\n## Dataset Creation\r\n\r\n- We employ a recasting technique from Poliak et al. (2018a,b) to convert publicly available BBC Hindi news text classification datasets in Hindi and pose them as TE problems\r\n- In this recasting process, we build template hypotheses for each class in the label taxonomy\r\n- Then, we pair the original annotated sentence with each of the template hypotheses to create TE samples.\r\n- For more information on the recasting process, refer to paper \"https://www.aclweb.org/anthology/2020.aacl-main.71\"\r\n\r\n### Source Data\r\n\r\nSource Dataset for the recasting process is the BBC Hindi Headlines Dataset(https://github.com/NirantK/hindi2vec/releases/tag/bbc-hindi-v0.1)\r\n\r\n#### Initial Data Collection and Normalization\r\n\r\n-  BBC Hindi News Classification Dataset contains 4, 335 Hindi news headlines tagged across 14 categories: India, Pakistan,news, International, entertainment, sport, science, China, learning english, social, southasia, business, institutional, multimedia\r\n-  We processed this dataset to combine two sets of relevant but low prevalence classes.\r\n- Namely, we merged the samples from Pakistan, China, international, and southasia as one class called international.\r\n- Likewise, we also merged samples from news, business, social, learning english, and institutional as news.\r\n- Lastly, we also removed the class multimedia because there were very few samples.\r\n\r\n#### Who are the source language producers?\r\n\r\nPls refer to this paper: \"https://www.aclweb.org/anthology/2020.aacl-main.71\"\r\n\r\n### Annotations\r\n\r\n#### Annotation process\r\n\r\nAnnotation process has been described in Dataset Creation Section.\r\n\r\n#### Who are the annotators?\r\n\r\nAnnotation is done automatically.\r\n\r\n### Personal and Sensitive Information\r\n\r\nNo Personal and Sensitive Information is mentioned in the Datasets.\r\n\r\n## Considerations for Using the Data\r\n\r\nPls refer to this paper: https://www.aclweb.org/anthology/2020.aacl-main.71\r\n\r\n### Discussion of Biases\r\n\r\nPls refer to this paper: https://www.aclweb.org/anthology/2020.aacl-main.71\r\n\r\n### Other Known Limitations\r\n\r\nNo other known limitations\r\n\r\n## Additional Information\r\n\r\nPls refer to this link: https://github.com/midas-research/hindi-nli-data\r\n\r\n### Dataset Curators\r\n\r\nIt is written in the repo : https://github.com/avinsit123/hindi-nli-data that \r\n- This corpus can be used freely for research purposes.\r\n- The paper listed below provide details of the creation and use of the corpus. If you use the corpus, then please cite the paper.\r\n- If interested in commercial use of the corpus, send email to midas@iiitd.ac.in.\r\n- If you use the corpus in a product or application, then please credit the authors and Multimodal Digital Media Analysis Lab - Indraprastha Institute of Information Technology, New Delhi appropriately. Also, if you send us an email, we will be thrilled to know about how you have used the corpus.\r\n- Multimodal Digital Media Analysis Lab - Indraprastha Institute of Information Technology, New Delhi, India disclaims any responsibility for the use of the corpus and does not provide technical support. However, the contact listed above will be happy to respond to queries and clarifications.\r\n- Rather than redistributing the corpus, please direct interested parties to this page\r\n- Please feel free to send us an email:\r\n  - with feedback regarding the corpus.\r\n  - with information on how you have used the corpus.\r\n  - if interested in having us analyze your data for natural language inference.\r\n  - if interested in a collaborative research project.\r\n\r\n\r\n### Licensing Information\r\n\r\nCopyright (C) 2019 Multimodal Digital Media Analysis Lab - Indraprastha Institute of Information Technology, New Delhi (MIDAS, IIIT-Delhi).\r\nPls contact authors for any information on the dataset.\r\n\r\n### Citation Information\r\n\r\n```\r\n    @inproceedings{uppal-etal-2020-two,\r\n    title = \"Two-Step Classification using Recasted Data for Low Resource Settings\",\r\n    author = \"Uppal, Shagun  and\r\n      Gupta, Vivek  and\r\n      Swaminathan, Avinash  and\r\n      Zhang, Haimin  and\r\n      Mahata, Debanjan  and\r\n      Gosangi, Rakesh  and\r\n      Shah, Rajiv Ratn  and\r\n      Stent, Amanda\",\r\n    booktitle = \"Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing\",\r\n    month = dec,\r\n    year = \"2020\",\r\n    address = \"Suzhou, China\",\r\n    publisher = \"Association for Computational Linguistics\",\r\n    url = \"https://www.aclweb.org/anthology/2020.aacl-main.71\",\r\n    pages = \"706--719\",\r\n    abstract = \"An NLP model{'}s ability to reason should be independent of language. Previous works utilize Natural Language Inference (NLI) to understand the reasoning ability of models, mostly focusing on high resource languages like English. To address scarcity of data in low-resource languages such as Hindi, we use data recasting to create NLI datasets for four existing text classification datasets. Through experiments, we show that our recasted dataset is devoid of statistical irregularities and spurious patterns. We further study the consistency in predictions of the textual entailment models and propose a consistency regulariser to remove pairwise-inconsistencies in predictions. We propose a novel two-step classification method which uses textual-entailment predictions for classification task. We further improve the performance by using a joint-objective for classification and textual entailment. We therefore highlight the benefits of data recasting and improvements on classification performance using our approach with supporting experimental results.\",\r\n}\r\n```\r\n",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 757657888,
    "title": "Add dataset XhosaNavy English -Xhosa",
    "dateCreated": "2020-12-05T11:19:54Z",
    "dateModified": "2020-12-05T11:19:54Z",
    "description": "Add dataset XhosaNavy English -Xhosa\r\nMore info : http://opus.nlpl.eu/XhosaNavy.php",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 757656094,
    "title": "add telugu-news corpus",
    "dateCreated": "2020-12-05T11:07:56Z",
    "dateModified": "2020-12-05T11:07:56Z",
    "description": "Adding Telugu News Corpus to datasets.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 757652517,
    "title": "Add BSD ",
    "dateCreated": "2020-12-05T10:43:48Z",
    "dateModified": "2020-12-05T10:43:48Z",
    "description": "This PR adds BSD, the Japanese-English business dialogue corpus by \r\n[Rikters et al., 2020](https://www.aclweb.org/anthology/D19-5204.pdf). ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 757651669,
    "title": "Opus sardware",
    "dateCreated": "2020-12-05T10:38:02Z",
    "dateModified": "2020-12-05T10:38:02Z",
    "description": "Added Opus sardware dataset for machine translation English to Sardinian.\r\nfor more info : http://opus.nlpl.eu/sardware.php",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 757643302,
    "title": "Adding dataset for proto_qa in huggingface datasets library",
    "dateCreated": "2020-12-05T09:43:28Z",
    "dateModified": "2020-12-05T09:43:28Z",
    "description": "Added dataset for ProtoQA: A Question Answering Dataset for Prototypical Common-Sense Reasoning\r\nFollowed all steps for adding a new dataset.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 757640506,
    "title": "hindi discourse analysis dataset commit",
    "dateCreated": "2020-12-05T09:24:01Z",
    "dateModified": "2020-12-05T09:24:01Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 757517092,
    "title": "adding psc dataset",
    "dateCreated": "2020-12-05T02:40:01Z",
    "dateModified": "2020-12-05T02:40:01Z",
    "description": "",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 757512441,
    "title": "adding dyk dataset",
    "dateCreated": "2020-12-05T02:11:42Z",
    "dateModified": "2020-12-05T02:11:42Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 757504068,
    "title": "Fix typo in the comment in _info function",
    "dateCreated": "2020-12-05T01:26:20Z",
    "dateModified": "2020-12-05T01:26:20Z",
    "description": "",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 757503918,
    "title": "adding polemo2 dataset",
    "dateCreated": "2020-12-05T01:25:29Z",
    "dateModified": "2020-12-05T01:25:29Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 757502199,
    "title": "Vinay/add/telugu books",
    "dateCreated": "2020-12-05T01:17:02Z",
    "dateModified": "2020-12-05T01:17:02Z",
    "description": "Real data tests are failing as this dataset needs to be manually downloaded",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 757498565,
    "title": "Add LINNAEUS",
    "dateCreated": "2020-12-05T01:01:09Z",
    "dateModified": "2020-12-05T01:01:09Z",
    "description": "",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 757477349,
    "title": "Add Species-800",
    "dateCreated": "2020-12-04T23:44:51Z",
    "dateModified": "2020-12-04T23:44:51Z",
    "description": "",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 757452831,
    "title": "Add JFLEG",
    "dateCreated": "2020-12-04T22:36:38Z",
    "dateModified": "2020-12-04T22:36:38Z",
    "description": "This PR adds [JFLEG ](https://www.aclweb.org/anthology/E17-2037/), an English grammatical error correction benchmark. \r\n\r\nThe tests were successful on real data, although it would be great if I can get some guidance on the **dummy data**. Basically, **for each source sentence there are 4 possible gold standard target sentences**. The original dataset comprise files in a flat structure, labelled by split then by source/target (e.g., dev.src, dev.ref0, ..., dev.ref3). Not sure what is the best way of adding this.\r\n\r\nI imagine I can treat each distinct source-target pair as its own split? But having so many copies of the source sentence feels redundant, and it would make it less convenient to end-users who might want to access multiple gold standard targets simultaneously. ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 757448920,
    "title": "Add the Winograd Schema Challenge",
    "dateCreated": "2020-12-04T22:26:59Z",
    "dateModified": "2020-12-04T22:26:59Z",
    "description": "Adds the Winograd Schema Challenge, including configs for the more canonical wsc273 as well as wsc285 with 12 new examples.\r\n\r\n- https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html\r\n\r\nThe data format was a bit of a nightmare but I think I got it to a workable format.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 757413920,
    "title": "Fix PerSenT",
    "dateCreated": "2020-12-04T21:21:02Z",
    "dateModified": "2020-12-04T21:21:02Z",
    "description": "New PR for dataset PerSenT",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 757411057,
    "title": "Add GitHub version of ETH Py150 Corpus",
    "dateCreated": "2020-12-04T21:16:08Z",
    "dateModified": "2020-12-04T21:16:08Z",
    "description": "Add the redistributable version of **ETH Py150 Corpus**",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 757399142,
    "title": "Add Urdu Sentiment Corpus (USC). ",
    "dateCreated": "2020-12-04T20:55:27Z",
    "dateModified": "2020-12-04T20:55:27Z",
    "description": "Added Urdu Sentiment Corpus. More details about the dataset over <a href=\"https://github.com/MuhammadYaseenKhan/Urdu-Sentiment-Corpus\">here</a>. ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 757393158,
    "title": "Add ReFreSD dataset",
    "dateCreated": "2020-12-04T20:45:11Z",
    "dateModified": "2020-12-04T20:45:11Z",
    "description": "This PR adds the **ReFreSD dataset**. \r\nThe original data is hosted [on this github repo](https://github.com/Elbria/xling-SemDiv) and we use the `REFreSD_rationale` to expose all the data. \r\n\r\n\r\nNeed feedback on:\r\n- I couldn't generate the dummy data. The file we download is a tsv file, but without extension, I suppose this is the problem. I'm sure there is a simple trick to make this work. \r\n- The feature names. \r\n  - I don't know if it's better to stick to the classic `sentence1`, `sentence2` or to `sentence_en`, `sentence_fr` to be more explicit. \r\n  - There is a binary label (called `label`, no problem here), and a 3-class label called `#3_labels` in the original tsv. I changed it to `all_labels` but I'm sure there is better. \r\n- The rationales are lists of integers, extracted as a string at first. I wonder what's the best way to treat them, any idea? Also, I couldn't manage to make a `Sequence` of `int8` but I'm sure I've missed something simple. \r\n\r\nThanks in advance ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 757378406,
    "title": "updated after the class name update",
    "dateCreated": "2020-12-04T20:19:43Z",
    "dateModified": "2020-12-04T20:19:43Z",
    "description": "@lhoestq <--- ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 757358145,
    "title": "add wmt mlqe 2020 shared task",
    "dateCreated": "2020-12-04T19:45:34Z",
    "dateModified": "2020-12-04T19:45:34Z",
    "description": "First commit for Shared task 1 (wmt_mlqw_task1) of WMT20 MLQE (quality estimation of machine translation)\r\nNote that I copied the tags in the README for only one (of the 7 configurations): `en-de`.\r\nThere is one configuration for each pair of languages.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 757341607,
    "title": "minor change in description in paws-x.py  and updated dataset_infos",
    "dateCreated": "2020-12-04T19:17:49Z",
    "dateModified": "2020-12-04T19:17:49Z",
    "description": "",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 757325741,
    "title": "added paws",
    "dateCreated": "2020-12-04T18:52:38Z",
    "dateModified": "2020-12-04T18:52:38Z",
    "description": "Updating README and tags for dataset card in a while",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 757317651,
    "title": "adding xquad-r dataset",
    "dateCreated": "2020-12-04T18:39:13Z",
    "dateModified": "2020-12-04T18:39:13Z",
    "description": "",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 757307660,
    "title": "Adding XQUAD-R Dataset",
    "dateCreated": "2020-12-04T18:22:29Z",
    "dateModified": "2020-12-04T18:22:29Z",
    "description": "",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 757301368,
    "title": "Add Urdu Sentiment Corpus (USC).",
    "dateCreated": "2020-12-04T18:12:24Z",
    "dateModified": "2020-12-04T18:12:24Z",
    "description": "Added Urdu Sentiment Corpus. More details about the dataset over <a href=\"https://github.com/MuhammadYaseenKhan/Urdu-Sentiment-Corpus\">here</a>.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 757278341,
    "title": "Adding XQUAD-R Dataset",
    "dateCreated": "2020-12-04T17:35:43Z",
    "dateModified": "2020-12-04T17:35:43Z",
    "description": "",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 757265075,
    "title": "adding discovery",
    "dateCreated": "2020-12-04T17:16:54Z",
    "dateModified": "2020-12-04T17:16:54Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 757255492,
    "title": "Adding initial version of cord-19 dataset",
    "dateCreated": "2020-12-04T17:03:17Z",
    "dateModified": "2020-12-04T17:03:17Z",
    "description": "Initial version only reading the metadata in CSV.\r\n\r\n### Checklist:\r\n- [x] Create the dataset script /datasets/my_dataset/my_dataset.py using the template\r\n- [x] Fill the _DESCRIPTION and _CITATION variables\r\n- [x] Implement _infos(), _split_generators() and _generate_examples()\r\n- [x] Make sure that the BUILDER_CONFIGS class attribute is filled with the different configurations of the dataset and that the BUILDER_CONFIG_CLASS is specified if there is a custom config class.\r\n- [x] Generate the metadata file dataset_infos.json for all configurations\r\n- [x] Generate the dummy data dummy_data.zip files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card README.md using the template and at least fill the tags\r\n- [x] Both tests for the real data and the dummy data pass.\r\n\r\n### TODO:\r\n- [x] add more metadata\r\n- [x] add full text\r\n- [x] add pre-computed document embedding",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 757245404,
    "title": "Add xquad-r dataset",
    "dateCreated": "2020-12-04T16:48:53Z",
    "dateModified": "2020-12-04T16:48:53Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 757229684,
    "title": "Add wikiqaar dataset",
    "dateCreated": "2020-12-04T16:26:18Z",
    "dateModified": "2020-12-04T16:26:18Z",
    "description": "Arabic Wiki Question Answering Corpus.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 757197735,
    "title": "Adding babi dataset",
    "dateCreated": "2020-12-04T15:42:34Z",
    "dateModified": "2020-12-04T15:42:34Z",
    "description": "Adding the English version of bAbI.\r\n\r\nSamples are taken from ParlAI for consistency with the main users at the moment.\r\n\r\nSupersede #945 (problem with the rebase) and adresses the issues mentioned in the review (dummy data are smaller now and code comments are fixed).",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 757194531,
    "title": "Add Urdu fake news dataset.",
    "dateCreated": "2020-12-04T15:38:17Z",
    "dateModified": "2020-12-04T15:38:17Z",
    "description": "Added Urdu fake news dataset. More information about the dataset can be found <a href=\"https://github.com/MaazAmjad/Datasets-for-Urdu-news\">here</a>.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 757186983,
    "title": "Add Xitsonga Ner",
    "dateCreated": "2020-12-04T15:27:44Z",
    "dateModified": "2020-12-04T15:27:44Z",
    "description": "Clean Xitsonga Ner PR",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 757181014,
    "title": "adding cdt dataset",
    "dateCreated": "2020-12-04T15:19:36Z",
    "dateModified": "2020-12-04T15:19:36Z",
    "description": "",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 757176172,
    "title": "Add Urdu fake news.",
    "dateCreated": "2020-12-04T15:13:10Z",
    "dateModified": "2020-12-04T15:13:10Z",
    "description": "Added Urdu fake news dataset. More information about the dataset can be found <a href=\"https://github.com/MaazAmjad/Datasets-for-Urdu-news\">here</a>.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 757169944,
    "title": "adding cdt dataset",
    "dateCreated": "2020-12-04T15:04:33Z",
    "dateModified": "2020-12-04T15:04:33Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 757166342,
    "title": "Add conda environment activation",
    "dateCreated": "2020-12-04T14:59:43Z",
    "dateModified": "2020-12-04T14:59:43Z",
    "description": "Added activation of Conda environment before installing.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 757156781,
    "title": "Add Google Great Code Dataset",
    "dateCreated": "2020-12-04T14:46:28Z",
    "dateModified": "2020-12-04T14:46:28Z",
    "description": "",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 757142350,
    "title": "Add Tashkeela dataset",
    "dateCreated": "2020-12-04T14:26:18Z",
    "dateModified": "2020-12-04T14:26:18Z",
    "description": "Arabic Vocalized Words Dataset.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 757133789,
    "title": "Fix incorrect MRQA train+SQuAD URL",
    "dateCreated": "2020-12-04T14:14:26Z",
    "dateModified": "2020-12-04T14:14:26Z",
    "description": "Fix issue #1115 \r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 757133502,
    "title": "add dbpedia_14 dataset",
    "dateCreated": "2020-12-04T14:13:59Z",
    "dateModified": "2020-12-04T14:13:59Z",
    "description": "This dataset corresponds to the DBpedia dataset requested in https://github.com/huggingface/datasets/issues/353.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 757127527,
    "title": "Incorrect URL for MRQA SQuAD train subset",
    "dateCreated": "2020-12-04T14:05:24Z",
    "dateModified": "2020-12-04T14:05:24Z",
    "description": "https://github.com/huggingface/datasets/blob/4ef4c8f8b7a60e35c6fa21115fca9faae91c9f74/datasets/mrqa/mrqa.py#L53\r\n\r\nThe URL for `train+SQuAD` subset of MRQA points to the dev set instead of train set. It should be `https://s3.us-east-2.amazonaws.com/mrqa/release/v2/train/SQuAD.jsonl.gz`.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 757123638,
    "title": "Add sesotho ner corpus",
    "dateCreated": "2020-12-04T13:59:41Z",
    "dateModified": "2020-12-04T13:59:41Z",
    "description": "Clean Sesotho PR",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 757115557,
    "title": "add qed",
    "dateCreated": "2020-12-04T13:47:57Z",
    "dateModified": "2020-12-04T13:47:57Z",
    "description": "adding QED: Dataset for Explanations in Question Answering\r\nhttps://github.com/google-research-datasets/QED\r\nhttps://arxiv.org/abs/2009.06354",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 757108151,
    "title": "Initial version of cord-19 dataset from AllenAI with only the abstract",
    "dateCreated": "2020-12-04T13:36:39Z",
    "dateModified": "2020-12-04T13:36:39Z",
    "description": "Initial version only reading the metadata in CSV.\r\n\r\n### Checklist:\r\n- [x] Create the dataset script /datasets/my_dataset/my_dataset.py using the template\r\n- [x] Fill the _DESCRIPTION and _CITATION variables\r\n- [x] Implement _infos(), _split_generators() and _generate_examples()\r\n- [x] Make sure that the BUILDER_CONFIGS class attribute is filled with the different configurations of the dataset and that the BUILDER_CONFIG_CLASS is specified if there is a custom config class.\r\n- [x] Generate the metadata file dataset_infos.json for all configurations\r\n- [x] Generate the dummy data dummy_data.zip files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card README.md using the template and at least fill the tags\r\n- [ ] Both tests for the real data and the dummy data pass.\r\n\r\n### TODO:\r\n- [ ] add more metadata\r\n- [ ] add full text\r\n- [ ] add pre-computed document embedding",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 757083266,
    "title": "Add Siswati Ner corpus",
    "dateCreated": "2020-12-04T12:57:31Z",
    "dateModified": "2020-12-04T12:57:31Z",
    "description": "Clean Siswati PR",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 757082677,
    "title": "Using a feature named \"_type\" fails with certain operations",
    "dateCreated": "2020-12-04T12:56:33Z",
    "dateModified": "2020-12-04T12:56:33Z",
    "description": "A column named `_type` leads to a `TypeError: unhashable type: 'dict'` for certain operations:\r\n```python\r\nfrom datasets import Dataset, concatenate_datasets\r\n\r\nds = Dataset.from_dict({\"_type\": [\"whatever\"]}).map()\r\nconcatenate_datasets([ds])\r\n# or simply\r\nDataset(ds._data)\r\n```\r\nContext: We are using datasets to persist data coming from elasticsearch to feed to our pipeline, and elasticsearch has a `_type` field, hence the strange name of the column.\r\n\r\nNot sure if you wish to support this specific column name, but if you do i would be happy to try a fix and provide a PR. I already had a look into it and i think the culprit is the `datasets.features.generate_from_dict` function. It uses the hard coded `_type` string to figure out if it reached the end of the nested feature object from a serialized dict.\r\n\r\nBest wishes and keep up the awesome work!",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 757055702,
    "title": "add woz_dialogue",
    "dateCreated": "2020-12-04T12:13:07Z",
    "dateModified": "2020-12-04T12:13:07Z",
    "description": "Adding Wizard-of-Oz task oriented dialogue dataset \r\nhttps://github.com/nmrksic/neural-belief-tracker/tree/master/data/woz\r\nhttps://arxiv.org/abs/1604.04562",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 757054732,
    "title": "Add Sepedi NER corpus",
    "dateCreated": "2020-12-04T12:11:24Z",
    "dateModified": "2020-12-04T12:11:24Z",
    "description": "Finally a clean PR for Sepedi",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 757031179,
    "title": "Add arsentd_lev dataset",
    "dateCreated": "2020-12-04T11:31:04Z",
    "dateModified": "2020-12-04T11:31:04Z",
    "description": "Add The Arabic Sentiment Twitter Dataset for Levantine dialect (ArSenTD-LEV)\r\n\r\nPaper: [ArSentD-LEV: A Multi-Topic Corpus for Target-based Sentiment Analysis in Arabic Levantine Tweets](https://arxiv.org/abs/1906.01830)\r\nHomepage: http://oma-project.com/",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 757027158,
    "title": "Add Urdu fake news",
    "dateCreated": "2020-12-04T11:24:14Z",
    "dateModified": "2020-12-04T11:24:14Z",
    "description": "Added Urdu fake news dataset. More information about the dataset can be found <a href=\"https://github.com/MaazAmjad/Datasets-for-Urdu-news\">here</a>.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 757024162,
    "title": "add xquad_r dataset",
    "dateCreated": "2020-12-04T11:19:35Z",
    "dateModified": "2020-12-04T11:19:35Z",
    "description": "",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 757020934,
    "title": "add TLC",
    "dateCreated": "2020-12-04T11:14:58Z",
    "dateModified": "2020-12-04T11:14:58Z",
    "description": "Added TLC dataset",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 757016820,
    "title": "Add support to download kaggle datasets",
    "dateCreated": "2020-12-04T11:08:37Z",
    "dateModified": "2020-12-04T11:08:37Z",
    "description": "We can use API key",
    "status": "open",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 757016515,
    "title": "Add retries to download manager",
    "dateCreated": "2020-12-04T11:08:11Z",
    "dateModified": "2020-12-04T11:08:11Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 757009226,
    "title": "Add Wikicorpus dataset",
    "dateCreated": "2020-12-04T10:57:26Z",
    "dateModified": "2020-12-04T10:57:26Z",
    "description": "Add dataset.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 756998433,
    "title": "Urdu fake news",
    "dateCreated": "2020-12-04T10:41:20Z",
    "dateModified": "2020-12-04T10:41:20Z",
    "description": "Added Bend the Truth urdu fake news dataset. More inforation <a href=\"https://github.com/MaazAmjad/Datasets-for-Urdu-news\">here</a>.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 756993540,
    "title": "Add tamilmixsentiment data",
    "dateCreated": "2020-12-04T10:34:07Z",
    "dateModified": "2020-12-04T10:34:07Z",
    "description": "",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 756975414,
    "title": "Add ToTTo Dataset",
    "dateCreated": "2020-12-04T10:07:25Z",
    "dateModified": "2020-12-04T10:07:25Z",
    "description": "Adds a brand new table to text dataset: https://github.com/google-research-datasets/ToTTo",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 756955729,
    "title": "Add MSRA NER labels",
    "dateCreated": "2020-12-04T09:38:16Z",
    "dateModified": "2020-12-04T09:38:16Z",
    "description": "Fixes #940 ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 756952461,
    "title": "FIX matinf link in ADD_NEW_DATASET.md",
    "dateCreated": "2020-12-04T09:33:25Z",
    "dateModified": "2020-12-04T09:33:25Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 756934964,
    "title": "Add TupleInf Open IE Dataset",
    "dateCreated": "2020-12-04T09:08:07Z",
    "dateModified": "2020-12-04T09:08:07Z",
    "description": "For more information: https://allenai.org/data/tuple-ie",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 756927060,
    "title": "add urdu fake news dataset",
    "dateCreated": "2020-12-04T08:57:38Z",
    "dateModified": "2020-12-04T08:57:38Z",
    "description": "Added Urdu fake news dataset. The dataset can be found <a href=\"https://github.com/MaazAmjad/Datasets-for-Urdu-news\">here</a>.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 756916565,
    "title": "Add NCBI Disease Corpus dataset",
    "dateCreated": "2020-12-04T08:42:32Z",
    "dateModified": "2020-12-04T08:42:32Z",
    "description": "",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 756913134,
    "title": "Add Coached Conversation Preference Dataset",
    "dateCreated": "2020-12-04T08:36:49Z",
    "dateModified": "2020-12-04T08:36:49Z",
    "description": "Adding [Coached Conversation Preference Dataset](https://research.google/tools/datasets/coached-conversational-preference-elicitation/)\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 756841254,
    "title": "Add Google wellformed query dataset",
    "dateCreated": "2020-12-04T06:25:54Z",
    "dateModified": "2020-12-04T06:25:54Z",
    "description": "This pull request will add Google wellformed_query dataset. Link of dataset is https://github.com/google-research-datasets/query-wellformedness",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 756825941,
    "title": "add thaisum",
    "dateCreated": "2020-12-04T05:54:48Z",
    "dateModified": "2020-12-04T05:54:48Z",
    "description": "ThaiSum, a large-scale corpus for Thai text summarization obtained from several online news websites namely Thairath, ThaiPBS, Prachathai, and The Standard. This dataset consists of over 350,000 article and summary pairs written by journalists. We evaluate the performance of various existing summarization models on ThaiSum dataset and analyse the characteristic of the dataset to present its difficulties.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 756823690,
    "title": "add sharc_modified",
    "dateCreated": "2020-12-04T05:49:49Z",
    "dateModified": "2020-12-04T05:49:49Z",
    "description": "Adding modified ShARC dataset https://github.com/nikhilweee/neural-conv-qa",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 756822017,
    "title": "add xquad_r dataset",
    "dateCreated": "2020-12-04T05:45:55Z",
    "dateModified": "2020-12-04T05:45:55Z",
    "description": "",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 756794430,
    "title": "Add Big Patent dataset",
    "dateCreated": "2020-12-04T04:37:30Z",
    "dateModified": "2020-12-04T04:37:30Z",
    "description": "* More info on the dataset: https://evasharma.github.io/bigpatent/\r\n* There's another raw version of the dataset available from tfds. However, they're quite large so I don't have the resources to fully test all the configs for that version yet. We'll try to add it later.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 756720643,
    "title": "adding cdt dataset",
    "dateCreated": "2020-12-04T01:28:11Z",
    "dateModified": "2020-12-04T01:28:11Z",
    "description": "- **Name:** *Cyberbullying Detection Task*\r\n- **Description:** *The Cyberbullying Detection task was part of 2019 edition of PolEval competition. The goal is to predict if a given Twitter message contains a cyberbullying (harmful) content.*\r\n- **Data:** *https://github.com/ptaszynski/cyberbullying-Polish*\r\n- **Motivation:** *The KLEJ benchmark (Kompleksowa Lista Ewaluacji J\u0119zykowych) is a set of nine evaluation tasks for the Polish language understanding.*",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 756704563,
    "title": "add mutual friends conversational dataset",
    "dateCreated": "2020-12-04T00:48:21Z",
    "dateModified": "2020-12-04T00:48:21Z",
    "description": "Mutual friends dataset\r\nWIP\r\n\r\nTODO:\r\n- scenario_kbs (bug with pyarrow conversion)\r\n- download from codalab checksums bug",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 756688727,
    "title": "adding cdsc dataset",
    "dateCreated": "2020-12-04T00:10:05Z",
    "dateModified": "2020-12-04T00:10:05Z",
    "description": "- **Name**: *cdsc (domains: cdsc-e & cdsc-r)*\r\n- **Description**: *Polish CDSCorpus consists of 10K Polish sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for the evaluation of compositional distributional semantics models of Polish. The dataset was presented at ACL 2017. Please refer to the Wr\u00f3blewska and Krasnowska-Kiera\u015b (2017) for a detailed description of the resource.*\r\n- **Data**: *http://2019.poleval.pl/index.php/tasks/*\r\n- **Motivation**: *The KLEJ benchmark (Kompleksowa Lista Ewaluacji J\u0119zykowych) is a set of nine evaluation tasks for the Polish language understanding.*",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 756687101,
    "title": "Add the multilingual Exams dataset",
    "dateCreated": "2020-12-04T00:06:04Z",
    "dateModified": "2020-12-04T00:06:04Z",
    "description": "https://github.com/mhardalov/exams-qa\r\n\r\n`multilingual` configs have all languages mixed together\r\n\r\n`crosslingual` mixes the languages for test but separates them for train and dec, so I've made one config per language for train/dev data and one config with the joint test set",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 756676218,
    "title": "Myanmar news dataset",
    "dateCreated": "2020-12-03T23:39:00Z",
    "dateModified": "2020-12-03T23:39:00Z",
    "description": "Add news topic classification dataset in Myanmar / Burmese languagess\r\n\r\nThis data was collected in 2017 by Aye Hninn Khine , and published on GitHub with a GPL license\r\nhttps://github.com/ayehninnkhine/MyanmarNewsClassificationSystem\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 756672527,
    "title": "Add Knowledge-Enhanced Language Model Pre-training (KELM)",
    "dateCreated": "2020-12-03T23:30:09Z",
    "dateModified": "2020-12-03T23:30:09Z",
    "description": "Adds the KELM dataset.\r\n\r\n- Webpage/repo: https://github.com/google-research-datasets/KELM-corpus\r\n- Paper: https://arxiv.org/pdf/2010.12688.pdf",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 756663464,
    "title": "Add WikiANN NER dataset",
    "dateCreated": "2020-12-03T23:09:24Z",
    "dateModified": "2020-12-03T23:09:24Z",
    "description": "This PR adds the full set of 176 languages from the balanced train/dev/test splits of WikiANN / PAN-X from: https://github.com/afshinrahimi/mmner\r\n\r\nUntil now, only 40 of these languages were available in `datasets` as part of the XTREME benchmark\r\n\r\nCourtesy of the dataset author, we can now download this dataset from a Dropbox URL without needing a manual download anymore \ud83e\udd73, so at some point it would be worth updating the PAN-X subset of XTREME as well \ud83d\ude04 \r\n\r\nLink to gist with some snippets for producing dummy data: https://gist.github.com/lewtun/5b93294ab6dbcf59d1493dbe2cfd6bb9\r\n\r\nP.S. @yjernite I think I was confused about needing to generate a set of YAML tags per config, so ended up just adding a single one in the README.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 756652427,
    "title": "nkjp-ner",
    "dateCreated": "2020-12-03T22:47:26Z",
    "dateModified": "2020-12-03T22:47:26Z",
    "description": "- **Name:** *nkjp-ner*\r\n- **Description:** *The NKJP-NER is based on a human-annotated part of NKJP. We extracted sentences with named entities of exactly one type. The task is to predict the type of the named entity.*\r\n- **Data:** *https://klejbenchmark.com/tasks/*\r\n- **Motivation:** *The KLEJ benchmark (Kompleksowa Lista Ewaluacji J\u0119zykowych) is a set of nine evaluation tasks for the Polish language understanding.*\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 756633215,
    "title": "add AJGT dataset",
    "dateCreated": "2020-12-03T22:16:31Z",
    "dateModified": "2020-12-03T22:16:31Z",
    "description": "Arabic Jordanian General Tweets.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 756617964,
    "title": "Added glucose dataset",
    "dateCreated": "2020-12-03T21:49:01Z",
    "dateModified": "2020-12-03T21:49:01Z",
    "description": "This PR adds the [Glucose](https://github.com/ElementalCognition/glucose) dataset.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 756584328,
    "title": "quac quac / coin coin",
    "dateCreated": "2020-12-03T20:55:29Z",
    "dateModified": "2020-12-03T20:55:29Z",
    "description": "Add QUAC (Question Answering in Context)\r\nI linearized most of the dictionnaries to lists.\r\nReferenced to the authors' datasheet for the dataset card.\r\n\ud83e\udd86\ud83e\udd86\ud83e\udd86\r\nCoin coin",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 756501235,
    "title": "adding cleaned verion of E2E NLG",
    "dateCreated": "2020-12-03T19:21:07Z",
    "dateModified": "2020-12-03T19:21:07Z",
    "description": "Found at: https://github.com/tuetschek/e2e-cleaning",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 756483172,
    "title": "Swedish MT STS-B",
    "dateCreated": "2020-12-03T19:06:25Z",
    "dateModified": "2020-12-03T19:06:25Z",
    "description": "Added a Swedish machine translated version of the well known STS-B Corpus",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 756468034,
    "title": "Add DialogRE dataset",
    "dateCreated": "2020-12-03T18:56:40Z",
    "dateModified": "2020-12-03T18:56:40Z",
    "description": "Adding the [DialogRE](https://github.com/nlpdata/dialogre) dataset Version 2.\r\n\r\n- All tests passed successfully.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 756454511,
    "title": "actually uses the previously declared VERSION on the configs in the template",
    "dateCreated": "2020-12-03T18:44:27Z",
    "dateModified": "2020-12-03T18:44:27Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 756447296,
    "title": "add xlrd to test package requirements",
    "dateCreated": "2020-12-03T18:32:47Z",
    "dateModified": "2020-12-03T18:32:47Z",
    "description": "Adds `xlrd` package to the test requirements to handle scripts that use `pandas` to load excel files",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 756442481,
    "title": "add conv_ai",
    "dateCreated": "2020-12-03T18:25:20Z",
    "dateModified": "2020-12-03T18:25:20Z",
    "description": "Adding ConvAI dataset https://github.com/DeepPavlov/convai/tree/master/2017",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 756425737,
    "title": "Test",
    "dateCreated": "2020-12-03T18:01:45Z",
    "dateModified": "2020-12-03T18:01:45Z",
    "description": "",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 756417337,
    "title": "Add Pubmed (citation + abstract) dataset (2020).",
    "dateCreated": "2020-12-03T17:54:10Z",
    "dateModified": "2020-12-03T17:54:10Z",
    "description": null,
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 756414212,
    "title": "add xquad-r dataset",
    "dateCreated": "2020-12-03T17:50:01Z",
    "dateModified": "2020-12-03T17:50:01Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 756391957,
    "title": "Add ChrEn",
    "dateCreated": "2020-12-03T17:17:48Z",
    "dateModified": "2020-12-03T17:17:48Z",
    "description": "Adding the Cherokee English machine translation dataset of https://github.com/ZhangShiyue/ChrEn",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 756383414,
    "title": "add xquad-r dataset",
    "dateCreated": "2020-12-03T17:06:23Z",
    "dateModified": "2020-12-03T17:06:23Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 756382186,
    "title": "Not support links with 302 redirect ",
    "dateCreated": "2020-12-03T17:04:43Z",
    "dateModified": "2020-12-03T17:04:43Z",
    "description": "I have an issue adding this download link https://github.com/jitkapat/thailitcorpus/releases/download/v.2.0/tlc_v.2.0.tar.gz\r\n\r\nit might be because it is not a direct link (it returns 302 and redirects to aws that returns 403 for head requests). \r\n\r\n```\r\nr.head(\"https://github.com/jitkapat/thailitcorpus/releases/download/v.2.0/tlc_v.2.0.tar.gz\", allow_redirects=True)                                      \r\n# <Response [403]>\r\n```",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 756376374,
    "title": "Add the Ud treebank",
    "dateCreated": "2020-12-03T16:56:41Z",
    "dateModified": "2020-12-03T16:56:41Z",
    "description": "This PR adds the 183 datasets in 104 languages of the UD Treebank.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 756373187,
    "title": "Add KorNLU dataset",
    "dateCreated": "2020-12-03T16:52:39Z",
    "dateModified": "2020-12-03T16:52:39Z",
    "description": "Added Korean NLU datasets. The link to the dataset can be found [here](https://github.com/kakaobrain/KorNLUDatasets) and the paper can be found [here](https://arxiv.org/abs/2004.03289)\r\n\r\n**Note**: The MNLI tsv file is broken, so this code currently excludes the file. Please suggest other alternative if any @lhoestq \r\n\r\n- [x] Followed the instructions in CONTRIBUTING.md\r\n- [x] Ran the tests successfully\r\n- [x] Created the dummy data",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 756362661,
    "title": "add labr dataset",
    "dateCreated": "2020-12-03T16:38:57Z",
    "dateModified": "2020-12-03T16:38:57Z",
    "description": "Arabic Book Reviews dataset. ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 756349001,
    "title": "Fix squad V2 metric script",
    "dateCreated": "2020-12-03T16:23:32Z",
    "dateModified": "2020-12-03T16:23:32Z",
    "description": "The current squad v2 metric doesn't work with the squad (v1 or v2) datasets. The script is copied from `squad_evaluate` in transformers that requires the labels (with multiple answers) to be like this:\r\n```\r\nreferences = [{'id': 'a', 'answers': [\r\n    {'text': 'Denver Broncos', 'answer_start': 177},\r\n    {'text': 'Denver Broncos', 'answer_start': 177}\r\n]}]\r\n```\r\nwhile the dataset had references like this:\r\n```\r\nreferences = [{'id': 'a', 'answers': \r\n    {'text': ['Denver Broncos' 'Denver Broncos'], 'answer_start': [177, 177]}\r\n}]\r\n```\r\n\r\nUsing one or the other format fails with the current squad v2 metric:\r\n```\r\nfrom datasets import load_metric\r\nmetric = load_metric(\"squad_v2\")\r\npredictions = [{'id': 'a', 'prediction_text': 'Denver Broncos', 'no_answer_probability': 0.0}]\r\nreferences = [{'id': 'a', 'answers': [\r\n    {'text': 'Denver Broncos', 'answer_start': 177},\r\n    {'text': 'Denver Broncos', 'answer_start': 177}\r\n]}]\r\nmetric.compute(predictions=predictions, references=references)\r\n```\r\nfails as well as\r\n```\r\nfrom datasets import load_metric\r\nmetric = load_metric(\"squad_v2\")\r\npredictions = [{'id': 'a', 'prediction_text': 'Denver Broncos', 'no_answer_probability': 0.0}]\r\nreferences = [{'id': 'a', 'answers': \r\n    {'text': ['Denver Broncos' 'Denver Broncos'], 'answer_start': [177, 177]}\r\n}]\r\nmetric.compute(predictions=predictions, references=references)\r\n```\r\n\r\nThis is because arrow reformats the references behind the scene.\r\n\r\nWith this PR (tested locally), both the snippets up there work and return proper results.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 756348623,
    "title": "Add TLC",
    "dateCreated": "2020-12-03T16:23:06Z",
    "dateModified": "2020-12-03T16:23:06Z",
    "description": "Added TLC dataset",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 756332704,
    "title": "added paws-x dataset",
    "dateCreated": "2020-12-03T16:06:01Z",
    "dateModified": "2020-12-03T16:06:01Z",
    "description": "Added paws-x dataset. Updating README and tags in the dataset card in a while",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 756331419,
    "title": "Adding TamilMixSentiment",
    "dateCreated": "2020-12-03T16:04:25Z",
    "dateModified": "2020-12-03T16:04:25Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 756309828,
    "title": "Add deal_or_no_dialog",
    "dateCreated": "2020-12-03T15:38:07Z",
    "dateModified": "2020-12-03T15:38:07Z",
    "description": "Add deal_or_no_dialog Dataset\r\n\r\ngithub: https://github.com/facebookresearch/end-to-end-negotiator\r\nPaper: [Deal or No Deal? End-to-End Learning for Negotiation Dialogues](https://arxiv.org/abs/1706.05125)",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 756298372,
    "title": "Add hebrew-sentiment",
    "dateCreated": "2020-12-03T15:24:31Z",
    "dateModified": "2020-12-03T15:24:31Z",
    "description": "hebrew-sentiment dataset is ready! (including tests, tags etc)",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 756265688,
    "title": "Add dataset - SemEval 2014 - Task 1",
    "dateCreated": "2020-12-03T14:52:59Z",
    "dateModified": "2020-12-03T14:52:59Z",
    "description": "Adding the dataset of SemEval 2014 Task 1\r\n\r\nFound the dataset under the shared Google Sheet > Recurring Task Datasets\r\nTask Homepage - https://alt.qcri.org/semeval2014/task1\r\n\r\nThank you!",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 756176061,
    "title": "Fix dataset URL and file names, and add column name in \"Social Bias Frames\" dataset",
    "dateCreated": "2020-12-03T13:03:05Z",
    "dateModified": "2020-12-03T13:03:05Z",
    "description": "# Why I did\r\nWhen I use \"social_bias_frames\" datasets in this library, I got 404 Errors.\r\nSo, I fixed this error and another some problems that I faced to use the dataset.\r\n\r\n# What I did\r\n* Modify this dataset URL\r\n* Modify this dataset file names\r\n* Add a \"dataSource\" column\r\n\r\nThank you!",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 756171798,
    "title": "add sharc dataset",
    "dateCreated": "2020-12-03T12:57:23Z",
    "dateModified": "2020-12-03T12:57:23Z",
    "description": "This PR adds the ShARC dataset.\r\n\r\nMore info:\r\nhttps://sharc-data.github.io/index.html",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 756169049,
    "title": "Add Facebook SimpleQuestionV2",
    "dateCreated": "2020-12-03T12:53:20Z",
    "dateModified": "2020-12-03T12:53:20Z",
    "description": "Add simple questions v2: https://research.fb.com/downloads/babi/",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 756166728,
    "title": "Add GoEmotions",
    "dateCreated": "2020-12-03T12:49:53Z",
    "dateModified": "2020-12-03T12:49:53Z",
    "description": "Adds the GoEmotions dataset, a nice emotion classification dataset with 27 (multi-)label annotations on reddit comments. Includes both a large raw version and a narrowed version with predefined train/test/val splits, which I've included as separate configs with the latter as a default.\r\n\r\n- Webpage/repo: https://github.com/google-research/google-research/tree/master/goemotions\r\n- Paper: https://arxiv.org/abs/2005.00547",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 756157602,
    "title": "Add siswati ner corpus",
    "dateCreated": "2020-12-03T12:36:00Z",
    "dateModified": "2020-12-03T12:36:00Z",
    "description": "",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 756133072,
    "title": "Adding NCHLT dataset",
    "dateCreated": "2020-12-03T11:59:25Z",
    "dateModified": "2020-12-03T11:59:25Z",
    "description": "https://repo.sadilar.org/handle/20.500.12185/7/discover?filtertype_0=database&filtertype_1=title&filter_relational_operator_1=contains&filter_relational_operator_0=equals&filter_1=&filter_0=Monolingual+Text+Corpora%3A+Annotated&filtertype=project&filter_relational_operator=equals&filter=NCHLT+Text+II",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 756127490,
    "title": "Add KorNLU",
    "dateCreated": "2020-12-03T11:50:54Z",
    "dateModified": "2020-12-03T11:50:54Z",
    "description": "Added Korean NLU datasets. The link to the dataset can be found [here](https://github.com/kakaobrain/KorNLUDatasets) and the paper can be found [here](https://arxiv.org/abs/2004.03289)\r\n\r\n**Note**: The MNLI tsv file is broken, so this code currently excludes the file. Please suggest other alternative if any @lhoestq \r\n\r\n- [x] Followed the instructions in CONTRIBUTING.md\r\n- [x] Ran the tests successfully\r\n- [x] Created the dummy data",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 756122709,
    "title": "Dataset.map() turns tensors into lists?",
    "dateCreated": "2020-12-03T11:43:46Z",
    "dateModified": "2020-12-03T11:43:46Z",
    "description": "I apply `Dataset.map()` to a function that returns a dict of torch tensors (like a tokenizer from the repo transformers). However, in the mapped dataset, these tensors have turned to lists!\r\n\r\n```import datasets\r\nimport torch  \r\nfrom datasets import load_dataset                                                                                                                 \r\nprint(\"version datasets\", datasets.__version__)\r\n\r\ndataset = load_dataset(\"snli\", split='train[0:50]')  \r\n\r\ndef tokenizer_fn(example):\r\n    # actually uses a tokenizer which does something like:\r\n    return {'input_ids': torch.tensor([[0, 1, 2]])}\r\n\r\nprint(\"First item in dataset:\\n\", dataset[0])\r\ntokenized = tokenizer_fn(dataset[0])\r\nprint(\"Tokenized hyp:\\n\", tokenized)\r\ndataset_tok = dataset.map(tokenizer_fn, batched=False,\r\n        remove_columns=['label', 'premise', 'hypothesis'])\r\nprint(\"Tokenized using map:\\n\", dataset_tok[0])\r\nprint(type(tokenized['input_ids']), type(dataset_tok[0]['input_ids']))\r\ndataset_tok = dataset.map(tokenizer_fn, batched=False,\r\n                          remove_columns=['label', 'premise', 'hypothesis'])\r\nprint(\"Tokenized using map:\\n\", dataset_tok[0])\r\nprint(type(tokenized['input_ids']), type(dataset_tok[0]['input_ids']))\r\n```\r\n\r\nThe output is:\r\n\r\n```\r\nversion datasets 1.1.3\r\nReusing dataset snli (/home/tom/.cache/huggingface/datasets/snli/plain_text/1.0.0/bb1102591c6230bd78813e229d5dd4c7fbf4fc478cec28f298761eb69e5b537c)\r\nFirst item in dataset:\r\n {'premise': 'A person on a horse jumps over a broken down airplane.', 'hypothesis': 'A person is training his horse for a competition.', 'label': 1}\r\nTokenized hyp:\r\n {'input_ids': tensor([[0, 1, 2]])}\r\nLoading cached processed dataset at /home/tom/.cache/huggingface/datasets/snli/plain_text/1.0.0/bb1102591c6230bd78813e229d5dd4c7fbf4fc478cec28f298761eb69e5b537c/cache-fe38f449fe9ac46f.arrow\r\nTokenized using map:\r\n {'input_ids': [[0, 1, 2]]}\r\n<class 'torch.Tensor'> <class 'list'>\r\n```\r\n\r\nOr am I doing something wrong?\r\n",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 756120760,
    "title": "Add xitsonga ner corpus",
    "dateCreated": "2020-12-03T11:40:48Z",
    "dateModified": "2020-12-03T11:40:48Z",
    "description": "",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 756111647,
    "title": "Add AMTTL Chinese Word Segmentation Dataset",
    "dateCreated": "2020-12-03T11:27:52Z",
    "dateModified": "2020-12-03T11:27:52Z",
    "description": "",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 756100717,
    "title": "Add TSAC: Tunisian Sentiment Analysis Corpus",
    "dateCreated": "2020-12-03T11:12:35Z",
    "dateModified": "2020-12-03T11:12:35Z",
    "description": "github: https://github.com/fbougares/TSAC\r\n\r\npaper: https://www.aclweb.org/anthology/W17-1307/",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 756097583,
    "title": "Add Big Patent dataset",
    "dateCreated": "2020-12-03T11:07:59Z",
    "dateModified": "2020-12-03T11:07:59Z",
    "description": "- More info on the dataset: https://evasharma.github.io/bigpatent/\r\n- There's another raw version of the dataset available from tfds. However, they're quite large so I don't have the resources to fully test all the configs for that version yet. We'll try to add it later.\r\n- ~Currently, there are no dummy data for this dataset yet as I'm facing some problems with generating them. I'm trying to add them later.~",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 756055102,
    "title": "Add SuperGLUE metric",
    "dateCreated": "2020-12-03T10:11:34Z",
    "dateModified": "2020-12-03T10:11:34Z",
    "description": "Adds a new metric for the SuperGLUE benchmark (similar to the GLUE benchmark metric).",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 756050387,
    "title": "Add UN Universal Declaration of Human Rights (UDHR)",
    "dateCreated": "2020-12-03T10:04:58Z",
    "dateModified": "2020-12-03T10:04:58Z",
    "description": "Universal declaration of human rights with translations in 464 languages and dialects.\r\n\r\n- UN page: https://www.ohchr.org/EN/UDHR/Pages/UDHRIndex.aspx\r\n- Raw data source: https://unicode.org/udhr/index.html\r\n\r\nEach instance of the dataset corresponds to one translation of the document. Since there's only one instance per language (and because there are 500 languages so the dummy data would be messy), I opted to just include them all under the same single config. I wasn't able to find any kind of license so I just copied the copyright notice.\r\n\r\nI was pretty careful careful generating the language tags so they _should_ all be correct & consistent BCP-47 codes per the docs.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 756000478,
    "title": "Update ADD NEW DATASET",
    "dateCreated": "2020-12-03T08:58:32Z",
    "dateModified": "2020-12-03T08:58:32Z",
    "description": "This PR adds a couple of detail on cloning/rebasing the repo.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 755987997,
    "title": "add med_hop",
    "dateCreated": "2020-12-03T08:40:27Z",
    "dateModified": "2020-12-03T08:40:27Z",
    "description": "This PR adds the MedHop dataset from the QAngaroo multi hop reading comprehension datasets\r\n\r\nMore info:\r\nhttp://qangaroo.cs.ucl.ac.uk/index.html",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 755975586,
    "title": "Fix docs indentation issues",
    "dateCreated": "2020-12-03T08:21:34Z",
    "dateModified": "2020-12-03T08:21:34Z",
    "description": "Replace tabs with spaces.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 755953294,
    "title": "Add PerSenT",
    "dateCreated": "2020-12-03T07:43:58Z",
    "dateModified": "2020-12-03T07:43:58Z",
    "description": "Added [Person's SentimenT](https://stonybrooknlp.github.io/PerSenT/) dataset. ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 755947097,
    "title": "add wiki_hop",
    "dateCreated": "2020-12-03T07:32:26Z",
    "dateModified": "2020-12-03T07:32:26Z",
    "description": "This PR adds the WikiHop dataset from the QAngaroo multi hop reading comprehension datasets\r\n\r\nMore info:\r\nhttp://qangaroo.cs.ucl.ac.uk/index.html\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 755936327,
    "title": "add scb_mt_enth_2020",
    "dateCreated": "2020-12-03T07:13:49Z",
    "dateModified": "2020-12-03T07:13:49Z",
    "description": "## scb-mt-en-th-2020: A Large English-Thai Parallel Corpus\r\n\r\nThe primary objective of our work is to build a large-scale English-Thai dataset for machine translation.\r\nWe construct an English-Thai machine translation dataset with over 1 million segment pairs, curated from various sources,\r\nnamely news, Wikipedia articles, SMS messages, task-based dialogs, web-crawled data and government documents.\r\nMethodology for gathering data, building parallel texts and removing noisy sentence pairs are presented in a reproducible manner.\r\nWe train machine translation models based on this dataset. Our models' performance are comparable to that of\r\nGoogle Translation API (as of May 2020) for Thai-English and outperform Google when the Open Parallel Corpus (OPUS) is\r\nincluded in the training data for both Thai-English and English-Thai translation.\r\nThe dataset, pre-trained models, and source code to reproduce our work are available for public use.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 755921927,
    "title": "Add support for \".txm\" format",
    "dateCreated": "2020-12-03T06:52:08Z",
    "dateModified": "2020-12-03T06:52:08Z",
    "description": "In dummy data generation, add support for XML-like \".txm\" file format.\r\n\r\nAlso support filenames with additional compression extension: \".txm.gz\".",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 755858785,
    "title": "IIT B English to Hindi machine translation dataset",
    "dateCreated": "2020-12-03T05:18:45Z",
    "dateModified": "2020-12-03T05:18:45Z",
    "description": "Adding IIT Bombay English-Hindi Corpus dataset\r\nmore info : http://www.cfilt.iitb.ac.in/iitb_parallel/",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 755844004,
    "title": "add crows_pairs",
    "dateCreated": "2020-12-03T05:05:11Z",
    "dateModified": "2020-12-03T05:05:11Z",
    "description": "This PR adds CrowS-Pairs datasets.\r\n\r\nMore info:\r\nhttps://github.com/nyu-mll/crows-pairs/\r\nhttps://arxiv.org/pdf/2010.00133.pdf",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 755777438,
    "title": "allegro_reviews dataset ",
    "dateCreated": "2020-12-03T03:11:39Z",
    "dateModified": "2020-12-03T03:11:39Z",
    "description": "- **Name:** *allegro_reviews*\r\n- **Description:** *Allegro Reviews is a sentiment analysis dataset, consisting of 11,588 product reviews written in Polish and extracted from Allegro.pl - a popular e-commerce marketplace. Each review contains at least 50 words and has a rating on a scale from one (negative review) to five (positive review).*\r\n- **Data:** *https://github.com/allegro/klejbenchmark-allegroreviews*\r\n- **Motivation:** *The KLEJ benchmark (Kompleksowa Lista Ewaluacji J\u0119zykowych) is a set of nine evaluation tasks for the Polish language understanding.*",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 755767616,
    "title": "Add PEC",
    "dateCreated": "2020-12-03T02:46:08Z",
    "dateModified": "2020-12-03T02:46:08Z",
    "description": "A persona-based empathetic conversation dataset.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 755712854,
    "title": "Add ASSET dataset for text simplification evaluation",
    "dateCreated": "2020-12-03T00:28:29Z",
    "dateModified": "2020-12-03T00:28:29Z",
    "description": "Adding the ASSET dataset from https://github.com/facebookresearch/asset\r\n\r\nOne config for the simplification data, one for the human ratings of quality.\r\n\r\nThe README.md borrows from that written by @juand-r",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 755695420,
    "title": "Hi",
    "dateCreated": "2020-12-02T23:47:14Z",
    "dateModified": "2020-12-02T23:47:14Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 755689195,
    "title": "L\u00edo o",
    "dateCreated": "2020-12-02T23:32:25Z",
    "dateModified": "2020-12-02T23:32:25Z",
    "description": "````l`````````\n\n```\nO\n```\n`````\n\u00d1o\n```\n````\n\n```",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 755673371,
    "title": "Add Sesotho Ner",
    "dateCreated": "2020-12-02T23:00:15Z",
    "dateModified": "2020-12-02T23:00:15Z",
    "description": "",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 755664113,
    "title": "Add ZEST: ZEroShot learning from Task descriptions",
    "dateCreated": "2020-12-02T22:41:20Z",
    "dateModified": "2020-12-02T22:41:20Z",
    "description": "Adds the ZEST dataset on zero-shot learning from task descriptions from AI2.\r\n\r\n- Webpage: https://allenai.org/data/zest\r\n- Paper: https://arxiv.org/abs/2011.08115\r\n\r\nThe nature of this dataset made the supported task tags tricky if you wouldn't mind giving any feedback @yjernite. Also let me know if you think we should have a `other-task-generalization` or something like that...",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 755655752,
    "title": "Add Schema Guided Dialogue dataset",
    "dateCreated": "2020-12-02T22:26:01Z",
    "dateModified": "2020-12-02T22:26:01Z",
    "description": "This PR adds the Schema Guided Dialogue dataset created for the DSTC8 challenge\r\n- https://github.com/google-research-datasets/dstc8-schema-guided-dialogue\r\n\r\nA bit simpler than MultiWOZ, the only tricky thing is the sequence of dictionaries that had to be linearized. There is a config for the data proper, and a config for the schemas.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 755651377,
    "title": "add MRQA",
    "dateCreated": "2020-12-02T22:17:56Z",
    "dateModified": "2020-12-02T22:17:56Z",
    "description": "MRQA (shared task 2019)\r\nout of distribution generalization\r\nFramed as extractive question answering\r\nDataset is the concatenation (of subsets) of existing QA datasets processed to match the SQuAD format",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 755644559,
    "title": "Add Gutenberg time references dataset",
    "dateCreated": "2020-12-02T22:05:26Z",
    "dateModified": "2020-12-02T22:05:26Z",
    "description": "This PR adds the gutenberg_time dataset: https://arxiv.org/abs/2011.04124",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 755601450,
    "title": "Add Setswana NER",
    "dateCreated": "2020-12-02T20:52:07Z",
    "dateModified": "2020-12-02T20:52:07Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 755582090,
    "title": "Add caWaC dataset",
    "dateCreated": "2020-12-02T20:18:55Z",
    "dateModified": "2020-12-02T20:18:55Z",
    "description": "Add dataset.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 755570882,
    "title": "Add Sepedi NER",
    "dateCreated": "2020-12-02T20:01:05Z",
    "dateModified": "2020-12-02T20:01:05Z",
    "description": "This is a new branch created for this dataset",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 755558175,
    "title": "Specify file encoding",
    "dateCreated": "2020-12-02T19:40:45Z",
    "dateModified": "2020-12-02T19:40:45Z",
    "description": "If not specified, Python uses system default, which for Windows is not \"utf-8\".",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 755521862,
    "title": "Add CLINC150 dataset",
    "dateCreated": "2020-12-02T18:44:30Z",
    "dateModified": "2020-12-02T18:44:30Z",
    "description": "Added CLINC150 Dataset. The link to the dataset can be found [here](https://github.com/clinc/oos-eval) and the paper can be found [here](https://www.aclweb.org/anthology/D19-1131.pdf)\r\n\r\n- [x] Followed the instructions in CONTRIBUTING.md\r\n- [x] Ran the tests successfully\r\n- [x] Created the dummy data",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 755508841,
    "title": "add hard dataset",
    "dateCreated": "2020-12-02T18:27:36Z",
    "dateModified": "2020-12-02T18:27:36Z",
    "description": "Hotel Reviews in Arabic language.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 755505851,
    "title": "Add SciTLDR Dataset (Take 2)",
    "dateCreated": "2020-12-02T18:22:50Z",
    "dateModified": "2020-12-02T18:22:50Z",
    "description": "Adds the SciTLDR Dataset by AI2\r\nAdded the `README.md` card with tags to the best of my knowledge\r\n\r\nMulti-target summaries or TLDRs of Scientific Documents\r\n\r\nContinued from #986 ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 755493075,
    "title": "Adding CS restaurants dataset",
    "dateCreated": "2020-12-02T18:02:30Z",
    "dateModified": "2020-12-02T18:02:30Z",
    "description": "This PR adds the CS restaurants dataset; this is a re-opening of a previous PR with a chaotic commit history.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 755485658,
    "title": "Adding  Evidence Inference Data:",
    "dateCreated": "2020-12-02T17:51:35Z",
    "dateModified": "2020-12-02T17:51:35Z",
    "description": "http://evidence-inference.ebm-nlp.com/download/\nhttps://arxiv.org/pdf/2005.04177.pdf",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 755463726,
    "title": "Add Bilingual Corpus of Arabic-English Parallel Tweets",
    "dateCreated": "2020-12-02T17:20:02Z",
    "dateModified": "2020-12-02T17:20:02Z",
    "description": "Added Bilingual Corpus of Arabic-English Parallel Tweets. The link to the dataset can be found [here](https://alt.qcri.org/wp-content/uploads/2020/08/Bilingual-Corpus-of-Arabic-English-Parallel-Tweets.zip) and the paper can be found [here](https://www.aclweb.org/anthology/2020.bucc-1.3.pdf)\r\n\r\n- [x] Followed the instructions in CONTRIBUTING.md\r\n- [x] Ran the tests successfully\r\n- [x] Created the dummy data",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 755432143,
    "title": "Add NoReC: Norwegian Review Corpus",
    "dateCreated": "2020-12-02T16:38:29Z",
    "dateModified": "2020-12-02T16:38:29Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 755384433,
    "title": "Adding C3 dataset: the first free-form multiple-Choice Chinese machine reading Comprehension dataset.",
    "dateCreated": "2020-12-02T15:40:36Z",
    "dateModified": "2020-12-02T15:40:36Z",
    "description": "https://github.com/nlpdata/c3\nhttps://arxiv.org/abs/1904.09679",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 755372798,
    "title": "Adding C3 dataset: the first free-form multiple-Choice Chinese machine reading Comprehension dataset.      https://github.com/nlpdata/c3 https://arxiv.org/abs/1904.09679",
    "dateCreated": "2020-12-02T15:28:05Z",
    "dateModified": "2020-12-02T15:28:05Z",
    "description": null,
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 755364078,
    "title": "Include license file in source distribution",
    "dateCreated": "2020-12-02T15:17:43Z",
    "dateModified": "2020-12-02T15:17:43Z",
    "description": "It would be helpful to include the license file in the source distribution.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 755362766,
    "title": "add yahoo_answers_topics",
    "dateCreated": "2020-12-02T15:16:13Z",
    "dateModified": "2020-12-02T15:16:13Z",
    "description": "This PR adds yahoo answers topic classification dataset.\r\n\r\nMore info:\r\nhttps://github.com/LC-John/Yahoo-Answers-Topic-Classification-Dataset\r\n\r\ncc @joeddav, @yjernite ",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 755337255,
    "title": "Adding Autshumato South african langages:",
    "dateCreated": "2020-12-02T14:47:33Z",
    "dateModified": "2020-12-02T14:47:33Z",
    "description": "https://repo.sadilar.org/handle/20.500.12185/7/discover?filtertype=database&filter_relational_operator=equals&filter=Multilingual+Text+Corpora%3A+Aligned",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 755325368,
    "title": "how large datasets are handled under the hood ",
    "dateCreated": "2020-12-02T14:32:40Z",
    "dateModified": "2020-12-02T14:32:40Z",
    "description": "Hi\r\nI want to use multiple large datasets with a mapping style dataloader, where they cannot fit into memory, could you tell me how you handled the datasets under the hood? is this you bring all in memory in case of mapping style ones? or is this some sharding under the hood and you bring in memory when necessary, thanks ",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 755310318,
    "title": "Add multi_x_science_sum",
    "dateCreated": "2020-12-02T14:14:01Z",
    "dateModified": "2020-12-02T14:14:01Z",
    "description": "Add Multi-XScience Dataset. \r\n\r\ngithub repo: https://github.com/yaolu/Multi-XScience\r\npaper: [Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles](https://arxiv.org/abs/2010.14235)",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 755309758,
    "title": "Adding Medal: MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining",
    "dateCreated": "2020-12-02T14:13:17Z",
    "dateModified": "2020-12-02T14:13:17Z",
    "description": null,
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 755309071,
    "title": "Adding Medal: MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining",
    "dateCreated": "2020-12-02T14:12:30Z",
    "dateModified": "2020-12-02T14:12:30Z",
    "description": null,
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 755292066,
    "title": "UM005: Urdu <> English Translation Dataset",
    "dateCreated": "2020-12-02T13:51:35Z",
    "dateModified": "2020-12-02T13:51:35Z",
    "description": "Adds Urdu-English dataset for machine translation: http://ufal.ms.mff.cuni.cz/umc/005-en-ur/",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 755246786,
    "title": "add generated_reviews_enth",
    "dateCreated": "2020-12-02T12:50:43Z",
    "dateModified": "2020-12-02T12:50:43Z",
    "description": "`generated_reviews_enth` is created as part of [scb-mt-en-th-2020](https://arxiv.org/pdf/2007.03541.pdf) for machine translation task. This dataset (referred to as `generated_reviews_yn` in [scb-mt-en-th-2020](https://arxiv.org/pdf/2007.03541.pdf)) are English product reviews generated by [CTRL](https://arxiv.org/abs/1909.05858), translated by Google Translate API and annotated as accepted or rejected (`correct`) based on fluency and adequacy of the translation by human annotators. This allows it to be used for English-to-Thai translation quality esitmation (binary label), machine translation, and sentiment analysis.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 755235356,
    "title": "adding yahoo_answers_qa",
    "dateCreated": "2020-12-02T12:33:54Z",
    "dateModified": "2020-12-02T12:33:54Z",
    "description": "Adding Yahoo Answers QA dataset.\r\n\r\nMore info:\r\nhttps://ciir.cs.umass.edu/downloads/nfL6/",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 755185517,
    "title": "Microsoft CodeXGlue",
    "dateCreated": "2020-12-02T11:21:18Z",
    "dateModified": "2020-12-02T11:21:18Z",
    "description": "Datasets from https://github.com/microsoft/CodeXGLUE\r\n\r\nThis contains 13 datasets:\r\n\r\ncode_x_glue_cc_clone_detection_big_clone_bench\r\ncode_x_glue_cc_clone_detection_poj_104\r\ncode_x_glue_cc_cloze_testing_all\r\ncode_x_glue_cc_cloze_testing_maxmin\r\ncode_x_glue_cc_code_completion_line\r\ncode_x_glue_cc_code_completion_token\r\ncode_x_glue_cc_code_refinement\r\ncode_x_glue_cc_code_to_code_trans\r\ncode_x_glue_cc_defect_detection\r\ncode_x_glue_ct_code_to_text\r\ncode_x_glue_tc_nl_code_search_adv\r\ncode_x_glue_tc_text_to_code\r\ncode_x_glue_tt_text_to_text\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 755176084,
    "title": "NotADirectoryError while loading the CNN/Dailymail dataset",
    "dateCreated": "2020-12-02T11:07:56Z",
    "dateModified": "2020-12-02T11:07:56Z",
    "description": "\r\nDownloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.28 GiB, post-processed: Unknown size, total: 1.82 GiB) to /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602...\r\n\r\n---------------------------------------------------------------------------\r\n\r\nNotADirectoryError                        Traceback (most recent call last)\r\n\r\n<ipython-input-9-cd4bf8bea840> in <module>()\r\n     22 \r\n     23 \r\n---> 24 train = load_dataset('cnn_dailymail', '3.0.0', split='train')\r\n     25 validation = load_dataset('cnn_dailymail', '3.0.0', split='validation')\r\n     26 test = load_dataset('cnn_dailymail', '3.0.0', split='test')\r\n\r\n5 frames\r\n\r\n/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _find_files(dl_paths, publisher, url_dict)\r\n    132     else:\r\n    133         logging.fatal(\"Unsupported publisher: %s\", publisher)\r\n--> 134     files = sorted(os.listdir(top_dir))\r\n    135 \r\n    136     ret_files = []\r\n\r\nNotADirectoryError: [Errno 20] Not a directory: '/root/.cache/huggingface/datasets/downloads/1bc05d24fa6dda2468e83a73cf6dc207226e01e3c48a507ea716dc0421da583b/cnn/stories'",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 755175199,
    "title": "added dataset circa",
    "dateCreated": "2020-12-02T11:06:39Z",
    "dateModified": "2020-12-02T11:06:39Z",
    "description": "Dataset Circa added. Only README.md and dataset card left",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 755146834,
    "title": "Add Sepedi ner corpus",
    "dateCreated": "2020-12-02T10:30:07Z",
    "dateModified": "2020-12-02T10:30:07Z",
    "description": "",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 755135768,
    "title": "Problem downloading amazon_reviews_multi",
    "dateCreated": "2020-12-02T10:15:57Z",
    "dateModified": "2020-12-02T10:15:57Z",
    "description": "Thanks for adding the dataset. \r\nAfter trying to load the dataset, I am getting the following error: \r\n`ConnectionError: Couldn't reach https://amazon-reviews-ml.s3-us-west-2.amazonaws.com/json/train/dataset_fr_train.json\r\n`\r\nI used the following code to load the dataset: \r\n`load_dataset(\r\n            dataset_name,\r\n            \"all_languages\",\r\n            cache_dir=\".data\"\r\n        )`\r\n\r\nI am using version 1.1.3 of `datasets`\r\n\r\nNote that I can perform a successfull `wget https://amazon-reviews-ml.s3-us-west-2.amazonaws.com/json/train/dataset_fr_train.json`",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 755124963,
    "title": "Add CAIL 2018 dataset",
    "dateCreated": "2020-12-02T10:01:40Z",
    "dateModified": "2020-12-02T10:01:40Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 755117902,
    "title": "Adding farsi_news dataset (https://github.com/sci2lab/Farsi-datasets)",
    "dateCreated": "2020-12-02T09:52:19Z",
    "dateModified": "2020-12-02T09:52:19Z",
    "description": null,
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 755097798,
    "title": "Add E2E NLG",
    "dateCreated": "2020-12-02T09:25:12Z",
    "dateModified": "2020-12-02T09:25:12Z",
    "description": "Adding the E2E NLG dataset.\r\n\r\nMore info here : http://www.macs.hw.ac.uk/InteractionLab/E2E/\r\n\r\n### Checkbox\r\n\r\n- [x] Create the dataset script `/datasets/my_dataset/my_dataset.py` using the template\r\n- [x] Fill the `_DESCRIPTION` and `_CITATION` variables\r\n- [x] Implement `_infos()`, `_split_generators()` and `_generate_examples()`\r\n- [x] Make sure that the `BUILDER_CONFIGS` class attribute is filled with the different configurations of the dataset and that the `BUILDER_CONFIG_CLASS` is specified if there is a custom config class.\r\n- [x] Generate the metadata file `dataset_infos.json` for all configurations\r\n- [x] Generate the dummy data `dummy_data.zip` files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card `README.md` using the template and at least fill the tags \r\n- [x] Both tests for the real data and the dummy data pass.\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 755079394,
    "title": "Fix SV -> NO",
    "dateCreated": "2020-12-02T08:59:59Z",
    "dateModified": "2020-12-02T08:59:59Z",
    "description": "This PR fixes the small typo as seen in #956 ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 755069159,
    "title": "making sure datasets are not loaded in memory and distributed training of them",
    "dateCreated": "2020-12-02T08:45:15Z",
    "dateModified": "2020-12-02T08:45:15Z",
    "description": "Hi\r\nI am dealing with large-scale datasets which I need to train distributedly, I used the shard function to divide the dataset across the cores, without any sampler, this does not work for distributed training and does not become any faster than 1 TPU core. 1) how I can make sure data is not loaded in memory 2) in case of distributed training with iterative datasets which measures needs to be taken? Is this all sharding the data only. I was wondering if there can be possibility for me to discuss this with someone with distributed training with iterative datasets using dataset library. thanks ",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 755059469,
    "title": "Add OPUS DOGC dataset",
    "dateCreated": "2020-12-02T08:30:32Z",
    "dateModified": "2020-12-02T08:30:32Z",
    "description": "",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 755047470,
    "title": "Add SciTLDR Dataset",
    "dateCreated": "2020-12-02T08:11:16Z",
    "dateModified": "2020-12-02T08:11:16Z",
    "description": "Adds the SciTLDR Dataset by AI2\r\nAdded README card with tags to the best of my knowledge\r\n\r\nMulti-target summaries or TLDRs of Scientific Documents",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 755020564,
    "title": "Add GAP dataset",
    "dateCreated": "2020-12-02T07:25:11Z",
    "dateModified": "2020-12-02T07:25:11Z",
    "description": "GAP dataset\r\nGender bias coreference resolution",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 755009916,
    "title": "committing Whoa file",
    "dateCreated": "2020-12-02T07:07:46Z",
    "dateModified": "2020-12-02T07:07:46Z",
    "description": "",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 754966620,
    "title": "add mc taco",
    "dateCreated": "2020-12-02T05:54:55Z",
    "dateModified": "2020-12-02T05:54:55Z",
    "description": "MC-TACO\r\nTemporal commonsense knowledge",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 754946337,
    "title": "add prachathai67k take2",
    "dateCreated": "2020-12-02T05:12:01Z",
    "dateModified": "2020-12-02T05:12:01Z",
    "description": "I decided it will be faster to create a new pull request instead of fixing the rebase issues.\r\ncontinuing from https://github.com/huggingface/datasets/pull/954\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 754937612,
    "title": "add wisesight_sentiment take2",
    "dateCreated": "2020-12-02T04:50:59Z",
    "dateModified": "2020-12-02T04:50:59Z",
    "description": "Take 2 since last time the rebase issues were taking me too much time to fix as opposed to just open a new one.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 754899301,
    "title": "Wongnai - Thai reviews dataset",
    "dateCreated": "2020-12-02T03:20:08Z",
    "dateModified": "2020-12-02T03:20:08Z",
    "description": "40,000 reviews, previously released on GitHub ( https://github.com/wongnai/wongnai-corpus ) with an LGPL license, and on a closed Kaggle competition ( https://www.kaggle.com/c/wongnai-challenge-review-rating-prediction/ )",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 754893337,
    "title": "[WIP] Add multi woz",
    "dateCreated": "2020-12-02T03:05:42Z",
    "dateModified": "2020-12-02T03:05:42Z",
    "description": "This PR adds version 2.2 of the Multi-domain Wizard of OZ dataset: https://github.com/budzianowski/multiwoz/tree/master/data/MultiWOZ_2.2\r\n\r\nIt was a pretty big chunk of work to figure out the structure, so I stil have tol add the description to the README.md\r\n\r\nOn the plus side the structure is broadly similar to that of the Google Schema Guided dialogue [dataset](https://github.com/google-research-datasets/dstc8-schema-guided-dialogue), so will take care of that one next.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 754854478,
    "title": "Add code refinement",
    "dateCreated": "2020-12-02T01:29:58Z",
    "dateModified": "2020-12-02T01:29:58Z",
    "description": "### OVERVIEW\r\nMillions of open-source projects with numerous bug fixes\r\nare available in code repositories. This proliferation\r\nof software development histories can be leveraged to\r\nlearn how to fix common programming bugs\r\nCode refinement aims to automatically fix bugs in the code,\r\nwhich can contribute to reducing the cost of bug-fixes for developers.\r\nGiven a piece of Java code with bugs,\r\nthe task is to remove the bugs to output the refined code.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 754839594,
    "title": "Add ROPES dataset",
    "dateCreated": "2020-12-02T00:52:10Z",
    "dateModified": "2020-12-02T00:52:10Z",
    "description": "ROPES dataset \r\nReasoning over paragraph effects in situations - testing a system's ability to apply knowledge from a passage of text to a new situation. The task is framed into a reading comprehension task following squad-style extractive qa.\r\n\r\nOne thing to note: labels of the test set are hidden (leaderboard submission) so I encoded that as an empty list (ropes.py:L125)",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 754826146,
    "title": "Arabic pos dialect",
    "dateCreated": "2020-12-02T00:21:13Z",
    "dateModified": "2020-12-02T00:21:13Z",
    "description": "A README.md and loading script for the Arabic POS Dialect dataset. The README is missing the sections on personal information, biases, and limitations, as it would probably be better for those to be filled by someone who can read the contents of the dataset and is familiar with Arabic NLP. ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 754823701,
    "title": "add MeTooMA dataset",
    "dateCreated": "2020-12-02T00:15:55Z",
    "dateModified": "2020-12-02T00:15:55Z",
    "description": "This PR adds the #MeToo MA dataset. It presents multi-label data points for tweets mined in the backdrop of the #MeToo movement. The dataset includes data points in the form of Tweet ids and appropriate labels. Please refer to the accompanying paper for detailed information regarding annotation, collection, and guidelines.\r\n\r\nPaper: https://ojs.aaai.org/index.php/ICWSM/article/view/7292\r\nDataset Link: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JN4EYU\r\n\r\n\r\n---\r\nannotations_creators:\r\n- expert-generated\r\nlanguage_creators:\r\n- found\r\nlanguages:\r\n- en\r\nmultilinguality:\r\n- monolingual\r\nsize_categories:\r\n- 1K<n<10K\r\nsource_datasets:\r\n- original\r\ntask_categories:\r\n- text-classification\r\n- text-retrieval\r\ntask_ids:\r\n- multi-class-classification\r\n- multi-label-classification\r\n---\r\n\r\n# Dataset Card for #MeTooMA dataset\r\n\r\n## Table of Contents\r\n- [Dataset Description](#dataset-description)\r\n  - [Dataset Summary](#dataset-summary)\r\n  - [Supported Tasks](#supported-tasks-and-leaderboards)\r\n  - [Languages](#languages)\r\n- [Dataset Structure](#dataset-structure)\r\n  - [Data Instances](#data-instances)\r\n  - [Data Fields](#data-instances)\r\n  - [Data Splits](#data-instances)\r\n- [Dataset Creation](#dataset-creation)\r\n  - [Curation Rationale](#curation-rationale)\r\n  - [Source Data](#source-data)\r\n  - [Annotations](#annotations)\r\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\r\n- [Considerations for Using the Data](#considerations-for-using-the-data)\r\n  - [Social Impact of Dataset](#social-impact-of-dataset)\r\n  - [Discussion of Biases](#discussion-of-biases)\r\n  - [Other Known Limitations](#other-known-limitations)\r\n- [Additional Information](#additional-information)\r\n  - [Dataset Curators](#dataset-curators)\r\n  - [Licensing Information](#licensing-information)\r\n  - [Citation Information](#citation-information)\r\n\r\n## Dataset Description\r\n\r\n- **Homepage:** https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JN4EYU\r\n- **Paper:** https://ojs.aaai.org//index.php/ICWSM/article/view/7292\r\n- **Point of Contact:** https://github.com/midas-research/MeTooMA\r\n\r\n\r\n### Dataset Summary\r\n\r\n- The dataset consists of tweets belonging to #MeToo movement on Twitter, labeled into different categories.\r\n- This dataset includes more data points and has more labels than any of the previous datasets that contain social media\r\nposts about sexual abuse disclosures. Please refer to the Related Datasets of the publication for detailed information about this.\r\n- Due to Twitter's development policies, the authors provide only the tweet IDs and corresponding labels,\r\nother data can be fetched via Twitter API.\r\n- The data has been labeled by experts, with the majority taken into the account for deciding the final label.\r\n- The authors provide these labels for each of the tweets.\r\n  - Relevance\r\n  - Directed Hate\r\n  - Generalized Hate\r\n  - Sarcasm\r\n  - Allegation\r\n  - Justification\r\n  - Refutation\r\n  - Support\r\n  - Oppose\r\n- The definitions for each task/label are in the main publication.\r\n- Please refer to the accompanying paper https://aaai.org/ojs/index.php/ICWSM/article/view/7292 for statistical analysis on the textual data\r\nextracted from this dataset.\r\n- The language of all the tweets in this dataset is English\r\n- Time period: October 2018 - December 2018\r\n- Suggested Use Cases of this dataset:\r\n  - Evaluating usage of linguistic acts such as hate-speech and sarcasm in the context of public sexual abuse disclosures.\r\n  - Extracting actionable insights and virtual dynamics of gender roles in sexual abuse revelations.\r\n  - Identifying how influential people were portrayed on the public platform in the\r\n  events of mass social movements.\r\n  - Polarization analysis based on graph simulations of social nodes of users involved\r\n  in the #MeToo movement.\r\n\r\n\r\n### Supported Tasks and Leaderboards\r\n\r\nMulti-Label and Multi-Class Classification\r\n\r\n### Languages\r\n\r\nEnglish\r\n\r\n## Dataset Structure\r\n- The dataset is structured into CSV format with TweetID and accompanying labels.\r\n- Train and Test sets are split into respective files.\r\n\r\n### Data Instances\r\n\r\nTweet ID and the appropriate labels\r\n\r\n### Data Fields\r\n\r\nTweet ID and appropriate labels (binary label applicable for a data point) and multiple labels for each Tweet ID\r\n\r\n### Data Splits\r\n\r\n- Train: 7979\r\n- Test: 1996\r\n\r\n## Dataset Creation\r\n\r\n### Curation Rationale\r\n\r\n- Twitter was the major source of all the public disclosures of sexual abuse incidents during the #MeToo movement.\r\n- People expressed their opinions over issues that were previously missing from the social media space.\r\n- This provides an option to study the linguistic behaviors of social media users in an informal setting,\r\ntherefore the authors decide to curate this annotated dataset.\r\n- The authors expect this dataset would be of great interest and use to both computational and socio-linguists.\r\n- For computational linguists, it provides an opportunity to model three new complex dialogue acts (allegation, refutation, and justification) and also to study how these acts interact with some of the other linguistic components like stance, hate, and sarcasm. For socio-linguists, it provides an opportunity to explore how a movement manifests in social media.\r\n\r\n\r\n### Source Data\r\n- Source of all the data points in this dataset is a Twitter social media platform.\r\n\r\n#### Initial Data Collection and Normalization\r\n\r\n- All the tweets are mined from Twitter with initial search parameters identified using keywords from the #MeToo movement.\r\n- Redundant keywords were removed based on manual inspection.\r\n- Public streaming APIs of Twitter was used for querying with the selected keywords.\r\n- Based on text de-duplication and cosine similarity score, the set of tweets were pruned.\r\n- Non-English tweets were removed.\r\n- The final set was labeled by experts with the majority label taken into the account for deciding the final label.\r\n- Please refer to this paper for detailed information: https://ojs.aaai.org//index.php/ICWSM/article/view/7292\r\n\r\n#### Who are the source language producers?\r\n\r\nPlease refer to this paper for detailed information: https://ojs.aaai.org//index.php/ICWSM/article/view/7292\r\n\r\n### Annotations\r\n\r\n#### Annotation process\r\n\r\n- The authors chose against crowdsourcing for labeling this dataset due to its highly sensitive nature.\r\n- The annotators are domain experts having degrees in advanced clinical psychology and gender studies.\r\n- They were provided a guidelines document with instructions about each task and its definitions, labels, and examples.\r\n- They studied the document, worked on a few examples to get used to this annotation task.\r\n- They also provided feedback for improving the class definitions.\r\n- The annotation process is not mutually exclusive, implying that the presence of one label does not mean the\r\nabsence of the other one.\r\n\r\n\r\n#### Who are the annotators?\r\n\r\n- The annotators are domain experts having a degree in clinical psychology and gender studies.\r\n- Please refer to the accompanying paper for a detailed annotation process.\r\n\r\n### Personal and Sensitive Information\r\n\r\n- Considering Twitter's policy for distribution of data, only Tweet ID and applicable labels are shared for public use.\r\n- It is highly encouraged to use this dataset for scientific purposes only.\r\n- This dataset collection completely follows the Twitter mandated guidelines for distribution and usage.\r\n\r\n## Considerations for Using the Data\r\n\r\n### Social Impact of Dataset\r\n\r\n- The authors of this dataset do not intend to conduct a population-centric analysis of the #MeToo movement on Twitter.\r\n- The authors acknowledge that findings from this dataset cannot be used as-is for any direct social intervention, these\r\nshould be used to assist already existing human intervention tools and therapies.\r\n- Enough care has been taken to ensure that this work comes off as trying to target a specific person for their\r\nthe personal stance of issues pertaining to the #MeToo movement.\r\n- The authors of this work do not aim to vilify anyone accused in the #MeToo movement in any manner.\r\n- Please refer to the ethics and discussion section of the mentioned publication for appropriate sharing of this dataset\r\nand the social impact of this work.\r\n\r\n\r\n### Discussion of Biases\r\n\r\n- The #MeToo movement acted as a catalyst for implementing social policy changes to benefit the members of\r\nthe community affected by sexual abuse.\r\n- Any work undertaken on this dataset should aim to minimize the bias against minority groups which\r\nmight amplify in cases of a sudden outburst of public reactions over sensitive social media discussions.\r\n\r\n### Other Known Limitations\r\n\r\n- Considering privacy concerns, social media practitioners should be aware of making automated interventions\r\nto aid the victims of sexual abuse as some people might not prefer to disclose their notions.\r\n- Concerned social media users might also repeal their social information if they found out that their\r\ninformation is being used for computational purposes, hence it is important to seek subtle individual consent\r\nbefore trying to profile authors involved in online discussions to uphold personal privacy.\r\n\r\n## Additional Information\r\n\r\nPlease refer to this link: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JN4EYU\r\n\r\n### Dataset Curators\r\n\r\n- If you use the corpus in a product or application, then please credit the authors\r\nand [Multimodal Digital Media Analysis Lab - Indraprastha Institute of Information Technology, New Delhi]\r\n(http://midas.iiitd.edu.in) appropriately.\r\nAlso, if you send us an email, we will be thrilled to know about how you have used the corpus.\r\n- If interested in the commercial use of the corpus, send an email to midas@iiitd.ac.in.\r\n- Multimodal Digital Media Analysis Lab - Indraprastha Institute of Information Technology, New Delhi, India\r\ndisclaims any responsibility for the use of the corpus and does not provide technical support.\r\nHowever, the contact listed above will be happy to respond to queries and clarifications\r\n- Please feel free to send us an email:\r\n  - with feedback regarding the corpus.\r\n  - with information on how you have used the corpus.\r\n  - if interested in having us analyze your social media data.\r\n  - if interested in a collaborative research project.\r\n\r\n### Licensing Information\r\n\r\n[More Information Needed]\r\n\r\n### Citation Information\r\n\r\nPlease cite the following publication if you make use of the dataset: https://ojs.aaai.org/index.php/ICWSM/article/view/7292\r\n\r\n```\r\n\r\n@article{Gautam_Mathur_Gosangi_Mahata_Sawhney_Shah_2020, title={#MeTooMA: Multi-Aspect Annotations of Tweets Related to the MeToo Movement}, volume={14}, url={https://aaai.org/ojs/index.php/ICWSM/article/view/7292}, abstractNote={&lt;p&gt;In this paper, we present a dataset containing 9,973 tweets related to the MeToo movement that were manually annotated for five different linguistic aspects: relevance, stance, hate speech, sarcasm, and dialogue acts. We present a detailed account of the data collection and annotation processes. The annotations have a very high inter-annotator agreement (0.79 to 0.93 k-alpha) due to the domain expertise of the annotators and clear annotation instructions. We analyze the data in terms of geographical distribution, label correlations, and keywords. Lastly, we present some potential use cases of this dataset. We expect this dataset would be of great interest to psycholinguists, socio-linguists, and computational linguists to study the discursive space of digitally mobilized social movements on sensitive issues like sexual harassment.&lt;/p&#38;gt;}, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Gautam, Akash and Mathur, Puneet and Gosangi, Rakesh and Mahata, Debanjan and Sawhney, Ramit and Shah, Rajiv Ratn}, year={2020}, month={May}, pages={209-216} }\r\n\r\n```\r\n\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 754811185,
    "title": "Add MeTooMA Dataset",
    "dateCreated": "2020-12-01T23:44:01Z",
    "dateModified": "2020-12-01T23:44:01Z",
    "description": "",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 754807963,
    "title": "Adding The Microsoft Terminology Collection dataset.",
    "dateCreated": "2020-12-01T23:36:23Z",
    "dateModified": "2020-12-01T23:36:23Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 754787314,
    "title": "Add Children's Book Test (CBT) dataset",
    "dateCreated": "2020-12-01T22:53:26Z",
    "dateModified": "2020-12-01T22:53:26Z",
    "description": "Add the Children's Book Test (CBT) from Facebook (Hill et al. 2016).\r\n\r\nSentence completion given a few sentences as context from a children's book.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 754784041,
    "title": "add piqa",
    "dateCreated": "2020-12-01T22:47:04Z",
    "dateModified": "2020-12-01T22:47:04Z",
    "description": "Physical Interaction: Question Answering (commonsense)\r\nhttps://yonatanbisk.com/piqa/",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 754697489,
    "title": "Add SWAG",
    "dateCreated": "2020-12-01T20:21:05Z",
    "dateModified": "2020-12-01T20:21:05Z",
    "description": "Commonsense NLI -> https://rowanzellers.com/swag/",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 754681940,
    "title": "Add wiki auto dataset",
    "dateCreated": "2020-12-01T19:58:11Z",
    "dateModified": "2020-12-01T19:58:11Z",
    "description": "This PR adds the WikiAuto sentence simplification dataset\r\n\r\nhttps://github.com/chaojiang06/wiki-auto\r\n\r\nThis is also a prospective GEM task, hence the README.md",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 754659015,
    "title": "ADD Afrikaans NER",
    "dateCreated": "2020-12-01T19:23:03Z",
    "dateModified": "2020-12-01T19:23:03Z",
    "description": "Afrikaans NER corpus",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 754578988,
    "title": "Add CS Restaurants dataset",
    "dateCreated": "2020-12-01T17:17:37Z",
    "dateModified": "2020-12-01T17:17:37Z",
    "description": "This PR adds the Czech restaurants dataset for Czech NLG.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 754558686,
    "title": "Add CLINC150 Dataset",
    "dateCreated": "2020-12-01T16:50:13Z",
    "dateModified": "2020-12-01T16:50:13Z",
    "description": "Added CLINC150 Dataset. The link to the dataset can be found [here](https://github.com/clinc/oos-eval) and the paper can be found [here](https://www.aclweb.org/anthology/D19-1131.pdf)\r\n\r\n- [x] Followed the instructions in CONTRIBUTING.md\r\n- [x] Ran the tests successfully\r\n- [x] Created the dummy data",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 754553169,
    "title": "Add CLINC150 Dataset",
    "dateCreated": "2020-12-01T16:43:00Z",
    "dateModified": "2020-12-01T16:43:00Z",
    "description": "Added CLINC150 Dataset. The link to the dataset can be found [here](https://github.com/clinc/oos-eval) and the paper can be found [here](https://www.aclweb.org/anthology/D19-1131.pdf)\r\n\r\n- [x] Followed the instructions in CONTRIBUTING.md\r\n- [x] Ran the tests successfully\r\n- [x] Created the dummy data",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 754474660,
    "title": "Adding the WebNLG dataset",
    "dateCreated": "2020-12-01T15:05:23Z",
    "dateModified": "2020-12-01T15:05:23Z",
    "description": "This PR adds data from the WebNLG challenge, with one configuration per release and challenge iteration.\r\n\r\nMore information can be found [here](https://webnlg-challenge.loria.fr/)\r\n\r\nUnfortunately, the data itself comes from a pretty large number of small XML files, so the dummy data ends up being quite large (8.4 MB even keeping only one example per file).",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 754451234,
    "title": "add CODAH dataset",
    "dateCreated": "2020-12-01T14:37:05Z",
    "dateModified": "2020-12-01T14:37:05Z",
    "description": "Adding CODAH dataset.\r\n\r\nMore info:\r\nhttps://github.com/Websail-NU/CODAH",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 754441428,
    "title": "Add Danish Political Comments Dataset",
    "dateCreated": "2020-12-01T14:28:32Z",
    "dateModified": "2020-12-01T14:28:32Z",
    "description": "",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 754434398,
    "title": "sample multiple datasets ",
    "dateCreated": "2020-12-01T14:20:02Z",
    "dateModified": "2020-12-01T14:20:02Z",
    "description": "Hi\r\nI am dealing with multiple datasets, I need to have a dataloader over them with a condition that in each batch data samples are coming from one of the datasets. My main question is: \r\n-  I need to have a way to sample the datasets first with some weights, lets say 2x dataset1 1x dataset2, could you point me how I can do it\r\n\r\nsub-questions:\r\n- I want to concat sampled datasets and define one dataloader on it, then I need a way to make sure batches come from 1 dataset in each iteration, could you assist me how I can do?\r\n- I use iterative-type of datasets, but I need a method of shuffling still since it brings accuracy performance issues if not doing it, thanks for the help. ",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 754422710,
    "title": "Add code to automate parts of the dataset card",
    "dateCreated": "2020-12-01T14:04:51Z",
    "dateModified": "2020-12-01T14:04:51Z",
    "description": "Most parts of the \"Dataset Structure\" section can be generated automatically. This PR adds some code to do so.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 754418610,
    "title": "Add Tunizi Dataset",
    "dateCreated": "2020-12-01T13:59:39Z",
    "dateModified": "2020-12-01T13:59:39Z",
    "description": "",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 754404095,
    "title": "dataset(ncslgr): add initial loading script",
    "dateCreated": "2020-12-01T13:41:17Z",
    "dateModified": "2020-12-01T13:41:17Z",
    "description": "clean #789",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 754380073,
    "title": "Isixhosa ner corpus",
    "dateCreated": "2020-12-01T13:08:36Z",
    "dateModified": "2020-12-01T13:08:36Z",
    "description": "",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 754368378,
    "title": "Add Norwegian NER",
    "dateCreated": "2020-12-01T12:51:02Z",
    "dateModified": "2020-12-01T12:51:02Z",
    "description": "This PR adds the [Norwegian NER](https://github.com/ljos/navnkjenner) dataset.\r\n\r\nI have added the `conllu` package as a test dependency. This is required to properly parse the `.conllu` files.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 754367291,
    "title": "Added PragmEval benchmark",
    "dateCreated": "2020-12-01T12:49:15Z",
    "dateModified": "2020-12-01T12:49:15Z",
    "description": "",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 754362012,
    "title": "add prachathai67k",
    "dateCreated": "2020-12-01T12:40:55Z",
    "dateModified": "2020-12-01T12:40:55Z",
    "description": "`prachathai-67k`: News Article Corpus and Multi-label Text Classificdation from Prachathai.com\r\nThe prachathai-67k dataset was scraped from the news site Prachathai.\r\nWe filtered out those articles with less than 500 characters of body text, mostly images and cartoons.\r\nIt contains 67,889 articles wtih 12 curated tags from August 24, 2004 to November 15, 2018.\r\nThe dataset was originally scraped by @lukkiddd and cleaned by @cstorm125.\r\nYou can also see preliminary exploration at https://github.com/PyThaiNLP/prachathai-67k/blob/master/exploration.ipynb",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 754359942,
    "title": "added health_fact dataset ",
    "dateCreated": "2020-12-01T12:37:44Z",
    "dateModified": "2020-12-01T12:37:44Z",
    "description": "Added dataset Explainable Fact-Checking for Public Health Claims (dataset_id: health_fact)",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 754357270,
    "title": "Add orange sum",
    "dateCreated": "2020-12-01T12:33:34Z",
    "dateModified": "2020-12-01T12:33:34Z",
    "description": "Add OrangeSum a french abstractive summarization dataset. \r\n\r\nPaper: [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321)",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 754349979,
    "title": "Prachathai67k",
    "dateCreated": "2020-12-01T12:21:52Z",
    "dateModified": "2020-12-01T12:21:52Z",
    "description": "Add `prachathai-67k`: News Article Corpus and Multi-label Text Classificdation from Prachathai.com\r\n\r\nThe `prachathai-67k` dataset was scraped from the news site [Prachathai](prachathai.com). We filtered out those articles with less than 500 characters of body text, mostly images and cartoons. It contains 67,889 articles wtih 12 curated tags from August 24, 2004 to November 15, 2018. The dataset was originally scraped by [@lukkiddd](https://github.com/lukkiddd) and cleaned by [@cstorm125](https://github.com/cstorm125). Download the dataset [here](https://www.dropbox.com/s/fsxepdka4l2pr45/prachathai-67k.zip?dl=1). You can also see preliminary exploration in [exploration.ipynb](https://github.com/PyThaiNLP/prachathai-67k/blob/master/exploration.ipynb).\r\n\r\nThis dataset is a part of [pyThaiNLP](https://github.com/PyThaiNLP/) Thai text [classification-benchmarks](https://github.com/PyThaiNLP/classification-benchmarks). For the benchmark, we selected the following tags with substantial volume that resemble **classifying types of articles**:\r\n\r\n* `\u0e01\u0e32\u0e23\u0e40\u0e21\u0e37\u0e2d\u0e07` - politics\r\n* `\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e21\u0e19\u0e38\u0e29\u0e22\u0e0a\u0e19` - human_rights\r\n* `\u0e04\u0e38\u0e13\u0e20\u0e32\u0e1e\u0e0a\u0e35\u0e27\u0e34\u0e15` - quality_of_life\r\n* `\u0e15\u0e48\u0e32\u0e07\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28` - international\r\n* `\u0e2a\u0e31\u0e07\u0e04\u0e21` - social\r\n* `\u0e2a\u0e34\u0e48\u0e07\u0e41\u0e27\u0e14\u0e25\u0e49\u0e2d\u0e21` - environment\r\n* `\u0e40\u0e28\u0e23\u0e29\u0e10\u0e01\u0e34\u0e08` - economics\r\n* `\u0e27\u0e31\u0e12\u0e19\u0e18\u0e23\u0e23\u0e21` - culture\r\n* `\u0e41\u0e23\u0e07\u0e07\u0e32\u0e19` - labor\r\n* `\u0e04\u0e27\u0e32\u0e21\u0e21\u0e31\u0e48\u0e19\u0e04\u0e07` - national_security\r\n* `\u0e44\u0e2d\u0e0b\u0e35\u0e17\u0e35` - ict\r\n* `\u0e01\u0e32\u0e23\u0e28\u0e36\u0e01\u0e29\u0e32` - education",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 754318686,
    "title": "Support .xz file format",
    "dateCreated": "2020-12-01T11:34:48Z",
    "dateModified": "2020-12-01T11:34:48Z",
    "description": "Add support to extract/uncompress files in .xz format.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 754317777,
    "title": "Add GermaNER Dataset",
    "dateCreated": "2020-12-01T11:33:31Z",
    "dateModified": "2020-12-01T11:33:31Z",
    "description": "",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 754306260,
    "title": "docs(ADD_NEW_DATASET): correct indentation for script",
    "dateCreated": "2020-12-01T11:17:38Z",
    "dateModified": "2020-12-01T11:17:38Z",
    "description": "",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 754286658,
    "title": "Add europeana newspapers",
    "dateCreated": "2020-12-01T10:52:18Z",
    "dateModified": "2020-12-01T10:52:18Z",
    "description": "This PR adds the [Europeana newspapers](https://github.com/EuropeanaNewspapers/ner-corpora) dataset.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 754278632,
    "title": "add PEC dataset",
    "dateCreated": "2020-12-01T10:41:41Z",
    "dateModified": "2020-12-01T10:41:41Z",
    "description": "A persona-based empathetic conversation dataset published at EMNLP 2020.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 754273920,
    "title": "Adding Babi dataset - English version",
    "dateCreated": "2020-12-01T10:35:36Z",
    "dateModified": "2020-12-01T10:35:36Z",
    "description": "Adding the English version of bAbI.\r\n\r\nSamples are taken from ParlAI for consistency with the main users at the moment.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 754228947,
    "title": "Add German Legal Entity Recognition Dataset",
    "dateCreated": "2020-12-01T09:38:22Z",
    "dateModified": "2020-12-01T09:38:22Z",
    "description": "",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 754192491,
    "title": "The FLUE Benchmark",
    "dateCreated": "2020-12-01T09:00:50Z",
    "dateModified": "2020-12-01T09:00:50Z",
    "description": "This PR adds the [FLUE](https://github.com/getalp/Flaubert/tree/master/flue) benchmark which is a set of different datasets to evaluate models for French content.\r\n\r\nTwo datasets are missing, the French Treebank that we can use only for research purpose and we are not allowed to distribute, and the Word Sense disambiguation for Nouns that will be added later.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 754162318,
    "title": "D",
    "dateCreated": "2020-12-01T08:17:10Z",
    "dateModified": "2020-12-01T08:17:10Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 754141321,
    "title": "Add People's Daily NER dataset",
    "dateCreated": "2020-12-01T07:48:53Z",
    "dateModified": "2020-12-01T07:48:53Z",
    "description": "",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 754010753,
    "title": "Add MSRA NER dataset",
    "dateCreated": "2020-12-01T05:02:11Z",
    "dateModified": "2020-12-01T05:02:11Z",
    "description": "",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 753965405,
    "title": "add wisesight_sentiment",
    "dateCreated": "2020-12-01T03:06:39Z",
    "dateModified": "2020-12-01T03:06:39Z",
    "description": "Add `wisesight_sentiment` Social media messages in Thai language with sentiment label (positive, neutral, negative, question)\r\n\r\nModel Card:\r\n---\r\nYAML tags:\r\nannotations_creators:\r\n- expert-generated\r\nlanguage_creators:\r\n- found\r\nlanguages:\r\n- th\r\nlicenses:\r\n- cc0-1.0\r\nmultilinguality:\r\n- monolingual\r\nsize_categories:\r\n- 10K<n<100K\r\nsource_datasets:\r\n- original\r\ntask_categories:\r\n- text-classification\r\ntask_ids:\r\n- sentiment-classification\r\n---\r\n\r\n# Dataset Card for wisesight_sentiment\r\n\r\n## Table of Contents\r\n- [Dataset Description](#dataset-description)\r\n  - [Dataset Summary](#dataset-summary)\r\n  - [Supported Tasks](#supported-tasks-and-leaderboards)\r\n  - [Languages](#languages)\r\n- [Dataset Structure](#dataset-structure)\r\n  - [Data Instances](#data-instances)\r\n  - [Data Fields](#data-instances)\r\n  - [Data Splits](#data-instances)\r\n- [Dataset Creation](#dataset-creation)\r\n  - [Curation Rationale](#curation-rationale)\r\n  - [Source Data](#source-data)\r\n  - [Annotations](#annotations)\r\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\r\n- [Considerations for Using the Data](#considerations-for-using-the-data)\r\n  - [Social Impact of Dataset](#social-impact-of-dataset)\r\n  - [Discussion of Biases](#discussion-of-biases)\r\n  - [Other Known Limitations](#other-known-limitations)\r\n- [Additional Information](#additional-information)\r\n  - [Dataset Curators](#dataset-curators)\r\n  - [Licensing Information](#licensing-information)\r\n  - [Citation Information](#citation-information)\r\n\r\n## Dataset Description\r\n\r\n- **Homepage:** https://github.com/PyThaiNLP/wisesight-sentiment\r\n- **Repository:** https://github.com/PyThaiNLP/wisesight-sentiment\r\n- **Paper:**\r\n- **Leaderboard:** https://www.kaggle.com/c/wisesight-sentiment/\r\n- **Point of Contact:** https://github.com/PyThaiNLP/\r\n\r\n### Dataset Summary\r\n\r\nWisesight Sentiment Corpus: Social media messages in Thai language with sentiment label (positive, neutral, negative, question)\r\n- Released to public domain under Creative Commons Zero v1.0 Universal license.\r\n- Labels: {\"pos\": 0, \"neu\": 1, \"neg\": 2, \"q\": 3}\r\n- Size: 26,737 messages\r\n- Language: Central Thai\r\n- Style: Informal and conversational. With some news headlines and advertisement.\r\n- Time period: Around 2016 to early 2019. With small amount from other period.\r\n- Domains: Mixed. Majority are consumer products and services (restaurants, cosmetics, drinks, car, hotels), with some current affairs.\r\n- Privacy:\r\n    - Only messages that made available to the public on the internet (websites, blogs, social network sites).\r\n    - For Facebook, this means the public comments (everyone can see) that made on a public page.\r\n    - Private/protected messages and messages in groups, chat, and inbox are not included.\r\n- Alternations and modifications:\r\n    - Keep in mind that this corpus does not statistically represent anything in the language register.\r\n    - Large amount of messages are not in their original form. Personal data are removed or masked.\r\n    - Duplicated, leading, and trailing whitespaces are removed. Other punctuations, symbols, and emojis are kept intact.\r\n    (Mis)spellings are kept intact.\r\n    - Messages longer than 2,000 characters are removed.\r\n    - Long non-Thai messages are removed. Duplicated message (exact match) are removed.\r\n- More characteristics of the data can be explore [this notebook](https://github.com/PyThaiNLP/wisesight-sentiment/blob/master/exploration.ipynb)\r\n\r\n### Supported Tasks and Leaderboards\r\n\r\nSentiment analysis / [Kaggle Leaderboard](https://www.kaggle.com/c/wisesight-sentiment/)\r\n\r\n### Languages\r\n\r\nThai\r\n\r\n## Dataset Structure\r\n\r\n### Data Instances\r\n\r\n```\r\n{'category': 'pos', 'texts': '\u0e19\u0e48\u0e32\u0e2a\u0e19\u0e19\u0e19'}\r\n{'category': 'neu', 'texts': '\u0e04\u0e23\u0e31\u0e1a #phithanbkk'}\r\n{'category': 'neg', 'texts': '\u0e0b\u0e37\u0e49\u0e2d\u0e41\u0e15\u0e48\u0e1c\u0e49\u0e32\u0e2d\u0e19\u0e32\u0e21\u0e31\u0e22\u0e41\u0e1a\u0e1a\u0e40\u0e22\u0e47\u0e19\u0e21\u0e32\u0e04\u0e48\u0e30 \u0e41\u0e1a\u0e1a\u0e27\u0e48\u0e32\u0e2d\u0e35\u0e2b\u0e48\u0e32\u0e01\u0e39\u0e19\u0e2d\u0e19\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49'}\r\n{'category': 'q', 'texts': '\u0e21\u0e35\u0e41\u0e2d\u0e25\u0e01\u0e2d\u0e2e\u0e2d\u0e25\u0e21\u0e31\u0e49\u0e22\u0e04\u0e30'}\r\n```\r\n\r\n### Data Fields\r\n\r\n- `texts`: texts \r\n- `category`: sentiment of texts ranging from `pos` (positive; 0), `neu` (neutral; 1), `neg` (negative; 2) and `q` (question; 3)\r\n\r\n### Data Splits\r\n\r\n|           | train | valid | test  |\r\n|-----------|-------|-------|-------|\r\n| # samples | 21628 | 2404  | 2671  |\r\n| # neu     | 11795 | 1291  | 1453  |\r\n| # neg     | 5491  | 637   | 683   |\r\n| # pos     | 3866  | 434   | 478   |\r\n| # q       | 476   | 42    | 57    |\r\n| avg words | 27.21 | 27.18 | 27.12 |\r\n| avg chars | 89.82 | 89.50 | 90.36 |\r\n\r\n## Dataset Creation\r\n\r\n### Curation Rationale\r\n\r\nOriginally, the dataset was conceived for the [In-class Kaggle Competition](https://www.kaggle.com/c/wisesight-sentiment/) at Chulalongkorn university by [Ekapol Chuangsuwanich](https://www.cp.eng.chula.ac.th/en/about/faculty/ekapolc/) (Faculty of Engineering, Chulalongkorn University). It has since become one of the  benchmarks for sentiment analysis in Thai.\r\n\r\n### Source Data\r\n\r\n#### Initial Data Collection and Normalization\r\n\r\n- Style: Informal and conversational. With some news headlines and advertisement.\r\n- Time period: Around 2016 to early 2019. With small amount from other period.\r\n- Domains: Mixed. Majority are consumer products and services (restaurants, cosmetics, drinks, car, hotels), with some current affairs.\r\n- Privacy:\r\n  - Only messages that made available to the public on the internet (websites, blogs, social network sites).\r\n  - For Facebook, this means the public comments (everyone can see) that made on a public page.\r\n  - Private/protected messages and messages in groups, chat, and inbox are not included.\r\n  - Usernames and non-public figure names are removed\r\n  - Phone numbers are masked (e.g. 088-888-8888, 09-9999-9999, 0-2222-2222)\r\n  - If you see any personal data still remain in the set, please tell us - so we can remove them.\r\n- Alternations and modifications:\r\n  - Keep in mind that this corpus does not statistically represent anything in the language register.\r\n  - Large amount of messages are not in their original form. Personal data are removed or masked.\r\n  - Duplicated, leading, and trailing whitespaces are removed. Other punctuations, symbols, and emojis are kept intact.\r\n  - (Mis)spellings are kept intact.\r\n  - Messages longer than 2,000 characters are removed.\r\n  - Long non-Thai messages are removed. Duplicated message (exact match) are removed.\r\n\r\n\r\n#### Who are the source language producers?\r\n\r\nSocial media users in Thailand\r\n\r\n### Annotations\r\n\r\n#### Annotation process\r\n\r\n- Sentiment values are assigned by human annotators.\r\n- A human annotator put his/her best effort to assign just one label, out of four, to a message.\r\n- Agreement, enjoyment, and satisfaction are positive. Disagreement, sadness, and disappointment are negative.\r\n- Showing interest in a topic or in a product is counted as positive. In this sense, a question about a particular product could has a positive sentiment value, if it shows the interest in the product.\r\n- Saying that other product or service is better is counted as negative.\r\n- General information or news title tend to be counted as neutral.\r\n\r\n#### Who are the annotators?\r\n\r\nOutsourced annotators hired by [Wisesight (Thailand) Co., Ltd.](https://github.com/wisesight/)\r\n\r\n### Personal and Sensitive Information\r\n\r\n- We trying to exclude any known personally identifiable information from this data set.\r\n- Usernames and non-public figure names are removed\r\n- Phone numbers are masked (e.g. 088-888-8888, 09-9999-9999, 0-2222-2222)\r\n- If you see any personal data still remain in the set, please tell us - so we can remove them.\r\n\r\n## Considerations for Using the Data\r\n\r\n### Social Impact of Dataset\r\n\r\n- `wisesight_sentiment` is the first and one of the few open datasets for sentiment analysis of social media data in Thai\r\n- There are risks of personal information that escape the anonymization process\r\n\r\n### Discussion of Biases\r\n\r\n- A message can be ambiguous. When possible, the judgement will be based solely on the text itself.\r\n  - In some situation, like when the context is missing, the annotator may have to rely on his/her own world knowledge and just guess.\r\n  - In some cases, the human annotator may have an access to the message's context, like an image. These additional information are not included as part of this corpus.\r\n\r\n### Other Known Limitations\r\n\r\n- The labels are imbalanced; over half of the texts are `neu` (neutral) whereas there are very few `q` (question).\r\n- Misspellings in social media texts make word tokenization process for Thai difficult, thus impacting the model performance\r\n\r\n## Additional Information\r\n\r\n### Dataset Curators\r\n\r\nThanks [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp) community, [Kitsuchart Pasupa](http://www.it.kmitl.ac.th/~kitsuchart/) (Faculty of Information Technology, King Mongkut's Institute of Technology Ladkrabang), and [Ekapol Chuangsuwanich](https://www.cp.eng.chula.ac.th/en/about/faculty/ekapolc/) (Faculty of Engineering, Chulalongkorn University) for advice. The original Kaggle competition, using the first version of this corpus, can be found at https://www.kaggle.com/c/wisesight-sentiment/ \r\n\r\n### Licensing Information\r\n\r\n- If applicable, copyright of each message content belongs to the original poster.\r\n- **Annotation data (labels) are released to public domain.**\r\n- [Wisesight (Thailand) Co., Ltd.](https://github.com/wisesight/) helps facilitate the annotation, but does not necessarily agree upon the labels made by the human annotators. This annotation is for research purpose and does not reflect the professional work that Wisesight has been done for its customers.\r\n- The human annotator does not necessarily agree or disagree with the message. Likewise, the label he/she made to the message does not necessarily reflect his/her personal view towards the message.\r\n\r\n### Citation Information\r\n\r\nPlease cite the following if you make use of the dataset:\r\n\r\nArthit Suriyawongkul, Ekapol Chuangsuwanich, Pattarawat Chormai, and Charin Polpanumas. 2019. **PyThaiNLP/wisesight-sentiment: First release.** September.\r\n\r\nBibTeX:\r\n```\r\n@software{bact_2019_3457447,\r\n  author       = {Suriyawongkul, Arthit and\r\n                  Chuangsuwanich, Ekapol and\r\n                  Chormai, Pattarawat and\r\n                  Polpanumas, Charin},\r\n  title        = {PyThaiNLP/wisesight-sentiment: First release},\r\n  month        = sep,\r\n  year         = 2019,\r\n  publisher    = {Zenodo},\r\n  version      = {v1.0},\r\n  doi          = {10.5281/zenodo.3457447},\r\n  url          = {https://doi.org/10.5281/zenodo.3457447}\r\n}\r\n```\r\n\r\n ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 753940979,
    "title": "V-1.0.0 of isizulu_ner_corpus",
    "dateCreated": "2020-12-01T02:04:32Z",
    "dateModified": "2020-12-01T02:04:32Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 753921078,
    "title": "Local machine/cluster Beam Datasets example/tutorial",
    "dateCreated": "2020-12-01T01:11:43Z",
    "dateModified": "2020-12-01T01:11:43Z",
    "description": "Hi,\r\n\r\nI'm wondering if https://huggingface.co/docs/datasets/beam_dataset.html has an non-GCP or non-Dataflow version example/tutorial? I tried to migrate it to run on DirectRunner and SparkRunner, however, there were way too many runtime errors that I had to fix during the process, and even so I wasn't able to get either runner correctly producing the desired output.\r\n\r\nThanks!\r\nShang",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 753915603,
    "title": "Added HANS parses and categories",
    "dateCreated": "2020-12-01T00:58:16Z",
    "dateModified": "2020-12-01T00:58:16Z",
    "description": "This pull request adds HANS missing information: the sentence parses, as well as the heuristic category.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 753863055,
    "title": "add PIB dataset",
    "dateCreated": "2020-11-30T22:55:43Z",
    "dateModified": "2020-11-30T22:55:43Z",
    "description": "This pull request will add PIB dataset.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 753860095,
    "title": "small updates to the \"add new dataset\" guide",
    "dateCreated": "2020-11-30T22:49:10Z",
    "dateModified": "2020-11-30T22:49:10Z",
    "description": "small updates (corrections/typos) to the \"add new dataset\" guide",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 753854272,
    "title": "Add NumerSense",
    "dateCreated": "2020-11-30T22:36:33Z",
    "dateModified": "2020-11-30T22:36:33Z",
    "description": "Adds the NumerSense dataset\r\n- Webpage/leaderboard: https://inklab.usc.edu/NumerSense/\r\n- Paper: https://arxiv.org/abs/2005.00683\r\n- Description: NumerSense is a new numerical commonsense reasoning probing task, with a diagnostic dataset consisting of 3,145 masked-word-prediction probes. Basically, it's a benchmark to see whether your MLM can figure out the right number in a fill-in-the-blank task based on commonsense knowledge (a bird has **two** legs)",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 753840300,
    "title": "adding metooma dataset",
    "dateCreated": "2020-11-30T22:09:49Z",
    "dateModified": "2020-11-30T22:09:49Z",
    "description": "",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 753818193,
    "title": "[WIP] complex_webqa - Error zipfile.BadZipFile: Bad CRC-32",
    "dateCreated": "2020-11-30T21:30:21Z",
    "dateModified": "2020-11-30T21:30:21Z",
    "description": "Have a string `zipfile.BadZipFile: Bad CRC-32 for file 'web_snippets_train.json'` error when downloading the largest file from dropbox: `https://www.dropbox.com/sh/7pkwkrfnwqhsnpo/AABVENv_Q9rFtnM61liyzO0La/web_snippets_train.json.zip?dl=1`\r\n\r\nDidn't managed to see how to solve that.\r\n\r\nPutting aside for now.\r\n",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 753801204,
    "title": "Lambada",
    "dateCreated": "2020-11-30T21:02:33Z",
    "dateModified": "2020-11-30T21:02:33Z",
    "description": "Added LAMBADA dataset.\r\n\r\nA couple of points of attention (mostly because I am not sure)\r\n- The training data are compressed in a .tar file inside the main tar.gz file. I had to manually un-tar the training file to access the examples.\r\n- The dev and test splits don't have the `category` field so I put `None` by default.\r\n\r\nHappy to make changes if it doesn't respect the guidelines!\r\n\r\nVictor",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 753737794,
    "title": "Add weibo NER dataset",
    "dateCreated": "2020-11-30T19:22:47Z",
    "dateModified": "2020-11-30T19:22:47Z",
    "description": "",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 753722324,
    "title": "Add the Multilingual Amazon Reviews Corpus",
    "dateCreated": "2020-11-30T18:58:06Z",
    "dateModified": "2020-11-30T18:58:06Z",
    "description": "- **Name:** Multilingual Amazon Reviews Corpus* (`amazon_reviews_multi`)\r\n- **Description:** A collection of Amazon reviews in English, Japanese, German, French, Spanish and Chinese.\r\n- **Paper:** https://arxiv.org/abs/2010.02573\r\n\r\n### Checkbox\r\n\r\n- [x] Create the dataset script `/datasets/my_dataset/my_dataset.py` using the template\r\n- [x] Fill the `_DESCRIPTION` and `_CITATION` variables\r\n- [x] Implement `_infos()`, `_split_generators()` and `_generate_examples()`\r\n- [x] Make sure that the `BUILDER_CONFIGS` class attribute is filled with the different configurations of the dataset and that the `BUILDER_CONFIG_CLASS` is specified if there is a custom config class.\r\n- [x] Generate the metadata file `dataset_infos.json` for all configurations\r\n- [x] Generate the dummy data `dummy_data.zip` files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card `README.md` using the template : fill the tags and the various paragraphs\r\n- [x] Both tests for the real data and the dummy data pass.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 753679020,
    "title": "Hello",
    "dateCreated": "2020-11-30T17:50:05Z",
    "dateModified": "2020-11-30T17:50:05Z",
    "description": "",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 753676069,
    "title": "add inquisitive",
    "dateCreated": "2020-11-30T17:45:22Z",
    "dateModified": "2020-11-30T17:45:22Z",
    "description": "Adding inquisitive qg dataset\r\n\r\nMore info: https://github.com/wjko2/INQUISITIVE",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 753672661,
    "title": "Add Turku NLP Corpus for Finnish NER",
    "dateCreated": "2020-11-30T17:40:19Z",
    "dateModified": "2020-11-30T17:40:19Z",
    "description": "",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 753631951,
    "title": "Add DART",
    "dateCreated": "2020-11-30T16:42:37Z",
    "dateModified": "2020-11-30T16:42:37Z",
    "description": "- **Name:** *DART*\r\n- **Description:** *DART is a large dataset for open-domain structured data record to text generation.*\r\n- **Paper:** *https://arxiv.org/abs/2007.02871*\r\n- **Data:** *https://github.com/Yale-LILY/dart#leaderboard*\r\n\r\n### Checkbox\r\n\r\n- [x] Create the dataset script `/datasets/my_dataset/my_dataset.py` using the template\r\n- [x] Fill the `_DESCRIPTION` and `_CITATION` variables\r\n- [x] Implement `_infos()`, `_split_generators()` and `_generate_examples()`\r\n- [x] Make sure that the `BUILDER_CONFIGS` class attribute is filled with the different configurations of the dataset and that the `BUILDER_CONFIG_CLASS` is specified if there is a custom config class.\r\n- [x] Generate the metadata file `dataset_infos.json` for all configurations\r\n- [x] Generate the dummy data `dummy_data.zip` files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card `README.md` using the template : fill the tags and the various paragraphs\r\n- [x] Both tests for the real data and the dummy data pass.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 753569220,
    "title": "Add CC-100 dataset",
    "dateCreated": "2020-11-30T15:23:22Z",
    "dateModified": "2020-11-30T15:23:22Z",
    "description": "Add CC-100.\r\n\r\nClose #773 ",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 753559130,
    "title": "Add XOR QA Dataset",
    "dateCreated": "2020-11-30T15:10:54Z",
    "dateModified": "2020-11-30T15:10:54Z",
    "description": "Added XOR Question Answering Dataset. The link to the dataset can be found [here](https://nlp.cs.washington.edu/xorqa/)\r\n\r\n- [x] Followed the instructions in CONTRIBUTING.md\r\n- [x] Ran the tests successfully\r\n- [x] Created the dummy data",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 753445747,
    "title": "add dream dataset",
    "dateCreated": "2020-11-30T12:40:14Z",
    "dateModified": "2020-11-30T12:40:14Z",
    "description": "Adding Dream: a Dataset and for Dialogue-Based Reading Comprehension\r\n\r\nMore details:\r\nhttps://dataset.org/dream/\r\nhttps://github.com/nlpdata/dream",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 753434472,
    "title": "wrong length with datasets ",
    "dateCreated": "2020-11-30T12:23:39Z",
    "dateModified": "2020-11-30T12:23:39Z",
    "description": "Hi\r\nI have a MRPC dataset which I convert it to seq2seq format, then this is of this format:\r\n\r\n`Dataset(features: {'src_texts': Value(dtype='string', id=None), 'tgt_texts': Value(dtype='string', id=None)}, num_rows: 10)\r\n`\r\n\r\nI feed it to a dataloader:\r\n```\r\ndataloader = DataLoader(\r\n            train_dataset,\r\n            batch_size=self.args.train_batch_size,\r\n            sampler=train_sampler,\r\n            collate_fn=self.data_collator,\r\n            drop_last=self.args.dataloader_drop_last,\r\n            num_workers=self.args.dataloader_num_workers,\r\n        )\r\n```\r\n\r\nnow if I type len(dataloader) this is 1, which is wrong, and this needs to be 10. could you assist me please? thanks \r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 753397440,
    "title": "Add conll2002",
    "dateCreated": "2020-11-30T11:29:35Z",
    "dateModified": "2020-11-30T11:29:35Z",
    "description": "Adding the Conll2002 dataset for NER.\r\n\r\nMore info here : https://www.clips.uantwerpen.be/conll2002/ner/\r\n\r\n### Checkbox\r\n\r\n- [x] Create the dataset script `/datasets/my_dataset/my_dataset.py` using the template\r\n- [x] Fill the `_DESCRIPTION` and `_CITATION` variables\r\n- [x] Implement `_infos()`, `_split_generators()` and `_generate_examples()`\r\n- [x] Make sure that the `BUILDER_CONFIGS` class attribute is filled with the different configurations of the dataset and that the `BUILDER_CONFIG_CLASS` is specified if there is a custom config class.\r\n- [x] Generate the metadata file `dataset_infos.json` for all configurations\r\n- [x] Generate the dummy data `dummy_data.zip` files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card `README.md` using the template : fill the tags and the various paragraphs\r\n- [x] Both tests for the real data and the dummy data pass.\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 753391591,
    "title": "Addition of Concode Dataset ",
    "dateCreated": "2020-11-30T11:20:59Z",
    "dateModified": "2020-11-30T11:20:59Z",
    "description": "##Overview\r\nConcode Dataset contains pairs of Nl Queries and the corresponding Code.(Contextual Code Generation)\r\n\r\nReference Links\r\nPaper Link = https://arxiv.org/pdf/1904.09086.pdf\r\nGithub Link =https://github.com/microsoft/CodeXGLUE/tree/main/Text-Code/text-to-code",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 753376643,
    "title": "Add Swedish NER Corpus",
    "dateCreated": "2020-11-30T10:59:51Z",
    "dateModified": "2020-11-30T10:59:51Z",
    "description": "",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 753118481,
    "title": "Shall we change the hashing to encoding to reduce potential replicated cache files?",
    "dateCreated": "2020-11-30T03:50:46Z",
    "dateModified": "2020-11-30T03:50:46Z",
    "description": "Hi there. For now, we are using `xxhash` to hash the transformations to fingerprint and we will save a copy of the processed dataset to disk if there is a new hash value. However, there are some transformations that are idempotent or commutative to each other. I think that encoding the transformation chain as the fingerprint may help in those cases, for example, use `base64.urlsafe_b64encode`. In this way, before we want to save a new copy, we can decode the transformation chain and normalize it to prevent omit potential reuse. As the main targets of this project are the really large datasets that cannot be loaded entirely in memory, I believe it would save a lot of time if we can avoid some write.\r\n\r\nIf you have interest in this, I'd love to help :).",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 752956106,
    "title": "Add list_github_datasets api for retrieving dataset name list in github repo",
    "dateCreated": "2020-11-29T16:42:15Z",
    "dateModified": "2020-11-29T16:42:15Z",
    "description": "Thank you for your great effort on unifying data processing for NLP!\r\n\r\nThis pr is trying to add a new api `list_github_datasets` in the `inspect` module. The reason for it is that the current `list_datasets` api need to access https://huggingface.co/api/datasets to get a large json. However, this connection can be really slow... (I was visiting from China) and from my own experience, most of the time `requests.get` failed to download the whole json after a long wait and will trigger fault in `r.json()`.\r\nI also noticed that the current implementation will first try to download from github, which makes me be able to smoothly run `load_dataset('squad')` in the example.\r\nTherefore, I think it would be better if we can have an api to get the list of datasets that are available on github, and it will also improve newcomers' experience (it is a little frustrating if one cannot successfully run the first function in the README example.) before we have faster source for huggingface.co.\r\n\r\nAs for the implementation, I've added a `dataset_infos.json` file under the `datasets` folder, and it has the following structure:\r\n```json\r\n    {\r\n        \"id\": \"aeslc\",\r\n        \"folder\": \"datasets/aeslc\",\r\n        \"dataset_infos\": \"datasets/aeslc/dataset_infos.json\"\r\n    },\r\n    ...\r\n    {\r\n        \"id\": \"json\",\r\n        \"folder\": \"datasets/json\"\r\n    },\r\n   ...\r\n```\r\nThe script I used to get this file is:\r\n```python\r\nimport json\r\nimport os\r\n\r\nDATASETS_BASE_DIR = \"/root/datasets\" \r\nDATASET_INFOS_JSON = \"dataset_infos.json\"\r\n\r\ndatasets = []\r\nfor item in os.listdir(os.path.join(DATASETS_BASE_DIR, \"datasets\")):\r\n    if os.path.isdir(os.path.join(DATASETS_BASE_DIR, \"datasets\", item)):\r\n        datasets.append(item)\r\n\r\ndatasets.sort()\r\n\r\ntotal_ds_info = []\r\nfor ds in datasets:\r\n    ds_dir = os.path.join(\"datasets\", ds)\r\n    ds_info_dir = os.path.join(ds_dir, DATASET_INFOS_JSON)\r\n    if os.path.isfile(os.path.join(DATASETS_BASE_DIR, ds_info_dir)):\r\n        total_ds_info.append({\"id\": ds,\r\n                              \"folder\": ds_dir,\r\n                              \"dataset_infos\": ds_info_dir})\r\n    else:\r\n        total_ds_info.append({\"id\": ds,\r\n                              \"folder\": ds_dir})\r\n\r\nwith open(DATASET_INFOS_JSON, \"w\") as f:\r\n    json.dump(total_ds_info, f)\r\n```\r\nThe new `dataset_infos.json` was saved as a formated json so that it will be easy to add new dataset.\r\n\r\nWhen calling `list_github_datasets`, the user will get the list of dataset names in this github repo and if `with_details` is set to be `True`, they can get the url of specific dataset info.\r\n\r\nThank you for your time on reviewing this pr :).",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 752892020,
    "title": "My new dataset PEC",
    "dateCreated": "2020-11-29T11:10:37Z",
    "dateModified": "2020-11-29T11:10:37Z",
    "description": "A new dataset PEC published in EMNLP 2020.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 752806215,
    "title": "datasets module not found",
    "dateCreated": "2020-11-29T01:24:15Z",
    "dateModified": "2020-11-29T01:24:15Z",
    "description": "Currently, running `from datasets import load_dataset` will throw a `ModuleNotFoundError: No module named 'datasets'` error.\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 752772723,
    "title": "Grindr meeting app web.Grindr",
    "dateCreated": "2020-11-28T21:36:23Z",
    "dateModified": "2020-11-28T21:36:23Z",
    "description": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 752508299,
    "title": "Add FiNER dataset",
    "dateCreated": "2020-11-27T23:54:20Z",
    "dateModified": "2020-11-27T23:54:20Z",
    "description": "Hi,\r\n\r\nthis PR adds \"A Finnish News Corpus for Named Entity Recognition\" as new `finer` dataset.\r\n\r\nThe dataset is described in [this paper](https://arxiv.org/abs/1908.04212). The data is publicly available in [this GitHub](https://github.com/mpsilfve/finer-data).\r\n\r\nNotice: they provide two testsets. The additional test dataset taken from Wikipedia is named as \"test_wikipedia\" split.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 752428652,
    "title": "Add dependency on black for tests",
    "dateCreated": "2020-11-27T19:12:48Z",
    "dateModified": "2020-11-27T19:12:48Z",
    "description": "Add package 'black' as an installation requirement for tests.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 752422351,
    "title": "Remove os.path.join from all URLs",
    "dateCreated": "2020-11-27T18:55:30Z",
    "dateModified": "2020-11-27T18:55:30Z",
    "description": "Remove `os.path.join` from all URLs in dataset scripts.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 752403395,
    "title": "Fix url with backslash in windows for blimp and pg19",
    "dateCreated": "2020-11-27T17:59:11Z",
    "dateModified": "2020-11-27T17:59:11Z",
    "description": "Following #903 I also fixed blimp and pg19 which were using the `os.path.join` to create urls\r\n\r\ncc @albertvillanova",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 752395456,
    "title": "Disallow backslash in urls",
    "dateCreated": "2020-11-27T17:38:28Z",
    "dateModified": "2020-11-27T17:38:28Z",
    "description": "Following #903 @albertvillanova noticed that there are sometimes bad usage of `os.path.join` in datasets scripts to create URLS. However this should be avoided since it doesn't work on windows.\r\n\r\nI'm suggesting a test to make sure we that all the urls don't have backslashes in them in the datasets scripts.\r\nThe tests works by adding a callback feature to the MockDownloadManager used to test the dataset scripts. In a download callback I just make sure that the url is valid.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 752372743,
    "title": "Very detailed step-by-step on how to add a dataset",
    "dateCreated": "2020-11-27T16:45:21Z",
    "dateModified": "2020-11-27T16:45:21Z",
    "description": "Add very detailed step-by-step instructions to add a new dataset to the library.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 752360614,
    "title": "Fix URL with backslash in Windows",
    "dateCreated": "2020-11-27T16:26:24Z",
    "dateModified": "2020-11-27T16:26:24Z",
    "description": "In Windows, `os.path.join` generates URLs containing backslashes, when the first \"path\" does not end with a slash. \r\n\r\nIn general, `os.path.join` should be avoided to generate URLs.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 752345739,
    "title": "Follow cache_dir parameter to gcs downloader",
    "dateCreated": "2020-11-27T16:02:06Z",
    "dateModified": "2020-11-27T16:02:06Z",
    "description": "As noticed in #900 the cache_dir parameter was not followed to the downloader in the case of an already processed dataset hosted on our google storage (one of them is natural questions).\r\n\r\nFix #900 ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 752233851,
    "title": "Addition of Nl2Bash Dataset",
    "dateCreated": "2020-11-27T12:53:55Z",
    "dateModified": "2020-11-27T12:53:55Z",
    "description": "## Overview\r\nThe NL2Bash data contains over 10,000 instances of linux shell commands and their corresponding natural language descriptions provided by experts, from the Tellina system. The dataset features 100+ commonly used shell utilities.\r\n## Footnotes\r\nThe following dataset marks the first ML on source code related Dataset in datasets module. It'll be really useful as a lot of the research direction involves Transformer Based Model.\r\nThanks.\r\n### Reference Links\r\n\r\n> Paper Link = https://arxiv.org/pdf/1802.08979.pdf \r\n> Github Link = https://github.com/TellinaTool/nl2bash\r\n\r\n\r\n\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 752214066,
    "title": "datasets.load_dataset() custom chaching directory bug",
    "dateCreated": "2020-11-27T12:18:53Z",
    "dateModified": "2020-11-27T12:18:53Z",
    "description": "Hello,\r\nI'm having issue with loading a dataset with a custom `cache_dir`. Despite specifying the output dir, it is still downloaded to \r\n `~/.cache`.\r\n\r\n## Environment info\r\n- `datasets` version: 1.1.3\r\n- Platform: Linux-4.19.129-aufs-1-x86_64-with-debian-10.1\r\n- Python version: 3.7.3\r\n\r\n## The code I'm running:\r\n```python\r\nimport datasets\r\nfrom pathlib import Path\r\n\r\nvalidation_dataset = datasets.load_dataset(\"natural_questions\", split=\"validation[:5%]\", cache_dir=Path(\"./data\"))  \r\n```\r\n\r\n## The output:\r\n\r\n* The dataset is downloaded to my home directory's `.cache` \r\n* A new empty directory named \"`natural_questions` is created in the specified directory `.data`\r\n* `tree data` in the shell outputs:\r\n```\r\ndata\r\n\u2514\u2500\u2500 natural_questions\r\n    \u2514\u2500\u2500 default\r\n        \u2514\u2500\u2500 0.0.2\r\n3 directories, 0 files\r\n```\r\n\r\nThe output:\r\n```\r\nDownloading: 8.61kB [00:00, 5.11MB/s]                                                                                                                                                                              \r\nDownloading: 13.6kB [00:00, 7.89MB/s]                                                                                                                                                                              \r\nUsing custom data configuration default                                                                                                                                                                            \r\nDownloading and preparing dataset natural_questions/default (download: 41.97 GiB, generated: 92.95 GiB, post-processed: Unknown size, total: 134.92 GiB) to ./data/natural_questions/default/0.0.2/867dbbaf9137c1b8\r\n3ecb19f5eb80559e1002ea26e702c6b919cfa81a17a8c531...                                                                                                                                                                \r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13.6k/13.6k [00:00<00:00, 1.51MB/s]                                                                                                          \r\nDownloading:   7%|\u2588\u2588\u2588\u258e                                            | 6.70G/97.4G [03:46<1:37:05, 15.6MB/s]\r\n```\r\n\r\n## Expected behaviour:\r\nThe dataset \"Natural Questions\" should be downloaded to the directory \"./data\"\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 752191227,
    "title": "Allow arrow based builder in auto dummy data generation",
    "dateCreated": "2020-11-27T11:39:38Z",
    "dateModified": "2020-11-27T11:39:38Z",
    "description": "Following #898 I added support for arrow based builder for the auto dummy data generator",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 752148284,
    "title": "Adding SQA dataset",
    "dateCreated": "2020-11-27T10:29:18Z",
    "dateModified": "2020-11-27T10:29:18Z",
    "description": "As discussed in #880 \r\n\r\nSeems like automatic dummy-data generation doesn't work if the builder is a `ArrowBasedBuilder`, do you think you could take a look @lhoestq ?",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 752100256,
    "title": "Dataset viewer issues",
    "dateCreated": "2020-11-27T09:14:34Z",
    "dateModified": "2020-11-27T09:14:34Z",
    "description": "I was looking through the dataset viewer and I like it a lot. Version numbers, citation information, everything's there! I've spotted a few issues/bugs though:\r\n\r\n- the URL is still under `nlp`, perhaps an alias for `datasets` can be made\r\n- when I remove a **feature** (and the feature list is empty), I get an error. This is probably expected, but perhaps a better error message can be shown to the user\r\n\r\n```bash\r\nIndexError: list index out of range\r\nTraceback:\r\nFile \"/home/sasha/streamlit/lib/streamlit/ScriptRunner.py\", line 322, in _run_script\r\n    exec(code, module.__dict__)\r\nFile \"/home/sasha/nlp-viewer/run.py\", line 316, in <module>\r\n    st.table(style)\r\nFile \"/home/sasha/streamlit/lib/streamlit/DeltaGenerator.py\", line 122, in wrapped_method\r\n    return dg._enqueue_new_element_delta(marshall_element, delta_type, last_index)\r\nFile \"/home/sasha/streamlit/lib/streamlit/DeltaGenerator.py\", line 367, in _enqueue_new_element_delta\r\n    rv = marshall_element(msg.delta.new_element)\r\nFile \"/home/sasha/streamlit/lib/streamlit/DeltaGenerator.py\", line 120, in marshall_element\r\n    return method(dg, element, *args, **kwargs)\r\nFile \"/home/sasha/streamlit/lib/streamlit/DeltaGenerator.py\", line 2944, in table\r\n    data_frame_proto.marshall_data_frame(data, element.table)\r\nFile \"/home/sasha/streamlit/lib/streamlit/elements/data_frame_proto.py\", line 54, in marshall_data_frame\r\n    _marshall_styles(proto_df.style, df, styler)\r\nFile \"/home/sasha/streamlit/lib/streamlit/elements/data_frame_proto.py\", line 73, in _marshall_styles\r\n    translated_style = styler._translate()\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/pandas/io/formats/style.py\", line 351, in _translate\r\n    * (len(clabels[0]) - len(hidden_columns))\r\n```\r\n\r\n- there seems to be **an encoding issue** in the default view, the dataset examples are shown as raw monospace text, without a decent encoding. That makes it hard to read for languages that use a lot of special characters. Take for instance the [cs-en WMT19 set](https://huggingface.co/nlp/viewer/?dataset=wmt19&config=cs-en). This problem goes away when you enable \"List view\", because then some syntax highlighteris used, and the special characters are coded correctly.\r\n",
    "status": "open",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 751834265,
    "title": "Add template and documentation for dataset card",
    "dateCreated": "2020-11-26T21:30:25Z",
    "dateModified": "2020-11-26T21:30:25Z",
    "description": "This PR adds a template for dataset cards, as well as a guide to filling out the template and a completed example for the ELI5 dataset, building on the work of @mcmillanmajora \r\n\r\nNew pull requests adding datasets should now have a README.md file which serves both to hold the tags we will have to index the datasets and as a data statement.\r\n\r\nThe template is designed to be pretty extensive. The idea is that the person who uploads the dataset should put in all the basic information (at least the Dataset Description section) and whatever else they feel comfortable adding and leave the `[More Information Needed]` annotation everywhere else as a placeholder.\r\n\r\nWe will then work with @mcmillanmajora to involve the data authors more directly in filling out the remaining information.\r\n\r\nDirect links to:\r\n- [Documentation](https://github.com/yjernite/datasets/blob/add_dataset_card_doc/templates/README_guide.md)\r\n- [Empty template](https://github.com/yjernite/datasets/blob/add_dataset_card_doc/templates/README.md)\r\n- [ELI5 example](https://github.com/yjernite/datasets/blob/add_dataset_card_doc/datasets/eli5/README.md)",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 751782295,
    "title": "Better messages regarding split naming",
    "dateCreated": "2020-11-26T18:55:46Z",
    "dateModified": "2020-11-26T18:55:46Z",
    "description": "I made explicit the error message when a bad split name is used.\r\n\r\nAlso I wanted to allow the `-` symbol for split names but actually this symbol is used to name the arrow files `{dataset_name}-{dataset_split}.arrow` so we should probably keep it this way, i.e. not allowing the `-` symbol in split names. Moreover in the future we might want to use `{dataset_name}-{dataset_split}-{shard_id}_of_{n_shards}.arrow` and reuse the `-` symbol.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 751734905,
    "title": "Allow several tags sets",
    "dateCreated": "2020-11-26T17:04:13Z",
    "dateModified": "2020-11-26T17:04:13Z",
    "description": "Hi !\r\n\r\nCurrently we have three dataset cards : snli, cnn_dailymail and allocine.\r\nFor each one of those datasets a set of tag is defined. The set of tags contains fields like `multilinguality`, `task_ids`, `licenses` etc.\r\n\r\nFor certain datasets like `glue` for example, there exist several configurations: `sst2`, `mnli` etc. Therefore we should define one set of tags per configuration. However the current format used for tags only supports one set of tags per dataset.\r\n\r\nIn this PR I propose a simple change in the yaml format used for tags to allow for several sets of tags.\r\n\r\nLet me know what you think, especially @julien-c let me know if it's good for you since it's going to be parsed by moon-landing",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 751703696,
    "title": "add metrec: arabic poetry dataset",
    "dateCreated": "2020-11-26T16:10:16Z",
    "dateModified": "2020-11-26T16:10:16Z",
    "description": "",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 751658262,
    "title": "Add a few datasets of reference in the documentation",
    "dateCreated": "2020-11-26T15:02:39Z",
    "dateModified": "2020-11-26T15:02:39Z",
    "description": "I started making a small list of various datasets of reference in the documentation.\r\nSince many datasets share a lot in common I think it's good to have a list of datasets scripts to get some inspiration from.\r\n\r\nLet me know what you think, and if you have ideas of other datasets that we may add to this list, please let me know.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 751576869,
    "title": "gitignore .python-version",
    "dateCreated": "2020-11-26T13:05:58Z",
    "dateModified": "2020-11-26T13:05:58Z",
    "description": "ignore `.python-version` added by `pyenv`",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 751534050,
    "title": "Add LER",
    "dateCreated": "2020-11-26T11:58:23Z",
    "dateModified": "2020-11-26T11:58:23Z",
    "description": "",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 751115691,
    "title": "Optional per-dataset default config name",
    "dateCreated": "2020-11-25T21:02:30Z",
    "dateModified": "2020-11-25T21:02:30Z",
    "description": "This PR adds a `DEFAULT_CONFIG_NAME` class attribute to `DatasetBuilder`. This allows a dataset to have a specified default config name when a dataset has more than one config but the user does not specify it. For example, after defining `DEFAULT_CONFIG_NAME = \"combined\"` in PolyglotNER, a user can now do the following:\r\n\r\n```python\r\nds = load_dataset(\"polyglot_ner\")\r\n```\r\nwhich is equivalent to,\r\n```python\r\nds = load_dataset(\"polyglot_ner\", \"combined\")\r\n```\r\nIn effect (for this particular dataset configuration), this means that if the user doesn't specify a language, they are given the combined dataset including all languages.\r\n\r\nSince it doesn't always make sense to have a default config, this feature is opt-in. If `DEFAULT_CONFIG_NAME` is not defined and a user does not pass a config for a dataset with multiple configs available, a ValueError is raised like usual.\r\n\r\nLet me know what you think about this approach @lhoestq @thomwolf and I'll add some documentation and define a default for some of our existing datasets.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 750944422,
    "title": "Nested lists are zipped unexpectedly",
    "dateCreated": "2020-11-25T16:07:46Z",
    "dateModified": "2020-11-25T16:07:46Z",
    "description": "I might misunderstand something, but I expect that if I define:\r\n```python\r\n\"top\": datasets.features.Sequence({\r\n  \"middle\": datasets.features.Sequence({\r\n    \"bottom\": datasets.Value(\"int32\")\r\n  })\r\n})\r\n```\r\n\r\nAnd I then create an example:\r\n```python\r\nyield 1, {\r\n  \"top\": [{\r\n    \"middle\": [\r\n      {\"bottom\": 1},\r\n      {\"bottom\": 2}\r\n    ]\r\n  }]\r\n}\r\n```\r\n\r\nI then load my dataset:\r\n```python\r\ntrain = load_dataset(\"my dataset\")[\"train\"]\r\n```\r\n\r\nand expect to be able to access `data[0][\"top\"][0][\"middle\"][0]`.\r\n\r\nThat is not the case. Here is `data[0]` as JSON:\r\n\r\n```json\r\n{\"top\": {\"middle\": [{\"bottom\": [1, 2]}]}}\r\n```\r\n\r\nClearly different than the thing I inputted.\r\n```json\r\n{\"top\": [{\"middle\": [{\"bottom\": 1},{\"bottom\": 2}]}]}\r\n```",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 750868831,
    "title": "pyarrow.lib.ArrowNotImplementedError: MakeBuilder: cannot construct builder for type extension<arrow.py_extension_type>",
    "dateCreated": "2020-11-25T14:32:21Z",
    "dateModified": "2020-11-25T14:32:21Z",
    "description": "I set up a new dataset, with a sequence of arrays (really, I want to have an array of (None, 137, 2), and the first dimension is dynamic) \r\n\r\n```python\r\n    def _info(self):\r\n        return datasets.DatasetInfo(\r\n            description=_DESCRIPTION,\r\n            # This defines the different columns of the dataset and their types\r\n            features=datasets.Features(\r\n                {\r\n                    \"pose\": datasets.features.Sequence(datasets.features.Array2D(shape=(137, 2), dtype=\"float32\"))\r\n                }\r\n            ),\r\n            homepage=_HOMEPAGE,\r\n            citation=_CITATION,\r\n        )\r\n    def _generate_examples(self):\r\n        \"\"\" Yields examples. \"\"\"\r\n\r\n        yield 1, {\r\n            \"pose\": [np.zeros(shape=(137, 2), dtype=np.float32)]\r\n        }\r\n```\r\n\r\nBut this doesn't work -\r\n> pyarrow.lib.ArrowNotImplementedError: MakeBuilder: cannot construct builder for type extension<arrow.py_extension_type>",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 750829314,
    "title": "Fix wikipedia custom config",
    "dateCreated": "2020-11-25T13:44:12Z",
    "dateModified": "2020-11-25T13:44:12Z",
    "description": "It should be possible to use the wikipedia dataset with any `language` and `date`.\r\nHowever it was not working as noticed in #784 . Indeed the custom wikipedia configurations were not enabled for some reason.\r\n\r\nI fixed that and was able to run \r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"./datasets/wikipedia\", language=\"zh\", date=\"20201120\", beam_runner='DirectRunner')\r\n```\r\n\r\ncc @stvhuang @SamuelCahyawijaya\r\n\r\nFix #784",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 750789052,
    "title": "Very slow cold-start",
    "dateCreated": "2020-11-25T12:47:58Z",
    "dateModified": "2020-11-25T12:47:58Z",
    "description": "Hi,\r\nI expect when importing `datasets` that nothing major happens in the background, and so the import should be insignificant.\r\nWhen I load a metric, or a dataset, its fine that it takes time.\r\n\r\nThe following ranges from 3 to 9 seconds:\r\n```\r\npython -m timeit -n 1 -r 1 'from datasets import load_dataset'\r\n```\r\n\r\nedit:\r\nsorry for the mis-tag, not sure how I added it.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 749862034,
    "title": "Auto generate dummy data",
    "dateCreated": "2020-11-24T16:31:34Z",
    "dateModified": "2020-11-24T16:31:34Z",
    "description": "When adding a new dataset to the library, dummy data creation can take some time.\r\nTo make things easier I added a command line tool that automatically generates dummy data when possible.\r\n\r\nThe tool only supports certain data files types: txt, csv, tsv, jsonl, json and xml.\r\n\r\nHere are some examples:\r\n```\r\npython datasets-cli dummy_data ./datasets/snli --auto_generate\r\npython datasets-cli dummy_data ./datasets/squad --auto_generate --json_field data\r\npython datasets-cli dummy_data ./datasets/iwslt2017 --auto_generate --xml_tag seg --match_text_files \"train*\" --n_lines 15\r\n# --xml_tag seg => each sample corresponds to a \"seg\" tag in the xml tree\r\n# --match_text_files \"train*\" =>  also match text files that don't have a proper text file extension (no suffix like \".txt\" for example)\r\n# --n_lines 15 => some text files have headers so we have to use at least 15 lines\r\n```\r\n\r\nand here is the command usage:\r\n\r\n```\r\nusage: datasets-cli <command> [<args>] dummy_data [-h] [--auto_generate]\r\n                                  [--n_lines N_LINES]\r\n                                  [--json_field JSON_FIELD]\r\n                                  [--xml_tag XML_TAG]\r\n                                  [--match_text_files MATCH_TEXT_FILES]\r\n                                  [--keep_uncompressed]\r\n                                  [--cache_dir CACHE_DIR]\r\n                                  path_to_dataset\r\n\r\npositional arguments:\r\npath_to_dataset       Path to the dataset (example: ./datasets/squad)\r\n\r\noptional arguments:\r\n-h, --help            show this help message and exit\r\n--auto_generate       Try to automatically generate dummy data\r\n--n_lines N_LINES     Number of lines or samples to keep when auto-\r\n                                                generating dummy data\r\n--json_field JSON_FIELD\r\n                                                Optional, json field to read the data from when auto-\r\n                                                generating dummy data. In the json data files, this\r\n                                                field must point to a list of samples as json objects\r\n                                                (ex: the 'data' field for squad-like files)\r\n--xml_tag XML_TAG     Optional, xml tag name of the samples inside the xml\r\n                                                files when auto-generating dummy data.\r\n--match_text_files MATCH_TEXT_FILES\r\n                                                Optional, a comma separated list of file patterns that\r\n                                                looks for line-by-line text files other than *.txt or\r\n                                                *.csv. Example: --match_text_files *.label\r\n--keep_uncompressed   Don't compress the dummy data folders when auto-\r\n                                                generating dummy data. Useful for debugging for to do\r\n                                                manual adjustements before compressing.\r\n--cache_dir CACHE_DIR\r\n                                                Cache directory to download and cache files when auto-\r\n                                                generating dummy data\r\n```\r\n\r\nThe command generates all the necessary `dummy_data.zip` files (one per config).\r\n\r\nHow it works:\r\n- it runs the split_generators() method of the dataset script to download the original data files\r\n- when downloading it records a mapping between the downloaded files and the corresponding expected dummy data files paths\r\n- then for each data file it creates the dummy data file keeping only the first samples (the strategy depends on the type of file)\r\n- finally it compresses the dummy data folders into dummy_zip files ready for dataset tests\r\n\r\nLet me know if that makes sense or if you have ideas to improve this tool !\r\n\r\nI also added a unit test.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 749750801,
    "title": "Downloading/caching only a part of a datasets' dataset.",
    "dateCreated": "2020-11-24T14:25:18Z",
    "dateModified": "2020-11-24T14:25:18Z",
    "description": "Hi,\r\nI want to use the validation data *only* (of natural question).\r\nI don't want to have the whole dataset cached in my machine, just the dev set.\r\nIs this possible? I can't find a way to do it in the docs.\r\n\r\nThank you,\r\nSapir",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 749662188,
    "title": "Update README.md",
    "dateCreated": "2020-11-24T12:23:52Z",
    "dateModified": "2020-11-24T12:23:52Z",
    "description": "\"no label\" is \"-\" in the original dataset but \"-1\" in Huggingface distribution.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 749548107,
    "title": "Use GCP download url instead of tensorflow custom download for boolq",
    "dateCreated": "2020-11-24T09:47:11Z",
    "dateModified": "2020-11-24T09:47:11Z",
    "description": "BoolQ is a dataset that used tf.io.gfile.copy to download the file from a GCP bucket.\r\nIt prevented the dataset to be downloaded twice because of a FileAlreadyExistsError.\r\nEven though the error could be fixed by providing `overwrite=True` to the tf.io.gfile.copy call, I changed the script to use GCP download urls and use regular downloads instead and remove the tensorflow dependency.\r\n\r\nFix #875 ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 748949606,
    "title": "Add SQA",
    "dateCreated": "2020-11-23T16:31:55Z",
    "dateModified": "2020-11-23T16:31:55Z",
    "description": "## Adding a Dataset\r\n- **Name:** SQA (Sequential Question Answering) by Microsoft. \r\n- **Description:** The SQA dataset was created to explore the task of answering sequences of inter-related questions on HTML tables. It has 6,066 sequences with 17,553 questions in total.\r\n- **Paper:** https://www.microsoft.com/en-us/research/publication/search-based-neural-structured-learning-sequential-question-answering/\r\n- **Data:** https://www.microsoft.com/en-us/download/details.aspx?id=54253\r\n- **Motivation:** currently, the [Tapas](https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html) algorithm by Google AI is being added to the Transformers library (see https://github.com/huggingface/transformers/pull/8113). It would be great to use that model in combination with this dataset, on which it achieves SOTA results (average question accuracy of 0.71).\r\n\r\nNote 1: this dataset actually consists of 2 types of files: \r\n1) TSV files, containing the questions, answer coordinates and answer texts (for training, dev and test)\r\n2) a folder of csv files, which contain the actual tabular data\r\n\r\nNote 2: if you download the dataset straight from the download link above, then you will see that the `answer_coordinates` and `answer_text` columns are string lists of string tuples and strings respectively, which is not ideal. It would be better to make them true Python lists of tuples and strings respectively (using `ast.literal_eval`), before uploading them to the HuggingFace hub.\r\n\r\nAdding this would be great! Then we could possibly also add [WTQ (WikiTable Questions)](https://github.com/ppasupat/WikiTableQuestions) and [TabFact (Tabular Fact Checking)](https://github.com/wenhuchen/Table-Fact-Checking) on which TAPAS also achieves state-of-the-art results. Note that the TAPAS algorithm requires these datasets to first be converted into the SQA format.\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 748848847,
    "title": "boolq does not load ",
    "dateCreated": "2020-11-23T14:28:28Z",
    "dateModified": "2020-11-23T14:28:28Z",
    "description": "Hi\r\nI am getting these errors trying to load boolq thanks \r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 5, in <module>\r\n    data = AutoTask().get(\"boolq\").get_dataset(\"train\", n_obs=10)\r\n  File \"/remote/idiap.svm/user.active/rkarimi/dev/internship/seq2seq/tasks/tasks.py\", line 42, in get_dataset\r\n    dataset = self.load_dataset(split=split)\r\n  File \"/remote/idiap.svm/user.active/rkarimi/dev/internship/seq2seq/tasks/tasks.py\", line 38, in load_dataset\r\n    return datasets.load_dataset(self.task.name, split=split)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 531, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \" /idiap/home/rkarimi/.cache/huggingface/modules/datasets_modules/datasets/boolq/2987db1f15deaa19500ae24de560eabeaf1f8ef51df88c0470beeec72943bf11/boolq.py\", line 74, in _split_generators\r\n    downloaded_files = dl_manager.download_custom(urls_to_download, tf.io.gfile.copy)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 150, in download_custom\r\n    get_from_cache(url, cache_dir=cache_dir, local_files_only=True, use_etag=False)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 472, in get_from_cache\r\n    f\"Cannot find the requested files in the cached path at {cache_path} and outgoing traffic has been\"\r\nFileNotFoundError: Cannot find the requested files in the cached path at /idiap/home/rkarimi/.cache/huggingface/datasets/eaee069e38f6ceaa84de02ad088c34e63ec97671f2cd1910ddb16b10dc60808c and outgoing traffic has been disabled. To enable file online look-ups, set 'local_files_only' to False.\r\n",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 748621981,
    "title": "Loading Data From S3 Path in Sagemaker",
    "dateCreated": "2020-11-23T09:17:22Z",
    "dateModified": "2020-11-23T09:17:22Z",
    "description": "In Sagemaker Im tring to load the data set from S3 path as follows\r\n\r\n`train_path = 's3://xxxxxxxxxx/xxxxxxxxxx/train.csv'\r\n    valid_path = 's3://xxxxxxxxxx/xxxxxxxxxx/validation.csv'\r\n    test_path = 's3://xxxxxxxxxx/xxxxxxxxxx/test.csv'\r\n    \r\n    data_files = {}\r\n    data_files[\"train\"] = train_path\r\n    data_files[\"validation\"] = valid_path\r\n    data_files[\"test\"] = test_path\r\n    extension = train_path.split(\".\")[-1]\r\n    datasets = load_dataset(extension, data_files=data_files, s3_enabled=True)\r\n    print(datasets)`\r\n\r\n\r\nI getting an error of\r\n\r\n`algo-1-7plil_1  |   File \"main.py\", line 21, in <module>\r\nalgo-1-7plil_1  |     datasets = load_dataset(extension, data_files=data_files)\r\nalgo-1-7plil_1  |   File \"/opt/conda/lib/python3.6/site-packages/datasets/load.py\", line 603, in load_dataset\r\nalgo-1-7plil_1  |     **config_kwargs,\r\nalgo-1-7plil_1  |   File \"/opt/conda/lib/python3.6/site-packages/datasets/builder.py\", line 155, in __init__\r\nalgo-1-7plil_1  |     **config_kwargs,\r\nalgo-1-7plil_1  |   File \"/opt/conda/lib/python3.6/site-packages/datasets/builder.py\", line 305, in _create_builder_config\r\nalgo-1-7plil_1  |     m.update(str(os.path.getmtime(data_file)))\r\nalgo-1-7plil_1  |   File \"/opt/conda/lib/python3.6/genericpath.py\", line 55, in getmtime\r\nalgo-1-7plil_1  |     return os.stat(filename).st_mtime\r\nalgo-1-7plil_1  | FileNotFoundError: [Errno 2] No such file or directory: 's3://lsmv-sagemaker/pubmedbert/test.csv`\r\n\r\nBut when im trying with pandas , it is able to load from S3\r\n\r\nDoes the datasets library support S3 path to load",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 748234438,
    "title": "DataLoader(datasets) become more and more slowly within iterations",
    "dateCreated": "2020-11-22T12:41:10Z",
    "dateModified": "2020-11-22T12:41:10Z",
    "description": "Hello, when I for loop my dataloader, the loading speed is becoming more and more slowly!\r\n```\r\ndataset = load_from_disk(dataset_path)  # around 21,000,000 lines\r\n\r\nlineloader = tqdm(DataLoader(dataset, batch_size=1))\r\nfor idx, line in enumerate(lineloader):\r\n     # do some thing for each line\r\n```\r\nIn the begining, the loading speed is around 2000it/s, but after 1 minutes later, the speed is much slower, just around 800it/s.\r\n\r\nAnd when I set `num_workers=4` in DataLoader, the loading speed is much lower, just 130it/s.\r\n\r\nCould you please help me with this problem?\r\nThanks a lot!",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 748195104,
    "title": "imdb dataset cannot be loaded ",
    "dateCreated": "2020-11-22T08:24:43Z",
    "dateModified": "2020-11-22T08:24:43Z",
    "description": "Hi\r\nI am trying to load the imdb train dataset\r\n\r\n`dataset = datasets.load_dataset(\"imdb\", split=\"train\")`\r\n\r\ngetting following errors, thanks for your help \r\n```\r\nTraceback (most recent call last):        \r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 558, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/info_utils.py\", line 73, in verify_splits\r\n    raise NonMatchingSplitsSizesError(str(bad_splits))\r\ndatasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='test', num_bytes=32660064, num_examples=25000, dataset_name='imdb'), 'recorded': SplitInfo(name='test', num_bytes=26476338, num_examples=20316, dataset_name='imdb')}, {'expected': SplitInfo(name='train', num_bytes=33442202, num_examples=25000, dataset_name='imdb'), 'recorded': SplitInfo(name='train', num_bytes=0, num_examples=0, dataset_name='imdb')}, {'expected': SplitInfo(name='unsupervised', num_bytes=67125548, num_examples=50000, dataset_name='imdb'), 'recorded': SplitInfo(name='unsupervised', num_bytes=0, num_examples=0, dataset_name='imdb')}]\r\n>>> dataset = datasets.load_dataset(\"imdb\", split=\"train\")\r\n\r\n```\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 748194311,
    "title": "bug in boolq dataset loading",
    "dateCreated": "2020-11-22T08:18:34Z",
    "dateModified": "2020-11-22T08:18:34Z",
    "description": "Hi\r\nI am trying to load boolq dataset:\r\n\r\n```\r\nimport datasets\r\ndatasets.load_dataset(\"boolq\")\r\n```\r\n\r\nI am getting the following errors, thanks for your help \r\n\r\n```\r\n>>> import datasets\r\n2020-11-22 09:16:30.070332: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2020-11-22 09:16:30.070389: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n>>> datasets.load_dataset(\"boolq\")\r\ncahce dir  /idiap/temp/rkarimi/cache_home/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home/datasets\r\nUsing custom data configuration default\r\nDownloading and preparing dataset boolq/default (download: 8.36 MiB, generated: 7.47 MiB, post-processed: Unknown size, total: 15.83 MiB) to /idiap/temp/rkarimi/cache_home/datasets/boolq/default/0.1.0/2987db1f15deaa19500ae24de560eabeaf1f8ef51df88c0470beeec72943bf11...\r\ncahce dir  /idiap/temp/rkarimi/cache_home/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home/datasets/downloads\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 531, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \" /idiap/home/rkarimi/.cache/huggingface/modules/datasets_modules/datasets/boolq/2987db1f15deaa19500ae24de560eabeaf1f8ef51df88c0470beeec72943bf11/boolq.py\", line 74, in _split_generators\r\n    downloaded_files = dl_manager.download_custom(urls_to_download, tf.io.gfile.copy)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 149, in download_custom\r\n    custom_download(url, path)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 516, in copy_v2\r\n    compat.path_to_bytes(src), compat.path_to_bytes(dst), overwrite)\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: file already exists\r\n\r\n\r\n```",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 748193140,
    "title": "trec dataset unavailable ",
    "dateCreated": "2020-11-22T08:09:36Z",
    "dateModified": "2020-11-22T08:09:36Z",
    "description": "Hi\r\nwhen I try to load the trec dataset I am getting these errors, thanks for your help\r\n\r\n`datasets.load_dataset(\"trec\",  split=\"train\")\r\n`\r\n```\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 531, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \" /idiap/home/rkarimi/.cache/huggingface/modules/datasets_modules/datasets/trec/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/trec.py\", line 140, in _split_generators\r\n    dl_files = dl_manager.download_and_extract(_URLs)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 254, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 179, in download\r\n    num_proc=download_config.num_proc,\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 225, in map_nested\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 225, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 163, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 477, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach http://cogcomp.org/Data/QA/QC/train_5500.label\r\n```",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 747959523,
    "title": "load_dataset('cnn_dalymail', '3.0.0') gives a 'Not a directory' error",
    "dateCreated": "2020-11-21T06:30:45Z",
    "dateModified": "2020-11-21T06:30:45Z",
    "description": "```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('cnn_dailymail', '3.0.0')\r\n```\r\nStack trace:\r\n```\r\n---------------------------------------------------------------------------\r\n\r\nNotADirectoryError                        Traceback (most recent call last)\r\n\r\n<ipython-input-6-2e06a8332652> in <module>()\r\n      1 from datasets import load_dataset\r\n----> 2 dataset = load_dataset('cnn_dailymail', '3.0.0')\r\n\r\n5 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)\r\n    608         download_config=download_config,\r\n    609         download_mode=download_mode,\r\n--> 610         ignore_verifications=ignore_verifications,\r\n    611     )\r\n    612 \r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    513                     if not downloaded_from_gcs:\r\n    514                         self._download_and_prepare(\r\n--> 515                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    516                         )\r\n    517                     # Sync info\r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    568         split_dict = SplitDict(dataset_name=self.name)\r\n    569         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 570         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    571 \r\n    572         # Checksums verification\r\n\r\n/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _split_generators(self, dl_manager)\r\n    252     def _split_generators(self, dl_manager):\r\n    253         dl_paths = dl_manager.download_and_extract(_DL_URLS)\r\n--> 254         train_files = _subset_filenames(dl_paths, datasets.Split.TRAIN)\r\n    255         # Generate shared vocabulary\r\n    256 \r\n\r\n/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _subset_filenames(dl_paths, split)\r\n    153     else:\r\n    154         logging.fatal(\"Unsupported split: %s\", split)\r\n--> 155     cnn = _find_files(dl_paths, \"cnn\", urls)\r\n    156     dm = _find_files(dl_paths, \"dm\", urls)\r\n    157     return cnn + dm\r\n\r\n/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _find_files(dl_paths, publisher, url_dict)\r\n    132     else:\r\n    133         logging.fatal(\"Unsupported publisher: %s\", publisher)\r\n--> 134     files = sorted(os.listdir(top_dir))\r\n    135 \r\n    136     ret_files = []\r\n\r\nNotADirectoryError: [Errno 20] Not a directory: '/root/.cache/huggingface/datasets/downloads/1bc05d24fa6dda2468e83a73cf6dc207226e01e3c48a507ea716dc0421da583b/cnn/stories'\r\n```\r\nI have ran the code on Google Colab",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 747653697,
    "title": "Add IndicGLUE dataset and Metrics",
    "dateCreated": "2020-11-20T17:09:34Z",
    "dateModified": "2020-11-20T17:09:34Z",
    "description": "Added IndicGLUE benchmark for evaluating models on 11 Indian Languages. The descriptions of the tasks and the corresponding paper can be found [here](https://indicnlp.ai4bharat.org/indic-glue/)\r\n\r\n- [x] Followed the instructions in CONTRIBUTING.md\r\n- [x] Ran the tests successfully \r\n- [x] Created the dummy data",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 747470136,
    "title": "terminate called after throwing an instance of 'google::protobuf::FatalException'",
    "dateCreated": "2020-11-20T12:56:24Z",
    "dateModified": "2020-11-20T12:56:24Z",
    "description": "Hi\r\nI am using the dataset \"iwslt2017-en-nl\", and after downloading it I am getting this error when trying to evaluate it on T5-base with seq2seq_trainer.py in the huggingface repo could you assist me please? thanks \r\n\r\n\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63/63 [02:47<00:00,  2.18s/it][libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0): \r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: (index) >= (0): \r\nrun_t5_base_eval.sh: line 19:  5795 Aborted ",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 747021996,
    "title": "[Feature Request] Add optional parameter in text loading script to preserve linebreaks",
    "dateCreated": "2020-11-19T23:51:31Z",
    "dateModified": "2020-11-19T23:51:31Z",
    "description": "I'm working on a project about rhyming verse using phonetic poetry and song lyrics, and line breaks are a vital part of the data. \r\n\r\nI recently switched over to use the datasets library when my various corpora grew larger than my computer's memory. And so far, it is SO great. \r\n\r\nBut the first time I processed all of my data into a dataset, I hadn't realized the text loader script was processing the source files line-by-line and stripping off the newlines. \r\n\r\nOnce I caught the issue, I made my own data loader by modifying one line in the default text loader (changing `batch = batch.splitlines()` to `batch = batch.splitlines(True)` inside `_generate_tables`). And so I'm all set as far as my project is concerned.\r\n\r\nBut if my use case is more general, it seems like it'd be pretty trivial to add a kwarg to the default text loader called keeplinebreaks or something, which would default to False and get passed to `splitlines()`. ",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 746495711,
    "title": "Update ner datasets infos",
    "dateCreated": "2020-11-19T11:28:03Z",
    "dateModified": "2020-11-19T11:28:03Z",
    "description": "Update the dataset_infos.json files for changes made in #850 regarding the ner datasets feature types (and the change to ClassLabel)\r\nI also fixed the ner types of conll2003",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 745889882,
    "title": "Consistent metric outputs",
    "dateCreated": "2020-11-18T18:05:59Z",
    "dateModified": "2020-11-18T18:05:59Z",
    "description": "To automate the use of metrics, they should return consistent outputs.\r\nIn particular I'm working on adding a conversion of metrics to keras metrics.\r\nTo achieve this we need two things:\r\n- have each metric return dictionaries of string -> floats since each keras metrics should return one float\r\n- define in the metric info the different fields of the output dictionary\r\n\r\nIn this PR I'm adding these two features.\r\nI also fixed a few bugs in some metrics\r\n\r\n#867 needs to be merged first",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 745773955,
    "title": "Fix some metrics feature types",
    "dateCreated": "2020-11-18T15:46:11Z",
    "dateModified": "2020-11-18T15:46:11Z",
    "description": "Replace `int` feature type to `int32` since `int` is not a pyarrow dtype in those metrics:\r\n- accuracy\r\n- precision\r\n- recall\r\n- f1\r\nI also added the sklearn citation and used keyword arguments to remove future warnings",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 745719222,
    "title": "OSCAR from Inria group",
    "dateCreated": "2020-11-18T14:40:54Z",
    "dateModified": "2020-11-18T14:40:54Z",
    "description": "## Adding a Dataset\r\n- **Name:** *OSCAR* (Open Super-large Crawled ALMAnaCH coRpus), multilingual parsing of Common Crawl (separate crawls for many different languages), [here](https://oscar-corpus.com/).\r\n- **Description:** *OSCAR or Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.*\r\n- **Paper:** *[here](https://hal.inria.fr/hal-02148693)*\r\n- **Data:** *[here](https://oscar-corpus.com/)*\r\n- **Motivation:** *useful for unsupervised tasks in separate languages. In an ideal world, your team would be able to obtain the unshuffled version, that could be used to train GPT-2-like models (the shuffled version, I suppose, could be used for translation).*\r\n\r\nI am aware that you do offer the \"colossal\" Common Crawl dataset already, but this has the advantage to be available in many subcorpora for different languages.\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 745430497,
    "title": "Have Trouble importing `datasets`",
    "dateCreated": "2020-11-18T08:04:41Z",
    "dateModified": "2020-11-18T08:04:41Z",
    "description": "I'm failing to import transformers (v4.0.0-dev), and tracing the cause seems to be failing to import datasets.\r\n\r\nI cloned the newest version of datasets (master branch), and do `pip install -e .`.\r\n\r\nThen, `import datasets` causes the error below.\r\n\r\n```\r\n~/workspace/Clone/datasets/src/datasets/utils/file_utils.py in <module>\r\n    116 sys.path.append(str(HF_MODULES_CACHE))\r\n    117 \r\n--> 118 os.makedirs(HF_MODULES_CACHE, exist_ok=True)\r\n    119 if not os.path.exists(os.path.join(HF_MODULES_CACHE, \"__init__.py\")):\r\n    120     with open(os.path.join(HF_MODULES_CACHE, \"__init__.py\"), \"w\"):\r\n\r\n~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/os.py in makedirs(name, mode, exist_ok)\r\n    221             return\r\n    222     try:\r\n--> 223         mkdir(name, mode)\r\n    224     except OSError:\r\n    225         # Cannot rely on checking for EEXIST, since the operating system \r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: '<MY_HOME_DIRECTORY>/.cache/huggingface/modules'\r\n```\r\n\r\nThe error occurs in `os.makedirs` in `file_utils.py`, even though `exist_ok = True` option is set.\r\n(I use Python 3.8, so `exist_ok` is expected to work.)\r\n\r\nI've checked some environment variables, and they are set as below.\r\n\r\n```\r\n*** NameError: name 'HF_MODULES_CACHE' is not defined\r\n*** NameError: name 'hf_cache_home' is not defined\r\n*** NameError: name 'XDG_CACHE_HOME' is not defined\r\n```\r\n\r\nShould I set some environment variables before using this library?\r\nAnd, do you have any idea why \"No such file or directory\" occurs even though the `exist_ok = True` option is set?\r\n\r\nThank you in advance.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 745322357,
    "title": "Unable to download cnn_dailymail dataset",
    "dateCreated": "2020-11-18T04:38:02Z",
    "dateModified": "2020-11-18T04:38:02Z",
    "description": "### Script to reproduce the error\r\n```\r\nfrom datasets import load_dataset\r\n\r\ntrain_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split= 'train[:10%')\r\nvalid_dataset = load_dataset(\"cnn_dailymail\",\"3.0.0\", split=\"validation[:5%]\")\r\n```\r\n\r\n\r\n### Error\r\n```\r\n---------------------------------------------------------------------------\r\nNotADirectoryError                        Traceback (most recent call last)\r\n<ipython-input-8-47c39c228935> in <module>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 train_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split= 'train[:10%')\r\n      4 valid_dataset = load_dataset(\"cnn_dailymail\",\"3.0.0\", split=\"validation[:5%]\")\r\n\r\n5 frames\r\n/usr/local/lib/python3.6/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)\r\n    609         download_config=download_config,\r\n    610         download_mode=download_mode,\r\n--> 611         ignore_verifications=ignore_verifications,\r\n    612     )\r\n    613 \r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    469                     if not downloaded_from_gcs:\r\n    470                         self._download_and_prepare(\r\n--> 471                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    472                         )\r\n    473                     # Sync info\r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    524         split_dict = SplitDict(dataset_name=self.name)\r\n    525         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 526         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    527 \r\n    528         # Checksums verification\r\n\r\n/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _split_generators(self, dl_manager)\r\n    252     def _split_generators(self, dl_manager):\r\n    253         dl_paths = dl_manager.download_and_extract(_DL_URLS)\r\n--> 254         train_files = _subset_filenames(dl_paths, datasets.Split.TRAIN)\r\n    255         # Generate shared vocabulary\r\n    256 \r\n\r\n/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _subset_filenames(dl_paths, split)\r\n    153     else:\r\n    154         logging.fatal(\"Unsupported split: %s\", split)\r\n--> 155     cnn = _find_files(dl_paths, \"cnn\", urls)\r\n    156     dm = _find_files(dl_paths, \"dm\", urls)\r\n    157     return cnn + dm\r\n\r\n/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _find_files(dl_paths, publisher, url_dict)\r\n    132     else:\r\n    133         logging.fatal(\"Unsupported publisher: %s\", publisher)\r\n--> 134     files = sorted(os.listdir(top_dir))\r\n    135 \r\n    136     ret_files = []\r\n\r\nNotADirectoryError: [Errno 20] Not a directory: '/root/.cache/huggingface/datasets/downloads/1bc05d24fa6dda2468e83a73cf6dc207226e01e3c48a507ea716dc0421da583b/cnn/stories'\r\n\r\n```\r\n\r\nThanks for any suggestions.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 744954534,
    "title": "Add clear_cache parameter in the test command",
    "dateCreated": "2020-11-17T17:52:29Z",
    "dateModified": "2020-11-17T17:52:29Z",
    "description": "For certain datasets like OSCAR #348 there are lots of different configurations and each one of them can take a lot of disk space.\r\n\r\nI added a `--clear_cache` flag to the `datasets-cli test` command to be able to clear the cache after each configuration test to avoid filling up the disk. It should enable an easier generation for the `dataset_infos.json` file for OSCAR.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 744906131,
    "title": "Update head requests",
    "dateCreated": "2020-11-17T16:49:06Z",
    "dateModified": "2020-11-17T16:49:06Z",
    "description": "Get requests and Head requests didn't have the same parameters.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 744753458,
    "title": "Possible Bug: Small training/dataset file creates gigantic output",
    "dateCreated": "2020-11-17T13:48:59Z",
    "dateModified": "2020-11-17T13:48:59Z",
    "description": "Hey guys,\r\n\r\nI was trying to create a new bert model from scratch via _huggingface transformers + tokenizers + dataets_ (actually using this example script by your team: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py). It was supposed to be a first test with a small 5 GB raw text file but I can't even end the preprocessing handled by datasets because this tiny 5 GB text file becomes more than 1 TB when processing. My system was running out of space and crashed prematurely.\r\n\r\nI've done training from scratch via Google's bert repo in the past and I can remember that the resulting pretraining data can become quite big. But 5 GB becoming 1 TB was never the case. Is this considered normal or is it a bug?\r\n\r\nI've used the following CMD:\r\n`python xla_spawn.py --num_cores=8 run_mlm.py --model_type bert --config_name config.json --tokenizer_name tokenizer.json --train_file dataset_full.txt --do_train --output_dir out --max_steps 500000 --save_steps 2500 --save_total_limit 2 --prediction_loss_only --line_by_line --max_seq_length 128 --pad_to_max_length --preprocessing_num_workers 16 --per_device_train_batch_size 128 --overwrite_output_dir --debug`\r\n\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 744750691,
    "title": "wmt16 cs-en does not donwload ",
    "dateCreated": "2020-11-17T13:45:35Z",
    "dateModified": "2020-11-17T13:45:35Z",
    "description": "Hi\r\nI am trying with wmt16, cs-en pair, thanks for the help, perhaps similar to the ro-en issue. thanks\r\n\r\n split=\"train\", n_obs=data_args.n_train) for task in data_args.task}\r\n  File \"finetune_t5_trainer.py\", line 109, in <dictcomp>\r\n    split=\"train\", n_obs=data_args.n_train) for task in data_args.task}\r\n  File \"/home/rabeeh/internship/seq2seq/tasks/tasks.py\", line 82, in get_dataset\r\n    dataset = load_dataset(\"wmt16\", self.pair, split=split)\r\n  File \"/opt/conda/envs/internship/lib/python3.7/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/opt/conda/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/opt/conda/envs/internship/lib/python3.7/site-packages/datasets/builder.py\", line 531, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/rabeeh/.cache/huggingface/modules/datasets_modules/datasets/wmt16/7b2c4443a7d34c2e13df267eaa8cab4c62dd82f6b62b0d9ecc2e3a673ce17308/wmt_utils.py\", line 755, in _split_generators\r\n    downloaded_files = dl_manager.download_and_extract(urls_to_download)\r\n  File \"/opt/conda/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 254, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/opt/conda/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 179, in download\r\n    num_proc=download_config.num_proc,\r\n  File \"/opt/conda/envs/internship/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 225, in map_nested\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/opt/conda/envs/internship/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 225, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/opt/conda/envs/internship/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 181, in _single_map_nested\r\n    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]\r\n  File \"/opt/conda/envs/internship/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 181, in <listcomp>\r\n    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]\r\n  File \"/opt/conda/envs/internship/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 163, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/opt/conda/envs/internship/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/opt/conda/envs/internship/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 475, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 743917091,
    "title": "Integrate file_lock inside the lib for better logging control",
    "dateCreated": "2020-11-16T15:13:39Z",
    "dateModified": "2020-11-16T15:13:39Z",
    "description": "Previously the locking system of the lib was based on the file_lock package. However as noticed in #812 there were too many logs printed even when the datasets logging was set to warnings or errors.\r\n\r\nFor example\r\n```python\r\nimport logging\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\nimport datasets\r\ndatasets.set_verbosity_warning()\r\ndatasets.load_dataset(\"squad\")\r\n```\r\nwould still log the file lock events:\r\n```\r\nINFO:filelock:Lock 5737989232 acquired on /Users/quentinlhoest/.cache/huggingface/datasets/44801f118d500eff6114bfc56ab4e6def941f1eb14b70ac1ecc052e15cdac49d.85f43de978b9b25921cb78d7a2f2b350c04acdbaedb9ecb5f7101cd7c0950e68.py.lock\r\nINFO:filelock:Lock 5737989232 released on /Users/quentinlhoest/.cache/huggingface/datasets/44801f118d500eff6114bfc56ab4e6def941f1eb14b70ac1ecc052e15cdac49d.85f43de978b9b25921cb78d7a2f2b350c04acdbaedb9ecb5f7101cd7c0950e68.py.lock\r\nINFO:filelock:Lock 4393489968 acquired on /Users/quentinlhoest/.cache/huggingface/datasets/_Users_quentinlhoest_.cache_huggingface_datasets_squad_plain_text_1.0.0_1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41.lock\r\nINFO:filelock:Lock 4393489968 released on /Users/quentinlhoest/.cache/huggingface/datasets/_Users_quentinlhoest_.cache_huggingface_datasets_squad_plain_text_1.0.0_1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41.lock\r\nINFO:filelock:Lock 4393490808 acquired on /Users/quentinlhoest/.cache/huggingface/datasets/_Users_quentinlhoest_.cache_huggingface_datasets_squad_plain_text_1.0.0_1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41.lock\r\nReusing dataset squad (/Users/quentinlhoest/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41)\r\nINFO:filelock:Lock 4393490808 released on /Users/quentinlhoest/.cache/huggingface/datasets/_Users_quentinlhoest_.cache_huggingface_datasets_squad_plain_text_1.0.0_1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41.lock\r\n```\r\n\r\nWith the integration of file_lock in the library, the ouput is much cleaner:\r\n```\r\nReusing dataset squad (/Users/quentinlhoest/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41)\r\n```\r\n\r\nSince the file_lock package is only a 450 lines file I think it's fine to have it inside the lib.\r\n\r\nFix #812 ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 743904516,
    "title": "Add SemEval-2010 task 8",
    "dateCreated": "2020-11-16T14:57:57Z",
    "dateModified": "2020-11-16T14:57:57Z",
    "description": "Hi,\r\nI don't know how to add dummy data, since I create the validation set out of the last 1000 examples of the train set. If you have a suggestion, I am happy to implement it.\r\nCheers,\r\nJoel",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 743863214,
    "title": "Use pandas reader in csv",
    "dateCreated": "2020-11-16T14:05:45Z",
    "dateModified": "2020-11-16T14:05:45Z",
    "description": "The pyarrow CSV reader has issues that the pandas one doesn't (see #836 ).\r\nTo fix that I switched to the pandas csv reader.\r\nThe new reader is compatible with all the pandas parameters to read csv files.\r\nMoreover it reads csv by chunk in order to save RAM, while the pyarrow one loads everything in memory.\r\n\r\nFix #836 \r\nFix #794 \r\n\r\nBreaking: now all the parameters to read to csv file can be used in the `load_dataset` kwargs when loading csv, and the previous pyarrow objects `pyarrow.csv.ReadOptions`, `pyarrow.csv.ParseOptions` and `pyarrow.csv.ConvertOptions` are not used anymore.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 743799239,
    "title": "Add open book corpus",
    "dateCreated": "2020-11-16T12:30:02Z",
    "dateModified": "2020-11-16T12:30:02Z",
    "description": "Adds book corpus based on Shawn Presser's [work](https://github.com/soskek/bookcorpus/issues/27)  @richarddwang, the author of the original BookCorpus dataset, suggested it should be named [OpenBookCorpus](https://github.com/huggingface/datasets/issues/486). I named it BookCorpusOpen to be easily located alphabetically. But, of course, we can rename it if needed. \r\n\r\nIt contains 17868 dataset items; each item contains two fields: title and text. The title is the name of the book (just the file name) while the text contains unprocessed book text. Note that bookcorpus is pre-segmented into a sentence while this bookcorpus is not. This is intentional (see https://github.com/huggingface/datasets/issues/486) as some users might want to further process the text themselves. \r\n\r\n@lhoestq and others please review this PR thoroughly. cc @shawwn ",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 743690839,
    "title": "Fix kor nli csv reader",
    "dateCreated": "2020-11-16T09:53:41Z",
    "dateModified": "2020-11-16T09:53:41Z",
    "description": "The kor_nli dataset had an issue with the csv reader that was not able to parse the lines correctly. Some lines were merged together for some reason.\r\nI fixed that by iterating through the lines directly instead of using a csv reader.\r\nI also changed the feature names to match the other NLI datasets (i.e. use \"premise\", \"hypothesis\", \"label\" features)\r\n\r\nFix #821 ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 743675376,
    "title": "wmt16 does not download ",
    "dateCreated": "2020-11-16T09:31:51Z",
    "dateModified": "2020-11-16T09:31:51Z",
    "description": "Hi, I appreciate your help with the following error, thanks \r\n\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"wmt16\", \"ro-en\", split=\"train\")\r\nDownloading and preparing dataset wmt16/ro-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/7b2c4443a7d34c2e13df267eaa8cab4c62dd82f6b62b0d9ecc2e3a673ce17308...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/builder.py\", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/builder.py\", line 531, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/7b2c4443a7d34c2e13df267eaa8cab4c62dd82f6b62b0d9ecc2e3a673ce17308/wmt_utils.py\", line 755, in _split_generators\r\n    downloaded_files = dl_manager.download_and_extract(urls_to_download)\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/download_manager.py\", line 254, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/download_manager.py\", line 179, in download\r\n    num_proc=download_config.num_proc,\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 225, in map_nested\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 225, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 181, in _single_map_nested\r\n    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 181, in <listcomp>\r\n    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 163, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 475, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach http://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-ro.tmx.gz",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 743426583,
    "title": "concatenate_datasets support axis=0 or 1 \uff1f",
    "dateCreated": "2020-11-16T02:46:23Z",
    "dateModified": "2020-11-16T02:46:23Z",
    "description": "I want to achieve the following result\r\n![image](https://user-images.githubusercontent.com/12437751/99207426-f0c8db80-27f8-11eb-820a-4d9f7287b742.png)\r\n",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 743396240,
    "title": "wmt cannot be downloaded ",
    "dateCreated": "2020-11-16T01:04:41Z",
    "dateModified": "2020-11-16T01:04:41Z",
    "description": "Hi, I appreciate your help with the following error, thanks \r\n\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"wmt16\", \"ro-en\", split=\"train\")\r\nDownloading and preparing dataset wmt16/ro-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/7b2c4443a7d34c2e13df267eaa8cab4c62dd82f6b62b0d9ecc2e3a673ce17308...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/builder.py\", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/builder.py\", line 531, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/7b2c4443a7d34c2e13df267eaa8cab4c62dd82f6b62b0d9ecc2e3a673ce17308/wmt_utils.py\", line 755, in _split_generators\r\n    downloaded_files = dl_manager.download_and_extract(urls_to_download)\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/download_manager.py\", line 254, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/download_manager.py\", line 179, in download\r\n    num_proc=download_config.num_proc,\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 225, in map_nested\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 225, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 181, in _single_map_nested\r\n    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 181, in <listcomp>\r\n    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 163, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 475, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach http://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-ro.tmx.gz",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 743343278,
    "title": "Add support for other languages for rouge",
    "dateCreated": "2020-11-15T20:57:45Z",
    "dateModified": "2020-11-15T20:57:45Z",
    "description": "I calculate rouge with\r\n```\r\nfrom datasets import load_metric\r\nrouge = load_metric(\"rouge\")\r\nrouge_output = rouge.compute(predictions=['\u0442\u0435\u0441\u0442 \u0442\u0435\u0441\u0442 \u043f\u0440\u0438\u0432\u0435\u0442'], references=['\u0442\u0435\u0441\u0442 \u0442\u0435\u0441\u0442 \u043f\u043e\u043a\u0430'], rouge_types=[\r\n    \"rouge2\"])[\"rouge2\"].mid\r\nprint(rouge_output)\r\n```\r\nthe result is\r\n`Score(precision=0.0, recall=0.0, fmeasure=0.0)`\r\nIt seems like the `rouge_score` library that this metric uses filters all non-alphanueric latin characters \r\nin `rouge_scorer/tokenize.py` with `text = re.sub(r\"[^a-z0-9]+\", \" \", six.ensure_str(text))`.\r\nPlease add support for other languages. ",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 742369419,
    "title": "Create ClassLabel for labelling tasks datasets",
    "dateCreated": "2020-11-13T11:07:22Z",
    "dateModified": "2020-11-13T11:07:22Z",
    "description": "This PR adds a specific `ClassLabel` for the datasets that are about a labelling task such as POS, NER or Chunking.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 742263333,
    "title": "Load amazon dataset",
    "dateCreated": "2020-11-13T08:34:24Z",
    "dateModified": "2020-11-13T08:34:24Z",
    "description": "Hi,\r\nI was going through amazon_us_reviews dataset and found that example API usage given on website is different from the API usage while loading dataset. \r\n\r\nEg. what API usage is on the [website](https://huggingface.co/datasets/amazon_us_reviews) \r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"amazon_us_reviews\")\r\n```\r\nHow it is when I tried (the error generated does point me to the right direction though)\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"amazon_us_reviews\", 'Books_v1_00')\r\n``` \r\nAlso, there is some issue with formatting as it's not showing bullet list in description with new line. Can I work on it?",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 742240942,
    "title": "Error when concatenate_datasets",
    "dateCreated": "2020-11-13T07:56:02Z",
    "dateModified": "2020-11-13T07:56:02Z",
    "description": "Hello, when I concatenate two dataset loading  from disk, I encountered a problem:\r\n```\r\ntest_dataset = load_from_disk('data/test_dataset')\r\ntrn_dataset = load_from_disk('data/train_dataset')\r\n\r\ntrain_dataset = concatenate_datasets([trn_dataset, test_dataset])\r\n```\r\nAnd it reported ValueError blow:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-38-74fa525512ca> in <module>\r\n----> 1 train_dataset = concatenate_datasets([trn_dataset, test_dataset])\r\n\r\n/opt/miniconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py in concatenate_datasets(dsets, info, split)\r\n   2547                 \"However datasets' indices {} come from memory and datasets' indices {} come from disk.\".format(\r\n   2548                     [i for i in range(len(dsets)) if indices_mappings_in_memory[i]],\r\n-> 2549                     [i for i in range(len(dsets)) if not indices_mappings_in_memory[i]],\r\n   2550                 )\r\n   2551             )\r\n\r\nValueError: Datasets' indices should ALL come from memory, or should ALL come from disk.\r\nHowever datasets' indices [1] come from memory and datasets' indices [0] come from disk.\r\n```\r\n\r\nBut it's curious both of my datasets loading from disk, so I check the source code in `arrow_dataset.py` about the Error:\r\n```\r\ntrn_dataset._data_files\r\n# output\r\n[{'filename': 'data/train_dataset/csv-train.arrow', 'skip': 0, 'take': 593264}]\r\n\r\ntest_dataset._data_files\r\n# output\r\n[{'filename': 'data/test_dataset/csv-test.arrow', 'skip': 0, 'take': 424383}]\r\n\r\nprint([not dset._data_files for dset in [trn_dataset, test_dataset]])\r\n# [False, False]\r\n\r\n# And I tested the code the same as arrow_dataset, but nothing happened\r\ndsets = [trn_dataset, test_dataset]\r\ndsets_in_memory = [not dset._data_files for dset in dsets]\r\nif any(dset_in_memory != dsets_in_memory[0] for dset_in_memory in dsets_in_memory):\r\n    raise ValueError(\r\n        \"Datasets should ALL come from memory, or should ALL come from disk.\\n\"\r\n        \"However datasets {} come from memory and datasets {} come from disk.\".format(\r\n            [i for i in range(len(dsets)) if dsets_in_memory[i]],\r\n            [i for i in range(len(dsets)) if not dsets_in_memory[i]],\r\n        )\r\n    )\r\n```\r\n\r\nAny suggestions would be greatly appreciated! \r\nThanks!",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 742179495,
    "title": "multiprocessing in dataset map \"can only test a child process\"",
    "dateCreated": "2020-11-13T06:01:04Z",
    "dateModified": "2020-11-13T06:01:04Z",
    "description": "Using a dataset with a single 'text' field and a fast tokenizer in a jupyter notebook.\r\n\r\n``` \r\ndef tokenizer_fn(example):\r\n    return tokenizer.batch_encode_plus(example['text'])\r\n\r\nds_tokenized = text_dataset.map(tokenizer_fn, batched=True, num_proc=6, remove_columns=['text'])\r\n```\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRemoteTraceback                           Traceback (most recent call last)\r\nRemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/multiprocess/pool.py\", line 119, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/datasets/arrow_dataset.py\", line 156, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/datasets/fingerprint.py\", line 163, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/datasets/arrow_dataset.py\", line 1510, in _map_single\r\n    for i in pbar:\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/tqdm/notebook.py\", line 228, in __iter__\r\n    for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/tqdm/std.py\", line 1186, in __iter__\r\n    self.close()\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/tqdm/notebook.py\", line 251, in close\r\n    super(tqdm_notebook, self).close(*args, **kwargs)\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/tqdm/std.py\", line 1291, in close\r\n    fp_write('')\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/tqdm/std.py\", line 1288, in fp_write\r\n    self.fp.write(_unicode(s))\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/wandb/sdk/lib/redirect.py\", line 91, in new_write\r\n    cb(name, data)\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/wandb/sdk/wandb_run.py\", line 598, in _console_callback\r\n    self._backend.interface.publish_output(name, data)\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/wandb/sdk/interface/interface.py\", line 146, in publish_output\r\n    self._publish_output(o)\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/wandb/sdk/interface/interface.py\", line 151, in _publish_output\r\n    self._publish(rec)\r\n  File \"/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/wandb/sdk/interface/interface.py\", line 431, in _publish\r\n    if self._process and not self._process.is_alive():\r\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\r\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\r\nAssertionError: can only test a child process\r\n\"\"\"\r\n```",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 741885174,
    "title": "Add HoVer multi-hop fact verification dataset",
    "dateCreated": "2020-11-12T19:55:46Z",
    "dateModified": "2020-11-12T19:55:46Z",
    "description": "## Adding a Dataset\r\n- **Name:** HoVer\r\n- **Description:** https://twitter.com/YichenJiang9/status/1326954363806429186 contains 20K claim verification examples\r\n- **Paper:** https://arxiv.org/abs/2011.03088\r\n- **Data:** https://hover-nlp.github.io/\r\n- **Motivation:** There are still few multi-hop information extraction benchmarks (HotpotQA, which dataset wase based off, notwithstanding)\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 741841350,
    "title": "amazon description fields as bullets",
    "dateCreated": "2020-11-12T18:50:41Z",
    "dateModified": "2020-11-12T18:50:41Z",
    "description": "One more minor formatting change to amazon reviews's description (in addition to #844). Just reformatting the fields to display as a bulleted list in markdown.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 741835661,
    "title": "add newlines to amazon desc",
    "dateCreated": "2020-11-12T18:41:20Z",
    "dateModified": "2020-11-12T18:41:20Z",
    "description": "Just a quick formatting fix to hopefully make it render nicer on Viewer",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 741531121,
    "title": "use_custom_baseline still produces errors for bertscore",
    "dateCreated": "2020-11-12T11:44:32Z",
    "dateModified": "2020-11-12T11:44:32Z",
    "description": "`metric = load_metric('bertscore')`\r\n`a1 = \"random sentences\"`\r\n`b1 = \"random sentences\"`\r\n`metric.compute(predictions = [a1], references = [b1], lang = 'en')`\r\n\r\n`Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/stephen_chan/.local/lib/python3.6/site-packages/datasets/metric.py\", line 393, in compute\r\n    output = self._compute(predictions=predictions, references=references, **kwargs)\r\n  File \"/home/stephen_chan/.cache/huggingface/modules/datasets_modules/metrics/bertscore/361e597a01a41d6cf95d94bbfb01dea16261687abc0c6c74cc9930f80488f363/bertscore.py\", line 108, in _compute\r\n    hashcode = bert_score.utils.get_hash(model_type, num_layers, idf, rescale_with_baseline)\r\nTypeError: get_hash() missing 1 required positional argument: 'use_custom_baseline'`\r\n\r\nAdding 'use_custom_baseline = False' as an argument produces this error\r\n\r\n`Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/stephen_chan/.local/lib/python3.6/site-packages/datasets/metric.py\", line 393, in compute\r\n    output = self._compute(predictions=predictions, references=references, **kwargs)\r\nTypeError: _compute() got an unexpected keyword argument 'use_custom_baseline'`\r\n\r\nThis is on Ubuntu 18.04, Python 3.6.9, datasets version 1.1.2",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 741208428,
    "title": "How to enable `.map()` pre-processing pipelines to support multi-node parallelism?",
    "dateCreated": "2020-11-12T02:04:38Z",
    "dateModified": "2020-11-12T02:04:38Z",
    "description": "Hi,\r\n\r\nCurrently, multiprocessing can be enabled for the `.map()` stages on a single node. However, in the case of multi-node training, (since more than one node would be available) I'm wondering if it's possible to extend the parallel processing among nodes, instead of only 1 node running the `.map()` while the other node is waiting for it to finish?\r\n\r\nThanks!",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 740737448,
    "title": "Can not reuse datasets already downloaded",
    "dateCreated": "2020-11-11T12:42:15Z",
    "dateModified": "2020-11-11T12:42:15Z",
    "description": "Hello,\r\nI need to connect to a frontal node (with http proxy, no gpu) before connecting to a gpu node (but no http proxy, so can not use wget so on).\r\nI successfully downloaded and reuse the wikipedia datasets in a frontal node. \r\nWhen I connect to the gpu node, I supposed to use the downloaded datasets from cache, but failed and end with time out error.\r\n\r\nOn frontal node:\r\n```\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset('wikipedia', '20200501.en')\r\nReusing dataset wikipedia (/linkhome/rech/genini01/uua34ms/.cache/huggingface/datasets/wikipedia/20200501.en/1.0.0/f92599dfccab29832c442b82870fa8f6983e5b4ebbf5e6e2dcbe894e325339cd)\r\n/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\r\n  return torch._C._cuda_getDeviceCount() > 0\r\n```\r\n\r\nOn gpu node:\r\n```\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset('wikipedia', '20200501.en')\r\nTraceback (most recent call last):\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connection.py\", line 160, in _new_conn\r\n    (self._dns_host, self.port), self.timeout, **extra_kw\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/util/connection.py\", line 84, in create_connection\r\n    raise err\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/util/connection.py\", line 74, in create_connection\r\n    sock.connect(sa)\r\nTimeoutError: [Errno 110] Connection timed out\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 677, in urlopen\r\n    chunked=chunked,\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\r\n    self._validate_conn(conn)\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\r\n    conn.connect()\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connection.py\", line 309, in connect\r\n    conn = self._new_conn()\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connection.py\", line 172, in _new_conn\r\n    self, \"Failed to establish a new connection: %s\" % e\r\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x14b7b73e4908>: Failed to establish a new connection: [Errno 110] Connection timed out\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/requests/adapters.py\", line 449, in send\r\n    timeout=timeout\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 727, in urlopen\r\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/util/retry.py\", line 446, in increment\r\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/wikipedia/wikipedia.py (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x14b7b73e4908>: Failed to establish a new connection: [Errno 110] Connection timed out',))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/datasets/load.py\", line 590, in load_dataset\r\n    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/datasets/load.py\", line 264, in prepare_module\r\n    head_hf_s3(path, filename=name, dataset=dataset)\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 200, in head_hf_s3\r\n    return requests.head(hf_bucket_url(identifier=identifier, filename=filename, use_cdn=use_cdn, dataset=dataset))\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/requests/api.py\", line 104, in head\r\n    return request('head', url, **kwargs)\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/requests/api.py\", line 61, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/requests/sessions.py\", line 530, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/requests/sessions.py\", line 643, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/requests/adapters.py\", line 516, in send\r\n    raise ConnectionError(e, request=request)\r\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/wikipedia/wikipedia.py (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x14b7b73e4908>: Failed to establish a new connection: [Errno 110] Connection timed out',))\r\n\r\n```\r\n\r\nAny advice?Thanks!\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 740632771,
    "title": "Update squad_v2.py",
    "dateCreated": "2020-11-11T09:58:41Z",
    "dateModified": "2020-11-11T09:58:41Z",
    "description": "Change lines 100 and 102 to prevent overwriting ```predictions``` variable.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 740355270,
    "title": "XSum dataset missing spaces between sentences",
    "dateCreated": "2020-11-11T00:34:43Z",
    "dateModified": "2020-11-11T00:34:43Z",
    "description": "I noticed that the XSum dataset has no space between sentences. This could lead to worse results for anyone training or testing on it. Here's an example (0th entry in the test set):\r\n\r\n`The London trio are up for best UK act and best album, as well as getting two nominations in the best song category.\"We got told like this morning 'Oh I think you're nominated'\", said Dappy.\"And I was like 'Oh yeah, which one?' And now we've got nominated for four awards. I mean, wow!\"Bandmate Fazer added: \"We thought it's best of us to come down and mingle with everyone and say hello to the cameras. And now we find we've got four nominations.\"The band have two shots at the best song prize, getting the nod for their Tynchy Stryder collaboration Number One, and single Strong Again.Their album Uncle B will also go up against records by the likes of Beyonce and Kanye West.N-Dubz picked up the best newcomer Mobo in 2007, but female member Tulisa said they wouldn't be too disappointed if they didn't win this time around.\"At the end of the day we're grateful to be where we are in our careers.\"If it don't happen then it don't happen - live to fight another day and keep on making albums and hits for the fans.\"Dappy also revealed they could be performing live several times on the night.The group will be doing Number One and also a possible rendition of the War Child single, I Got Soul.The charity song is a  re-working of The Killers' All These Things That I've Done and is set to feature artists like Chipmunk, Ironik and Pixie Lott.This year's Mobos will be held outside of London for the first time, in Glasgow on 30 September.N-Dubz said they were looking forward to performing for their Scottish fans and boasted about their recent shows north of the border.\"We just done Edinburgh the other day,\" said Dappy.\"We smashed up an N-Dubz show over there. We done Aberdeen about three or four months ago - we smashed up that show over there! Everywhere we go we smash it up!\"`",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 740328382,
    "title": "CNN/Dailymail Dataset Card",
    "dateCreated": "2020-11-10T23:56:43Z",
    "dateModified": "2020-11-10T23:56:43Z",
    "description": "Link to the card page: https://github.com/mcmillanmajora/datasets/tree/cnn_dailymail_card/datasets/cnn_dailymail\n\nOne of the questions this dataset brings up is how we want to handle versioning of the cards to mirror versions of the dataset. The different versions of this dataset are used for different tasks (which may not be reflected in the versions that we currently have in the repo?), but it's only the structure that's changing rather than the content in this particular case, at least between versions 2.0.0 and 3.0.0. ",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 740250215,
    "title": "AlloCin\u00e9 dataset card",
    "dateCreated": "2020-11-10T21:19:53Z",
    "dateModified": "2020-11-10T21:19:53Z",
    "description": "Link to the card page: https://github.com/mcmillanmajora/datasets/blob/allocine_card/datasets/allocine/README.md\n\nThere wasn't as much information available for this dataset, so I'm wondering what's the best way to address open questions about the dataset. For example, where did the list of films that the dataset creator used come from?\n\nI'm also wondering how best to go about talking about limitations when so little is known about the data. ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 740187613,
    "title": "load_dataset with 'csv' is not working. while the same file is loading with 'text' mode or with pandas",
    "dateCreated": "2020-11-10T19:35:40Z",
    "dateModified": "2020-11-10T19:35:40Z",
    "description": "Hi All\r\nI am trying to load a custom dataset  and I am trying to load a single file to make sure the file is loading correctly:\r\ndataset = load_dataset('csv', data_files=files)\r\nWhen I run it I get:\r\n\r\nDownloading and preparing dataset csv/default-35575a1051604c88 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) tocache/huggingface/datasets/csv/default-35575a1051604c88/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4...\r\n\r\nI am getting this error:\r\n6a4ac4/csv.py in _generate_tables(self, files)\r\n     78     def _generate_tables(self, files):\r\n     79         for i, file in enumerate(files):\r\n---> 80             pa_table = pac.read_csv(\r\n     81                 file,\r\n     82                 read_options=self.config.pa_read_options,\r\n\r\n~/anaconda2/envs/nlp/lib/python3.8/site-packages/pyarrow/_csv.pyx in pyarrow._csv.read_csv()\r\n\r\n~/anaconda2/envs/nlp/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/anaconda2/envs/nlp/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\n**ArrowInvalid: straddling object straddles two block boundaries (try to increase block size?)**\r\n\r\n\r\n\r\nThe size of the file is 3.5 GB. When I try smaller files I do not have an issue. When I load it with 'text' parser I can see all data but it is not what I need.\r\nThere is no issue reading the file with pandas. any idea what could be the issue?\r\nWhen I am running a different CSV I do not get  this line:\r\n (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size)\r\n\r\nAny ideas?\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 740102210,
    "title": "Wikipedia postprocessing",
    "dateCreated": "2020-11-10T17:26:38Z",
    "dateModified": "2020-11-10T17:26:38Z",
    "description": "Hi, thanks for this library!\r\n\r\nRunning this code:\r\n\r\n```py\r\nimport datasets\r\nwikipedia = datasets.load_dataset(\"wikipedia\", \"20200501.de\")\r\nprint(wikipedia['train']['text'][0])\r\n```\r\n\r\nI get:\r\n\r\n```\r\nmini|Ricardo Flores Mag\u00f3n\r\nmini|Mexikanische Revolution\u00e4re, Mag\u00f3n in der Mitte anf\u00fchrend, gegen die Diktatur von Porfirio Diaz, Ausschnitt des Gem\u00e4lde \u201eTierra y Libertad\u201c von Idelfonso Carrara (?) von 1930.\r\n\r\nRicardo Flores Mag\u00f3n (* 16. September 1874 in San Antonio Eloxochitl\u00e1n im mexikanischen Bundesstaat Oaxaca; \u2020 22. November 1922 im Bundesgef\u00e4ngnis Leavenworth im US-amerikanischen Bundesstaat Kansas) war als Journalist, Gewerkschafter und Literat ein f\u00fchrender anarchistischer Theoretiker und Aktivist, der die revolution\u00e4re mexikanische Bewegung radikal beeinflusste. Mag\u00f3n war Gr\u00fcnder der Partido Liberal Mexicano und Mitglied der Industrial Workers of the World.\r\n\r\nPolitische Biografie \r\nJournalistisch und politisch k\u00e4mpfte er und sein Bruder sehr kompromisslos gegen die Diktatur Porfirio Diaz. Philosophisch und politisch orientiert an radikal anarchistischen Idealen und den Erfahrungen seiner indigenen Vorfahren bei der gemeinschaftlichen Bewirtschaftung des Gemeindelandes, machte er die Forderung \u201eLand und Freiheit\u201c (Tierra y Libertad) popul\u00e4r. Besonders Francisco Villa und Emiliano Zapata griffen die Forderung Land und Freiheit auf. Seine Philosophie hatte gro\u00dfen Einfluss auf die Landarbeiter. 1904 floh er in die USA und gr\u00fcndete 1906 die Partido Liberal Mexicano. Im Exil lernte er u. a. Emma Goldman kennen. Er verbrachte die meiste Zeit seines Lebens in Gef\u00e4ngnissen und im Exil und wurde 1918 in den USA wegen \u201eBehinderung der Kriegsanstrengungen\u201c zu zwanzig Jahren Gef\u00e4ngnis verurteilt. Zu seinem Tod gibt es drei verschiedene Theorien. Offiziell starb er an Herzversagen. Librado Rivera, der die Leiche mit eigenen Augen gesehen hat, geht davon aus, dass Mag\u00f3n von einem Mitgefangenen erdrosselt wurde. Die staatstreue Gewerkschaftszeitung CROM ver\u00f6ffentlichte 1923 einen Beitrag, nachdem Mag\u00f3n von einem Gef\u00e4ngnisw\u00e4rter erschlagen wurde.\r\nmini|Die Br\u00fcder Ricardo (links) und Enrique Flores Mag\u00f3n (rechts) vor dem Los Angeles County Jail, 1917\r\n\r\n[...]\r\n```\r\n\r\nso some Markup like `mini|` is still left. Should I run another parser on this text before feeding it to an ML model or is this a known imperfection of parsing Wiki markup?\r\n\r\nApologies if this has been asked before.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 740082890,
    "title": "[GEM] add WikiLingua cross-lingual abstractive summarization dataset",
    "dateCreated": "2020-11-10T17:00:43Z",
    "dateModified": "2020-11-10T17:00:43Z",
    "description": "## Adding a Dataset\r\n- **Name:** WikiLingua\r\n- **Description:** The dataset includes ~770k article and summary pairs in 18 languages from WikiHow. The gold-standard article-summary alignments across languages were extracted by aligning the images that are used to describe each how-to step in an article.\r\n- **Paper:** https://arxiv.org/pdf/2010.03093.pdf\r\n- **Data:** https://github.com/esdurmus/Wikilingua\r\n- **Motivation:** Included in the GEM shared task. Multilingual.\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 740079692,
    "title": "[GEM] add ASSET text simplification dataset",
    "dateCreated": "2020-11-10T16:56:30Z",
    "dateModified": "2020-11-10T16:56:30Z",
    "description": "## Adding a Dataset\r\n- **Name:** ASSET\r\n- **Description:** ASSET is a crowdsourced\r\nmulti-reference corpus for assessing sentence simplification in English where each simplification was produced by executing several rewriting transformations.\r\n- **Paper:** https://www.aclweb.org/anthology/2020.acl-main.424.pdf\r\n- **Data:** https://github.com/facebookresearch/asset\r\n- **Motivation:** Included in the GEM shared task\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 740077228,
    "title": "[GEM] add WikiAuto text simplification dataset",
    "dateCreated": "2020-11-10T16:53:23Z",
    "dateModified": "2020-11-10T16:53:23Z",
    "description": "## Adding a Dataset\r\n- **Name:** WikiAuto\r\n- **Description:** Sentences in English Wikipedia and their corresponding sentences in Simple English Wikipedia that are written with simpler grammar and word choices. A lot of lexical and syntactic paraphrasing. \r\n- **Paper:** https://www.aclweb.org/anthology/2020.acl-main.709.pdf\r\n- **Data:** https://github.com/chaojiang06/wiki-auto\r\n- **Motivation:** Included in the GEM shared task\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 740071697,
    "title": "[GEM] Add WebNLG dataset",
    "dateCreated": "2020-11-10T16:46:48Z",
    "dateModified": "2020-11-10T16:46:48Z",
    "description": "## Adding a Dataset\r\n- **Name:** WebNLG\r\n- **Description:** WebNLG consists of Data/Text pairs where the data is a set of triples extracted from DBpedia and the text is a verbalisation of these triples (16,095 data inputs and 42,873 data-text pairs). The data is available in English and Russian\r\n- **Paper:** https://www.aclweb.org/anthology/P17-1017.pdf\r\n- **Data:** https://webnlg-challenge.loria.fr/download/\r\n- **Motivation:** Included in the GEM shared task, multilingual\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 740065376,
    "title": "[GEM] add ToTTo Table-to-text dataset",
    "dateCreated": "2020-11-10T16:38:34Z",
    "dateModified": "2020-11-10T16:38:34Z",
    "description": "## Adding a Dataset\r\n- **Name:** ToTTo\r\n- **Description:** ToTTo is an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description.\r\n- **Paper:** https://arxiv.org/abs/2004.14373\r\n- **Data:** https://github.com/google-research-datasets/totto\r\n- **Motivation:** Included in the GEM shared task\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 740061699,
    "title": "[GEM] add Schema-Guided Dialogue",
    "dateCreated": "2020-11-10T16:33:44Z",
    "dateModified": "2020-11-10T16:33:44Z",
    "description": "## Adding a Dataset\r\n- **Name:** The Schema-Guided Dialogue Dataset\r\n- **Description:** The Schema-Guided Dialogue (SGD) dataset consists of over 20k annotated multi-domain, task-oriented conversations between a human and a virtual assistant. These conversations involve interactions with services and APIs spanning 20 domains, ranging from banks and events to media, calendar, travel, and weather.\r\n- **Paper:** https://arxiv.org/pdf/2002.01359.pdf https://arxiv.org/pdf/2004.15006.pdf\r\n- **Data:** https://github.com/google-research-datasets/dstc8-schema-guided-dialogue\r\n- **Motivation:** Included in the GEM shared task\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 740008683,
    "title": "Add writer_batch_size attribute to GeneratorBasedBuilder",
    "dateCreated": "2020-11-10T15:28:19Z",
    "dateModified": "2020-11-10T15:28:19Z",
    "description": "As specified in #741 one would need to specify a custom ArrowWriter batch size to avoid filling the RAM. Indeed the defaults buffer size is 10 000 examples but for multimodal datasets that contain images or videos we may want to reduce that.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 739983024,
    "title": "[GEM] MultiWOZ dialogue dataset",
    "dateCreated": "2020-11-10T14:57:50Z",
    "dateModified": "2020-11-10T14:57:50Z",
    "description": "## Adding a Dataset\r\n- **Name:** MultiWOZ (Multi-Domain Wizard-of-Oz)\r\n- **Description:** 10k annotated human-human dialogues. Each dialogue consists of a goal, multiple user and system utterances as well as a belief state. Only system utterances are annotated with dialogue acts \u2013 there are no annotations from the user side.\r\n- **Paper:** https://arxiv.org/pdf/2007.12720.pdf\r\n- **Data:** https://github.com/budzianowski/multiwoz\r\n- **Motivation:** Will likely be part of the GEM shared task\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 739976716,
    "title": "[GEM] Add E2E dataset",
    "dateCreated": "2020-11-10T14:50:40Z",
    "dateModified": "2020-11-10T14:50:40Z",
    "description": "## Adding a Dataset\r\n- **Name:** E2E NLG dataset (for End-to-end natural language generation)\r\n- **Description:**a dataset for training end-to-end, datadriven natural language generation systems in the restaurant domain, the datasets consists of 5,751 dialogue-act Meaning Representations (structured data) and 8.1 reference free-text utterances per dialogue-act on average\r\n- **Paper:** https://arxiv.org/pdf/1706.09254.pdf https://arxiv.org/abs/1901.07931\r\n- **Data:** http://www.macs.hw.ac.uk/InteractionLab/E2E/#data\r\n- **Motivation:** This dataset will likely be included in the GEM shared task\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 739925960,
    "title": "Add accuracy, precision, recall and F1 metrics",
    "dateCreated": "2020-11-10T13:50:35Z",
    "dateModified": "2020-11-10T13:50:35Z",
    "description": "This PR adds several single metrics, namely:\r\n\r\n- Accuracy\r\n- Precision\r\n- Recall\r\n- F1\r\n\r\nThey all uses under the hood the sklearn metrics of the same name. They allow different useful features when training a multilabel/multiclass model:\r\n- have a macro/micro/per label/weighted/binary/per sample score\r\n- score only the selected labels (usually what we call the positive labels) and ignore the negative ones. For example in case of a Named Entity Recognition task, positive labels are (`PERSON`, `LOCATION` or `ORGANIZATION`) and the negative one is `O`.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 739896526,
    "title": "Discussion using datasets in offline mode",
    "dateCreated": "2020-11-10T13:10:51Z",
    "dateModified": "2020-11-10T13:10:51Z",
    "description": "`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\r\n\r\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\r\n\r\nHere are some points to open discussion:\r\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\r\n- AFAIK, you can make it work if you manually put the python files (csv.py for example) on this offline machine and change your code to `datasets.load_dataset(\"MY_PATH/csv.py\", ...)`. But it would be much better if you could run ths same code without modification if files are available locally.\r\n- I've also been considering the requirement of downloading Python code and execute on your machine to use datasets. This can be an issue in a professional context. Downloading a CSV/H5 file is acceptable, downloading an executable script can open many security issues. We certainly need a mechanism to at least \"freeze\" the dataset code you retrieved once so that you can review it if you want and then be sure you use this one everywhere and not a version dowloaded from internet.\r\n \r\nWDYT? (thks)\r\n\r\n",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 739815763,
    "title": "how processing in batch works in datasets ",
    "dateCreated": "2020-11-10T11:11:17Z",
    "dateModified": "2020-11-10T11:11:17Z",
    "description": "Hi,\r\nI need to process my datasets before it is passed to dataloader in batch, \r\nhere is my codes \r\n\r\n```\r\nclass AbstractTask(ABC):\r\n    task_name: str = NotImplemented\r\n    preprocessor: Callable = NotImplemented\r\n    split_to_data_split: Mapping[str, str] = NotImplemented\r\n    tokenizer: Callable = NotImplemented\r\n    max_source_length: str = NotImplemented\r\n    max_target_length: str = NotImplemented\r\n    # TODO: should not be a task item, but cannot see other ways.\r\n    tpu_num_cores: int = None\r\n\r\n    # The arguments set are for all tasks and needs to be kept common.\r\n    def __init__(self, config):\r\n        self.max_source_length = config['max_source_length']\r\n        self.max_target_length = config['max_target_length']\r\n        self.tokenizer = config['tokenizer']\r\n        self.tpu_num_cores = config['tpu_num_cores']\r\n\r\n    def _encode(self, batch) -> Dict[str, torch.Tensor]:\r\n        batch_encoding = self.tokenizer.prepare_seq2seq_batch(\r\n            [x[\"src_texts\"] for x in batch],\r\n            tgt_texts=[x[\"tgt_texts\"] for x in batch],\r\n            max_length=self.max_source_length,\r\n            max_target_length=self.max_target_length,\r\n            padding=\"max_length\" if self.tpu_num_cores is not None else \"longest\",  # TPU hack\r\n            return_tensors=\"pt\"\r\n        )\r\n        return batch_encoding.data\r\n\r\n\r\n    def data_split(self, split):\r\n        return self.split_to_data_split[split]\r\n\r\n    def get_dataset(self, split, n_obs=None):\r\n        split = self.data_split(split)\r\n        if n_obs is not None:\r\n            split = split+\"[:{}]\".format(n_obs)\r\n        dataset = load_dataset(self.task_name, split=split)\r\n        dataset = dataset.map(self.preprocessor, remove_columns=dataset.column_names)\r\n        dataset = dataset.map(lambda batch: self._encode(batch), batched=True)\r\n        dataset.set_format(type=\"torch\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\r\n        return dataset\r\n\r\n```\r\n\r\nI call it like \r\n\r\n`AutoTask.get(task, train_dataset_config).get_dataset(split=\"train\", n_obs=data_args.n_train) \r\n`\r\n\r\nThis gives the following error, to me because the data inside the   dataset = dataset.map(lambda batch: self._encode(batch), batched=True) is not processed in batch, could you tell me how I can process dataset in batch inside my function? thanks \r\n\r\n  File \"finetune_multitask_trainer.py\", line 192, in main\r\n    if training_args.do_train else None\r\n  File \"finetune_multitask_trainer.py\", line 191, in <dictcomp>\r\n    split=\"train\", n_obs=data_args.n_train) for task in data_args.task}\r\n  File \"/remote/idiap.svm/user.active/rkarimi/dev/internship/seq2seq/tasks.py\", line 56, in get_dataset\r\n    dataset = dataset.map(lambda batch: self._encode(batch), batched=True)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1236, in map\r\n    update_data = does_function_return_dict(test_inputs, test_indices)\r\n  File \"/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1207, in does_function_return_dict\r\n    function(*fn_args, indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n  File \"/remote/idiap.svm/user.active/rkarimi/dev/internship/seq2seq/tasks.py\", line 56, in <lambda>\r\n    dataset = dataset.map(lambda batch: self._encode(batch), batched=True)\r\n  File \"/remote/idiap.svm/user.active/rkarimi/dev/internship/seq2seq/tasks.py\", line 37, in _encode\r\n    [x[\"src_texts\"] for x in batch],\r\n  File \"/remote/idiap.svm/user.active/rkarimi/dev/internship/seq2seq/tasks.py\", line 37, in <listcomp>\r\n    [x[\"src_texts\"] for x in batch],\r\nTypeError: string indices must be integers\r\n\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 739579314,
    "title": "datasets freezes ",
    "dateCreated": "2020-11-10T05:10:19Z",
    "dateModified": "2020-11-10T05:10:19Z",
    "description": "Hi, I want to load these two datasets and convert them to Dataset format in torch and the code freezes for me, could you have a look please? thanks \r\n\r\ndataset1 = load_dataset(\"squad\", split=\"train[:10]\")\r\ndataset1 = dataset1.set_format(type='torch', columns=['context', 'answers', 'question'])\r\n\r\ndataset2 = load_dataset(\"imdb\", split=\"train[:10]\")\r\ndataset2 = dataset2.set_format(type=\"torch\", columns=[\"text\", \"label\"])\r\nprint(len(dataset1))\r\n",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 739506859,
    "title": "`kor_nli` dataset doesn't being loaded properly",
    "dateCreated": "2020-11-10T02:04:12Z",
    "dateModified": "2020-11-10T02:04:12Z",
    "description": "There are two issues from `kor_nli` dataset\r\n\r\n1. csv.DictReader failed to split features by tab\r\n    - Should not exist `None` value in label feature, but there it is.\r\n        ```python\r\n        kor_nli_train['train'].unique('gold_label')\r\n        # ['neutral', 'entailment', 'contradiction', None]\r\n        ```\r\n    - I found a reason why there is `None` values in label feature as following code\r\n        ```python\r\n        from datasets import load_dataset\r\n        kor_nli_train = load_dataset('kor_nli', 'multi_nli')\r\n    \r\n        for idx, example in enumerate(kor_nli_train['train']):\r\n            if example['gold_label'] is None:\r\n                print(idx, example)\r\n                break\r\n        # 16835 {'gold_label': None, 'sentence1': '\uadf8\ub294 \uc804\uc7c1 \uc804\uc5d0 \uac00\ubcbc\uc6b4 \ubc85\uc2a4\ud0a8 \uc554\ub9d0\uc744 \uac00\uc9c0\uace0 \ub2ec\ub9ac\uae30 \uc704\ud574 \uc6b0\uc720\ucc98\ub7fc \ud558\uc580 \uc2a4\ud130\ub4dc\ub97c \ub123\uc5c8\ub2e4.\\t\uc804\uc7c1 \uc804\uc5d0 \ub2e4\uc778\uc885 \uc5ec\uc131\ub4e4\uacfc \ud568\uaed8 \uc788\ub294 \ubc31\uc778 \ub0a8\uc790\uac00 \uc788\uc5c8\ub2e4.\\tentailment\\n\uc2ac\ub9bc\uc740 \uc7ac\ube68\ub9ac \uc637\uc744 \uc785\uc5c8\uace0, \uc21c\uac04\uc801\uc73c\ub85c \ubbf8\uc9c0\uadfc\ud55c \ubb3c\uc744 \ubfcc\ub9b4 \uc218 \uc788\ub294 \uc544\uce68 \uc138\ud0c1\ubb3c\uc744 \uae30\uaebc\uc774 \uac00\ub450\uc5c8\ub2e4.\\t\uc2ac\ub9bc\uc740 \uc9c1\uc7a5\uc5d0 \ub2a6\uc5c8\ub2e4.\\tneutral\\n\ub274\uc695\uc5d0\uc11c \uadf8 \uc2dd\uc0ac\ub97c \ud574\ubd24\ub294\ub370, \uac70\uae30\uc11c \uc18c\uace0\uae30\uc758 \uba4b\uc9c4 \uc18c\uace0\uae30 \ubd80\ubd84\uc744 \uc694\ub9ac\ud558\uace0 \ubc14\ubca0\ud050\ub85c \ub9cc\ub4e0 \ub110\ube64\uc9c0 \uac19\uc740 \uac78 \uac00\uc838\uc654\ub294\ub370, \uc815\ub9d0 \ub300\ub2e8\ud574.\\t\uadf8\ub4e4\uc774 \uac70\uae30\uc11c \uc694\ub9ac\ud558\ub294 \uc1e0\uace0\uae30\ub294 \uc5ed\uacb9\ub2e4. \uac70\uae30\uc11c \uc808\ub300 \uba39\uc9c0 \ub9c8\ub77c.\\tcontradiction\\n\ud310\ub9e4\uc6d0\uc758 \uc8fd\uc74c\uc5d0\uc11c \ube0c\ub77c\uc774\uc5b8 \ub370\ub124\ud788... \ud06c\ub9ac\uc2a4 \ucf08\ub9ac\\t\ud06c\ub9ac\uc2a4 \ucf08\ub9ac\ub294 \uc138\uc77c\uc988\ub9e8\uc758 \uc8fd\uc74c\uc744 \uc5b8\uae09\ud558\uc9c0 \uc54a\ub294\ub2e4.\\tcontradiction\\n\uadf8\ub7ec\ub294 \ub3d9\uc548 \uc694\ub9ac\uc0ac\ub294 \uadf8\ub0e5 \ud654\uac00 \ub0ac\uc5b4.\\t\uc2a4\ud29c\uac00 \ub053\ub294 \ub3d9\uc548 \uc694\ub9ac\uc0ac\ub294 \ud654\uac00 \ub0ac\ub2e4.\\tneutral\\n\ub9c8\uc9c0\ub9c9 \ub85c\ub9c8\uc758 \ub9f9\uacf5\uaca9 \uc804\ub0a0 \ubc24, 900\uba85 \uc774\uc0c1\uc758 \uc720\ub300\uc778 \uc218\ube44\uc218\ub4e4\uc774 \ub85c\ub9c8\uc778\ub4e4\uc5d0\uac8c \uadf8\ub4e4\uc744 \uc0ac\ub85c\uc7a1\ub294 \uc2b9\ub9ac\ub97c \uc8fc\uae30 \ubcf4\ub2e4\ub294 \ub300\ub7c9 \uc790\uc0b4\uc744 \uc800\uc9c8\ub800\ub2e4.\\t\ub85c\ub9c8\uc778\ub4e4\uc774 \uadf8\ub4e4\uc758 \ud3ec\ud68d\uc5d0 \uc2b9\ub9ac\ud558\ub3c4\ub85d \ub0b4\ubc84\ub824\ub450\uae30 \ubcf4\ub2e4\ub294 900\uba85\uc758 \uc720\ub300\uc778 \uc218\ube44\uc218\ub4e4\uc774 \uc790\uc0b4\ud588\ub2e4.\\tentailment\\n\uc55e\uc73c\ub85c \ubc1c\uc0ac\ud558\ub77c.\\t\ubc1c\uc0ac.\\tneutral\\n\uadf8\ub9ac\uace0 \ub2f9\uc2e0\uc740 \uc6b0\ub9ac \ub545\uc774 \uc5d0\uc774\ucee4\uc5d0 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c\uace0 \uc788\ub2e4. \uc6b0\ub9ac \uc0ac\ub78c\ub4e4\uc740 \uc5b4\ub5a4 \uac83\uc774 \uc5bc\ub9c8\ub098 \ub9ce\uc740\uc9c0 \uc774\ud574\ud558\uc9c0 \ubabb\ud560 \uac83\uc774\ub2e4.\\t\ubaa8\ub4e0 \uc0ac\ub78c\ub4e4\uc740 \uc6b0\ub9ac\uc758 \uce21\uc815 \uc2dc\uc2a4\ud15c\uc774 \uc5b4\ub5bb\uac8c \uc791\ub3d9\ud558\ub294\uc9c0 \uc54c\uace0 \uc774\ud574\ud569\ub2c8\ub2e4.\\tcontradiction\\n\uc8fc\ubbf8\uac8c\uc2a4\\tJumiyges\ub294 \ub3c4\uc2dc\uc758 \uc774\ub984\uc774\ub2e4.\\tneutral\\n\uc0ac\ub78c\uc740 \uc790\uae30 \ubbfc\uc871\uc744 \ub3cc\ubd10\uc57c \ud55c\ub2e4...\\t\uc0ac\ub78c\uc740 \uc870\uad6d\uc5d0 \uacf5\uac10\ud574\uc57c \ud55c\ub2e4.\\tentailment\\n\ub610\ud55c PDD 63\uc740 \uc815\ubd80\uc640 \uc5c5\uacc4\uac00 \ucef4\ud4e8\ud130 \uae30\ubc18 \uacf5\uaca9\uc5d0 \ub300\ud574 \uacbd\uace0\ud558\uace0 \ubc29\uc5b4\ud560 \uc900\ube44\ub97c \ub354 \uc798\ud560 \uc218 \uc788\ub3c4\ub85d \uc2dc\uc2a4\ud15c \ucde8\uc57d\uc131, \uc704\ud611, \uce68\uc785 \ubc0f \uc774\uc0c1\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uacf5\uc720\ud558\ub294 \uba54\ucee4\ub2c8\uc998\uc744 \uc218\ub9bd\ud558\ub294 \uac83\uc774 \uc911\uc694\ud558\ub2e4\ub294 \uac83\uc744 \uc778\uc2dd\ud588\uc2b5\ub2c8\ub2e4.\\t\uc815\ubcf4 \uc804\uc1a1 \ud504\ub85c\ud1a0\ucf5c\uc744 \ub9cc\ub4dc\ub294 \uac83\uc740 \uc911\uc694\ud558\ub2e4.\\tentailment\\n\uce74\ud398 \ub9c1 \ud53c\uc544\uc790 \ub378\ub77c \ub808\ud4cc\ube14\ub9ac\uce74 \ubc14\ub85c \ub0a8\ucabd\uc5d0\ub294 \ud53c\ub80c\uccb4\uac00 \uc54c\ub824\uc9c4 \uc9da \uc81c\ud488 \ub54c\ubb38\uc5d0 \ud55c\ub54c \uc2a4\ud2b8\ub85c \ub9c8\ucf13\uc774\ub77c\uace0 \ubd88\ub838\ub358 16\uc138\uae30 \ub85c\uc9c0\uc544\uc778 \uba54\ub974\uce74\ud1a0 \ub204\uc624\ubcf4(Mercato Nuovo)\uac00 \uc788\ub2e4.\\t\ud53c\uc544\uc790 \ub378\ub77c \ub808\ud4cc\ube14\ub9ac\uce74\uc5d0\ub294 \uce74\ud398\uac00 \ub9ce\uc774 \uc788\ub2e4.\\tentailment\\n\uc6b0\ub9ac\uac00 \uc5ec\uae30 \uc788\ub294 \ud55c \ud2b8\ub9b0\ud310\uc774 \ubb58 \uc8fc\uc6e0\ub294\uc9c0 \uc0b4\ud3b4\ubd10\uc57c\uaca0\uc5b4\\t\uc6b0\ub9ac\ub294 \ud2b8\ub9b0\ud310\uc774 \ubb34\uc5c7\uc744 \uc8fc\uc6e0\ub294\uc9c0 \ubcf4\ub294 \ub370 \uc2dc\uac04\uc744 \ub0ad\ube44\ud558\uc9c0 \uc54a\uc744 \uac83\uc774\ub2e4.\\tcontradiction\\n\uadf8\ub7ec\ub098 \ucf08\ud2b8\uc871\uc758 \ubb38\ud654\uc801 \uae30\ubc18\uc744 \uac00\uc9c4 \uc544\uc77c\ub79c\ub4dc \uad50\ud68c\ub294 \uc720\ub7fd\uc758 \uc2e0\ud765 \uae30\ub3c5\uad50 \uc138\uacc4\uc640\ub294 \ub2e4\ub974\uac8c \ubc1c\uc804\ud588\uace0 \uacb0\uad6d \ub85c\ub9c8\uc640 \uc911\uc559\uc9d1\uad8c\uc801 \ud589\uc815\uc73c\ub85c \ub300\uccb4\ub418\uc5c8\ub2e4.\\t\uc544\uc77c\ub79c\ub4dc \uad50\ud68c\uc5d0\ub294 \ucf08\ud2b8\uc871\uc758 \uae30\uc9c0\uac00 \uc788\uc5c8\ub2e4.\\tentailment\\n\uae00\uc384, \ub10c \uc120\ud0dd\uc758 \uc5ec\uc9c0\uac00 \uc5c6\uc5b4\\t\uae00\uc384, \ub108\uc5d0\uac90 \ub9ce\uc740 \uc120\ud0dd\uad8c\uc774 \uc788\uc5b4.\\tcontradiction\\n\uc0ac\uc2e4, \uacf5\uc2dd\uc801\uc778 \ubcf4\uc7a5\uc740 \uc5c6\ub2e4.\\t\ub0b4\uac00 \uc0b0 \ubb3c\uac74\uc5d0 \ub300\ud55c \ubcf4\uc99d\uc774 \uc5c6\uc5c8\ub2e4.\\tneutral\\n\ub35c \ud65c\uae30\ucc28\uae34 \ud558\uc9c0\ub9cc, \uc548\uc2dc\uc640 \ub974 \ubd80\ub974\uc82f\uc758 \uc0ac\ub791\uc2a4\ub7ec\uc6b4 \ud638\uc218\uc5d0\uc11c\ub3c4 \uc0b6\uc740 \ub611\uac19\uc774 \uc0c1\ucf8c\ud558\ub2e4.\\t\uc548\uc2dc\uc640 \ub974 \ubd80\ub974\uac9f\uc5d0\uc11c\ub294 \ud638\uc218\uc5d0\uc11c\uc758 \ud65c\ub3d9\uc774 \uc11c\ub450\ub974\uace0 \ubc14\uc05c \ubd84\uc704\uae30\ub97c \uc5f0\ucd9c\ud55c\ub2e4.\\tcontradiction\\n\uadf8\uc758 \uc5ec\ud589 \uc18c\uc2dd\uc774 \uc774\ubbf8 \ud37c\uc84c\ub2e4\uba74 \uacf5\uaca9 \uc18c\uc2dd\ub3c4 \ud37c\uc84c\uc744 \ud14c\uc9c0\ub9cc \ub9c8\uc744\uc5d0\uc11c\ub294 \uc804\ud600 \uacf5\ud669\uc758 \uae30\ubbf8\uac00 \ubcf4\uc774\uc9c0 \uc54a\uc558\ub2e4.\\t\uadf8\ub294 \uc65c \ub9c8\uc744\uc774 \ub2f9\ud669\ud558\uc9c0 \uc54a\uc558\ub294\uc9c0 \uc54c \uc218 \uc5c6\uc5c8\ub2e4.\\tneutral\\n\uacfc\uac70\uc5d0\ub294 \uc8fd\uc74c\uc758 \uc704\ud611\uc774 \ud1a0\uc9c0\uc758 \ud310\ub9e4\ub97c \ub9c9\ub294 \ub370 \uac70\uc758 \ub3c4\uc6c0\uc774 \ub418\uc9c0 \uc54a\uc558\ub2e4.\\t\ud1a0\uc9c0 \ud310\ub9e4\ub294 \uc5b4\ub5a0\ud55c \uc704\ud611\ub3c4 \uad50\ud658\ud558\uc9c0 \uc54a\uace0 \uc774\ub8e8\uc5b4\uc9c4\ub2e4.\\tcontradiction\\n\uc5b4\ub290 \uc2dc\uc810\uc5d0 \uc774\ub974\ub7ec \ub098\ub294 \uc9c0\uae08 \ub2e4\uac00\uc624\ub294 \uc0c8\ub85c\uc6b4 \uac83\ub4e4\uacfc \ub098\uc624\ub294 \ub9ce\uc740 \uc0c8\ub85c\uc6b4 \uac83\ub4e4\uc774 \ub0b4\uac00 \ub299\uc5b4\uac00\uace0 \uc788\ub2e4\uace0 \ub9d0\ud558\ub294 \uc2dc\ub300\ub85c \uc811\uc5b4\ub4e4\uace0 \uc788\ub2e4.\\t\ub098\ub294 \uc5ec\uc804\ud788 \ub0b4\uac00 \ubcf4\ub294 \ubaa8\ub4e0 \uc0c8\ub85c\uc6b4 \uac83\uc744 \uc0ac\ub791\ud55c\ub2e4.\\tcontradiction\\n\ub274\uc2a4\uc704\ud06c\ub294 \ubb3c\ub9ac\ud559\uc790\ub4e4\uc774 \uacbd\uae30\uc7a5 \ud589\uc0ac\uc5d0\uc11c \uace0\uc18d\ub3c4\ub85c\uc758 \uc790\ub3d9\ucc28 \uad50\ud1b5\uacfc \ubcf4\ud589\uc790 \uad50\ud1b5\uc744 \uac1c\uc120\ud558\uae30 \uc704\ud574 \uc0c8\ub5bc\uc758 \uc6c0\uc9c1\uc784\uc744 \uc5f0\uad6c\ud558\uace0 \uc788\ub2e4\uace0 \ub9d0\ud55c\ub2e4.\\t\uace0\uc18d\ub3c4\ub85c\uc758 \uc790\ub3d9\ucc28 \uad50\ud1b5 \ud750\ub984\uc744 \uac1c\uc120\ud558\ub294 \uac83\uc740 \ubb3c\ub9ac\ud559\uc790\ub4e4\uc774 \uc0c8\ub5bc\ub97c \uc5f0\uad6c\ud558\ub294 \uc774\uc720 \uc911 \ud558\ub098\uc774\ub2e4.\\tentailment\\n\uc5bc\ub9c8\ub098 \ub2e4\ub978\uac00? \uadf8\ub294 \uc7a0\uc2dc \ub9d0\uc744 \uba48\ucd94\uc5c8\ub2e4\uac00 \ub9d0\uc744 \uc774\uc5c8\ub2e4.\\t\uadf8\ub294 \uadf8 \uc18c\ub140\uac00 \uc5b4\ub514\uc5d0 \uc788\ub294\uc9c0 \uc54c\uace0 \uc2f6\uc5c8\ub2e4.\\tentailment\\n\uae00\uc384, \uadf8\uc5d0\uac8c \ub108\ubb34 \ub9ce\uc740 \uac83\uc744 \uc8fc\uc9c0\ub9c8.\\t\uadf8\ub294 \ud6e8\uc52c \ub354 \ub9ce\uc740 \uac83\uc744 \uc694\uad6c\ud560 \uac83\uc774\ub2e4.\\tneutral\\n\uc544\ubb34\ub9ac \uadf8\uc758 \ucc3d\uc791\ubb3c\uc774 \uc644\ubcbd\ud574 \ubcf4\uc778\ub2e4\uace0 \ud574\ub3c4, \uadf8\ub4e4\uc744 \ubbff\ub294 \uac83\uc740 \uc544\ub9c8\ub3c4 \uc88b\uc740 \uc0dd\uac01\uc774 \uc544\ub2d0 \uac83\uc774\ub2e4.\\'\\t\ub3c4\uc790\uae30\ub97c \uc798 \ub9cc\ub4e0\ub2e4\uace0 \ud574\uc11c \ub204\uad70\uac00\ub97c \ubbff\ub294 \uac83\uc740 \uc544\ub9c8 \uc88b\uc9c0 \uc54a\uc744 \uac83\uc774\ub2e4.\\tneutral\\n\ubc84\uc2a4\ud2c0\ub9c1 \uadf8\ub780 \ube44\uc544(Bustling Gran Via)\ub294 \ud638\ud154, \uc0c1\uc810, \uadf9\uc7a5, \ub098\uc774\ud2b8\ud074\ub7fd, \uce74\ud398 \ub4f1\uc774 \uc5b4\uc6b0\ub7ec\uc838 \uc0b0\ucc45\uacfc \ucc3d\uac00\ub97c \ubcfc \uc218 \uc788\ub2e4.\\tGran Via\ub294 \ud638\ud154, \uc0c1\uc810, \uadf9\uc7a5, \ub098\uc774\ud2b8\ud074\ub7fd, \uce74\ud398\uc758 \ubc88\ud654\ud55c \uc870\ud569\uc774\ub2e4.\\tentailment\\n\uc815\ubd80 \uc778\uc1c4\uc18c\\t\uadf8 \uc0ac\ubb34\uc2e4\uc740 \uc6cc\uc2f1\ud134\uc5d0 \uc704\uce58\ud574 \uc788\ub2e4.\\tneutral\\n\uc2e4\uc81c \ubb38\ud654 \uc804\uc7c1\uc774 \uc5b4\ub514 \uc788\ub294\uc9c0 \uc54c\uace0 \uc2f6\ub2e4\uba74 \ud559\uc6d0\uc744 \uc78a\uc5b4\ubc84\ub9ac\uace0 \uc2e4\ub9ac\ucf58 \ubc38\ub9ac\uc640 \ub808\ub4dc\ubaac\ub4dc\ub97c \uc0dd\uac01\ud574 \ubcf4\ub77c.\\t\uc2e4\uc81c \ubb38\ud654 \uc804\uc7c1\uc740 \ub808\ub4dc\ubaac\ub4dc\uc5d0\uc11c \uc77c\uc5b4\ub09c\ub2e4.\\tentailment\\n\uadf8\ub9ac\uace0 \ud398\ub2c8\uc2e4\ub9b0\uc744 \uc8fc\uc9c0 \uc54a\uae30 \uc704\ud574 \uce68\ub300 \uc704\uc5d0 \uc62c\ub824\ub1a8\uc5b4\\t\uadf8\ub140\uc758 \ubc29\uc5d0\ub294 \ud398\ub2c8\uc2e4\ub9b0\uc774 \uc5c6\ub2e4\ub294 \uc9d5\ud6c4\uac00 \uc804\ud600 \uc5c6\uc5c8\ub2e4.\\tcontradiction\\nL.A.\uc758 \uc57c\uc678 \uc2dc\uc7a5\uc744 \ud65c\ubcf4\ud558\ub294 \uac83\uc740 \ub9db\uc788\uace0 \uc800\ub834\ud55c \uadf8\ub8e8\ube0c\ub97c \uc7a1\uace0, \ub05d\uc774 \uc5c6\ub294 \ud587\ube5b\uc744 \uc990\uae30\uace0, \uc2e0\uc120\ud55c \ub18d\uc0b0\ubb3c, \uaf43, \ud5a5, \uadf8\ub9ac\uace0 \uac00\uc82f \uac08\ub85c\uc5b4\ub97c \uad6c\uc785\ud558\uba74\uc11c \ud604\uc9c0\uc778\ub4e4\uacfc \uc5b4\uc6b8\ub9b4 \uc218 \uc788\ub294 \ud6cc\ub96d\ud55c \ubc29\ubc95\uc774\ub2e4.\\tLA\uc758 \uc57c\uc678 \uc2dc\uc7a5\uc744 \ub3cc\uc544\ub2e4\ub2c8\ub294 \uac83\uc740 \uc2dc\uac04 \ub0ad\ube44\ub2e4.\\tcontradiction\\n\uc548\ub098\ub294 \ubc16\uc73c\ub85c \ub098\uc640 \uc548\ub3c4\uc758 \ud55c\uc228\uc744 \ub0b4\uc26c\uc5c8\ub2e4. \ub2e8 \ud55c \ubc88, \uadf8\ub9ac\uace0 \ub9c8\ub9ac\ud6c4\uc544\uc26c \ub9db\uc758 \uc220\ub85c \ub05d\ub0b4\uc790\ub294 \uacb0\uc2ec\uc774 \ub4a4\uc11e\uc5ec \uc788\uc5c8\ub2e4.\\t\uc548\ub098\ub294 \uc548\uc2ec\ud558\uace0 \ub9c8\ub9ac\ud6c4\uc544\uc26c \ub9db\uc758 \uc220\uc744 \ub2e4 \ub9c8\uc2dc\uae30\ub85c \uacb0\uc2ec\ud588\ub2e4.\\tentailment\\n5 \uc6d4\uc5d0 Vajpayee\ub294 \ud575 \uc2e4\ud5d8\uc758 \uc131\uacf5\uc801\uc778 \uc644\ub8cc\ub97c \ubc1c\ud45c\ud588\ub294\ub370, \uc778\ub3c4\uc778\ub4e4\uc740 \uc8fc\uad8c\uc758 \ud45c\uc2dc\ub85c \uc120\uc804\ud588\uc9c0\ub9cc \uc774\uc6c3 \uad6d\uac00\uc640 \uc11c\uad6c\uc640\uc758 \uc778\ub3c4 \uad00\uacc4\ub97c \ubcf5\uc7a1\ud558\uac8c \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\\t\uc778\ub3c4\ub294 \uc131\uacf5\uc801\uc778 \ud575\uc2e4\ud5d8\uc744 \ud55c \uc801\uc774 \uc5c6\ub2e4.\\tcontradiction\\n\ud50c\ub77c\ub178 \uc6d0\uc5d0\uc11c \ubcf4\ud1b5 \uc5bc\ub9c8\ub098 \ub9ce\uc740 \uac83\uc744 \uac00\uc9c0\uace0 \uc788\ub294\uac00?\\t\uc800 \uc0ac\ub78c\ub4e4 \uc911\uc5d0 \ud50c\ub77c\ub178 \uc6d0\uc5d0 \uac00\ubcf8 \uc0ac\ub78c \uc788\uc5b4?\\tcontradiction\\n\uadf8\uac83\uc758 \uc804\uccb4\uc801\uc778 \ud615\ud0dc\uc758 \uc6b0\uc544\ud568\uc740 \uc6b4\ud558 \uac74\ub108\ud3b8\uc5d0\uc11c \uac00\uc7a5 \uc798 \ubcfc \uc218 \uc788\ub2e4. \uc65c\ub0d0\ud558\uba74, \ub85c\ub9c8\uc5d0 \uc788\ub294 \uc131 \ubca0\ub4dc\ub85c\ucc98\ub7fc, \ub3d4\uc740 \uae38\ucb49\ud55c \ubcf8\ub2f9 \ub4a4\ub85c \ub354 \uac00\uae4c\uc6b4 \uacf3\uc5d0 \uc0ac\ub77c\uc9c0\uae30 \ub54c\ubb38\uc774\ub2e4.\\t\uc131 \ubca0\ub4dc\ub85c\uc758 \uae38\ucb49\ud55c \ubcf8\ub2f9\uc740 \ub3d4\uc744 \uac00\ub9b0\ub2e4.\\tentailment\\n\ub2f9\uc2e0\uc740 \uc218\ud2f4\uc774 \uc0b4\uc5d0 \uac15\ubc15\uc801\uc778 \uae30\uc068\uc744 \uac00\uc9c0\uace0 \ub204\ub4dc\ub97c \uadf8\ub9b4 \uac83\uc774\ub77c\uace0 \uc0dd\uac01\ud558\uaca0\uc9c0\ub9cc, \uc544\ub2c8\uc624; \uadf8\ub294 \uadf8\uc758 \ubaa8\ub4e0 \uacbd\ub825\uc5d0\uc11c \ub2e8 \ud55c \uc810\ub9cc\uc744 \uadf8\ub838\uace0, \uadf8\uac83\uc740 \uc0ac\uc18c\ud55c \uadf8\ub9bc\uc774\ub2e4.\\t\uadf8\ub294 \uadf8\uac83\uc774 \uadf8\ub97c \ubd88\ud3b8\ud558\uac8c \ub9cc\ub4e4\uc5c8\uae30 \ub54c\ubb38\uc5d0 \ud558\ub098\ub9cc \uadf8\ub838\ub2e4.\\tneutral\\n\uc774 \uc778\uc0c1\uc801\uc778 \ud48d\uacbd\uc740 \uc6d0\ub798 \ub098\ud3ec \ub808\uc628\uc774 \ub8e8\ube0c\ub974 \ubc15\ubb3c\uad00\uc758 \uce68\uc2e4\uc5d0\uc11c \ubcfc \uc218 \uc788\ub3c4\ub85d \uacc4\ud68d\ub418\uc5c8\ub294\ub370, \uadf8 \ub2f9\uc2dc \uad81\uc804\uc774\uc5c8\uc2b5\ub2c8\ub2e4.\\t\ub098\ud3f4\ub808\uc639\uc740 \uadf8\uc758 \ubaa8\ub4e0 \uad81\uc804\uc5d0 \uc788\ub294 \uadf8\uc758 \uce68\uc2e4\uc5d0\uc11c \ubcf4\ub294 \uacbd\uce58\uc5d0 \ub9ce\uc740 \uad00\uc2ec\uc744 \uac00\uc84c\ub2e4.\\tneutral\\n\uadf8\ub294 \uc6b0\ub9ac\uc5d0\uac8c \ubb38 \uc5f4\uc1e0\ub97c \uac74\ub124\uc8fc\uace0\ub294 \uae09\ud788 \ub5a0\ub0ac\ub2e4.\\t\uadf8\ub294 \uae34\uc7a5\ud574\uc11c \uc6b0\ub9ac\uc5d0\uac8c \uc5f4\uc1e0\ub97c \ube68\ub9ac \uc8fc\uc5c8\ub2e4.\\tneutral\\n\uc704\uc6d0\ud68c\ub294 \ub610\ud55c \ucd5c\uc885 \uaddc\uce59\uc744 OMB\uc5d0 \uc81c\ucd9c\ud588\ub2e4.\\t\uc704\uc6d0\ud68c\ub294 \ub610\ud55c \uc774 \uaddc\uce59\uc744 \ub2e4\ub978 \uadf8\ub8f9\uc5d0 \uc81c\ucd9c\ud588\uc9c0\ub9cc \ucd5c\uc885 \uaddc\uce59\uc740 OMB\uac00 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \uac83\uc774 \uc5c8\uc2b5\ub2c8\ub2e4.\\tneutral\\n\uc815\uc6d0\uac00\uac8c\uc5d0 \uac00\ubcf4\uba74 \uc62c\ub9ac\ube44\uc544\uc758 \ubcf5\uc81c \ud654\ud569\ubb3c \uac19\uc740 \uc720\ucf8c\ud55c \uc774\ub984\uc744 \uac00\uc9c4 \uc81c\ud488\ub4e4\uc744 \ucc3e\uc744 \uc218 \uc788\uc744 \uac81\ub2c8\ub2e4.\uc774 \uc81c\ud488\uc774 \ubfcc\ub9ac\ub97c \ub0b4\ub9ac\ub3c4\ub85d \ub3d5\uae30 \uc704\ud574 \ucd2c\uc601\uc758 \uc808\ub2e8\ub41c \ub05d\uc5d0 \ub369\ud06c\uc29b\uc744 \ud558\ub294 \ud638\ub974\ubaac\uc758 \ud63c\ud569\ubb3c\uc774\uc8e0.\\t\uc815\uc6d0 \uac00\uafb8\uae30 \uac00\uac8c\uc758 \uc81c\ud488\ub4e4\uc740 \uc885\uc885 \uadf8\ub4e4\uc758 \ubaa9\uc801\uc744 \uc124\uba85\ud558\uae30 \uc704\ud574 \uae30\uc220\uc801\uc73c\ub85c\ub098 \uacfc\ud559\uc801\uc73c\ub85c \ud30c\uc0dd\ub41c \uc774\ub984(\uc62c\ub9ac\ube44\uc544\uc758 \ubcf5\uc81c \ud654\ud569\ubb3c\ucc98\ub7fc)\uc744 \ubd80\uc5ec\ubc1b\ub294\ub2e4.\\tneutral\\n\uc2a4\ud0c0\ub294 \uc2a4\ud2f8 \uc790\uc2e0\uc774\ub098 \uc65c \uadf8\ub140\uc758 \uc774\uc57c\uae30\ub97c \ubc14\uafb8\uc5c8\ub294\uc9c0\uc5d0 \ud6e8\uc52c \ub354 \uad00\uc2ec\uc774 \uc788\uc744 \uac83\uc774\ub2e4.\\t\uc2a4\ud2f8\uc758 \uc774\uc57c\uae30\ub294 \uc870\uae08\ub3c4 \ubcc0\ud558\uc9c0 \uc54a\uc558\ub2e4.\\tcontradiction\\n\ub0a8\ud3b8\uacfc\uc758 \ub9c8\uc9c0\ub9c9 \ub300\uacb0\ub85c \ub9e5\ud2f0\uc5b4\ub294 \ub178\ub77c\uc758 \ubcc0\uc2e0\uc744 \ub108\ubb34\ub098 \ub2a5\uc219\ud558\uac8c \uc608\uace0\ud574 \uc654\uae30 \ub54c\ubb38\uc5d0, \uadf8\ub140\uc5d0\uac8c\ub294 \ub2f9\ud669\uc2a4\ub7ec\uc6b8 \uc815\ub3c4\ub85c \uac11\uc791\uc2a4\ub7ec\uc6b4 \uac83\ucc98\ub7fc \ubcf4\uc774\uc9c0\ub9cc, \uc6b0\ub9ac\uc5d0\uac8c\ub294 \uac10\uc815\uc801\uc73c\ub85c \ubd88\uac00\ud53c\ud574 \ubcf4\uc778\ub2e4.\\t\ub178\ub77c\uc758 \ubcc0\uc2e0\uc740 \ubd84\uba85\ud558\uace0 \ud544\uc5f0\uc801\uc774\uc5c8\ub2e4.\\tcontradiction\\n\uc774\uc9d1\ud2b8 \ucd5c\ub0a8\ub2e8 \ub3c4\uc2dc\uc778 \uc544\uc2a4\uc644\uc740 \uc624\ub79c \uc5ed\uc0ac\ub97c \ud1b5\ud574 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud574\uc654\ub2e4.\\t\uc544\uc2a4\uc644\uc740 \uc774\uc9d1\ud2b8 \uad6d\uacbd \ubc14\ub85c \uc704\uc5d0 \uc704\uce58\ud574 \uc788\uc2b5\ub2c8\ub2e4.\\tneutral\\n\uadf8\ub7ec\ub098 \ud6e8\uc52c \ub354 \uc6b0\uc544\ud55c \uac74\ucd95\uc801 \ud130\uce58\ub294 \uc2e0\uc131\ud55c \ucda4\uc778 Bharatanatyam\uc5d0\uc11c \uc218\ud589\ub41c 108 \uac00\uc9c0 \uae30\ubcf8 \ud3ec\uc988\ub97c \uc2dc\ubc14 \ud328\ub110\uc5d0\uc11c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\\t\ud328\ub110\uc5d0 \ub300\ud55c \uc2dc\ubc14\uc758 \ubb18\uc0ac\ub294 \uc77c\ubc18\uc801\uc778 \ubaa8\ud2f0\ube0c\ub2e4.\\tneutral\\n\ud638\ud654\ub86d\uac8c \uc2ec\uc5b4\uc9c4 \uacc4\ub2e8\uc2dd \uc815\uc6d0\uc740 \uc774\ud0c8\ub9ac\uc544 \ud615\uc2dd\uc758 \uac00\uc7a5 \ud6cc\ub96d\ud55c \uc559\uc0c1\ube14 \uc911 \ud558\ub098\uc785\ub2c8\ub2e4.\\t\uc544\ub984\ub2e4\uc6b4 \uc815\uc6d0\uacfc \ud76c\uadc0\ud55c \uaf43\uaf42\uc774 \ubaa8\ub450 \uc774\ud0c8\ub9ac\uc544\uc758 \ud615\uc2dd\uc801\uc778 \uc2a4\ud0c0\uc77c\uc744 \ubcf4\uc5ec\uc900\ub2e4.\\tneutral\\n\uc74c, \uadf8\ub7ac\uc73c\uba74 \uc88b\uc558\uc744 \ud150\ub370\\t\ub098\ub294 \uadf8\uac83\uc744 \ub2e4\ub974\uac8c \ud560 \uae30\ud68c\ub97c \ubab9\uc2dc \uac08\ub9dd\ud55c\ub2e4.\\tentailment\\n\ud3d0\ud5c8\uac00 \ub41c \uc131\uc758 \uae30\uc2ad\uc5d0 \uc790\ub9ac\uc7a1\uace0 \uc788\ub294 \uc608\uc05c \uc911\uc138 \ub3c4\uc2dc \ucf00\uc774\uc11c\uc2a4\ubc84\uadf8\ub294 \ub178\ubca8 \ud3c9\ud654\uc0c1 \uc218\uc0c1\uc790 \uc54c\ubc84\ud2b8 \uc288\ubc14\uc774\ucc98(1875\ub144)\uc758 \ucd9c\uc0dd\uc9c0\ub85c \ub110\ub9ac \uc54c\ub824\uc838 \uc788\ub2e4.\\t\uc54c\ubc84\ud2b8 \uc288\ubc14\uc774\ucc98\ub294 \ub458 \ub2e4 \ucf00\uc774\uc11c\uc2a4\ubc84\uadf8 \ub9c8\uc744\uc5d0 \uc788\uc5c8\ub2e4.\\tentailment\\n\uace0\uac10\ub3c4\ub294 \ubb38\uc81c\uac00 \uc788\ub294 \ub300\ubd80\ubd84\uc758 \ud658\uc790\ub4e4\uc774 \ubc1c\uacac\ub420 \uac83\uc744 \ubcf4\uc7a5\ud55c\ub2e4.\\t\uc7a5\ube44 \ubbfc\uac10\ub3c4\ub294 \ubb38\uc81c \ud0d0\uc9c0\uc640 \uad00\ub828\uc774 \uc5c6\uc2b5\ub2c8\ub2e4.\\tcontradiction\\n\uc624\ub298\uc740 \ud655\uc2e4\ud788 \ubc18\ubc14\uc9c0 \uac19\uc740 \ub0a0\uc774\uc5c8\uc5b4\\t\uc624\ub298 \uc0ac\ubb34\uc2e4\uc5d0 \uc788\ub294 \ubaa8\ub4e0 \uc0ac\ub78c\ub4e4\uc740 \ubc18\ubc14\uc9c0\ub97c \uc785\uc5c8\ub2e4.\\tneutral\\n\ubabb\uc0dd\uae34 \ud131\uc2dc\ub3c4\ub97c \uc785\uace0.\\t\uadf8\uac83\uc740 \ubd84\ud64d\uc0c9\uacfc \uc8fc\ud669\uc0c9\uc785\ub2c8\ub2e4.\\tneutral\\n\uc774\uc8fc \ub178\ub3d9 \uc218\uc6a9\uc18c \uc624 \ub9c8\uc774 \uac13 \uadf8\ub4e4\uc740 \ud310\uc9c0 \uc0c1\uc790\uc5d0 \uc0b0\ub2e4.\\t\ub178\ub3d9 \uc218\uc6a9\uc18c\uc5d0\ub294 \ud310\uc9c0 \uc0c1\uc790\uc5d0 \uc0ac\ub294 \uc774\uc8fc \ub178\ub3d9\uc790\ub4e4\uc758 \uc0ac\uc9c4\uc774 \uc788\ub2e4.\\tneutral\\n\uadf8\ub798, \uadf8\uac00 \uc804 \uc138\uacc4\ub97c \uc5ec\ud589\ud55c \ud6c4\uc5d0 \uadf8\ub7f0 \uac70\uc57c\\t\uadf8\uac83\uc740 \uc0ac\ub78c\ub4e4\uc758 \uc138\uacc4 \uc5ec\ud589\uc744 \ub530\ub978\ub2e4.\\tentailment\\n\uac74\ub108\ud3b8\uc5d0 \ud06c\uace0 \ud070 \ucc38\ub098\ubb34 \uba87 \uadf8\ub8e8\uac00 \uc788\ub2e4.\\t\uc6b0\ub9ac\ub294 \uc5ec\uae30 \uc624\ud06c\ub098 \uc5b4\ub5a4 \uc885\ub958\uc758 \ubbf8\uad6d \ub098\ubb34\ub3c4 \uc5c6\ub2e4.\\tcontradiction\\nFort-de-France\uc5d0\uc11c \ucd9c\ubc1c\ud558\ub294 \uc790\ub3d9\ucc28\ub098 \uc5ec\uac1d\uc120\uc73c\ub85c, \ub2f9\uc2e0\uc740 \uc548\uc138 ? \ubc14\ub2e4 \ud3ec\ub3c4\uac00 \uadf8\ub298\uc744 \uc81c\uacf5\ud558\ub294 \ucf8c\uc801\ud55c \uac08\uc0c9 \ubaa8\ub798 \ud574\ubcc0\uacfc \ud53c\ud06c\ub2c9 \ud14c\uc774\ube14, \uc5b4\ub9b0\uc774 \ubbf8\ub044\ub7fc\ud2c0, \uc2dd\ub2f9\uc774 \uc788\ub294 \uc548\ub290\uc5d0 \ub3c4\ucc29\ud560 \uc218 \uc788\ub2e4.\\t\ud504\ub791\uc2a4 \uc694\uc0c8\uc5d0\uc11c \uc790\ub3d9\ucc28\ub098 \ud398\ub9ac\ub97c \ud0c0\uace0 \uc548\uc138\ub85c \uac08 \uc218 \uc788\ub2e4.\\tentailment\\n\uadf8\ub9ac\uace0 \uadf8\uac83\uc740 \uc568\ub77c\ubc30\ub9c8\uc8fc\uac00 \uc608\uc0c1\ud588\ub358 \ub300\ub85c \uc608\uc0b0\uc5d0\uc11c 50\ub9cc \ub2ec\ub7ec\ub97c \uc0ad\uac10\ud558\uc9c0 \uc54a\uc744 \uac83\uc774\ub77c\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4.\\t\uc568\ub77c\ubc30\ub9c8 \uc8fc\ub294 \uc608\uc0b0 \uc0ad\uac10\uc744 \ud558\uc9c0 \uc54a\uc558\ub2e4. \uc65c\ub0d0\ud558\uba74 \uadf8\ub807\uac8c \ud558\ub294 \uac83\uc5d0 \ub300\ud55c \ucd08\uae30 \uc815\ub2f9\uc131\uc774 \uc815\ubc00 \uc870\uc0ac\uc5d0 \ub9de\uc11c\uc9c0 \uc54a\uc558\uae30 \ub54c\ubb38\uc774\ub2e4.\\tneutral\\n\uc54c\uc558\uc5b4 \uba3c\uc800 \uc5b4 .. \uc5b4 .. \ub178\uc778\uc774\ub098 \uac00\uc871\uc744 \uc694\uc591\uc6d0\uc5d0 \ubcf4\ub0b4\ub294 \uac83\uc5d0 \ub300\ud574 \uc5b4\ub5bb\uac8c \uc0dd\uac01\ud558\ub2c8?\\t\uac00\uc871\uc744 \uc694\uc591\uc6d0\uc5d0 \ubcf4\ub0b4\uc11c \uc0ac\ub294 \uac83\uc5d0 \ub300\ud574 \uc5b4\ub5bb\uac8c \uc0dd\uac01\ud558\ub294\uc9c0 \uc54c \ud544\uc694\uac00 \uc5c6\ub2e4.\\tcontradiction\\n\ub098\uba38\uc9c0\ub294 \ub108\uc5d0\uac8c \ub2ec\ub838\uc5b4.\\t\ub098\uba38\uc9c0\ub294 \ub108\uc5d0\uac8c \ub2ec\ub838\uc9c0\ub9cc \uc2dc\uac04\uc774 \ub9ce\uc9c0 \uc54a\ub2e4.\\tneutral\\n\uc74c-\ud760, 3\uc6d4\uc5d0 \ud587\ubcd5\uc5d0 \ud0c0\ub294 \uac83\uc5d0 \ub300\ud574 \uac71\uc815\ud558\uba74 \uc548 \ub41c\ub2e4\ub294 \uac83\uc744 \uc54c\uace0 \uc788\ub294 3\uc6d4\uc774\uc57c.\\t3\uc6d4\uc740 \uadf8\ub807\uac8c \ub365\uc9c0 \uc54a\ub2e4.\\tneutral\\n\uadf8\ub9ac\uace0 \uc5b4, \uadf8\ub7f0 \uc791\uc740 \uac83\ub4e4\ub85c \ub2e4\uc2dc \uc2dc\uc791\ud574\ubd10. \uc544\uc9c1 \ud6e8\uc52c \uc2f8. \uc5b4, \uadf8 \ud2b9\ubcc4\ud55c \ubaa8\ub378 \ucc28\ub294 150\ub2ec\ub7ec\uc57c.\\t\uadf8 \ubaa8\ud615\ucc28\ub294 4\ucc9c \ub2ec\ub7ec\uac00 \ub4e0\ub2e4.\\tcontradiction\\n\ub0b4\uc77c \ub3cc\uc544\uac00\uc57c \ud55c\ub2e4\uba74, \uce7c\uc774 \ub9d0\ud588\ub2e4.\\t\ub3cc\uc544\uac08 \uc218 \uc5c6\uc5b4. \uc624\ub298\uc740 \uc548 \ub3fc. \ub0b4\uc77c\uc740 \uc548 \ub3fc. \uc808\ub300 \uc548 \ub3fc.\" \uce7c\uc774 \ub9d0\ud588\ub2e4.', 'sentence2': 'contradiction'}\r\n        ```\r\n\r\n2. (Optional) Preferred to change the name of the features for the compatibility with `run_glue.py` in \ud83e\udd17 Transformers\r\n    - `kor_nli` dataset has same data structure of multi_nli, xnli\r\n    - Changing the name of features and the feature type of 'gold_label' to ClassLabel might be helpful\r\n    ```python\r\n        def _info(self):\r\n            return datasets.DatasetInfo(\r\n                description=_DESCRIPTION,\r\n                features=datasets.Features(\r\n                    {\r\n                        \"premise\": datasets.Value(\"string\"),\r\n                        \"hypothesis\": datasets.Value(\"string\"),\r\n                        \"label\": datasets.features.ClassLabel(names=[\"entailment\", \"neutral\", \"contradiction\"]),\r\n                    } \r\n                ),\r\n    ```\r\n\r\nIf you don't mind, I would like to fix this.\r\nThanks!",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 739387617,
    "title": "Update quail dataset to v1.3",
    "dateCreated": "2020-11-09T21:49:26Z",
    "dateModified": "2020-11-09T21:49:26Z",
    "description": "Updated quail to most recent version, to address the problem originally discussed [here](https://github.com/huggingface/datasets/issues/806).",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 739250624,
    "title": "Make save function use deterministic global vars order",
    "dateCreated": "2020-11-09T18:12:03Z",
    "dateModified": "2020-11-09T18:12:03Z",
    "description": "The `dumps` function need to be deterministic for the caching mechanism.\r\nHowever in #816 I noticed that one of dill's method to recursively check the globals of a function may return the globals in different orders each time it's used. To fix that I sort the globals by key in the `globs` dictionary.\r\nI had to add a rectified `save_function` to the saving functions registry of the Pickler to make it work.\r\n\r\nThis should fix #816 ",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 739173861,
    "title": "Fix type hints pickling in python 3.6",
    "dateCreated": "2020-11-09T16:27:47Z",
    "dateModified": "2020-11-09T16:27:47Z",
    "description": "Type hints can't be properly pickled in python 3.6. This was causing errors the `run_mlm.py` script from `transformers` with python 3.6\r\n\r\nHowever Cloupickle proposed a [fix](https://github.com/cloudpipe/cloudpickle/pull/318/files) to make it work anyway.\r\nThe idea is just to implement the pickling/unpickling of parameterized type hints. There is one detail though: since in python 3.6 we can't use `isinstance` on type hints, then we can't use pickle saving functions registry directly. Therefore we just wrap the `save_global` method of the Pickler.\r\n\r\nThis should fix https://github.com/huggingface/transformers/issues/8212 for python 3.6 and make `run_mlm.py` support python 3.6\r\n\r\ncc @sgugger ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 739145369,
    "title": "Add MRQA dataset",
    "dateCreated": "2020-11-09T15:52:19Z",
    "dateModified": "2020-11-09T15:52:19Z",
    "description": "## Adding a Dataset\r\n- **Name:** MRQA\r\n- **Description:** Collection of different (subsets of) QA datasets all converted to the same format to evaluate out-of-domain generalization (the datasets come from different domains, distributions, etc.). Some datasets are used for training and others are used for evaluation. This dataset was collected as part of MRQA 2019's shared task \r\n- **Paper:** https://arxiv.org/abs/1910.09753\r\n- **Data:** https://github.com/mrqa/MRQA-Shared-Task-2019\r\n- **Motivation:** Out-of-domain generalization is becoming (has become) a de-factor evaluation for NLU systems\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 739102686,
    "title": "[Caching] Dill globalvars() output order is not deterministic and can cause cache issues.",
    "dateCreated": "2020-11-09T15:01:20Z",
    "dateModified": "2020-11-09T15:01:20Z",
    "description": "Dill uses `dill.detect.globalvars` to get the globals used by a function in a recursive dump. `globalvars` returns a dictionary of all the globals that a dumped function needs. However the order of the keys in this dict is not deterministic and can cause caching issues.\r\n\r\nTo fix that one could register an implementation of dill's `save_function` in the `datasets` pickler that sorts the globals keys before dumping a function.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 738842092,
    "title": "Is dataset iterative or not?",
    "dateCreated": "2020-11-09T09:11:48Z",
    "dateModified": "2020-11-09T09:11:48Z",
    "description": "Hi\r\nI want to use your library for large-scale training, I am not sure if this is implemented as iterative datasets or not?\r\ncould you provide me with example how I can use datasets as iterative datasets?\r\nthanks",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 738500443,
    "title": "Joining multiple datasets ",
    "dateCreated": "2020-11-08T16:19:30Z",
    "dateModified": "2020-11-08T16:19:30Z",
    "description": "Hi\r\nI have multiple iterative datasets from your library with different size and I want to join them in a way that each datasets is sampled equally, so smaller datasets more, larger one less, could you tell me how to implement this in pytorch? thanks ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 738489852,
    "title": "How to implement DistributedSampler with datasets ",
    "dateCreated": "2020-11-08T15:27:11Z",
    "dateModified": "2020-11-08T15:27:11Z",
    "description": "Hi,\r\nI am using your datasets to define my dataloaders, and I am training finetune_trainer.py in huggingface repo on them.\r\nI need a distributedSampler to be able to train the models on TPUs being able to distribute the load across the TPU cores. Could you tell me how I can implement the distribued sampler when using datasets in which datasets are iterative? To give you more context, I have multiple of datasets and I need to write sampler for this case. thanks. ",
    "status": "open",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 738340217,
    "title": "Too much logging ",
    "dateCreated": "2020-11-07T23:56:30Z",
    "dateModified": "2020-11-07T23:56:30Z",
    "description": "I'm doing this in the beginning of my script:\r\n\r\nfrom datasets.utils import logging as datasets_logging\r\ndatasets_logging.set_verbosity_warning()\r\n\r\nbut I'm still getting these logs:\r\n\r\n[2020-11-07 15:45:41,908][filelock][INFO] - Lock 139958278886176 acquired on /home/username/.cache/huggingface/datasets/cfe20ffaa80ef1c145a0a210d5b9cdce2b60002831e6ed0edc7ab9275d6f0d48.1bd4ccbce9de3dad0698d84674a19d6cc66a84db736a6398110bd196795dde7e.py.lock\r\n\r\n[2020-11-07 15:45:41,909][filelock][INFO] - Lock 139958278886176 released on /home/username/.cache/huggingface/datasets/cfe20ffaa80ef1c145a0a210d5b9cdce2b60002831e6ed0edc7ab9275d6f0d48.1bd4ccbce9de3dad0698d84674a19d6cc66a84db736a6398110bd196795dde7e.py.lock\r\n\r\nusing datasets version = 1.1.2",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 738280132,
    "title": "nlp viewer error",
    "dateCreated": "2020-11-07T17:08:58Z",
    "dateModified": "2020-11-07T17:08:58Z",
    "description": "Hello, \r\nwhen I select amazon_us_reviews in nlp viewer, it shows error.\r\nhttps://huggingface.co/nlp/viewer/?dataset=amazon_us_reviews\r\n![image](https://user-images.githubusercontent.com/30210529/98447334-4aa81200-2124-11eb-9dca-82c3ab34ccc2.png)\r\n",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 737878370,
    "title": "Fix seqeval metric",
    "dateCreated": "2020-11-06T16:11:43Z",
    "dateModified": "2020-11-06T16:11:43Z",
    "description": "The current seqeval metric returns the following error when computed:\r\n```\r\n~/.cache/huggingface/modules/datasets_modules/metrics/seqeval/78a944d83252b5a16c9a2e49f057f4c6e02f18cc03349257025a8c9aea6524d8/seqeval.py in _compute(self, predictions, references, suffix)\r\n    102         scores = {}\r\n    103         for type_name, score in report.items():\r\n--> 104             scores[type_name][\"precision\"] = score[\"precision\"]\r\n    105             scores[type_name][\"recall\"] = score[\"recall\"]\r\n    106             scores[type_name][\"f1\"] = score[\"f1-score\"]\r\n\r\nKeyError: 'LOC'\r\n```\r\nThis is because the current code basically tries to do:\r\n```\r\nscores = {}\r\nscores[\"LOC\"][\"precision\"] = some_value\r\n```\r\nwhich does not work in python. This PR fixes that while keeping the previous nested structure of results, with the same keys.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 737832701,
    "title": "Add Google Taskmaster dataset",
    "dateCreated": "2020-11-06T15:10:41Z",
    "dateModified": "2020-11-06T15:10:41Z",
    "description": "## Adding a Dataset\r\n- **Name:** Taskmaster\r\n- **Description:** A large dataset of task-oriented dialogue with annotated goals (55K dialogues covering entertainment and travel reservations)\r\n- **Paper:** https://arxiv.org/abs/1909.05358\r\n- **Data:** https://github.com/google-research-datasets/Taskmaster\r\n- **Motivation:** One of few annotated datasets of this size for goal-oriented dialogue\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 737638942,
    "title": "dataset(dgs): initial dataset loading script",
    "dateCreated": "2020-11-06T10:14:43Z",
    "dateModified": "2020-11-06T10:14:43Z",
    "description": "When trying to create dummy data I get:\r\n\r\n> Dataset datasets with config None seems to already open files in the method `_split_generators(...)`. You might consider to instead only open files in the method `_generate_examples(...)` instead. If this is not possible the dummy data has t o be created with less guidance. Make sure you create the file dummy_data.\r\n\r\nI am not sure how to manually create the dummy_data (what exactly it should contain)\r\n\r\nAlso note, this library says:\r\n> ImportError: To be able to use this dataset, you need to install the following dependencies['pympi'] using 'pip install pympi' for instance'\r\n\r\nWhen you actually need to `pip install pympi-ling`\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 737509954,
    "title": "load_dataset for LOCAL CSV files report CONNECTION ERROR",
    "dateCreated": "2020-11-06T06:33:04Z",
    "dateModified": "2020-11-06T06:33:04Z",
    "description": "## load_dataset for LOCAL CSV files report CONNECTION ERROR\r\n- **Description:** \r\nA local demo csv file:\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom datasets import load_dataset\r\nimport torch\r\nimport transformers\r\n\r\ndf = pd.DataFrame(np.arange(1200).reshape(300,4))\r\ndf.to_csv('test.csv', header=False, index=False)\r\n\r\nprint('datasets version: ', datasets.__version__)\r\nprint('pytorch version: ', torch.__version__)\r\nprint('transformers version: ', transformers.__version__)\r\n\r\n# output:\r\ndatasets version:  1.1.2\r\npytorch version:  1.5.0\r\ntransformers version:  3.2.0\r\n```\r\n\r\nwhen I load data through `dataset`:\r\n```\r\ndataset = load_dataset('csv', data_files='./test.csv', delimiter=',', autogenerate_column_names=False)\r\n```\r\nError infos:\r\n```\r\nConnectionError                           Traceback (most recent call last)\r\n<ipython-input-17-bbdadb9a0c78> in <module>\r\n----> 1 dataset = load_dataset('csv', data_files='./test.csv', delimiter=',', autogenerate_column_names=False)\r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)\r\n    588     # Download/copy dataset processing script\r\n    589     module_path, hash = prepare_module(\r\n--> 590         path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n    591     )\r\n    592 \r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, **download_kwargs)\r\n    266         file_path = hf_github_url(path=path, name=name, dataset=dataset, version=script_version)\r\n    267         try:\r\n--> 268             local_path = cached_path(file_path, download_config=download_config)\r\n    269         except FileNotFoundError:\r\n    270             if script_version is not None:\r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/datasets/utils/file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    306             user_agent=download_config.user_agent,\r\n    307             local_files_only=download_config.local_files_only,\r\n--> 308             use_etag=download_config.use_etag,\r\n    309         )\r\n    310     elif os.path.exists(url_or_filename):\r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag)\r\n    473         elif response is not None and response.status_code == 404:\r\n    474             raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\n--> 475         raise ConnectionError(\"Couldn't reach {}\".format(url))\r\n    476 \r\n    477     # Try a second time\r\n\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/csv/csv.py\r\n```\r\n\r\nAnd I try to connect to the site with requests:\r\n```\r\nimport requests\r\n\r\nrequests.head(\"https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/csv/csv.py\")\r\n```\r\n\r\nSimilarly Error occurs:\r\n```\r\n---------------------------------------------------------------------------\r\nConnectionRefusedError                    Traceback (most recent call last)\r\n~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self)\r\n    159             conn = connection.create_connection(\r\n--> 160                 (self._dns_host, self.port), self.timeout, **extra_kw\r\n    161             )\r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)\r\n     83     if err is not None:\r\n---> 84         raise err\r\n     85 \r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)\r\n     73                 sock.bind(source_address)\r\n---> 74             sock.connect(sa)\r\n     75             return sock\r\n\r\nConnectionRefusedError: [Errno 111] Connection refused\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNewConnectionError                        Traceback (most recent call last)\r\n~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\r\n    676                 headers=headers,\r\n--> 677                 chunked=chunked,\r\n    678             )\r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\r\n    380         try:\r\n--> 381             self._validate_conn(conn)\r\n    382         except (SocketTimeout, BaseSSLError) as e:\r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn)\r\n    975         if not getattr(conn, \"sock\", None):  # AppEngine might not have  `.sock`\r\n--> 976             conn.connect()\r\n    977 \r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connection.py in connect(self)\r\n    307         # Add certificate verification\r\n--> 308         conn = self._new_conn()\r\n    309         hostname = self.host\r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self)\r\n    171             raise NewConnectionError(\r\n--> 172                 self, \"Failed to establish a new connection: %s\" % e\r\n    173             )\r\n\r\nNewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f3cceda5e48>: Failed to establish a new connection: [Errno 111] Connection refused\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nMaxRetryError                             Traceback (most recent call last)\r\n~/.conda/envs/py36/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\r\n    448                     retries=self.max_retries,\r\n--> 449                     timeout=timeout\r\n    450                 )\r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\r\n    724             retries = retries.increment(\r\n--> 725                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\r\n    726             )\r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)\r\n    438         if new_retry.is_exhausted():\r\n--> 439             raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\n    440 \r\n\r\nMaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /huggingface/datasets/1.1.2/datasets/csv/csv.py (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f3cceda5e48>: Failed to establish a new connection: [Errno 111] Connection refused',))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConnectionError                           Traceback (most recent call last)\r\n<ipython-input-20-18cc3eb4a049> in <module>\r\n      1 import requests\r\n      2 \r\n----> 3 requests.head(\"https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/csv/csv.py\")\r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs)\r\n    102 \r\n    103     kwargs.setdefault('allow_redirects', False)\r\n--> 104     return request('head', url, **kwargs)\r\n    105 \r\n    106 \r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs)\r\n     59     # cases, and look like a memory leak in others.\r\n     60     with sessions.Session() as session:\r\n---> 61         return session.request(method=method, url=url, **kwargs)\r\n     62 \r\n     63 \r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\r\n    528         }\r\n    529         send_kwargs.update(settings)\r\n--> 530         resp = self.send(prep, **send_kwargs)\r\n    531 \r\n    532         return resp\r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs)\r\n    641 \r\n    642         # Send the request\r\n--> 643         r = adapter.send(request, **kwargs)\r\n    644 \r\n    645         # Total elapsed time of the request (approximately)\r\n\r\n~/.conda/envs/py36/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\r\n    514                 raise SSLError(e, request=request)\r\n    515 \r\n--> 516             raise ConnectionError(e, request=request)\r\n    517 \r\n    518         except ClosedPoolError as e:\r\n\r\nConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /huggingface/datasets/1.1.2/datasets/csv/csv.py (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f3cceda5e48>: Failed to establish a new connection: [Errno 111] Connection refused',))\r\n```",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 737215430,
    "title": "Quail dataset urls are out of date",
    "dateCreated": "2020-11-05T19:40:19Z",
    "dateModified": "2020-11-05T19:40:19Z",
    "description": "<h3>Code</h3>\r\n\r\n```\r\nfrom datasets import load_dataset\r\nquail = load_dataset('quail')\r\n```\r\n\r\n<h3>Error</h3>\r\n\r\n```\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/text-machine-lab/quail/master/quail_v1.2/xml/ordered/quail_1.2_train.xml\r\n```\r\n\r\n\r\nAs per [quail v1.3 commit](https://github.com/text-machine-lab/quail/commit/506501cfa34d9ec6c042d31026ba6fea6bcec8ff) it looks like the location and suggested ordering has changed. In [https://github.com/huggingface/datasets/blob/master/datasets/quail/quail.py#L52-L58](https://github.com/huggingface/datasets/blob/master/datasets/quail/quail.py#L52-L58) the quail v1.2 datasets are being pointed to, which don't exist anymore.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 737019360,
    "title": "On loading a metric from datasets, I get the following error",
    "dateCreated": "2020-11-05T15:14:38Z",
    "dateModified": "2020-11-05T15:14:38Z",
    "description": "`from datasets import load_metric`\r\n\r\n`metric = load_metric('bleurt')`\r\n\r\nTraceback:\r\n210 class _ArrayXDExtensionType(pa.PyExtensionType):\r\n    211 \r\n    212     ndims: int = None\r\n\r\nAttributeError: module 'pyarrow' has no attribute 'PyExtensionType'\r\n\r\nAny help will be appreciated. Thank you. ",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 736858507,
    "title": "Empty output/answer in TriviaQA test set (both in 'kilt_tasks' and 'trivia_qa')",
    "dateCreated": "2020-11-05T11:38:01Z",
    "dateModified": "2020-11-05T11:38:01Z",
    "description": "# The issue\r\n\r\nIt's all in the title, it appears to be fine on the train and validation sets.\r\n\r\nIs there some kind of mapping to do like for the questions (see https://github.com/huggingface/datasets/blob/master/datasets/kilt_tasks/README.md) ? \r\n\r\n# How to reproduce\r\n```py\r\nfrom datasets import load_dataset\r\nkilt_tasks = load_dataset(\"kilt_tasks\")\r\ntrivia_qa = load_dataset('trivia_qa', 'unfiltered.nocontext')\r\n# both in \"kilt_tasks\"\r\nIn [18]: any([output['answer'] for output in kilt_tasks['test_triviaqa']['output']])                                                                                                                        \r\nOut[18]: False\r\n# and \"trivia_qa\"\r\nIn [13]: all([answer['value'] == '<unk>' for answer in trivia_qa['test']['answer']])                                                                                                                        \r\nOut[13]: True\r\n# appears to be fine on the train and validation sets.\r\nIn [14]: all([answer['value'] == '<unk>' for answer in trivia_qa['train']['answer']])                                                                                                                       \r\nOut[14]: False\r\n\r\nIn [15]: all([answer['value'] == '<unk>' for answer in trivia_qa['validation']['answer']])                                                                                                                  \r\nOut[15]: False\r\n\r\nIn [16]: any([output['answer'] for output in kilt_tasks['train_triviaqa']['output']])                                                                                                                       \r\nOut[16]: True\r\n\r\nIn [17]: any([output['answer'] for output in kilt_tasks['validation_triviaqa']['output']])                                                                                                                  \r\nOut[17]: True\r\n\r\n```",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 736818917,
    "title": "fix: typos in tutorial to map KILT and TriviaQA",
    "dateCreated": "2020-11-05T10:42:00Z",
    "dateModified": "2020-11-05T10:42:00Z",
    "description": "",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 736296343,
    "title": "Add XGlue",
    "dateCreated": "2020-11-04T17:29:54Z",
    "dateModified": "2020-11-04T17:29:54Z",
    "description": "Dataset is ready to merge. An important feature of this dataset is that for each config the train data is in English, while dev and test data are in multiple languages. Therefore, @lhoestq and I decided offline that we will give the dataset the following API, *e.g.* for \r\n\r\n```python\r\nload_dataset(\"xglue\", \"ner\")  # would give the splits 'train', 'validation.en', 'test.en', 'validation.es', 'test.es', ... \r\n```\r\n=> therefore one can load a single language test via\r\n\r\n```python\r\nload_dataset(\"xglue\", \"ner\", split=\"test.es\")\r\n```",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 735790876,
    "title": "How to join two datasets?",
    "dateCreated": "2020-11-04T03:53:11Z",
    "dateModified": "2020-11-04T03:53:11Z",
    "description": "Hi,\r\n\r\nI'm wondering if it's possible to join two (preprocessed) datasets with the same number of rows but different labels? \r\n\r\nI'm currently trying to create paired sentences for BERT from `wikipedia/'20200501.en`, and I couldn't figure out a way to create a paired sentence using `.map()` where the second sentence is **not** the next sentence (i.e., from a different article) of the first sentence.\r\n\r\nThanks!",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 735772775,
    "title": "Update loading_metrics.rst",
    "dateCreated": "2020-11-04T02:57:11Z",
    "dateModified": "2020-11-04T02:57:11Z",
    "description": "Minor bug",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 735551165,
    "title": "switch amazon reviews class label order",
    "dateCreated": "2020-11-03T18:38:58Z",
    "dateModified": "2020-11-03T18:38:58Z",
    "description": "Switches the label order to be more intuitive for amazon reviews, #791.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 735518805,
    "title": "Cannot load TREC dataset: ConnectionError",
    "dateCreated": "2020-11-03T17:45:22Z",
    "dateModified": "2020-11-03T17:45:22Z",
    "description": "## Problem\r\nI cannot load \"trec\" dataset, it results with ConnectionError as shown below. I've tried on both Google Colab and locally. \r\n* `requests.head('http://cogcomp.org/Data/QA/QC/train_5500.label')` returns <Response [302]>. \r\n* `requests.head('http://cogcomp.org/Data/QA/QC/train_5500.label', allow_redirects=True)` raises `requests.exceptions.TooManyRedirects: Exceeded 30 redirects.`\r\n* Opening `http://cogcomp.org/Data/QA/QC/train_5500.label' in a browser works, but opens a different address\r\n* Increasing max_redirects to 100 doesn't help\r\n\r\nAlso, while debugging I've seen that requesting 'https://storage.googleapis.com/huggingface-nlp/cache/datasets/trec/default/1.1.0/dataset_info.json' returns <Response [404]> before, but it doesn't raise any errors. Not sure if that's relevant.\r\n\r\n* datasets.__version__ == '1.1.2'\r\n* requests.__version__ == '2.24.0'\r\n\r\n## Error trace\r\n```\r\n>>> import datasets\r\n>>> datasets.__version__\r\n'1.1.2'\r\n>>> dataset = load_dataset(\"trec\", split=\"train\")\r\nUsing custom data configuration default\r\nDownloading and preparing dataset trec/default (download: 350.79 KiB, generated: 403.39 KiB, post-processed: Unknown size, total: 754.18 KiB) to /home/przemyslaw/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/builder.py\", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/builder.py\", line 531, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/przemyslaw/.cache/huggingface/modules/datasets_modules/datasets/trec/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/trec.py\", line 140, in _split_generators\r\n    dl_files = dl_manager.download_and_extract(_URLs)\r\n  File \"/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/download_manager.py\", line 254, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/download_manager.py\", line 179, in download\r\n    num_proc=download_config.num_proc,\r\n  File \"/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 225, in map_nested\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 225, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File \"/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 163, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File \"/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 475, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach http://cogcomp.org/Data/QA/QC/train_5500.label\r\n```\r\n\r\nI would appreciate some suggestions here. ",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 735420332,
    "title": "Token classification labels are strings and we don't have the list of labels",
    "dateCreated": "2020-11-03T15:33:30Z",
    "dateModified": "2020-11-03T15:33:30Z",
    "description": "Not sure if this is an issue we want to fix or not, putting it here so it's not forgotten. Right now, in token classification datasets, the labels for NER, POS and the likes are typed as `Sequence` of `strings`, which is wrong in my opinion. These should be `Sequence` of `ClassLabel` or some types that gives easy access to the underlying labels.\r\n\r\nThe main problem for preprocessing those datasets is that the list of possible labels is not stored inside the `Dataset` object which makes converting the labels to IDs quite difficult (you either have to know the list of labels in advance or run a full pass through the dataset to get the list of labels, the `unique` method being useless with the type `Sequence[str]`).",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 735414881,
    "title": "Seq2Seq Metrics QOL: Bleu, Rouge",
    "dateCreated": "2020-11-03T15:26:29Z",
    "dateModified": "2020-11-03T15:26:29Z",
    "description": "Putting all my QOL issues here, idt I will have time to propose fixes, but I didn't want these to be lost, in case they are useful. I tried using `rouge` and `bleu` for the first time and wrote down everything I didn't immediately understand:\r\n\r\n+ Bleu expects tokenization, can I just kwarg it like sacrebleu?\r\n+ different signatures, means that I would have had to add a lot of conditionals + pre and post processing: if I were going to replace the `calculate_rouge` and `calculate_bleu` functions here: https://github.com/huggingface/transformers/blob/master/examples/seq2seq/utils.py#L61\r\n\r\n\r\n#### What I tried\r\n\r\n\r\nRouge experience:\r\n```python\r\n\r\nrouge = load_metric('rouge')\r\nrouge.add_batch(['hi im sam'], ['im daniel']) # fails\r\nrouge.add_batch(predictions=['hi im sam'], references=['im daniel']) # works\r\nrouge.compute() # huge messy output, but reasonable. Not worth integrating b/c don't want to rewrite all the postprocessing.\r\n```\r\n\r\nBLEU experience:\r\n```python\r\nbleu = load_metric('bleu')\r\nbleu.add_batch(predictions=['hi im sam'], references=['im daniel'])\r\nbleu.add_batch(predictions=[['hi im sam']], references=[['im daniel']])\r\n\r\nbleu.add_batch(predictions=[['hi im sam']], references=[['im daniel']])\r\n```\r\nAll of these raise `ValueError: Got a string but expected a list instead: 'im daniel'`\r\n\r\n#### Doc Typo\r\nThis says `dataset=load_metric(...)` which seems wrong, will cause `NameError`\r\n\r\n![image](https://user-images.githubusercontent.com/6045025/98004483-ff0d0580-1dbd-11eb-9f35-6f35904611bb.png)\r\n\r\ncc @lhoestq, feel free to ignore.",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 735198265,
    "title": "Descriptions of raw and processed versions of wikitext are inverted",
    "dateCreated": "2020-11-03T10:24:51Z",
    "dateModified": "2020-11-03T10:24:51Z",
    "description": "Nothing of importance, but it looks like the descriptions of wikitext-n-v1 and wikitext-n-raw-v1 are inverted for both n=2 and n=103. I just verified by loading them and the `<unk>` tokens are present in the non-raw versions, which confirms that it's a mere inversion of the descriptions and not of the datasets themselves.\r\n\r\nAlso it would be nice if those descriptions appeared in the dataset explorer.\r\n\r\nhttps://github.com/huggingface/datasets/blob/87bd0864845ea0a1dd7167918dc5f341bf807bd3/datasets/wikitext/wikitext.py#L52",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 735158725,
    "title": "self.options cannot be converted to a Python object for pickling",
    "dateCreated": "2020-11-03T09:27:34Z",
    "dateModified": "2020-11-03T09:27:34Z",
    "description": "Hi,\r\n\r\nCurrently I am trying to load csv file with customized read_options. And the latest master seems broken if we pass the ReadOptions object.\r\n\r\nHere is a code snippet\r\n```python\r\nfrom datasets import load_dataset\r\nfrom pyarrow.csv import ReadOptions\r\nload_dataset(\"csv\", data_files=[\"out.csv\"], read_options=ReadOptions(block_size=16*1024*1024))\r\n```\r\nerror is `self.options cannot be converted to a Python object for pickling`\r\nWould you mind to take a look? Thanks!\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-28-ab83fec2ded4> in <module>\r\n----> 1 load_dataset(\"csv\", data_files=[\"out.csv\"], read_options=ReadOptions(block_size=16*1024*1024))\r\n\r\n/tmp/datasets/src/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)\r\n    602         hash=hash,\r\n    603         features=features,\r\n--> 604         **config_kwargs,\r\n    605     )\r\n    606 \r\n\r\n/tmp/datasets/src/datasets/builder.py in __init__(self, cache_dir, name, hash, features, **config_kwargs)\r\n    162             name,\r\n    163             custom_features=features,\r\n--> 164             **config_kwargs,\r\n    165         )\r\n    166 \r\n\r\n/tmp/datasets/src/datasets/builder.py in _create_builder_config(self, name, custom_features, **config_kwargs)\r\n    281                 )\r\n    282             else:\r\n--> 283                 suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\r\n    284 \r\n    285         if builder_config.data_files is not None:\r\n\r\n/tmp/datasets/src/datasets/fingerprint.py in hash(cls, value)\r\n     51             return cls.dispatch[type(value)](cls, value)\r\n     52         else:\r\n---> 53             return cls.hash_default(value)\r\n     54 \r\n     55     def update(self, value):\r\n\r\n/tmp/datasets/src/datasets/fingerprint.py in hash_default(cls, value)\r\n     44     @classmethod\r\n     45     def hash_default(cls, value):\r\n---> 46         return cls.hash_bytes(dumps(value))\r\n     47 \r\n     48     @classmethod\r\n\r\n/tmp/datasets/src/datasets/utils/py_utils.py in dumps(obj)\r\n    365     file = StringIO()\r\n    366     with _no_cache_fields(obj):\r\n--> 367         dump(obj, file)\r\n    368     return file.getvalue()\r\n    369 \r\n\r\n/tmp/datasets/src/datasets/utils/py_utils.py in dump(obj, file)\r\n    337 def dump(obj, file):\r\n    338     \"\"\"pickle an object to a file\"\"\"\r\n--> 339     Pickler(file, recurse=True).dump(obj)\r\n    340     return\r\n    341 \r\n\r\n~/.local/lib/python3.6/site-packages/dill/_dill.py in dump(self, obj)\r\n    444             raise PicklingError(msg)\r\n    445         else:\r\n--> 446             StockPickler.dump(self, obj)\r\n    447         stack.clear()  # clear record of 'recursion-sensitive' pickled objects\r\n    448         return\r\n\r\n/usr/lib/python3.6/pickle.py in dump(self, obj)\r\n    407         if self.proto >= 4:\r\n    408             self.framer.start_framing()\r\n--> 409         self.save(obj)\r\n    410         self.write(STOP)\r\n    411         self.framer.end_framing()\r\n\r\n/usr/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478 \r\n\r\n~/.local/lib/python3.6/site-packages/dill/_dill.py in save_module_dict(pickler, obj)\r\n    931             # we only care about session the first pass thru\r\n    932             pickler._session = False\r\n--> 933         StockPickler.save_dict(pickler, obj)\r\n    934         log.info(\"# D2\")\r\n    935     return\r\n\r\n/usr/lib/python3.6/pickle.py in save_dict(self, obj)\r\n    819 \r\n    820         self.memoize(obj)\r\n--> 821         self._batch_setitems(obj.items())\r\n    822 \r\n    823     dispatch[dict] = save_dict\r\n\r\n/usr/lib/python3.6/pickle.py in _batch_setitems(self, items)\r\n    850                 k, v = tmp[0]\r\n    851                 save(k)\r\n--> 852                 save(v)\r\n    853                 write(SETITEM)\r\n    854             # else tmp is empty, and we're done\r\n\r\n/usr/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    494             reduce = getattr(obj, \"__reduce_ex__\", None)\r\n    495             if reduce is not None:\r\n--> 496                 rv = reduce(self.proto)\r\n    497             else:\r\n    498                 reduce = getattr(obj, \"__reduce__\", None)\r\n\r\n~/.local/lib/python3.6/site-packages/pyarrow/_csv.cpython-36m-x86_64-linux-gnu.so in pyarrow._csv.ReadOptions.__reduce_cython__()\r\n\r\nTypeError: self.options cannot be converted to a Python object for pickling\r\n```",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 735105907,
    "title": "[Datasets] fix discofuse links",
    "dateCreated": "2020-11-03T08:03:45Z",
    "dateModified": "2020-11-03T08:03:45Z",
    "description": "The discofuse links were changed: https://github.com/google-research-datasets/discofuse/commit/d27641016eb5b3eb2af03c7415cfbb2cbebe8558. \r\nThe old links are broken\r\n\r\nI changed the links and created the new dataset_infos.json.\r\n\r\nPinging @thomwolf @lhoestq for notification.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 734693652,
    "title": "KILT dataset: empty string in triviaqa input field",
    "dateCreated": "2020-11-02T17:33:54Z",
    "dateModified": "2020-11-02T17:33:54Z",
    "description": "# What happened\r\nBoth train and test splits of the triviaqa dataset (part of the KILT benchmark) seem to have empty string in their input field (unlike the natural questions dataset, part of the same benchmark)\r\n\r\n# Versions\r\nKILT version is `1.0.0`\r\n`datasets` version is `1.1.2`\r\n[more here](https://gist.github.com/PaulLerner/3768c8d25f723edbac20d99b6a4056c1)\r\n\r\n# How to reproduce\r\n```py\r\nIn [1]: from datasets import load_dataset\r\nIn [4]: dataset = load_dataset(\"kilt_tasks\")                                                                                                                                                                \r\n# everything works fine, removed output for a better readibility\r\nDataset kilt_tasks downloaded and prepared to /people/lerner/.cache/huggingface/datasets/kilt_tasks/all_tasks/1.0.0/821c4295a2c35db2847585918d9c47d7f028f1a26b78825d8e77cd3aeb2621a1. Subsequent calls will reuse this data.\r\n\r\n# empty string in triviaqa input field\r\nIn [36]: dataset['train_triviaqa'][0]                                                                                                                                                                       \r\nOut[36]: \r\n{'id': 'dpql_5197',\r\n 'input': '',\r\n 'meta': {'left_context': '',\r\n  'mention': '',\r\n  'obj_surface': {'text': []},\r\n  'partial_evidence': {'end_paragraph_id': [],\r\n   'meta': [],\r\n   'section': [],\r\n   'start_paragraph_id': [],\r\n   'title': [],\r\n   'wikipedia_id': []},\r\n  'right_context': '',\r\n  'sub_surface': {'text': []},\r\n  'subj_aliases': {'text': []},\r\n  'template_questions': {'text': []}},\r\n 'output': {'answer': ['five  \u00a3', '5 \u00a3', '\u00a35', 'five \u00a3'],\r\n  'meta': [],\r\n  'provenance': [{'bleu_score': [1.0],\r\n    'end_character': [248],\r\n    'end_paragraph_id': [30],\r\n    'meta': [],\r\n    'section': ['Section::::Question of legal tender.\\n'],\r\n    'start_character': [246],\r\n    'start_paragraph_id': [30],\r\n    'title': ['Banknotes of the pound sterling'],\r\n    'wikipedia_id': ['270680']}]}}\r\nIn [35]: dataset['train_triviaqa']['input'][:10]                                                                                                                                                            \r\nOut[35]: ['', '', '', '', '', '', '', '', '', '']\r\n# same with test set \r\nIn [37]: dataset['test_triviaqa']['input'][:10]                                                                                                                                                             \r\nOut[37]: ['', '', '', '', '', '', '', '', '', '']\r\n# works fine with natural questions\r\nIn [34]: dataset['train_nq']['input'][:10]                                                                                                                                                                  \r\nOut[34]: \r\n['how i.met your mother who is the mother',\r\n 'who had the most wins in the nfl',\r\n 'who played mantis guardians of the galaxy 2',\r\n 'what channel is the premier league on in france',\r\n \"god's not dead a light in the darkness release date\",\r\n 'who is the current president of un general assembly',\r\n 'when do the eclipse supposed to take place',\r\n 'what is the name of the sea surrounding dubai',\r\n 'who holds the nba record for most points in a career',\r\n 'when did the new maze runner movie come out']\r\n```\r\n\r\nStay safe :)",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 734656518,
    "title": "add amazon reviews",
    "dateCreated": "2020-11-02T16:42:57Z",
    "dateModified": "2020-11-02T16:42:57Z",
    "description": "Adds the Amazon US Reviews dataset as requested in #353. Converted from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/amazon_us_reviews). cc @clmnt @sshleifer ",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 734470197,
    "title": "Error running pip install -e \".[dev]\" on MacOS 10.13.6: faiss/python does not exist",
    "dateCreated": "2020-11-02T12:36:35Z",
    "dateModified": "2020-11-02T12:36:35Z",
    "description": "I was following along with https://huggingface.co/docs/datasets/share_dataset.html#adding-tests-and-metadata-to-the-dataset when I ran into this error.\r\n\r\n```sh\r\ngit clone https://github.com/huggingface/datasets\r\ncd datasets\r\nvirtualenv venv -p python3 --system-site-packages\r\nsource venv/bin/activate\r\npip install -e \".[dev]\"\r\n```\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/59632/97868518-72871800-1cd5-11eb-9cd2-37d4e9d20b39.png)\r\n\r\n![image](https://user-images.githubusercontent.com/59632/97868592-977b8b00-1cd5-11eb-8f3c-0c409616149c.png)\r\n\r\nPython 3.7.7\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 734237839,
    "title": "dataset(ncslgr): add initial loading script",
    "dateCreated": "2020-11-02T06:50:10Z",
    "dateModified": "2020-11-02T06:50:10Z",
    "description": "Its a small dataset, but its heavily annotated\r\nhttps://www.bu.edu/asllrp/ncslgr.html\r\n\r\n![image](https://user-images.githubusercontent.com/5757359/97838609-3c539380-1ce9-11eb-885b-a15d4c91ea49.png)\r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 734136124,
    "title": "failed to reuse cache",
    "dateCreated": "2020-11-02T02:42:36Z",
    "dateModified": "2020-11-02T02:42:36Z",
    "description": "I packed the `load_dataset ` in a function of class, and cached data in a directory. But when I import the class and use the function, the data still have to be downloaded again. The information (Downloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.28 GiB, post-processed: Unknown size, total: 1.82 GiB) to ******) which logged to terminal shows the path is right to the cache directory, but the files still have to be downloaded again.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 734070162,
    "title": "Adding nli_tr dataset",
    "dateCreated": "2020-11-01T21:49:44Z",
    "dateModified": "2020-11-01T21:49:44Z",
    "description": "Hello,\r\n\r\nIn this pull request, we have implemented the necessary interface to add our recent dataset [NLI-TR](https://github.com/boun-tabi/NLI-TR).  The datasets will be presented on a full paper at EMNLP 2020 this month. [[arXiv link] ](https://arxiv.org/pdf/2004.14963.pdf)\r\n\r\nThe dataset is the neural machine translation of SNLI and MultiNLI datasets into Turkish.  So, we followed a similar format with the original datasets hosted in the HuggingFace datasets hub.  \r\n\r\nOur dataset is designed to be accessed as follows by following the interface of the GLUE dataset that provides multiple datasets in a single interface over the HuggingFace datasets hub.  \r\n\r\n```\r\nfrom datasets import load_dataset\r\nmultinli_tr = load_dataset(\"nli_tr\", \"multinli_tr\")\r\nsnli_tr = load_dataset(\"nli_tr\", \"snli_tr\")\r\n```\r\n\r\nThanks for your help in reviewing our pull request.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 733761717,
    "title": "feat(dataset): multiprocessing _generate_examples",
    "dateCreated": "2020-10-31T16:52:16Z",
    "dateModified": "2020-10-31T16:52:16Z",
    "description": "forking this out of #741, this issue is only regarding multiprocessing\r\n\r\nI'd love if there was a dataset configuration parameter `workers`, where when it is `1` it behaves as it does right now, and when its `>1` maybe `_generate_examples` can also get the `pool` and return an iterable using the pool.\r\n\r\nIn my use case, I would instead of:\r\n```python\r\nfor datum in data:\r\n     yield self.load_datum(datum)\r\n```\r\ndo:\r\n```python\r\nreturn pool.map(self.load_datum, data)\r\n```\r\n\r\nAs the dataset in question, as an example, has **only** 7000 rows, and takes 10 seconds to load each row on average, it takes almost 20 hours to load the entire dataset.\r\nIf this was a larger dataset (and many such datasets exist), it would take multiple days to complete.\r\n\r\nUsing multiprocessing, for example, 40 cores, could speed it up dramatically. For this dataset, hopefully to fully load in under an hour.",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 733719419,
    "title": "feat(aslg_pc12): add dev and test data splits",
    "dateCreated": "2020-10-31T13:25:38Z",
    "dateModified": "2020-10-31T13:25:38Z",
    "description": "For reproducibility sake, it's best if there are defined dev and test splits.\r\n\r\nThe original paper author did not define splits for the entire dataset, not for the sample loaded via this library, so I decided to define:\r\n- 5/7th for train\r\n- 1/7th for dev\r\n- 1/7th for test\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 733700463,
    "title": "Issue with downloading Wikipedia data for low resource language",
    "dateCreated": "2020-10-31T11:40:00Z",
    "dateModified": "2020-10-31T11:40:00Z",
    "description": "Hi, I tried to download Sundanese and Javanese wikipedia data with the following snippet\r\n```\r\njv_wiki = datasets.load_dataset('wikipedia', '20200501.jv', beam_runner='DirectRunner')\r\nsu_wiki = datasets.load_dataset('wikipedia', '20200501.su', beam_runner='DirectRunner')\r\n```\r\nAnd I get the following error for these two languages:\r\nJavanese\r\n```\r\nFileNotFoundError: Couldn't find file at https://dumps.wikimedia.org/jvwiki/20200501/dumpstatus.json\r\n```\r\n\r\nSundanese\r\n```\r\nFileNotFoundError: Couldn't find file at https://dumps.wikimedia.org/suwiki/20200501/dumpstatus.json\r\n```\r\n\r\nI found from https://github.com/huggingface/datasets/issues/577#issuecomment-688435085 that for small languages, they are directly downloaded and parsed from the Wikipedia dump site, but both of `https://dumps.wikimedia.org/jvwiki/20200501/dumpstatus.json` and `https://dumps.wikimedia.org/suwiki/20200501/dumpstatus.json` are no longer valid.\r\n\r\n Any suggestions on how to handle this issue? Thanks!",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 733536254,
    "title": "updated links to v1.3 of quail, fixed the description",
    "dateCreated": "2020-10-30T21:47:33Z",
    "dateModified": "2020-10-30T21:47:33Z",
    "description": "updated links to v1.3 of quail, fixed the description",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 733316463,
    "title": "Fix metric deletion when attribuets are missing",
    "dateCreated": "2020-10-30T16:16:10Z",
    "dateModified": "2020-10-30T16:16:10Z",
    "description": "When you call `del` on a metric we want to make sure that the arrow attributes are not already deleted.\r\nI just added `if hasattr(...)` to make sure it doesn't crash",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 733168609,
    "title": "Add XNLI train set",
    "dateCreated": "2020-10-30T13:21:53Z",
    "dateModified": "2020-10-30T13:21:53Z",
    "description": "I added the train set that was built using the translated MNLI.\r\nNow you can load the dataset specifying one language:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nxnli_en = load_dataset(\"xnli\", \"en\")\r\nprint(xnli_en[\"train\"][0])\r\n# {'hypothesis': 'Product and geography are what make cream skimming work .', 'label': 1, 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography .'}\r\nprint(xnli_en[\"test\"][0])                                                                                                                   \r\n# {'hypothesis': 'I havent spoken to him again.', 'label': 2, 'premise': \"Well, I wasn't even thinking about that, but I was so frustrated, and, I ended up talking to him again.\"}\r\n```\r\n\r\nCc @sgugger ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 732738647,
    "title": "Add ASNQ dataset",
    "dateCreated": "2020-10-29T23:31:56Z",
    "dateModified": "2020-10-29T23:31:56Z",
    "description": "This pull request adds the ASNQ dataset. It is a dataset for answer sentence selection derived from Google Natural Questions (NQ) dataset (Kwiatkowski et al. 2019). The dataset details can be found in the paper at https://arxiv.org/abs/1911.04118\r\n\r\nThe dataset is authored by Siddhant Garg, Thuy Vu and Alessandro Moschitti. \r\n\r\n_Please note that I have no affiliation with the authors._\r\n\r\nRepo: https://github.com/alexa/wqa_tanda\r\n\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 732514887,
    "title": "Feature/fidelity metrics from emnlp2020 evaluating and characterizing human rationales",
    "dateCreated": "2020-10-29T17:31:14Z",
    "dateModified": "2020-10-29T17:31:14Z",
    "description": "This metric computes fidelity (Yu et al. 2019, DeYoung et al. 2019) and normalized fidelity (Carton et al. 2020).",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 732449652,
    "title": "Unexpected behavior when loading cached csv file?",
    "dateCreated": "2020-10-29T16:06:10Z",
    "dateModified": "2020-10-29T16:06:10Z",
    "description": "I read a csv file from disk and forgot so specify the right delimiter. When i read the csv file again specifying the right delimiter it had no effect since it was using the cached dataset. I am not sure if this is unwanted behavior since i can always specify `download_mode=\"force_redownload\"`. But i think it would be nice if the information what `delimiter` or what `column_names` were used would influence the identifier of the cached dataset.\r\n\r\nSmall snippet to reproduce the behavior:\r\n```python\r\nimport datasets\r\n\r\nwith open(\"dummy_data.csv\", \"w\") as file:\r\n    file.write(\"test,this;text\\n\")\r\n\r\nprint(datasets.load_dataset(\"csv\", data_files=\"dummy_data.csv\", split=\"train\").column_names)\r\n# [\"test\", \"this;text\"]\r\n\r\nprint(datasets.load_dataset(\"csv\", data_files=\"dummy_data.csv\", split=\"train\", delimiter=\";\").column_names)\r\n# still [\"test\", \"this;text\"]\r\n```\r\n\r\nBy the way, thanks a lot for this amazing library! :)",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 732376648,
    "title": "Better error message for uninitialized metric",
    "dateCreated": "2020-10-29T14:42:50Z",
    "dateModified": "2020-10-29T14:42:50Z",
    "description": "When calling `metric.compute()` without having called `metric.add` or `metric.add_batch` at least once, the error was quite cryptic. I added a better error message\r\n\r\nFix #729 ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 732343550,
    "title": "Allow custom split names in text dataset",
    "dateCreated": "2020-10-29T14:04:06Z",
    "dateModified": "2020-10-29T14:04:06Z",
    "description": "The `text` dataset used to return only splits like train, test and validation. Other splits were ignored.\r\nNow any split name is allowed.\r\n\r\nI did the same for `json`, `pandas` and `csv`\r\n\r\nFix #735 ",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 732287504,
    "title": "Properly delete metrics when a process is killed",
    "dateCreated": "2020-10-29T12:52:07Z",
    "dateModified": "2020-10-29T12:52:07Z",
    "description": "Tests are flaky when using metrics in distributed setup.\r\nThere is because of one test that make sure that using two possibly incompatible metric computation (same exp id) either works or raises the right error.\r\nHowever if the error is raised, all the processes of the metric are killed, and the open files (arrow + lock files) are not closed correctly. This causes PermissionError on Windows when deleting the temporary directory.\r\n\r\nTo fix that I added a `finally` clause in the function passed to multiprocess to properly close the files when the process exits.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 732265741,
    "title": "[ROUGE] Add description to Rouge metric",
    "dateCreated": "2020-10-29T12:19:32Z",
    "dateModified": "2020-10-29T12:19:32Z",
    "description": "Add information about case sensitivity to ROUGE.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 731684153,
    "title": "Adding CC-100: Monolingual Datasets from Web Crawl Data",
    "dateCreated": "2020-10-28T18:20:41Z",
    "dateModified": "2020-10-28T18:20:41Z",
    "description": "## Adding a Dataset\r\n- **Name:** CC-100: Monolingual Datasets from Web Crawl Data\r\n- **Description:** https://twitter.com/alex_conneau/status/1321507120848625665\r\n- **Paper:** https://arxiv.org/abs/1911.02116\r\n- **Data:** http://data.statmt.org/cc-100/\r\n- **Motivation:** A large scale multi-lingual language modeling dataset. Text is de-duplicated and filtered by how \"Wikipedia-like\" it is, hopefully helping avoid some of the worst parts of the common crawl.\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 731612430,
    "title": "Fix metric with cache dir",
    "dateCreated": "2020-10-28T16:43:13Z",
    "dateModified": "2020-10-28T16:43:13Z",
    "description": "The cache_dir provided by the user was concatenated twice and therefore causing FileNotFound errors.\r\nThe tests didn't cover the case of providing `cache_dir=` for metrics because of a stupid issue (it was not using the right parameter).\r\n\r\nI remove the double concatenation and I fixed the tests\r\n\r\nFix #728 ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 731482213,
    "title": "Using `Dataset.map` with `n_proc>1` print multiple progress bars",
    "dateCreated": "2020-10-28T14:13:27Z",
    "dateModified": "2020-10-28T14:13:27Z",
    "description": "When using `Dataset.map` with `n_proc > 1`, only one of the processes should print a progress bar (to make the output readable). Right now, `n_proc` progress bars are printed.",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 731445222,
    "title": "Fix custom builder caching",
    "dateCreated": "2020-10-28T13:32:24Z",
    "dateModified": "2020-10-28T13:32:24Z",
    "description": "The cache directory of a dataset didn't take into account additional parameters that the user could specify such as `features` or any parameter of the builder configuration kwargs (ex: `encoding` for the `text` dataset).\r\n\r\nTo fix that, the cache directory name now has a suffix that depends on all of them.\r\n\r\nFix #730\r\nFix #750 ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 731257104,
    "title": "How to choose proper download_mode in function load_dataset?",
    "dateCreated": "2020-10-28T09:16:19Z",
    "dateModified": "2020-10-28T09:16:19Z",
    "description": "Hi, I am a beginner to datasets and I try to use datasets to load my csv file.\r\nmy csv file looks like this\r\n\r\n``` \r\ntext,label\r\n\"Effective but too-tepid biopic\",3\r\n\"If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .\",4\r\n\"Emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one .\",5\r\n```\r\n\r\nFirst I try to use this command to load my csv file .  \r\n\r\n``` python\r\ndataset=load_dataset('csv', data_files=['sst_test.csv'])\r\n```\r\n\r\nIt seems good, but when i try to overwrite the convert_options to convert  'label' columns from int64 to float32 like this.\r\n\r\n``` python\r\nimport pyarrow as pa\r\nfrom pyarrow import csv\r\nread_options = csv.ReadOptions(block_size=1024*1024)\r\nparse_options = csv.ParseOptions()\r\nconvert_options = csv.ConvertOptions(column_types={'text': pa.string(), 'label': pa.float32()})\r\ndataset = load_dataset('csv', data_files=['sst_test.csv'], read_options=read_options,\r\n                       parse_options=parse_options, convert_options=convert_options)\r\n```\r\n\r\nIt keeps the same:\r\n\r\n```shell\r\nDataset(features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}, num_rows: 2210)\r\n```\r\n\r\nI think this issue is caused by the parameter \"download_mode\" Default to REUSE_DATASET_IF_EXISTS because after I delete the cache_dir, it seems right.\r\n\r\nIs it a bug? How to choose proper download_mode to avoid this issue?\r\n",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 730908060,
    "title": "Add a `lazy_map` method to `Dataset` and `DatasetDict`",
    "dateCreated": "2020-10-27T22:33:03Z",
    "dateModified": "2020-10-27T22:33:03Z",
    "description": "The library is great, but it would be even more awesome with a `lazy_map` method implemented on `Dataset` and `DatasetDict`. This would apply a function on a give item but when the item is requested. Two use cases:\r\n\r\n1. load image on the fly\r\n2. apply a random function and get different outputs at each epoch (like data augmentation or randomly masking a part of a sentence for BERT-like objectives).",
    "status": "open",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 730771610,
    "title": "Add option for named splits when using ds.train_test_split",
    "dateCreated": "2020-10-27T19:59:44Z",
    "dateModified": "2020-10-27T19:59:44Z",
    "description": "### Feature Request \ud83d\ude80 \r\n\r\nCan we add a way to name your splits when using the `.train_test_split` function?\r\n\r\nIn almost every use case I've come across, I have a `train` and a `test` split in my `DatasetDict`, and I want to create a `validation` split. Therefore, its kinda useless to get a `test` split back from `train_test_split`, as it'll just overwrite my real `test` split that I intended to keep.\r\n\r\n### Workaround\r\n\r\nthis is my hack for dealin with this, for now :slightly_smiling_face:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\u200b\r\n\u200b\r\nds = load_dataset('imdb')\r\nds['train'], ds['validation'] = ds['train'].train_test_split(.1).values()\r\n```\r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 730669596,
    "title": "[GEM] add DART data-to-text generation dataset",
    "dateCreated": "2020-10-27T17:34:04Z",
    "dateModified": "2020-10-27T17:34:04Z",
    "description": "## Adding a Dataset\r\n- **Name:** DART\r\n- **Description:** DART consists of 82,191 examples across different domains with each input being a semantic RDF triple set derived from data records in tables and the tree ontology of the schema, annotated with sentence descriptions that cover all facts in the triple set.\r\n- **Paper:** https://arxiv.org/abs/2007.02871v1\r\n- **Data:** https://github.com/Yale-LILY/dart\r\n- **Motivation:** the dataset will likely be included in the GEM benchmark\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 730668332,
    "title": "[GEM] Add DART data-to-text generation dataset",
    "dateCreated": "2020-10-27T17:32:23Z",
    "dateModified": "2020-10-27T17:32:23Z",
    "description": "## Adding a Dataset\r\n- **Name:** DART\r\n- **Description:** DART consists of 82,191 examples across different domains with each input being a semantic RDF triple set derived from data records in tables and the tree ontology of the schema, annotated with sentence descriptions that cover all facts in the triple set.\r\n- **Paper:** https://arxiv.org/abs/2007.02871v1\r\n- **Data:** https://github.com/Yale-LILY/dart\r\n- **Motivation:** It will likely be included in the GEM generation evaluation benchmark\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 730617828,
    "title": "Adding Issue Template for Dataset Requests",
    "dateCreated": "2020-10-27T16:37:08Z",
    "dateModified": "2020-10-27T16:37:08Z",
    "description": "adding .github/ISSUE_TEMPLATE/add-dataset.md",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 730593631,
    "title": "Fixed errors in bertscore related to custom baseline",
    "dateCreated": "2020-10-27T16:08:35Z",
    "dateModified": "2020-10-27T16:08:35Z",
    "description": "[bertscore version 0.3.6 ](https://github.com/Tiiiger/bert_score) added support for custom baseline files. This update added extra argument `baseline_path` to BERTScorer class as well as some extra boolean parameters `use_custom_baseline` in functions like `get_hash(model, num_layers, idf, rescale_with_baseline, use_custom_baseline)`.\r\n\r\nThis PR fix those matching errors in bertscore metric implementation.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 730586972,
    "title": "[GEM] Add Czech Restaurant data-to-text generation dataset",
    "dateCreated": "2020-10-27T16:00:47Z",
    "dateModified": "2020-10-27T16:00:47Z",
    "description": "- Paper: https://www.aclweb.org/anthology/W19-8670.pdf\r\n- Data: https://github.com/UFAL-DSG/cs_restaurant_dataset\r\n- The dataset will likely be part of the GEM benchmark",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 729898867,
    "title": "Downloaded datasets are not usable offline",
    "dateCreated": "2020-10-26T20:54:46Z",
    "dateModified": "2020-10-26T20:54:46Z",
    "description": "I've been trying to use the IMDB dataset offline, but after downloading it and turning off the internet it still raises  an error from the ```requests``` library trying to reach for the online dataset.\r\nIs this the intended behavior ?\r\n(Sorry, I wrote the the first version of this issue while still on nlp 0.3.0).",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 729637917,
    "title": "Add meta-data to the HANS dataset",
    "dateCreated": "2020-10-26T14:56:53Z",
    "dateModified": "2020-10-26T14:56:53Z",
    "description": "The current version of the [HANS dataset](https://github.com/huggingface/datasets/blob/master/datasets/hans/hans.py) is missing the additional information provided for each example, including the sentence parses, heuristic and subcase.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 729046916,
    "title": "(Load dataset failure) ConnectionError: Couldn\u2019t reach https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/cnn_dailymail/cnn_dailymail.py",
    "dateCreated": "2020-10-25T15:34:57Z",
    "dateModified": "2020-10-25T15:34:57Z",
    "description": "Hey, I want to load the cnn-dailymail dataset for fine-tune.\r\nI write the code like this\r\nfrom datasets import load_dataset\r\n\r\ntest_dataset = load_dataset(\u201ccnn_dailymail\u201d, \u201c3.0.0\u201d, split=\u201ctrain\u201d)\r\n\r\nAnd I got the following errors.\r\n\r\nTraceback (most recent call last):\r\nFile \u201ctest.py\u201d, line 7, in\r\ntest_dataset = load_dataset(\u201ccnn_dailymail\u201d, \u201c3.0.0\u201d, split=\u201ctest\u201d)\r\nFile \u201cC:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\load.py\u201d, line 589, in load_dataset\r\nmodule_path, hash = prepare_module(\r\nFile \u201cC:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\load.py\u201d, line 268, in prepare_module\r\nlocal_path = cached_path(file_path, download_config=download_config)\r\nFile \u201cC:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\utils\\file_utils.py\u201d, line 300, in cached_path\r\noutput_path = get_from_cache(\r\nFile \u201cC:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\utils\\file_utils.py\u201d, line 475, in get_from_cache\r\nraise ConnectionError(\u201cCouldn\u2019t reach {}\u201d.format(url))\r\nConnectionError: Couldn\u2019t reach https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/cnn_dailymail/cnn_dailymail.py\r\n\r\nHow can I fix this ?",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 728638559,
    "title": "Process 0 very slow when using num_procs with map to tokenizer",
    "dateCreated": "2020-10-24T02:40:20Z",
    "dateModified": "2020-10-24T02:40:20Z",
    "description": "<img width=\"721\" alt=\"image\" src=\"https://user-images.githubusercontent.com/17930170/97066109-776d0d00-15ed-11eb-8bba-bb4d2e0fcc33.png\">\r\nThe code I am using is\r\n```\r\n\r\n        dataset = load_dataset(\"text\", data_files=[file_path], split='train')\r\n        dataset = dataset.map(lambda ex: tokenizer(ex[\"text\"], add_special_tokens=True,\r\n                                                truncation=True, max_length=args.block_size), num_proc=8)\r\n        dataset.set_format(type='torch', columns=['input_ids'])\r\n        dataset.save_to_disk(file_path+'.arrow')\r\n```\r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 728241494,
    "title": "CUDA out of memory",
    "dateCreated": "2020-10-23T13:57:00Z",
    "dateModified": "2020-10-23T13:57:00Z",
    "description": "In your dataset ,cuda run out of memory as long as the trainer begins:\r\nhowever, without changing any other element/parameter,just switch dataset to `LineByLineTextDataset`,everything becames OK.\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 728211373,
    "title": "Start community-provided dataset docs ",
    "dateCreated": "2020-10-23T13:17:41Z",
    "dateModified": "2020-10-23T13:17:41Z",
    "description": "Continuation of #736 with clean fork.\r\n\r\n#### Old description\r\nThis is what I did to get the pseudo-labels updated. Not sure if it generalizes, but I figured I would write it down. It was pretty easy because all I had to do was make properly formatted directories and change URLs.\r\n\r\nIn slack @thomwolf called it a user-namespace dataset, but the docs call it community dataset.\r\nI think the first naming is clearer, but I didn't address that here.\r\n\r\nI didn't add metadata, will try that.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 728203821,
    "title": "Start community-provided dataset docs V2",
    "dateCreated": "2020-10-23T13:07:30Z",
    "dateModified": "2020-10-23T13:07:30Z",
    "description": "",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 727863105,
    "title": "Use full released xsum dataset",
    "dateCreated": "2020-10-23T03:29:49Z",
    "dateModified": "2020-10-23T03:29:49Z",
    "description": "#672 Fix xsum to expand coverage and include IDs\r\nCode based on parser from older version of `datasets/xsum/xsum.py`\r\n@lhoestq ",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 727434935,
    "title": "Fix doc links to viewer",
    "dateCreated": "2020-10-22T14:20:16Z",
    "dateModified": "2020-10-22T14:20:16Z",
    "description": "It seems #733 forgot some links in the doc :)",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 726917801,
    "title": "Clicking on a metric in the search page points to datasets page giving \"Missing dataset\" warning",
    "dateCreated": "2020-10-21T22:56:23Z",
    "dateModified": "2020-10-21T22:56:23Z",
    "description": "Hi! Sorry if this isn't the right place to talk about the website, I just didn't exactly where to write this.\r\n\r\nSearching a metric in https://huggingface.co/metrics gives the right results but clicking on a metric (E.g ROUGE) points to https://huggingface.co/datasets/rouge. Clicking on a metric without searching points to the right page.\r\n\r\nThanks for all the great work!",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 726820191,
    "title": "Error loading ms_marco v2.1 using load_dataset()",
    "dateCreated": "2020-10-21T19:54:43Z",
    "dateModified": "2020-10-21T19:54:43Z",
    "description": "Code:\r\n`dataset = load_dataset('ms_marco', 'v2.1')`\r\n\r\nError:\r\n```\r\n`---------------------------------------------------------------------------\r\nJSONDecodeError                           Traceback (most recent call last)\r\n<ipython-input-16-34378c057212> in <module>()\r\n      9 \r\n     10 # Downloading and loading a dataset\r\n---> 11 dataset = load_dataset('ms_marco', 'v2.1')\r\n\r\n10 frames\r\n/usr/lib/python3.6/json/decoder.py in raw_decode(self, s, idx)\r\n    353         \"\"\"\r\n    354         try:\r\n--> 355             obj, end = self.scan_once(s, idx)\r\n    356         except StopIteration as err:\r\n    357             raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\n\r\nJSONDecodeError: Unterminated string starting at: line 1 column 388988661 (char 388988660)\r\n`\r\n```",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 726589446,
    "title": "load_dataset doesn't include `features` in its hash",
    "dateCreated": "2020-10-21T15:16:41Z",
    "dateModified": "2020-10-21T15:16:41Z",
    "description": "It looks like the function `load_dataset` does not include what's passed in the `features` argument when creating a hash for a given  dataset. As a result, if a user includes new features from an already downloaded dataset, those are ignored.\r\n\r\nExample: some models on the hub have a different ordering for the labels than what `datasets` uses for MNLI so I'd like to do something along the lines of:\r\n```\r\ndataset = load_dataset(\"glue\", \"mnli\")\r\nfeatures = dataset[\"train\"].features\r\nfeatures[\"label\"] = ClassLabel(names = ['entailment', 'contradiction', 'neutral'])  # new label order\r\ndataset = load_dataset(\"glue\", \"mnli\", features=features)\r\n```",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 726366062,
    "title": "[XGLUE] Adding new dataset",
    "dateCreated": "2020-10-21T10:51:36Z",
    "dateModified": "2020-10-21T10:51:36Z",
    "description": "XGLUE is a multilingual GLUE like dataset propesed in this [paper](https://arxiv.org/pdf/2004.01401.pdf).\r\n\r\nI'm planning on adding the dataset to the library myself in a couple of weeks.\r\nAlso tagging @JetRunner @qiweizhen in case I need some guidance ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 726196589,
    "title": "New version of CompGuessWhat?! with refined annotations",
    "dateCreated": "2020-10-21T06:55:41Z",
    "dateModified": "2020-10-21T06:55:41Z",
    "description": "This pull request introduces a few fixes to the annotations for VisualGenome in the CompGuessWhat?! original split.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 725884704,
    "title": "Add Quail question answering dataset",
    "dateCreated": "2020-10-20T19:33:14Z",
    "dateModified": "2020-10-20T19:33:14Z",
    "description": "QuAIL is a multi-domain RC dataset featuring news, blogs, fiction and user stories. Each domain is represented by 200 texts,    which gives us a 4-way data split. The texts are 300-350 word excerpts from CC-licensed texts that were hand-picked so as to make sense to human readers without larger context. Domain diversity mitigates the issue of possible overlap between training and test data of large pre-trained models, which the current SOTA systems are based on. For instance, BERT is trained on Wikipedia + BookCorpus, and was tested on Wikipedia-based SQuAD (Devlin, Chang, Lee, & Toutanova, 2019).\r\n\r\nhttps://text-machine-lab.github.io/blog/2020/quail/ @annargrs",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 725627235,
    "title": "dataset(ngt): add ngt dataset initial loading script",
    "dateCreated": "2020-10-20T14:04:58Z",
    "dateModified": "2020-10-20T14:04:58Z",
    "description": "Currently only making the paths to the annotation ELAN (eaf) file and videos available.\r\nThis is the first accessible way to download this dataset, which is not manual file-by-file.\r\n\r\nOnly downloading the necessary files, the annotation files are very small, 20MB for all of them, but the video files are large, 100GB in total, saved in `mpg` format. \r\nI do not intend to actually store these as an uncompressed array of frames, because it will be huge.\r\n\r\nFuture updates may add pose estimation files for all videos, making it easier to work with this data",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 725589352,
    "title": "Fix emotion description",
    "dateCreated": "2020-10-20T13:28:39Z",
    "dateModified": "2020-10-20T13:28:39Z",
    "description": "Fixes the description of the emotion dataset to reflect the class names observed in the data, not the ones described in the paper.\r\n\r\nI also took the liberty to make use of `ClassLabel` for the emotion labels.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 724918448,
    "title": "Dataset Explorer Doesn't Work for squad_es and squad_it",
    "dateCreated": "2020-10-19T19:34:12Z",
    "dateModified": "2020-10-19T19:34:12Z",
    "description": "https://huggingface.co/nlp/viewer/?dataset=squad_es\r\nhttps://huggingface.co/nlp/viewer/?dataset=squad_it\r\n\r\nBoth pages show \"OSError: [Errno 28] No space left on device\".",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 724703980,
    "title": "load_dataset for CSV files not working",
    "dateCreated": "2020-10-19T14:53:51Z",
    "dateModified": "2020-10-19T14:53:51Z",
    "description": "Similar to #622, I've noticed there is a problem when trying to load a CSV file with datasets.\r\n\r\n`\r\nfrom datasets import load_dataset\r\n`\r\n`\r\ndataset = load_dataset(\"csv\", data_files=[\"./sample_data.csv\"], delimiter=\"\\t\", column_names=[\"title\", \"text\"], script_version=\"master\")\r\n`\r\n\r\nDisplayed error:\r\n`\r\n...\r\nArrowInvalid: CSV parse error: Expected 2 columns, got 1\r\n`\r\n\r\nI should mention that when I've tried to read data from `https://github.com/lhoestq/transformers/tree/custom-dataset-in-rag-retriever/examples/rag/test_data/my_knowledge_dataset.csv` it worked without a problem. I've read that there might be some problems with /r character, so I've removed them from the custom dataset, but the problem still remains.\r\n\r\nI've added a colab reproducing the bug, but unfortunately I cannot provide the dataset.\r\nhttps://colab.research.google.com/drive/1Qzu7sC-frZVeniiWOwzoCe_UHZsrlxu8?usp=sharing\r\n\r\nAre there any work around for it ?\r\nThank you",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 724509974,
    "title": "Add OCNLI, a new CLUE dataset",
    "dateCreated": "2020-10-19T11:06:33Z",
    "dateModified": "2020-10-19T11:06:33Z",
    "description": "OCNLI stands for Original Chinese Natural Language Inference. It is a corpus for\r\n            Chinese Natural Language Inference, collected following closely the procedures of MNLI,\r\n            but with enhanced strategies aiming for more challenging inference pairs. We want to\r\n            emphasize we did not use human/machine translation in creating the dataset, and thus\r\n            our Chinese texts are original and not translated.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 723924275,
    "title": "Creating dataset consumes too much memory",
    "dateCreated": "2020-10-18T06:07:06Z",
    "dateModified": "2020-10-18T06:07:06Z",
    "description": "Moving this issue from https://github.com/huggingface/datasets/pull/722 here, because it seems like a general issue.\r\n\r\nGiven the following dataset example, where each example saves a sequence of 260x210x3 images (max length 400):\r\n```python\r\n    def _generate_examples(self, base_path, split):\r\n        \"\"\" Yields examples. \"\"\"\r\n\r\n        filepath = os.path.join(base_path, \"annotations\", \"manual\", \"PHOENIX-2014-T.\" + split + \".corpus.csv\")\r\n        images_path = os.path.join(base_path, \"features\", \"fullFrame-210x260px\", split)\r\n\r\n        with open(filepath, \"r\", encoding=\"utf-8\") as f:\r\n            data = csv.DictReader(f, delimiter=\"|\", quoting=csv.QUOTE_NONE)\r\n            for row in data:\r\n                frames_path = os.path.join(images_path, row[\"video\"])[:-7]\r\n                np_frames = []\r\n                for frame_name in os.listdir(frames_path):\r\n                    frame_path = os.path.join(frames_path, frame_name)\r\n                    im = Image.open(frame_path)\r\n                    np_frames.append(np.asarray(im))\r\n                    im.close()\r\n\r\n                yield row[\"name\"], {\"video\": np_frames}\r\n```\r\n\r\nThe dataset creation process goes out of memory on a machine with 500GB RAM.\r\nI was under the impression that the \"generator\" here is exactly for that, to avoid memory constraints.\r\n\r\n\r\nHowever, even if you want the entire dataset in memory, it would be in the worst case\r\n`260x210x3 x 400 max length x 7000 samples` in bytes (uint8) = 458.64 gigabytes\r\nSo I'm not sure why it's taking more than 500GB.\r\n\r\nAnd the dataset creation fails after 170 examples on a machine with 120gb RAM, and after 672 examples on a machine with 500GB RAM.\r\n\r\n\r\n---\r\n\r\n## Info that might help:\r\nIterating over examples is extremely slow.\r\n![image](https://user-images.githubusercontent.com/5757359/96359590-3c666780-111d-11eb-9347-1f833ad982a9.png)\r\nIf I perform this iteration in my own, custom loop (Without saving to file), it runs at 8-9 examples/sec\r\n\r\nAnd you can see at this state it is using 94% of the memory:\r\n![image](https://user-images.githubusercontent.com/5757359/96359606-7afc2200-111d-11eb-8c11-0afbdba1a6a3.png)\r\n\r\nAnd it is only using one CPU core, which is probably why it's so slow:\r\n![image](https://user-images.githubusercontent.com/5757359/96359630-a3841c00-111d-11eb-9ba0-7fd3cdf51d26.png)\r\n",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 723047958,
    "title": "Fix TREC urls",
    "dateCreated": "2020-10-16T09:11:28Z",
    "dateModified": "2020-10-16T09:11:28Z",
    "description": "The old TREC urls are now redirections.\r\nI updated the urls to the new ones, since we don't support redirections for downloads.\r\n\r\nFix #737 ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 723044066,
    "title": "Add wiki dpr multiset embeddings",
    "dateCreated": "2020-10-16T09:05:49Z",
    "dateModified": "2020-10-16T09:05:49Z",
    "description": "There are two DPR encoders, one trained on Natural Questions and one trained on a multiset/hybrid dataset.\r\nPreviously only the embeddings from the encoder trained on NQ were available. I'm adding the ones from the encoder trained on the multiset/hybrid dataset.\r\nIn the configuration you can now specify `embeddings_name=\"nq\"` or `embeddings_name=\"multiset\"`",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 723033923,
    "title": "Replace seqeval code with original classification_report for simplicity",
    "dateCreated": "2020-10-16T08:51:45Z",
    "dateModified": "2020-10-16T08:51:45Z",
    "description": "Recently, the original seqeval has enabled us to get per type scores and overall scores as a dictionary.\r\n\r\nThis PR replaces the current code with the original function(`classification_report`) to simplify it.\r\n\r\nAlso, the original code has been updated to fix #352.\r\n- Related issue: https://github.com/chakki-works/seqeval/pull/38\r\n\r\n\r\n```python\r\nfrom datasets import load_metric\r\nmetric = load_metric(\"seqeval\")\r\ny_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\r\ny_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\r\nmetric.compute(predictions=y_pred, references=y_true)\r\n# Output: {'MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0, 'number': 1}, 'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'overall_precision': 0.5, 'overall_recall': 0.5, 'overall_f1': 0.5, 'overall_accuracy': 0.8}\r\n```",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 722463923,
    "title": "Trec Dataset Connection Error",
    "dateCreated": "2020-10-15T15:57:53Z",
    "dateModified": "2020-10-15T15:57:53Z",
    "description": "**Datasets Version:**\r\n1.1.2\r\n\r\n**Python Version:**\r\n3.6/3.7\r\n\r\n\r\n**Code:**\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"trec\")\r\n```\r\n\r\n**Expected behavior:**\r\nDownload Trec dataset and load Dataset object\r\n\r\n**Current Behavior:**\r\nGet a connection error saying it couldn't reach http://cogcomp.org/Data/QA/QC/train_5500.label (but the link doesn't seem broken)\r\n\r\n<details>\r\n  <summary>Error Logs</summary>\r\n \r\n\r\nUsing custom data configuration default\r\nDownloading and preparing dataset trec/default (download: 350.79 KiB, generated: 403.39 KiB, post-processed: Unknown size, total: 754.18 KiB) to /root/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7...\r\n---------------------------------------------------------------------------\r\nConnectionError                           Traceback (most recent call last)\r\n<ipython-input-8-66bf1242096e> in <module>()\r\n----> 1 load_dataset(\"trec\")\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag)\r\n    473         elif response is not None and response.status_code == 404:\r\n    474             raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\n--> 475         raise ConnectionError(\"Couldn't reach {}\".format(url))\r\n    476 \r\n    477     # Try a second time\r\n\r\nConnectionError: Couldn't reach http://cogcomp.org/Data/QA/QC/train_5500.label\r\n\r\n</details>",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 722348191,
    "title": "Start community-provided dataset docs",
    "dateCreated": "2020-10-15T13:41:39Z",
    "dateModified": "2020-10-15T13:41:39Z",
    "description": "This is one I did to get the pseudo-labels updated. Not sure if it generalizes, but I figured I would write it down. It was pretty easy because all I had to do was make properly formatted directories and change URLs.\r\n\r\n+ In slack @thomwolf  called it a `user-namespace` dataset, but the docs call it `community dataset`.\r\nI think the first naming is clearer, but I didn't address that here.\r\n\r\n\r\n+ I didn't add metadata, will try that.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 722225270,
    "title": "Throw error when an unexpected key is used in data_files",
    "dateCreated": "2020-10-15T10:55:27Z",
    "dateModified": "2020-10-15T10:55:27Z",
    "description": "I have found that only \"train\", \"validation\" and \"test\" are valid keys in the `data_files` argument. When you use any other ones, those attached files are silently ignored - leading to unexpected behaviour for the users.\r\n\r\nSo the following, unintuitively, returns only one key (namely `train`).\r\n\r\n```python\r\ndatasets = load_dataset(\"text\", data_files={\"train\": train_f, \"valid\": valid_f})\r\nprint(datasets.keys())\r\n# dict_keys(['train'])\r\n```\r\n\r\nwhereas using `validation` instead, does return the expected result:\r\n\r\n```python\r\ndatasets = load_dataset(\"text\", data_files={\"train\": train_f, \"validation\": valid_f})\r\nprint(datasets.keys())\r\n# dict_keys(['train', 'validation'])\r\n```\r\n\r\nI would like to see more freedom in which keys one can use, but if that is not possible at least an error should be thrown when using an unexpected key.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 721767848,
    "title": "Fix GLUE metric description",
    "dateCreated": "2020-10-14T20:44:14Z",
    "dateModified": "2020-10-14T20:44:14Z",
    "description": "Small typo: the description says translation instead of prediction.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 721366744,
    "title": "Update link to dataset viewer",
    "dateCreated": "2020-10-14T11:13:23Z",
    "dateModified": "2020-10-14T11:13:23Z",
    "description": "Change 404 error links in quick tour to working ones",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 721359448,
    "title": "dataset(wlasl): initial loading script",
    "dateCreated": "2020-10-14T11:01:42Z",
    "dateModified": "2020-10-14T11:01:42Z",
    "description": "takes like 9-10 hours to download all of the videos for the dataset, but it does finish :) ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 721142985,
    "title": "dataset(aslg_pc12): initial loading script",
    "dateCreated": "2020-10-14T05:14:37Z",
    "dateModified": "2020-10-14T05:14:37Z",
    "description": "This contains the only current public part of this corpus.\r\n\r\nThe rest of the corpus is not yet been made public, but this sample is still being used by researchers.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 721073812,
    "title": "Possible caching bug",
    "dateCreated": "2020-10-14T02:02:34Z",
    "dateModified": "2020-10-14T02:02:34Z",
    "description": "The following code with `test1.txt` containing just \"\ud83e\udd17\ud83e\udd17\ud83e\udd17\":\r\n```\r\ndataset = datasets.load_dataset('text', data_files=['test1.txt'], split=\"train\", encoding=\"latin_1\")\r\nprint(dataset[0])\r\ndataset = datasets.load_dataset('text', data_files=['test1.txt'], split=\"train\", encoding=\"utf-8\")\r\nprint(dataset[0])\r\n``` \r\nproduces this output:\r\n```\r\nDownloading and preparing dataset text/default-15600e4d83254059 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155...\r\nDataset text downloaded and prepared to /home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155. Subsequent calls will reuse this data.\r\n{'text': '\u00f0\\x9f\u00a4\\x97\u00f0\\x9f\u00a4\\x97\u00f0\\x9f\u00a4\\x97'}\r\nUsing custom data configuration default\r\nReusing dataset text (/home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155)\r\n{'text': '\u00f0\\x9f\u00a4\\x97\u00f0\\x9f\u00a4\\x97\u00f0\\x9f\u00a4\\x97'}\r\n```\r\nJust changing the order (and deleting the temp files):\r\n```\r\ndataset = datasets.load_dataset('text', data_files=['test1.txt'], split=\"train\", encoding=\"utf-8\")\r\nprint(dataset[0])\r\ndataset = datasets.load_dataset('text', data_files=['test1.txt'], split=\"train\", encoding=\"latin_1\")\r\nprint(dataset[0])\r\n```\r\nproduces this:\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset text/default-15600e4d83254059 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155...\r\nDataset text downloaded and prepared to /home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155. Subsequent calls will reuse this data.\r\n{'text': '\ud83e\udd17\ud83e\udd17\ud83e\udd17'}\r\nUsing custom data configuration default\r\nReusing dataset text (/home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155)\r\n{'text': '\ud83e\udd17\ud83e\udd17\ud83e\udd17'}\r\n```\r\n\r\nIs it intended that the cache path does not depend on the config entries?\r\n\r\ntested with datasets==1.1.2 and python==3.8.5",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 719558876,
    "title": "Better error message when one forgets to call `add_batch` before `compute`",
    "dateCreated": "2020-10-12T17:59:22Z",
    "dateModified": "2020-10-12T17:59:22Z",
    "description": "When using metrics, if for some reason a user forgets to call `add_batch` to a metric before `compute` (with no arguments), the error message is a bit cryptic and could probably be made clearer.\r\n\r\n## Reproducer\r\n\r\n```python\r\nimport datasets\r\nimport torch\r\nfrom datasets import Metric\r\n\r\nclass GatherMetric(Metric):\r\n    def _info(self):\r\n        return datasets.MetricInfo(\r\n            description=\"description\",\r\n            citation=\"citation\",\r\n            inputs_description=\"kwargs\",\r\n            features=datasets.Features({\r\n                'predictions': datasets.Value('int64'),\r\n                'references': datasets.Value('int64'),\r\n            }),\r\n            codebase_urls=[],\r\n            reference_urls=[],\r\n            format='numpy'\r\n        )\r\n\r\n    def _compute(self, predictions, references):\r\n        return {\"predictions\": predictions, \"labels\": references}\r\n\r\nmetric = GatherMetric(cache_dir=\"test-metric\")\r\ninputs = torch.randint(0, 2, (1024,))\r\ntargets = torch.randint(0, 2, (1024,))\r\n\r\nbatch_size = 8\r\nfor i in range(0, 1024, batch_size):\r\n     pass # User forgets to call `add_batch`\r\nresult = metric.compute()\r\n```\r\n\r\n## Stack trace:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-13-267729d187fa> in <module>\r\n      3      pass\r\n      4     # metric.add_batch(predictions=inputs[i:i+batch_size], references=targets[i:i+batch_size])\r\n----> 5 result = metric.compute()\r\n\r\n~/git/datasets/src/datasets/metric.py in compute(self, *args, **kwargs)\r\n    380         if predictions is not None:\r\n    381             self.add_batch(predictions=predictions, references=references)\r\n--> 382         self._finalize()\r\n    383 \r\n    384         self.cache_file_name = None\r\n\r\n~/git/datasets/src/datasets/metric.py in _finalize(self)\r\n    343         elif self.process_id == 0:\r\n    344             # Let's acquire a lock on each node files to be sure they are finished writing\r\n--> 345             file_paths, filelocks = self._get_all_cache_files()\r\n    346 \r\n    347             # Read the predictions and references\r\n\r\n~/git/datasets/src/datasets/metric.py in _get_all_cache_files(self)\r\n    280         filelocks = []\r\n    281         for process_id, file_path in enumerate(file_paths):\r\n--> 282             filelock = FileLock(file_path + \".lock\")\r\n    283             try:\r\n    284                 filelock.acquire(timeout=self.timeout)\r\n\r\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\r\n```\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 719555780,
    "title": "Passing `cache_dir` to a metric does not work",
    "dateCreated": "2020-10-12T17:55:14Z",
    "dateModified": "2020-10-12T17:55:14Z",
    "description": "When passing `cache_dir` to a custom metric, the folder is concatenated to itself at some point and this results in a FileNotFoundError:\r\n\r\n## Reproducer\r\n\r\n```python\r\nimport datasets\r\nimport torch\r\nfrom datasets import Metric\r\n\r\nclass GatherMetric(Metric):\r\n    def _info(self):\r\n        return datasets.MetricInfo(\r\n            description=\"description\",\r\n            citation=\"citation\",\r\n            inputs_description=\"kwargs\",\r\n            features=datasets.Features({\r\n                'predictions': datasets.Value('int64'),\r\n                'references': datasets.Value('int64'),\r\n            }),\r\n            codebase_urls=[],\r\n            reference_urls=[],\r\n            format='numpy'\r\n        )\r\n\r\n    def _compute(self, predictions, references):\r\n        return {\"predictions\": predictions, \"labels\": references}\r\n\r\nmetric = GatherMetric(cache_dir=\"test-metric\")\r\ninputs = torch.randint(0, 2, (1024,))\r\ntargets = torch.randint(0, 2, (1024,))\r\n\r\nbatch_size = 8\r\nfor i in range(0, 1024, batch_size):\r\n    metric.add_batch(predictions=inputs[i:i+batch_size], references=targets[i:i+batch_size])\r\nresult = metric.compute()\r\n```\r\n\r\n## Stack trace:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n~/git/datasets/src/datasets/metric.py in _finalize(self)\r\n    349                 reader = ArrowReader(path=self.data_dir, info=DatasetInfo(features=self.features))\r\n--> 350                 self.data = Dataset(**reader.read_files([{\"filename\": f} for f in file_paths]))\r\n    351             except FileNotFoundError:\r\n\r\n~/git/datasets/src/datasets/arrow_reader.py in read_files(self, files, original_instructions)\r\n    227         # Prepend path to filename\r\n--> 228         pa_table = self._read_files(files)\r\n    229         files = copy.deepcopy(files)\r\n\r\n~/git/datasets/src/datasets/arrow_reader.py in _read_files(self, files)\r\n    166         for f_dict in files:\r\n--> 167             pa_table: pa.Table = self._get_dataset_from_filename(f_dict)\r\n    168             pa_tables.append(pa_table)\r\n\r\n~/git/datasets/src/datasets/arrow_reader.py in _get_dataset_from_filename(self, filename_skip_take)\r\n    291         )\r\n--> 292         mmap = pa.memory_map(filename)\r\n    293         f = pa.ipc.open_stream(mmap)\r\n\r\n~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.memory_map()\r\n\r\n~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.MemoryMappedFile._open()\r\n\r\n~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nFileNotFoundError: [Errno 2] Failed to open local file 'test-metric/gather_metric/default/test-metric/gather_metric/default/default_experiment-1-0.arrow'. Detail: [errno 2] No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-17-e42d43cc981f> in <module>\r\n      2 for i in range(0, 1024, batch_size):\r\n      3     metric.add_batch(predictions=inputs[i:i+batch_size], references=targets[i:i+batch_size])\r\n----> 4 result = metric.compute()\r\n\r\n~/git/datasets/src/datasets/metric.py in compute(self, *args, **kwargs)\r\n    380         if predictions is not None:\r\n    381             self.add_batch(predictions=predictions, references=references)\r\n--> 382         self._finalize()\r\n    383 \r\n    384         self.cache_file_name = None\r\n\r\n~/git/datasets/src/datasets/metric.py in _finalize(self)\r\n    351             except FileNotFoundError:\r\n    352                 raise ValueError(\r\n--> 353                     \"Error in finalize: another metric instance is already using the local cache file. \"\r\n    354                     \"Please specify an experiment_id to avoid colision between distributed metric instances.\"\r\n    355                 )\r\n\r\nValueError: Error in finalize: another metric instance is already using the local cache file. Please specify an experiment_id to avoid colision between distributed metric instances.\r\n```\r\n\r\nThe code works when we remove the `cache_dir=...` from the metric.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 719386366,
    "title": "Parallel downloads progress bar flickers",
    "dateCreated": "2020-10-12T13:36:05Z",
    "dateModified": "2020-10-12T13:36:05Z",
    "description": "When there are parallel downloads using the download manager, the tqdm progress bar flickers since all the progress bars are on the same line.\r\n\r\nTo fix that we could simply specify `position=i` for i=0 to n the number of files to download when instantiating the tqdm progress bar. \r\n\r\nAnother way would be to have one \"master\" progress bar that tracks the number of finished downloads, and then one progress bar per process that show the current downloads.",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 719313754,
    "title": "\"Checksums didn't match for dataset source files\" error while loading openwebtext dataset",
    "dateCreated": "2020-10-12T11:45:10Z",
    "dateModified": "2020-10-12T11:45:10Z",
    "description": "Hi,\r\nI have encountered this problem during loading the openwebtext dataset:\r\n```\r\n>>> dataset = load_dataset('openwebtext')\r\nDownloading and preparing dataset openwebtext/plain_text (download: 12.00 GiB, generated: 37.04 GiB, post-processed: Unknown size, total: 49.03 GiB) to /home/admin/.cache/huggingface/datasets/openwebtext/plain_text/1.0.0/5c636399c7155da97c982d0d70ecdce30fbca66a4eb4fc768ad91f8331edac02...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/admin/workspace/anaconda3/envs/torch1.6-py3.7/lib/python3.7/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/home/admin/workspace/anaconda3/envs/torch1.6-py3.7/lib/python3.7/site-packages/datasets/builder.py\", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/admin/workspace/anaconda3/envs/torch1.6-py3.7/lib/python3.7/site-packages/datasets/builder.py\", line 536, in _download_and_prepare\r\n    self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n  File \"/home/admin/workspace/anaconda3/envs/torch1.6-py3.7/lib/python3.7/site-packages/datasets/utils/info_utils.py\", line 39, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://zenodo.org/record/3834942/files/openwebtext.tar.xz']\r\n```\r\nI think this problem is caused because the released dataset has changed. Or I should download the dataset manually?\r\n\r\nSorry for release the unfinised issue by mistake.",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 718985641,
    "title": "pretty print dataset objects",
    "dateCreated": "2020-10-12T02:03:46Z",
    "dateModified": "2020-10-12T02:03:46Z",
    "description": "Currently, if I do:\r\n```\r\nfrom datasets import load_dataset\r\nload_dataset(\"wikihow\", 'all', data_dir=\"/hf/pegasus-datasets/wikihow/\")\r\n```\r\nI get:\r\n```\r\n\r\nDatasetDict({'train': Dataset(features: {'text': Value(dtype='string', id=None),\r\n'headline': Value(dtype='string', id=None), 'title': Value(dtype='string',\r\nid=None)}, num_rows: 157252), 'validation': Dataset(features: {'text':\r\nValue(dtype='string', id=None), 'headline': Value(dtype='string', id=None),\r\n'title': Value(dtype='string', id=None)}, num_rows: 5599), 'test':\r\nDataset(features: {'text': Value(dtype='string', id=None), 'headline':\r\nValue(dtype='string', id=None), 'title': Value(dtype='string', id=None)},\r\nnum_rows: 5577)})\r\n```\r\n\r\nThis is not very readable. \r\n\r\nCan we either have a better `__repr__` or have a custom method to nicely pprint the dataset object? \r\n\r\nHere is my very simple attempt. With this PR, it produces:\r\n```\r\nDatasetDict({\r\n  train:   Dataset({\r\n    features: ['text', 'headline', 'title'],\r\n    num_rows: 157252\r\n  })\r\n  validation:   Dataset({\r\n    features: ['text', 'headline', 'title'],\r\n    num_rows: 5599\r\n  })\r\n  test:   Dataset({\r\n    features: ['text', 'headline', 'title'],\r\n    num_rows: 5577\r\n  })\r\n})\r\n```\r\nI did omit the data types on purpose to make it more readable, but it shouldn't be too difficult to integrate those too.\r\n\r\nnote that this PR also fixes the inconsistency in output that in master misses enclosing `{}` for Dataset, but it is there for `DatasetDict` - or perhaps it was by design.\r\n\r\nI'm totally not attached to this format, just wanting something more readable. One approach could be to serialize to `json.dumps` or something similar. It'd make the indentation simpler.\r\n\r\nThank you.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 718947700,
    "title": "need to redirect /nlp to /datasets and remove outdated info",
    "dateCreated": "2020-10-11T23:12:12Z",
    "dateModified": "2020-10-11T23:12:12Z",
    "description": "It looks like the website still has all the `nlp` data, e.g.: https://huggingface.co/nlp/viewer/?dataset=wikihow&config=all\r\n\r\nshould probably redirect to: https://huggingface.co/datasets/wikihow\r\n\r\nalso for some reason the new information is slightly borked. If you look at the old one it was nicely formatted and had the links marked up, the new one is just a jumble of text in one chunk and no markup for links (i.e. not clickable).",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 718926723,
    "title": "Adding pseudo-labels to datasets",
    "dateCreated": "2020-10-11T21:05:45Z",
    "dateModified": "2020-10-11T21:05:45Z",
    "description": "I recently [uploaded pseudo-labels](https://github.com/huggingface/transformers/blob/master/examples/seq2seq/precomputed_pseudo_labels.md) for CNN/DM, XSUM and WMT16-en-ro to s3, and thom mentioned I should add them to this repo.\r\nSince pseudo-labels are just a large model's generations on an existing dataset, what is the right way to structure this contribution.\r\nI read https://huggingface.co/docs/datasets/add_dataset.html, but it doesn't really cover this type of contribution.\r\n\r\nI could, for example, make a new directory, `xsum_bart_pseudolabels` for each set of pseudolabels or add some sort of parametrization to `xsum.py`: https://github.com/huggingface/datasets/blob/5f4c6e830f603830117877b8990a0e65a2386aa6/datasets/xsum/xsum.py\r\n\r\nWhat do you think @lhoestq ?\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 718689117,
    "title": "datasets(RWTH-PHOENIX-Weather 2014 T): add initial loading script",
    "dateCreated": "2020-10-10T19:44:08Z",
    "dateModified": "2020-10-10T19:44:08Z",
    "description": "This is the first sign language dataset in this repo as far as I know.\r\nFollowing an old issue I opened https://github.com/huggingface/datasets/issues/302.\r\n\r\nI added the dataset official REAMDE file, but I see it's not very standard, so it can be removed.\r\n\r\n",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 718647147,
    "title": "feat(dl_manager): add support for ftp downloads",
    "dateCreated": "2020-10-10T15:50:20Z",
    "dateModified": "2020-10-10T15:50:20Z",
    "description": "I am working on a new dataset (#302) and encounter a problem downloading it.\r\n\r\n```python\r\n# This is the official download link from https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX-2014-T/\r\n_URL = \"ftp://wasserstoff.informatik.rwth-aachen.de/pub/rwth-phoenix/2016/phoenix-2014-T.v3.tar.gz\"\r\n\r\ndl_manager.download_and_extract(_URL)\r\n```\r\n\r\nI get an error:\r\n\r\n> ValueError: unable to parse ftp://wasserstoff.informatik.rwth-aachen.de/pub/rwth-phoenix/2016/phoenix-2014-T.v3.tar.gz as a URL or as a local path\r\n\r\nI checked, and indeed you don't consider `ftp` as a remote file.\r\nhttps://github.com/huggingface/datasets/blob/4c2af707a6955cf4b45f83ac67990395327c5725/src/datasets/utils/file_utils.py#L188\r\n\r\nAdding `ftp` to that list does not immediately solve the issue, so there probably needs to be some extra work.\r\n\r\n\r\n\r\n",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 716581266,
    "title": "OSError: Cannot find data file when not using the dummy dataset in RAG",
    "dateCreated": "2020-10-07T14:27:13Z",
    "dateModified": "2020-10-07T14:27:13Z",
    "description": "## Environment info\r\n\r\n    transformers version: 3.3.1\r\n    Platform: Linux-4.19\r\n    Python version: 3.7.7\r\n    PyTorch version (GPU?): 1.6.0\r\n    Tensorflow version (GPU?): No\r\n    Using GPU in script?: Yes\r\n    Using distributed or parallel set-up in script?: No\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behaviour:\r\n```\r\nimport os\r\nos.environ['HF_DATASETS_CACHE'] = '/workspace/notebooks/POCs/cache'\r\n\r\nfrom transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\r\n\r\ntokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\r\nretriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=False) \r\n```\r\n\r\nPlese note that I'm using the whole dataset: **use_dummy_dataset=False**\r\nAfter around 4 hours (downloading and some other things) this is returned:\r\n\r\n```\r\nDownloading and preparing dataset wiki_dpr/psgs_w100.nq.exact (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /workspace/notebooks/POCs/cache/wiki_dpr/psgs_w100.nq.exact/0.0.0/14b973bf2a456087ff69c0fd34526684eed22e48e0dfce4338f9a22b965ce7c2...\r\n\r\n---------------------------------------------------------------------------\r\nUnpicklingError                           Traceback (most recent call last)\r\n/opt/conda/lib/python3.7/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding)\r\n    459             try:\r\n--> 460                 return pickle.load(fid, **pickle_kwargs)\r\n    461             except Exception:\r\n\r\nUnpicklingError: pickle data was truncated\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nOSError                                   Traceback (most recent call last)\r\n/opt/conda/lib/python3.7/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    552                 # Prepare split will record examples associated to the split\r\n--> 553                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    554             except OSError:\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/builder.py in _prepare_split(self, split_generator)\r\n    840             for key, record in utils.tqdm(\r\n--> 841                 generator, unit=\" examples\", total=split_info.num_examples, leave=False, disable=not_verbose\r\n    842             ):\r\n\r\n/opt/conda/lib/python3.7/site-packages/tqdm/notebook.py in __iter__(self, *args, **kwargs)\r\n    217         try:\r\n--> 218             for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):\r\n    219                 # return super(tqdm...) will not catch exception\r\n\r\n/opt/conda/lib/python3.7/site-packages/tqdm/std.py in __iter__(self)\r\n   1128         try:\r\n-> 1129             for obj in iterable:\r\n   1130                 yield obj\r\n\r\n~/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/14b973bf2a456087ff69c0fd34526684eed22e48e0dfce4338f9a22b965ce7c2/wiki_dpr.py in _generate_examples(self, data_file, vectors_files)\r\n    131                         break\r\n--> 132                     vecs = np.load(open(vectors_files.pop(0), \"rb\"), allow_pickle=True)\r\n    133                     vec_idx = 0\r\n\r\n/opt/conda/lib/python3.7/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding)\r\n    462                 raise IOError(\r\n--> 463                     \"Failed to interpret file %s as a pickle\" % repr(file))\r\n    464     finally:\r\n\r\nOSError: Failed to interpret file <_io.BufferedReader name='/workspace/notebooks/POCs/cache/downloads/f34d5f091294259b4ca90e813631e69a6ded660d71b6cbedf89ddba50df94448'> as a pickle\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-10-f28df370ac47> in <module>\r\n      1 # ln -s /workspace/notebooks/POCs/cache /root/.cache/huggingface/datasets\r\n----> 2 retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=False)\r\n\r\n/opt/conda/lib/python3.7/site-packages/transformers/retrieval_rag.py in from_pretrained(cls, retriever_name_or_path, **kwargs)\r\n    307         generator_tokenizer = rag_tokenizer.generator\r\n    308         return cls(\r\n--> 309             config, question_encoder_tokenizer=question_encoder_tokenizer, generator_tokenizer=generator_tokenizer\r\n    310         )\r\n    311 \r\n\r\n/opt/conda/lib/python3.7/site-packages/transformers/retrieval_rag.py in __init__(self, config, question_encoder_tokenizer, generator_tokenizer)\r\n    298         self.config = config\r\n    299         if self._init_retrieval:\r\n--> 300             self.init_retrieval()\r\n    301 \r\n    302     @classmethod\r\n\r\n/opt/conda/lib/python3.7/site-packages/transformers/retrieval_rag.py in init_retrieval(self)\r\n    324 \r\n    325         logger.info(\"initializing retrieval\")\r\n--> 326         self.index.init_index()\r\n    327 \r\n    328     def postprocess_docs(self, docs, input_strings, prefix, n_docs, return_tensors=None):\r\n\r\n/opt/conda/lib/python3.7/site-packages/transformers/retrieval_rag.py in init_index(self)\r\n    238                 split=self.dataset_split,\r\n    239                 index_name=self.index_name,\r\n--> 240                 dummy=self.use_dummy_dataset,\r\n    241             )\r\n    242             self.dataset.set_format(\"numpy\", columns=[\"embeddings\"], output_all_columns=True)\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)\r\n    609         download_config=download_config,\r\n    610         download_mode=download_mode,\r\n--> 611         ignore_verifications=ignore_verifications,\r\n    612     )\r\n    613 \r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    474                     if not downloaded_from_gcs:\r\n    475                         self._download_and_prepare(\r\n--> 476                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    477                         )\r\n    478                     # Sync info\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    553                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    554             except OSError:\r\n--> 555                 raise OSError(\"Cannot find data file. \" + (self.manual_download_instructions or \"\"))\r\n    556 \r\n    557         if verify_infos:\r\n\r\nOSError: Cannot find data file. \r\n\r\n```\r\n\r\nThanks \r\n",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 716492263,
    "title": "Fix train_test_split output format",
    "dateCreated": "2020-10-07T12:39:01Z",
    "dateModified": "2020-10-07T12:39:01Z",
    "description": "There was an issue in the `transmit_format` wrapper that returned bad formats when using train_test_split.\r\nThis was due to `column_names` being handled as a List[str] instead of Dict[str, List[str]] when the dataset transform (train_test_split) returns a DatasetDict (one set of column names per split).\r\n\r\nThis should fix @timothyjlaurent 's issue in #620 and fix #676 \r\n\r\nI added tests for `transmit_format` so that it doesn't happen again",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 715694709,
    "title": "Don't use tqdm 4.50.0",
    "dateCreated": "2020-10-06T13:45:53Z",
    "dateModified": "2020-10-06T13:45:53Z",
    "description": "tqdm 4.50.0 introduced permission errors on windows\r\nsee [here](https://app.circleci.com/pipelines/github/huggingface/datasets/235/workflows/cfb6a39f-68eb-4802-8b17-2cd5e8ea7369/jobs/1111) for the error details.\r\n\r\nFor now I just added `<4.50.0` in the setup.py\r\nHopefully we can find what's wrong with this version soon",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 714959268,
    "title": "Fixes #712 Error in the Overview.ipynb notebook",
    "dateCreated": "2020-10-05T15:50:41Z",
    "dateModified": "2020-10-05T15:50:41Z",
    "description": "Fixes #712 Error in the Overview.ipynb notebook by adding `with_details=True` parameter to `list_datasets` function in Cell 3 of **overview** notebook",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 714952888,
    "title": "Fixes #712 Attribute error in cell 3 of the overview notebook",
    "dateCreated": "2020-10-05T15:42:09Z",
    "dateModified": "2020-10-05T15:42:09Z",
    "description": "Fixes the Attribute error in cell 3 of the overview notebook",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 714690192,
    "title": "Use python read for text dataset",
    "dateCreated": "2020-10-05T09:47:55Z",
    "dateModified": "2020-10-05T09:47:55Z",
    "description": "As mentioned in #622 the pandas reader used for text dataset doesn't work properly when there are \\r characters in the text file.\r\n\r\nInstead I switched to pure python using `open` and `read`.\r\nFrom my benchmark on a 100MB text file, it's the same speed as the previous pandas reader.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 714487881,
    "title": "Add the official dependabot implementation",
    "dateCreated": "2020-10-05T03:49:45Z",
    "dateModified": "2020-10-05T03:49:45Z",
    "description": "This will keep dependencies up to date. This will require a pr label `dependencies` being created in order to function correctly.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 714475732,
    "title": "Fix reading text files with carriage return symbols",
    "dateCreated": "2020-10-05T03:07:03Z",
    "dateModified": "2020-10-05T03:07:03Z",
    "description": "The new pandas-based text reader isn't able to work properly with files that contain carriage return symbols (`\\r`). \r\n\r\nIt fails with the following error message:\r\n\r\n```\r\n...\r\n  File \"pandas/_libs/parsers.pyx\", line 847, in pandas._libs.parsers.TextReader.read\r\n  File \"pandas/_libs/parsers.pyx\", line 874, in pandas._libs.parsers.TextReader._read_low_memory\r\n  File \"pandas/_libs/parsers.pyx\", line 918, in pandas._libs.parsers.TextReader._read_rows\r\n  File \"pandas/_libs/parsers.pyx\", line 905, in pandas._libs.parsers.TextReader._tokenize_rows\r\n  File \"pandas/_libs/parsers.pyx\", line 2042, in pandas._libs.parsers.raise_parser_error\r\npandas.errors.ParserError: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\r\n```\r\n\r\n___\r\nI figured out the pandas uses those symbols as line terminators and this eventually causes the error. Explicitly specifying the `lineterminator` fixes that issue and everything works fine. \r\n\r\nPlease, consider this PR as it seems to be a common issue to solve.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 714242316,
    "title": "Error in the notebooks/Overview.ipynb notebook",
    "dateCreated": "2020-10-04T05:58:31Z",
    "dateModified": "2020-10-04T05:58:31Z",
    "description": "Hi,\r\n\r\nI got the following error in **cell number 3** while exploring the **Overview.ipynb** notebook in google colab. I used the [link ](https://colab.research.google.com/github/huggingface/datasets/blob/master/notebooks/Overview.ipynb) provided in the main README file to open it in colab. \r\n\r\n```python\r\n# You can access various attributes of the datasets before downloading them\r\nsquad_dataset = list_datasets()[datasets.index('squad')]\r\n\r\npprint(squad_dataset.__dict__)  # It's a simple python dataclass\r\n```\r\n\r\nError message\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-5-8dc805c4949c> in <module>()\r\n      2 squad_dataset = list_datasets()[datasets.index('squad')]\r\n      3 \r\n ----> 4 pprint(squad_dataset.__dict__)  # It's a simple python dataclass\r\n    \r\nAttributeError: 'str' object has no attribute '__dict__'\r\n```\r\n\r\nThe object `squad_dataset` is a `str` not a `dataclass` .",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 714186999,
    "title": "fix README typos/ consistency",
    "dateCreated": "2020-10-03T22:20:56Z",
    "dateModified": "2020-10-03T22:20:56Z",
    "description": "",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 714067902,
    "title": "How to use similarity settings  other then \"BM25\" in Elasticsearch index ?",
    "dateCreated": "2020-10-03T11:18:49Z",
    "dateModified": "2020-10-03T11:18:49Z",
    "description": "**QUESTION : How should we use other similarity algorithms supported by Elasticsearch other than \"BM25\"  ?**\r\n**ES Reference**\r\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html\r\n**HF doc reference:**\r\nhttps://huggingface.co/docs/datasets/faiss_and_ea.html\r\n\r\n**context :**\r\n========\r\n\r\nI used the latest Elasticsearch server  version 7.9.2\r\nWhen I set DFR  which is one of the other similarity algorithms supported by elasticsearch  in the mapping, I get an error\r\n\r\nFor example DFR that I had tried in the first instance in mappings as below.,\r\n`\"mappings\": {\"properties\": {\"text\": {\"type\": \"text\", \"analyzer\": \"standard\", \"similarity\": \"DFR\"}}},`\r\n\r\nI get the following error \r\nRequestError: RequestError(400, 'mapper_parsing_exception', 'Unknown Similarity type [DFR] for field [text]')\r\n\r\nThe other thing as another option I had tried was to declare \"similarity\": \"my_similarity\" within settings and then assigning \"my_similarity\" inside the mappings as below \r\n\r\n`es_config = {\r\n        \"settings\": {\r\n            \"number_of_shards\": 1,\r\n             **\"similarity\":  \"my_similarity\"**: {\r\n          \"type\": \"DFR\",\r\n          \"basic_model\": \"g\",\r\n          \"after_effect\": \"l\",\r\n          \"normalization\": \"h2\",\r\n          \"normalization.h2.c\": \"3.0\"\r\n        } ,\r\n            \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\r\n            \r\n        },\r\n        \"mappings\": {\"properties\": {\"text\": {\"type\": \"text\", \"analyzer\": \"standard\", \"similarity\": \"my_similarity\"}}},\r\n    }`\r\n\r\nFor this , I got the following error\r\nRequestError: RequestError(400, 'illegal_argument_exception', 'unknown setting [index.similarity] please check that any required plugins are installed, or check the breaking changes documentation for removed settings')\r\n\r\n",
    "status": "open",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 714020953,
    "title": "Datasets performance slow? - 6.4x slower than in memory dataset",
    "dateCreated": "2020-10-03T06:44:07Z",
    "dateModified": "2020-10-03T06:44:07Z",
    "description": "I've been very excited about this amazing datasets project. However, I've noticed that the performance can be substantially slower than using an in-memory dataset.\r\n\r\nNow, this is expected I guess, due to memory mapping data using arrow files, and you don't get anything for free. But I was surprised at how much slower.\r\n\r\nFor example, in the `yelp_polarity` dataset (560000 datapoints, or 17500 batches of 32), it was taking me 3:31 to just get process the data and get it on the GPU (no model involved). Whereas, the equivalent in-memory dataset would finish in just 0:33.\r\n\r\nIs this expected? Given that one of the goals of this project is also accelerate dataset processing, this seems a bit slower than I would expect. I understand the advantages of being able to work on datasets that exceed memory, and that's very exciting to me, but thought I'd open this issue to discuss.\r\n\r\nFor reference I'm running a AMD Ryzen Threadripper 1900X 8-Core Processor CPU, with 128 GB of RAM and an NVME SSD Samsung 960 EVO. I'm running with an RTX Titan 24GB GPU.\r\n\r\nI can see with `iotop` that the dataset gets quickly loaded into the system read buffers, and thus doesn't incur any additional IO reads. Thus in theory, all the data *should* be in RAM, but in my benchmark code below it's still 6.4 times slower.\r\n\r\nWhat am I doing wrong? And is there a way to force the datasets to completely load into memory instead of being memory mapped in cases where you want maximum performance?\r\n\r\nAt 3:31 for 17500 batches, that's 12ms per batch. Does this 12ms just become insignificant as a proportion of forward and backward passes in practice, and thus it's not worth worrying about this in practice?\r\n\r\nIn any case, here's my code `benchmark.py`. If you run it with an argument of `memory` it will copy the data into memory before executing the same test.\r\n\r\n``` py\r\nimport sys\r\nfrom datasets import load_dataset\r\nfrom transformers import DataCollatorWithPadding, BertTokenizerFast\r\nfrom torch.utils.data import DataLoader\r\nfrom tqdm import tqdm\r\n\r\nif __name__ == '__main__':\r\n    tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\r\n    collate_fn = DataCollatorWithPadding(tokenizer, padding=True)\r\n\r\n    ds = load_dataset('yelp_polarity')\r\n\r\n    def do_tokenize(x):\r\n        return tokenizer(x['text'], truncation=True)\r\n\r\n    ds = ds.map(do_tokenize, batched=True)\r\n    ds.set_format('torch', ['input_ids', 'token_type_ids', 'attention_mask'])\r\n\r\n    if len(sys.argv) == 2 and sys.argv[1] == 'memory':\r\n        # copy to memory - probably a faster way to do this - but demonstrates the point\r\n        # approximately 530 batches per second - 17500 batches in 0:33\r\n        print('using memory')\r\n        _ds = [data for data in tqdm(ds['train'])]\r\n    else:\r\n        # approximately 83 batches per second - 17500 batches in 3:31\r\n        print('using datasets')\r\n        _ds = ds['train']\r\n\r\n    dl = DataLoader(_ds, shuffle=True, collate_fn=collate_fn, batch_size=32, num_workers=4)\r\n\r\n    for data in tqdm(dl):\r\n        for k, v in data.items():\r\n            data[k] = v.to('cuda')\r\n```\r\n\r\nFor reference, my conda environment is [here](https://gist.github.com/05b6101518ff70ed42a858b302a0405d)\r\n\r\nOnce again, I'm very excited about this library, and how easy it is to load datasets, and to do so without worrying about system memory constraints.\r\n\r\nThanks for all your great work.\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 713954666,
    "title": "Requirements should specify pyarrow<1",
    "dateCreated": "2020-10-02T23:39:39Z",
    "dateModified": "2020-10-02T23:39:39Z",
    "description": "I was looking at the docs on [Perplexity](https://huggingface.co/transformers/perplexity.html) via GPT2. When you load datasets and try to load Wikitext, you get the error,\r\n\r\n```\r\nmodule 'pyarrow' has no attribute 'PyExtensionType'\r\n```\r\nI traced it back to datasets having installed PyArrow 1.0.1 but there's not pinning in the setup file. \r\n\r\nhttps://github.com/huggingface/datasets/blob/e86a2a8f869b91654e782c9133d810bb82783200/setup.py#L68\r\n\r\nDowngrading by installing `pip install \"pyarrow<1\"` resolved the issue.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 713721959,
    "title": "Fix config creation for data files with NamedSplit",
    "dateCreated": "2020-10-02T15:46:49Z",
    "dateModified": "2020-10-02T15:46:49Z",
    "description": "During config creation, we need to iterate through the data files of all the splits to compute a hash.\r\nTo make sure the hash is unique given a certain combination of files/splits, we sort the split names.\r\nHowever the `NamedSplit` objects can't be passed to `sorted` and currently it raises an error: we need to sort the string of their names instead.\r\n\r\nFix #705 ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 713709100,
    "title": "TypeError: '<' not supported between instances of 'NamedSplit' and 'NamedSplit'",
    "dateCreated": "2020-10-02T15:27:55Z",
    "dateModified": "2020-10-02T15:27:55Z",
    "description": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.3.1 (installed from master)\r\n- `datasets` version: 1.0.2 (installed as a dependency from transformers)\r\n- Platform: Linux-4.15.0-118-generic-x86_64-with-debian-stretch-sid\r\n- Python version: 3.7.9\r\n\r\nI'm testing my own text classification dataset using [this example](https://github.com/huggingface/transformers/tree/master/examples/text-classification#run-generic-text-classification-script-in-tensorflow) from transformers. The dataset is split into train / dev / test, and in csv format, containing just a text and a label columns, using comma as sep. Here's a sample:\r\n```\r\ntext,label\r\n\"Registra-se a presen\u00e7a do acad\u00eamico <name> . <REL_SEP> Ao me deparar com a descri\u00e7\u00e3o de dois autores no polo ativo da a\u00e7\u00e3o junto ao PJe , margem esquerda foi informado pela procuradora do reclamante que se trata de uma reclama\u00e7\u00e3o trabalhista individual . <REL_SEP> Diante disso , face a aus\u00eancia injustificada do autor <name> , determina-se o ARQUIVAMENTO do presente processo , com rela\u00e7\u00e3o a este , nos termos do [[ art . 844 da CLT ]] . <REL_SEP> CUSTAS AUTOR - DISPENSADO <REL_SEP> Custas pelo autor no importe de R $326,82 , calculadas sobre R $16.341,03 , dispensadas na forma da lei , em virtude da concess\u00e3o dos benef\u00edcios da Justi\u00e7a Gratuita , ora deferida . <REL_SEP> Cientes os presentes . <REL_SEP> Audi\u00eancia encerrada \u00e0s 8h42min . <REL_SEP> <name> <REL_SEP> Ju\u00edza do Trabalho <REL_SEP> Ata redigida por << <name> >> , Secret\u00e1rio de Audi\u00eancia .\",NO_RELATION\r\n```\r\n\r\nHowever, @Santosh-Gupta reported in #7351 that he had the exact same problem using the ChemProt dataset. His colab notebook is referenced in the following section.\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Created a new conda environment using conda env -n transformers python=3.7\r\n2. Cloned transformers master, `cd` into it and installed using pip install --editable .  -r examples/requirements.txt \r\n3. Installed tensorflow with `pip install tensorflow`\r\n3. Ran `run_tf_text_classification.py` with the following parameters:\r\n\r\n```\r\n--train_file <DATASET_PATH>/train.csv \\\r\n--dev_file <DATASET_PATH>/dev.csv \\ \r\n--test_file <DATASET_PATH>/test.csv \\\r\n--label_column_id 1 \\\r\n--model_name_or_path neuralmind/bert-base-portuguese-cased \\\r\n--output_dir <OUTPUT_PATH> \\\r\n--num_train_epochs 4 \\\r\n--per_device_train_batch_size 4 \\\r\n--per_device_eval_batch_size 4 \\\r\n--do_train \\\r\n--do_eval \\\r\n--do_predict \\\r\n--logging_steps 1000 \\\r\n--evaluate_during_training \\\r\n--save_steps 1000 \\\r\n--overwrite_output_dir \\\r\n--overwrite_cache\r\n```\r\n\r\nI have also copied [@Santosh-Gupta 's colab notebook](https://colab.research.google.com/drive/11APei6GjphCZbH5wD9yVlfGvpIkh8pwr?usp=sharing) as a reference.\r\n\r\n<!-- If you have code snippets, error messages, stack traces please provide them here as well.\r\n     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting\r\n     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->\r\n\r\nHere is the stack trace:\r\n\r\n```\r\n2020-10-02 07:33:41.622011: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n/media/discoD/repositorios/transformers_pedro/src/transformers/training_args.py:333: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\r\n  FutureWarning,\r\n2020-10-02 07:33:43.471648: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-02 07:33:43.471791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 07:33:43.472664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7085GHz coreCount: 15 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-10-02 07:33:43.472684: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-02 07:33:43.472765: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-02 07:33:43.472809: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-02 07:33:43.472848: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-02 07:33:43.474209: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-02 07:33:43.474276: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-02 07:33:43.561219: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-02 07:33:43.561397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 07:33:43.562345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 07:33:43.563219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-02 07:33:43.563595: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-02 07:33:43.570091: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3591830000 Hz\r\n2020-10-02 07:33:43.570494: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560842432400 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-02 07:33:43.570511: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-02 07:33:43.570702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 07:33:43.571599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7085GHz coreCount: 15 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-10-02 07:33:43.571633: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-02 07:33:43.571645: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-02 07:33:43.571654: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-02 07:33:43.571664: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-02 07:33:43.571691: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-02 07:33:43.571704: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-02 07:33:43.571718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-02 07:33:43.571770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 07:33:43.572641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 07:33:43.573475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-02 07:33:47.139227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-02 07:33:47.139265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-10-02 07:33:47.139272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-10-02 07:33:47.140323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 07:33:47.141248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 07:33:47.142085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 07:33:47.142854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5371 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-10-02 07:33:47.146317: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5608b95dc5c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-02 07:33:47.146336: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1\r\n10/02/2020 07:33:47 - INFO - __main__ -   n_replicas: 1, distributed training: False, 16-bits training: False\r\n10/02/2020 07:33:47 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='/media/discoD/models/datalawyer/pedidos/transformers_tf', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=4.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct02_07-33-43_user-XPS-8700', logging_first_step=False, logging_steps=1000, save_steps=1000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name='/media/discoD/models/datalawyer/pedidos/transformers_tf', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False, tpu_name=None, xla=False)\r\n10/02/2020 07:33:53 - INFO - filelock -   Lock 140407857405776 acquired on /home/user/.cache/huggingface/datasets/e0f1e9ed46db1e2429189f06b479cbd4075c0976104c1aacf8f77d9a53d2ad87.03756fef6da334f50a7ff73608e21b5018229944ca250416ce7352e25d84a552.py.lock\r\n10/02/2020 07:33:53 - INFO - filelock -   Lock 140407857405776 released on /home/user/.cache/huggingface/datasets/e0f1e9ed46db1e2429189f06b479cbd4075c0976104c1aacf8f77d9a53d2ad87.03756fef6da334f50a7ff73608e21b5018229944ca250416ce7352e25d84a552.py.lock\r\nUsing custom data configuration default\r\nTraceback (most recent call last):\r\n  File \"run_tf_text_classification.py\", line 283, in <module>\r\n    main()\r\n  File \"run_tf_text_classification.py\", line 222, in main\r\n    max_seq_length=data_args.max_seq_length,\r\n  File \"run_tf_text_classification.py\", line 43, in get_tfds\r\n    ds = datasets.load_dataset(\"csv\", data_files=files)\r\n  File \"/media/discoD/anaconda3/envs/transformers/lib/python3.7/site-packages/datasets/load.py\", line 604, in load_dataset\r\n    **config_kwargs,\r\n  File \"/media/discoD/anaconda3/envs/transformers/lib/python3.7/site-packages/datasets/builder.py\", line 158, in __init__\r\n    **config_kwargs,\r\n  File \"/media/discoD/anaconda3/envs/transformers/lib/python3.7/site-packages/datasets/builder.py\", line 269, in _create_builder_config\r\n    for key in sorted(data_files.keys()):\r\nTypeError: '<' not supported between instances of 'NamedSplit' and 'NamedSplit'\r\n```\r\n\r\n## Expected behavior\r\n\r\nShould be able to run the text-classification example as described in [https://github.com/huggingface/transformers/tree/master/examples/text-classification#run-generic-text-classification-script-in-tensorflow](https://github.com/huggingface/transformers/tree/master/examples/text-classification#run-generic-text-classification-script-in-tensorflow)\r\n\r\nOriginally opened this issue at transformers' repository: [https://github.com/huggingface/transformers/issues/7535](https://github.com/huggingface/transformers/issues/7535). @jplu instructed me to open here, since according to [this](https://github.com/huggingface/transformers/issues/7535#issuecomment-702778885) evidence, the problem is from datasets.\r\n\r\nThanks!",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 713572556,
    "title": "Fix remote tests for new datasets",
    "dateCreated": "2020-10-02T12:08:04Z",
    "dateModified": "2020-10-02T12:08:04Z",
    "description": "When adding a new dataset, the remote tests fail because they try to get the new dataset from the master branch (i.e., where the dataset doesn't exist yet)\r\nTo fix that I reverted to the use of the HF API that fetch the available datasets on S3 that is synced with the master branch",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 713559718,
    "title": "Add hotpot QA",
    "dateCreated": "2020-10-02T11:44:28Z",
    "dateModified": "2020-10-02T11:44:28Z",
    "description": "Added the [HotpotQA](https://github.com/hotpotqa/hotpot) multi-hop question answering dataset.\r\n\r\n",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 713499628,
    "title": "Complete rouge kwargs",
    "dateCreated": "2020-10-02T09:59:01Z",
    "dateModified": "2020-10-02T09:59:01Z",
    "description": "In #701 we noticed that some kwargs were missing for rouge",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 713485757,
    "title": "Add rouge 2 and rouge Lsum to rouge metric outputs",
    "dateCreated": "2020-10-02T09:35:46Z",
    "dateModified": "2020-10-02T09:35:46Z",
    "description": "Continuation of #700 \r\n\r\nRouge 2 and Rouge Lsum were missing in Rouge's outputs.\r\nRouge Lsum is also useful to evaluate Rouge L for sentences with `\\n`\r\n\r\nFix #617 ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 713450295,
    "title": "Add rouge-2 in rouge_types for metric calculation",
    "dateCreated": "2020-10-02T08:36:45Z",
    "dateModified": "2020-10-02T08:36:45Z",
    "description": "The description of the ROUGE metric says, \r\n```\r\n_KWARGS_DESCRIPTION = \"\"\"\r\nCalculates average rouge scores for a list of hypotheses and references\r\nArgs:\r\n    predictions: list of predictions to score. Each predictions\r\n        should be a string with tokens separated by spaces.\r\n    references: list of reference for each prediction. Each\r\n        reference should be a string with tokens separated by spaces.\r\nReturns:\r\n    rouge1: rouge_1 f1,\r\n    rouge2: rouge_2 f1,\r\n    rougeL: rouge_l f1,\r\n    rougeLsum: rouge_l precision\r\n\"\"\"\r\n```\r\n\r\nbut the `rouge_types` argument defaults to  `rouge_types = [\"rouge1\", \"rougeL\"]`, this PR updates and add `rouge2` to the list so as to reflect the description card.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 713395642,
    "title": "XNLI dataset is not loading ",
    "dateCreated": "2020-10-02T06:53:16Z",
    "dateModified": "2020-10-02T06:53:16Z",
    "description": "`dataset = datasets.load_dataset(path='xnli')`\r\n\r\nshowing below error \r\n```\r\n/opt/conda/lib/python3.7/site-packages/nlp/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     36     if len(bad_urls) > 0:\r\n     37         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 38         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     39     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     40 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://www.nyu.edu/projects/bowman/xnli/XNLI-1.0.zip']\r\n```\r\n\r\nI think URL is now changed to \"https://cims.nyu.edu/~sbowman/xnli/XNLI-MT-1.0.zip\"",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 712979029,
    "title": "Update README.md",
    "dateCreated": "2020-10-01T16:02:42Z",
    "dateModified": "2020-10-01T16:02:42Z",
    "description": "Hey I was just telling my subscribers to check out your repositories \r\nThank you",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 712942977,
    "title": "Elasticsearch index docs",
    "dateCreated": "2020-10-01T15:18:58Z",
    "dateModified": "2020-10-01T15:18:58Z",
    "description": "I added the docs for ES indexes.\r\n\r\nI also added a `load_elasticsearch_index` method to load an index that has already been built.\r\n\r\nI checked the tests for the ES index and we have tests that mock ElasticSearch.\r\nI think this is good for now but at some point it would be cool to have an end-to-end test with a real ES running.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 712843949,
    "title": "Update XNLI download link",
    "dateCreated": "2020-10-01T13:27:22Z",
    "dateModified": "2020-10-01T13:27:22Z",
    "description": "The old link isn't working anymore. I updated it with the new official link.\r\nFix #690 ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 712827751,
    "title": "Use GitHub instead of aws in remote dataset tests",
    "dateCreated": "2020-10-01T13:07:50Z",
    "dateModified": "2020-10-01T13:07:50Z",
    "description": "Recently we switched from aws s3 to github to download dataset scripts.\r\nHowever in the tests, the dummy data were still downloaded from s3.\r\nSo I changed that to download them from github instead, in the MockDownloadManager.\r\n\r\nMoreover I noticed that `anli`'s dummy data were quite heavy (18MB compressed, i.e. the entire dataset) so I replaced them with dummy data with few examples.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 712822200,
    "title": "Rachel ker add dataset/mlsum",
    "dateCreated": "2020-10-01T13:01:10Z",
    "dateModified": "2020-10-01T13:01:10Z",
    "description": ".",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 712818968,
    "title": "Update README.md",
    "dateCreated": "2020-10-01T12:57:22Z",
    "dateModified": "2020-10-01T12:57:22Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 712389499,
    "title": "Add UI filter to filter datasets based on task",
    "dateCreated": "2020-10-01T00:56:18Z",
    "dateModified": "2020-10-01T00:56:18Z",
    "description": "This is great work, so huge shoutout to contributors and huggingface.\r\n\r\nThe [/nlp/viewer](https://huggingface.co/nlp/viewer/) is great and the [/datasets](https://huggingface.co/datasets) page is great. I was wondering if in both or either places we can have a filter that selects if a dataset is good for the following tasks (non exhaustive list)\r\n\r\n- Classification\r\n\t- Multi label\r\n\t- Multi class\r\n- Q&A\r\n- Summarization\r\n- Translation\r\n\r\nI believe this feature might have some value, for folks trying to find datasets for a particular task, and then testing their model capabilities.\r\n\r\nThank you :) ",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 712150321,
    "title": "XNLI dataset: NonMatchingChecksumError",
    "dateCreated": "2020-09-30T17:50:03Z",
    "dateModified": "2020-09-30T17:50:03Z",
    "description": "Hi,\r\nI tried to download \"xnli\" dataset in colab using \r\n`xnli = load_dataset(path='xnli')`\r\nbut got 'NonMatchingChecksumError' error\r\n\r\n`NonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-27-a87bedc82eeb> in <module>()\r\n----> 1 xnli = load_dataset(path='xnli')\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     37     if len(bad_urls) > 0:\r\n     38         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 39         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     40     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     41 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://www.nyu.edu/projects/bowman/xnli/XNLI-1.0.zip']`\r\n\r\nThe same code worked well several days ago in colab but stopped working now. Thanks!",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 712095262,
    "title": "Switch to pandas reader for text dataset",
    "dateCreated": "2020-09-30T16:28:12Z",
    "dateModified": "2020-09-30T16:28:12Z",
    "description": "Following the discussion in #622 , it appears that there's no appropriate ways to use the payrrow csv reader to read text files because of the separator.\r\n\r\nIn this PR I switched to pandas to read the file.\r\n\r\nMoreover pandas allows to read the file by chunk, which means that you can build the arrow dataset from a text file that is bigger than RAM (we used to have to shard text files an mentioned in https://github.com/huggingface/datasets/issues/610#issuecomment-691672919)\r\n\r\nFrom a test that I did locally on a 1GB text file, the pyarrow reader used to run in 150ms while the new one takes 650ms (multithreading off for pyarrow). This is probably due to chunking since I am having the same speed difference by calling `read()` and calling `read(chunksize)` + `readline()` to read the text file.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 711804828,
    "title": "Disable tokenizers parallelism in multiprocessed map",
    "dateCreated": "2020-09-30T09:53:34Z",
    "dateModified": "2020-09-30T09:53:34Z",
    "description": "It was reported in #620 that using multiprocessing with a tokenizers shows this message:\r\n```\r\nThe current process just got forked. Disabling parallelism to avoid deadlocks...\r\nTo disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\r\n```\r\nThis message is shown when TOKENIZERS_PARALLELISM is unset.\r\nMoreover if it is set to `true`, then the program just hangs.\r\n\r\nTo hide the message (if TOKENIZERS_PARALLELISM is unset) and avoid hanging (if TOKENIZERS_PARALLELISM is `true`), then I set TOKENIZERS_PARALLELISM to `false` when forking the process. After forking is gets back to its original value.\r\n\r\nAlso I added a warning if TOKENIZERS_PARALLELISM was `true` and is set to `false`:\r\n```\r\nSetting TOKENIZERS_PARALLELISM=false for forked processes.\r\n```\r\n\r\ncc @n1t0 ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 711664810,
    "title": "`ArrowInvalid` occurs while running `Dataset.map()` function",
    "dateCreated": "2020-09-30T06:16:50Z",
    "dateModified": "2020-09-30T06:16:50Z",
    "description": "It seems to fail to process the final batch. This [colab](https://colab.research.google.com/drive/1_byLZRHwGP13PHMkJWo62Wp50S_Z2HMD?usp=sharing) can reproduce the error.\r\n\r\nCode:\r\n\r\n```python\r\n# train_ds = Dataset(features: {\r\n#     'title': Value(dtype='string', id=None), \r\n#     'score': Value(dtype='float64', id=None)\r\n# }, num_rows: 99999)\r\n\r\n# suggested in #665 \r\nclass PicklableTokenizer(BertJapaneseTokenizer):\r\n    def __getstate__(self):\r\n        state = dict(self.__dict__)\r\n        state['do_lower_case'] = self.word_tokenizer.do_lower_case\r\n        state['never_split'] = self.word_tokenizer.never_split\r\n        del state['word_tokenizer']\r\n        return state\r\n    \r\n    def __setstate(self):\r\n        do_lower_case = state.pop('do_lower_case')\r\n        never_split = state.pop('never_split')\r\n        self.__dict__ = state\r\n        self.word_tokenizer = MecabTokenizer(\r\n            do_lower_case=do_lower_case, never_split=never_split\r\n        )\r\n\r\nt = PicklableTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')\r\n\r\nencoded = train_ds.map(\r\n    lambda examples: {'tokens': t.encode(examples['title'], max_length=1000)}, batched=True, batch_size=1000\r\n)\r\n```\r\n\r\nError Message:\r\n\r\n```\r\n 99% 99/100 [00:22<00:00, 39.07ba/s]\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\r\n   1242                 fn_kwargs=fn_kwargs,\r\n   1243                 new_fingerprint=new_fingerprint,\r\n-> 1244                 update_data=update_data,\r\n   1245             )\r\n   1246         else:\r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    151             \"output_all_columns\": self._output_all_columns,\r\n    152         }\r\n--> 153         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    154         if new_format[\"columns\"] is not None:\r\n    155             new_format[\"columns\"] = list(set(new_format[\"columns\"]) & set(out.column_names))\r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    161             # Call actual function\r\n    162 \r\n--> 163             out = func(self, *args, **kwargs)\r\n    164 \r\n    165             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/arrow_dataset.py in _map_single(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, update_data)\r\n   1496                     if update_data:\r\n   1497                         batch = cast_to_python_objects(batch)\r\n-> 1498                         writer.write_batch(batch)\r\n   1499             if update_data:\r\n   1500                 writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/arrow_writer.py in write_batch(self, batch_examples, writer_batch_size)\r\n    271             typed_sequence = TypedSequence(batch_examples[col], type=col_type, try_type=col_try_type)\r\n    272             typed_sequence_examples[col] = typed_sequence\r\n--> 273         pa_table = pa.Table.from_pydict(typed_sequence_examples)\r\n    274         self.write_table(pa_table)\r\n    275 \r\n\r\n/usr/local/lib/python3.6/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_pydict()\r\n\r\n/usr/local/lib/python3.6/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_arrays()\r\n\r\n/usr/local/lib/python3.6/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.validate()\r\n\r\n/usr/local/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Column 4 named tokens expected length 999 but got length 1000\r\n```\r\n",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 711385739,
    "title": "Dataset browser url is still https://huggingface.co/nlp/viewer/",
    "dateCreated": "2020-09-29T19:21:52Z",
    "dateModified": "2020-09-29T19:21:52Z",
    "description": "Might be worth updating to https://huggingface.co/datasets/viewer/",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 711182185,
    "title": "Add features parameter to CSV",
    "dateCreated": "2020-09-29T14:43:36Z",
    "dateModified": "2020-09-29T14:43:36Z",
    "description": "Add support for the `features` parameter when loading a csv dataset:\r\n\r\n```python\r\nfrom datasets import load_dataset, Features\r\n\r\nfeatures = Features({...})\r\ncsv_dataset = load_dataset(\"csv\", data_files=[\"path/to/my/file.csv\"], features=features)\r\n```\r\n\r\nI added tests to make sure that it is also compatible with the caching system\r\n\r\nFix #623 ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 711080947,
    "title": "Fix column order issue in cast",
    "dateCreated": "2020-09-29T12:49:13Z",
    "dateModified": "2020-09-29T12:49:13Z",
    "description": "Previously, the order of the columns in the features passes to `cast_` mattered.\r\nHowever even though features passed to `cast_` had the same order as the dataset features, it could fail because the schema that was built was always in alphabetical order.\r\nThis issue was reported by @lewtun  in #623 \r\n\r\nTo fix that I fixed the schema to follow the order of the arrow table columns.\r\nI also added the possibility to give features that are not ordered the same way as the dataset features.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 710942704,
    "title": "Fix wrong delimiter in text dataset",
    "dateCreated": "2020-09-29T09:43:24Z",
    "dateModified": "2020-09-29T09:43:24Z",
    "description": "The delimiter is set to the bell character as it is used nowhere is text files usually.\r\nHowever in the text dataset the delimiter was set to `\\b` which is backspace in python, while the bell character is `\\a`.\r\nI replace \\b by \\a\r\n\r\nHopefully it fixes issues mentioned by some users in #622 ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 710325399,
    "title": "Update navbar chapter titles color",
    "dateCreated": "2020-09-28T14:35:17Z",
    "dateModified": "2020-09-28T14:35:17Z",
    "description": "Consistency with the color change that was done in transformers at https://github.com/huggingface/transformers/pull/7423\r\nIt makes the background-color of the chapter titles in the docs navbar darker, to differentiate them from the inner sections.\r\n\r\nsee changes [here](https://691-250213286-gh.circle-artifacts.com/0/docs/_build/html/index.html)",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 710075721,
    "title": "Adding missing @property (+2 small flake8 fixes).",
    "dateCreated": "2020-09-28T08:53:53Z",
    "dateModified": "2020-09-28T08:53:53Z",
    "description": "Fixes #678",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 710066138,
    "title": "Fix bug related to boolean in GAP dataset.",
    "dateCreated": "2020-09-28T08:39:39Z",
    "dateModified": "2020-09-28T08:39:39Z",
    "description": "### Why I did\r\nThe value in `row[\"A-coref\"]` and `row[\"B-coref\"]` is `'TRUE'` or `'FALSE'`.\r\nThis type is `string`, then `bool('FALSE')` is equal to `True` in Python.\r\nSo, both rows are transformed into `True` now.\r\n\r\nSo, I modified this problem.\r\n\r\n### What I did\r\nI modified `bool(row[\"A-coref\"])` and `bool(row[\"B-coref\"])` to `row[\"A-coref\"] == \"TRUE\"` and `row[\"B-coref\"] == \"TRUE\"`.\r\n\r\nThank you!",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 710065838,
    "title": "Fix negative ids when slicing with an array",
    "dateCreated": "2020-09-28T08:39:08Z",
    "dateModified": "2020-09-28T08:39:08Z",
    "description": "```python\r\nfrom datasets import Dataset\r\n\r\nd = ds.Dataset.from_dict({\"a\": range(10)})\r\nprint(d[[0, -1]])\r\n# OverflowError\r\n```\r\n\r\nraises an error because of the negative id.\r\n\r\nThis PR fixes that.\r\nFix #668 ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 710060497,
    "title": "The download instructions for c4 datasets are not contained in the error message",
    "dateCreated": "2020-09-28T08:30:54Z",
    "dateModified": "2020-09-28T08:30:54Z",
    "description": "The manual download instructions are not clear \r\n```The dataset c4 with config en requires manual data. \r\n Please follow the manual download instructions: <bound method C4.manual_download_instructions of <datasets_modules.datasets.c4.830b0c218bd41fed439812c8dd19dbd4767d2a3faa385eb695cf8666c982b1b3.c4.C4 object at 0x7ff8c5969760>>. \r\n Manual data can be loaded with `datasets.load_dataset(c4, data_dir='<path/to/manual/data>')\r\n```\r\n\r\nEither `@property`  could be added to C4.manual_download_instrcutions (or make it a real property), or the manual_download_instructions function needs to be called I think.\r\n\r\nLet me know if you want a PR for this, but I'm not sure which possible fix is the correct one.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 710055239,
    "title": "Move cache dir root creation in builder's init",
    "dateCreated": "2020-09-28T08:22:46Z",
    "dateModified": "2020-09-28T08:22:46Z",
    "description": "We use lock files in the builder initialization but sometimes the cache directory where they're supposed to be was not created. To fix that I moved the builder's cache dir root creation in the builder's init.\r\n\r\nFix #671 ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 710014319,
    "title": "train_test_split returns empty dataset item",
    "dateCreated": "2020-09-28T07:19:33Z",
    "dateModified": "2020-09-28T07:19:33Z",
    "description": "I try to split my dataset by `train_test_split`, but after that the item in `train` and `test` `Dataset` is empty.\r\nThe codes:\r\n```\r\nyelp_data = datasets.load_from_disk('/home/ssd4/huanglianzhe/test_yelp')\r\n    print(yelp_data[0])\r\n    yelp_data = yelp_data.train_test_split(test_size=0.1)\r\n    print(yelp_data)\r\n    print(yelp_data['test'])\r\n    print(yelp_data['test'][0])\r\n```\r\nThe outputs:\r\n```\r\n{'stars': 2.0, 'text': 'xxxx'}\r\nLoading cached split indices for dataset at /home/ssd4/huanglianzhe/test_yelp/cache-f9b22d8b9d5a7346.arrow and /home/ssd4/huanglianzhe/test_yelp/cache-4aa26fa4005059d1.arrow\r\nDatasetDict({'train': Dataset(features: {'stars': Value(dtype='float64', id=None), 'text': Value(dtype='string', id=None)}, num_rows: 7219009), 'test': Dataset(features: {'stars': Value(dtype='float64', id=None), 'text': Value(dtype='string', id=None)}, num_rows: 802113)})\r\nDataset(features: {'stars': Value(dtype='float64', id=None), 'text': Value(dtype='string', id=None)}, num_rows: 802113)\r\n{}    # yelp_data['test'][0] is empty\r\n```",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 709818725,
    "title": "Add custom dataset to NLP?",
    "dateCreated": "2020-09-27T21:22:50Z",
    "dateModified": "2020-09-27T21:22:50Z",
    "description": "Is it possible to add a custom dataset such as a .csv to the NLP library?\r\n\r\nThanks.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 709661006,
    "title": "load_dataset() won't download in Windows",
    "dateCreated": "2020-09-27T03:56:25Z",
    "dateModified": "2020-09-27T03:56:25Z",
    "description": "I don't know if this is just me or Windows. Maybe other Windows users can chime in if they don't have this problem. I've been trying to get some of the tutorials working on Windows, but when I use the load_dataset() function, it just stalls and the script keeps running indefinitely without downloading anything. I've waited upwards of 18 hours to download the 'multi-news' dataset (which isn't very big), and still nothing. I've tried running it through different IDE's and the command line, but it had the same behavior. I've also tried it with all virus and malware protection turned off. I've made sure python and all IDE's are exceptions to the firewall and all the requisite permissions are enabled.\r\n\r\nAdditionally, I checked to see if other packages could download content such as an nltk corpus, and they could. I've also run the same script using Ubuntu and it downloaded fine (and quickly). When I copied the downloaded datasets from my Ubuntu drive to my Windows .cache folder it worked fine by reusing the already-downloaded dataset, but it's cumbersome to do that for every dataset I want to try in my Windows environment.\r\n\r\nCould this be a bug, or is there something I'm doing wrong or not thinking of?\r\n\r\nThanks.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 709603989,
    "title": "blog_authorship_corpus crashed",
    "dateCreated": "2020-09-26T20:15:28Z",
    "dateModified": "2020-09-26T20:15:28Z",
    "description": "This is just to report that When I pick blog_authorship_corpus in \r\nhttps://huggingface.co/nlp/viewer/?dataset=blog_authorship_corpus\r\nI get this:\r\n![image](https://user-images.githubusercontent.com/7553188/94349542-4364f300-0013-11eb-897d-b25660a449f0.png)\r\n\r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 709575527,
    "title": "Questions about XSUM ",
    "dateCreated": "2020-09-26T17:16:24Z",
    "dateModified": "2020-09-26T17:16:24Z",
    "description": "Hi there \u270b \r\n\r\nI'm looking into your `xsum` dataset and I have several questions on that. \r\nSo here is how I loaded the data: \r\n```\r\n>>> data = datasets.load_dataset('xsum', version='1.0.1')\r\n>>> data['train']\r\nDataset(features: {'document': Value(dtype='string', id=None), 'summary': Value(dtype='string', id=None)}, num_rows: 204017)\r\n>>> data['test']\r\nDataset(features: {'document': Value(dtype='string', id=None), 'summary': Value(dtype='string', id=None)}, num_rows: 11333)\r\n```\r\n\r\nThe first issue is, the instance counts don\u2019t match what I see on [the dataset's website](https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset#what-builds-the-xsum-dataset) (11,333 vs 11,334 for test set; 204,017 vs 204,045 for training set)\r\n```\r\n \u2026 training (90%, 204,045), validation (5%, 11,332), and test (5%, 11,334) set.\r\n```\r\nAny thoughts why? Perhaps @mariamabarham could help here, since she recently had a PR on this dataaset https://github.com/huggingface/datasets/pull/289  (reviewed by @patrickvonplaten) \r\n\r\nAnother issue is that the instances don't seem to have IDs. The original datasets provides IDs for the instances: https://github.com/EdinburghNLP/XSum/blob/master/XSum-Dataset/XSum-TRAINING-DEV-TEST-SPLIT-90-5-5.json but to be able to use them, the dataset sizes need to match. \r\n\r\nCC @jbragg \r\n\r\n",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 709093151,
    "title": "[BUG] No such file or directory",
    "dateCreated": "2020-09-25T16:38:54Z",
    "dateModified": "2020-09-25T16:38:54Z",
    "description": "This happens when both\r\n1. Huggingface datasets cache dir does not exist\r\n2. Try to load a local dataset script\r\n\r\nbuilder.py throws an error when trying to create a filelock in a directory (cache/datasets) that does not exist\r\nhttps://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L177\r\n\r\nTested on v1.0.2\r\n\r\n@lhoestq ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 709061231,
    "title": "Fix SQuAD metric kwargs description",
    "dateCreated": "2020-09-25T16:08:57Z",
    "dateModified": "2020-09-25T16:08:57Z",
    "description": "The `answer_start` field was missing in the kwargs docstring.\r\n\r\nThis should fix #657 \r\n\r\nFYI another fix was proposed by @tshrjn in #658 and suggests to remove this field.\r\nHowever IMO `answer_start` is useful to match the squad dataset format for consistency, even though it is not used in the metric computation. I think it's better to keep it this way, so that you can just give references=squad[\"answers\"] to .compute(). \r\n\r\nLet me know what sounds the best for you\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 708857595,
    "title": "How to skip a example when running dataset.map",
    "dateCreated": "2020-09-25T11:17:53Z",
    "dateModified": "2020-09-25T11:17:53Z",
    "description": "in processing func, I process examples and detect some invalid examples, which I did not want it to be added into train dataset. However I did not find how to skip this recognized invalid example when doing dataset.map. ",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 708310956,
    "title": "OverflowError when slicing with an array containing negative ids",
    "dateCreated": "2020-09-24T16:27:14Z",
    "dateModified": "2020-09-24T16:27:14Z",
    "description": "```python\r\nfrom datasets import Dataset\r\n\r\nd = ds.Dataset.from_dict({\"a\": range(10)})\r\n\r\nprint(d[0])\r\n# {'a': 0}\r\n\r\nprint(d[-1])\r\n# {'a': 9}\r\n\r\nprint(d[[0, -1]])\r\n# OverflowError\r\n```\r\nresults in\r\n```\r\n---------------------------------------------------------------------------\r\nOverflowError                             Traceback (most recent call last)\r\n<ipython-input-5-863dc3555598> in <module>\r\n----> 1 d[[0, -1]]\r\n\r\n~/Desktop/hf/nlp/src/datasets/arrow_dataset.py in __getitem__(self, key)\r\n   1070             format_columns=self._format_columns,\r\n   1071             output_all_columns=self._output_all_columns,\r\n-> 1072             format_kwargs=self._format_kwargs,\r\n   1073         )\r\n   1074 \r\n\r\n~/Desktop/hf/nlp/src/datasets/arrow_dataset.py in _getitem(self, key, format_type, format_columns, output_all_columns, format_kwargs)\r\n   1025                 indices = key\r\n   1026 \r\n-> 1027             indices_array = pa.array([int(i) for i in indices], type=pa.uint64())\r\n   1028 \r\n   1029             # Check if we need to convert indices\r\n\r\n~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib.array()\r\n\r\n~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib._sequence_to_array()\r\n\r\nOverflowError: can't convert negative value to unsigned int\r\n```",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 708258392,
    "title": "Loss not decrease with Datasets and Transformers",
    "dateCreated": "2020-09-24T15:14:43Z",
    "dateModified": "2020-09-24T15:14:43Z",
    "description": "HI,\r\n\r\nThe following script is used to fine-tune a BertForSequenceClassification model on SST2.\r\n\r\nThe script is adapted from [this colab](https://colab.research.google.com/github/huggingface/datasets/blob/master/notebooks/Overview.ipynb) that presents an example of fine-tuning BertForQuestionAnswering using squad dataset. In that colab, loss works fine. When I adapt it to SST2, the loss fails to decrease as it should. I attach the adapted script below and appreciate anyone pointing out what I miss?\r\n\r\n```python\r\nimport torch\r\nfrom datasets import load_dataset\r\nfrom transformers import BertForSequenceClassification\r\nfrom transformers import BertTokenizerFast\r\n# Load our training dataset and tokenizer\r\ndataset = load_dataset(\"glue\", 'sst2')\r\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\r\ndel dataset[\"test\"] # let's remove it in this demo\r\n\r\n# Tokenize our training dataset\r\ndef convert_to_features(example_batch):\r\n    encodings = tokenizer(example_batch[\"sentence\"])\r\n    encodings.update({\"labels\": example_batch[\"label\"]})\r\n    return encodings\r\n\r\nencoded_dataset = dataset.map(convert_to_features, batched=True)\r\n# Format our dataset to outputs torch.Tensor to train a pytorch model\r\ncolumns = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\r\nencoded_dataset.set_format(type='torch', columns=columns)\r\n\r\n# Instantiate a PyTorch Dataloader around our dataset\r\n# Let's do dynamic batching (pad on the fly with our own collate_fn)\r\ndef collate_fn(examples):\r\n    return tokenizer.pad(examples, return_tensors='pt')\r\n\r\ndataloader = torch.utils.data.DataLoader(encoded_dataset['train'], collate_fn=collate_fn, batch_size=8)\r\n# Now let's train our model\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n# Let's load a pretrained Bert model and a simple optimizer\r\nmodel = BertForSequenceClassification.from_pretrained('bert-base-cased', return_dict=True)\r\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\r\nmodel.train().to(device)\r\nfor i, batch in enumerate(dataloader):\r\n    batch.to(device)\r\n    outputs = model(**batch)\r\n    loss = outputs.loss\r\n    loss.backward()\r\n    optimizer.step()\r\n    model.zero_grad()\r\n    print(f'Step {i} - loss: {loss:.3}')\r\n\r\n\r\n```\r\nIn case needed.\r\n\r\n- datasets == 1.0.2\r\n- transformers == 3.2.0",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 707608578,
    "title": "Does both 'bookcorpus' and 'wikipedia' belong to the same datasets which Google used for pretraining BERT?",
    "dateCreated": "2020-09-23T19:02:25Z",
    "dateModified": "2020-09-23T19:02:25Z",
    "description": "",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 707037738,
    "title": "runing dataset.map, it raises TypeError: can't pickle Tokenizer objects",
    "dateCreated": "2020-09-23T04:28:14Z",
    "dateModified": "2020-09-23T04:28:14Z",
    "description": "I load squad dataset. Then want to process data use following function with `Huggingface Transformers LongformerTokenizer`.\r\n\r\n```\r\ndef convert_to_features(example):\r\n    # Tokenize contexts and questions (as pairs of inputs)\r\n    input_pairs = [example['question'], example['context']]\r\n    encodings = tokenizer.encode_plus(input_pairs, pad_to_max_length=True, max_length=512)\r\n    context_encodings = tokenizer.encode_plus(example['context'])\r\n    \r\n\r\n    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.\r\n    # this will give us the position of answer span in the context text\r\n    start_idx, end_idx = get_correct_alignement(example['context'], example['answers'])\r\n    start_positions_context = context_encodings.char_to_token(start_idx)\r\n    end_positions_context = context_encodings.char_to_token(end_idx-1)\r\n\r\n    # here we will compute the start and end position of the answer in the whole example\r\n    # as the example is encoded like this <s> question</s></s> context</s>\r\n    # and we know the postion of the answer in the context\r\n    # we can just find out the index of the sep token and then add that to position + 1 (+1 because there are two sep tokens)\r\n    # this will give us the position of the answer span in whole example \r\n    sep_idx = encodings['input_ids'].index(tokenizer.sep_token_id)\r\n    start_positions = start_positions_context + sep_idx + 1\r\n    end_positions = end_positions_context + sep_idx + 1\r\n\r\n    if end_positions > 512:\r\n      start_positions, end_positions = 0, 0\r\n\r\n    encodings.update({'start_positions': start_positions,\r\n                      'end_positions': end_positions,\r\n                      'attention_mask': encodings['attention_mask']})\r\n    return encodings\r\n```\r\n\r\nThen I run `dataset.map(convert_to_features)`, it raise\r\n```\r\nIn [59]: a.map(convert_to_features)                                                                                                                        \r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-59-c453b508761d> in <module>\r\n----> 1 a.map(convert_to_features)\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\r\n   1242                 fn_kwargs=fn_kwargs,\r\n   1243                 new_fingerprint=new_fingerprint,\r\n-> 1244                 update_data=update_data,\r\n   1245             )\r\n   1246         else:\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    151             \"output_all_columns\": self._output_all_columns,\r\n    152         }\r\n--> 153         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    154         if new_format[\"columns\"] is not None:\r\n    155             new_format[\"columns\"] = list(set(new_format[\"columns\"]) & set(out.column_names))\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    156                         kwargs_for_fingerprint[\"fingerprint_name\"] = fingerprint_name\r\n    157                         kwargs[fingerprint_name] = update_fingerprint(\r\n--> 158                             self._fingerprint, transform, kwargs_for_fingerprint\r\n    159                         )\r\n    160 \r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py in update_fingerprint(fingerprint, transform, transform_args)\r\n    103     for key in sorted(transform_args):\r\n    104         hasher.update(key)\r\n--> 105         hasher.update(transform_args[key])\r\n    106     return hasher.hexdigest()\r\n    107 \r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py in update(self, value)\r\n     55     def update(self, value):\r\n     56         self.m.update(f\"=={type(value)}==\".encode(\"utf8\"))\r\n---> 57         self.m.update(self.hash(value).encode(\"utf-8\"))\r\n     58 \r\n     59     def hexdigest(self):\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py in hash(cls, value)\r\n     51             return cls.dispatch[type(value)](cls, value)\r\n     52         else:\r\n---> 53             return cls.hash_default(value)\r\n     54 \r\n     55     def update(self, value):\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py in hash_default(cls, value)\r\n     44     @classmethod\r\n     45     def hash_default(cls, value):\r\n---> 46         return cls.hash_bytes(dumps(value))\r\n     47 \r\n     48     @classmethod\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/utils/py_utils.py in dumps(obj)\r\n    365     file = StringIO()\r\n    366     with _no_cache_fields(obj):\r\n--> 367         dump(obj, file)\r\n    368     return file.getvalue()\r\n    369 \r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/utils/py_utils.py in dump(obj, file)\r\n    337 def dump(obj, file):\r\n    338     \"\"\"pickle an object to a file\"\"\"\r\n--> 339     Pickler(file, recurse=True).dump(obj)\r\n    340     return\r\n    341 \r\n\r\n/opt/conda/lib/python3.7/site-packages/dill/_dill.py in dump(self, obj)\r\n    444             raise PicklingError(msg)\r\n    445         else:\r\n--> 446             StockPickler.dump(self, obj)\r\n    447         stack.clear()  # clear record of 'recursion-sensitive' pickled objects\r\n    448         return\r\n\r\n/opt/conda/lib/python3.7/pickle.py in dump(self, obj)\r\n    435         if self.proto >= 4:\r\n    436             self.framer.start_framing()\r\n--> 437         self.save(obj)\r\n    438         self.write(STOP)\r\n    439         self.framer.end_framing()\r\n\r\n/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    502         f = self.dispatch.get(t)\r\n    503         if f is not None:\r\n--> 504             f(self, obj) # Call unbound method with explicit self\r\n    505             return\r\n    506 \r\n\r\n/opt/conda/lib/python3.7/site-packages/dill/_dill.py in save_function(pickler, obj)\r\n   1436                                 globs, obj.__name__,\r\n   1437                                 obj.__defaults__, obj.__closure__,\r\n-> 1438                                 obj.__dict__, fkwdefaults), obj=obj)\r\n   1439         else:\r\n   1440             _super = ('super' in getattr(obj.func_code,'co_names',())) and (_byref is not None) and getattr(pickler, '_recurse', False)\r\n\r\n/opt/conda/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)\r\n    636         else:\r\n    637             save(func)\r\n--> 638             save(args)\r\n    639             write(REDUCE)\r\n    640 \r\n\r\n/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    502         f = self.dispatch.get(t)\r\n    503         if f is not None:\r\n--> 504             f(self, obj) # Call unbound method with explicit self\r\n    505             return\r\n    506 \r\n\r\n/opt/conda/lib/python3.7/pickle.py in save_tuple(self, obj)\r\n    787         write(MARK)\r\n    788         for element in obj:\r\n--> 789             save(element)\r\n    790 \r\n    791         if id(obj) in memo:\r\n\r\n/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    502         f = self.dispatch.get(t)\r\n    503         if f is not None:\r\n--> 504             f(self, obj) # Call unbound method with explicit self\r\n    505             return\r\n    506 \r\n\r\n/opt/conda/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)\r\n    931             # we only care about session the first pass thru\r\n    932             pickler._session = False\r\n--> 933         StockPickler.save_dict(pickler, obj)\r\n    934         log.info(\"# D2\")\r\n    935     return\r\n\r\n/opt/conda/lib/python3.7/pickle.py in save_dict(self, obj)\r\n    857 \r\n    858         self.memoize(obj)\r\n--> 859         self._batch_setitems(obj.items())\r\n    860 \r\n    861     dispatch[dict] = save_dict\r\n\r\n/opt/conda/lib/python3.7/pickle.py in _batch_setitems(self, items)\r\n    883                 for k, v in tmp:\r\n    884                     save(k)\r\n--> 885                     save(v)\r\n    886                 write(SETITEMS)\r\n    887             elif n:\r\n\r\n/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    547 \r\n    548         # Save the reduce() output and finally memoize the object\r\n--> 549         self.save_reduce(obj=obj, *rv)\r\n    550 \r\n    551     def persistent_id(self, obj):\r\n\r\n/opt/conda/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)\r\n    660 \r\n    661         if state is not None:\r\n--> 662             save(state)\r\n    663             write(BUILD)\r\n    664 \r\n\r\n/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    502         f = self.dispatch.get(t)\r\n    503         if f is not None:\r\n--> 504             f(self, obj) # Call unbound method with explicit self\r\n    505             return\r\n    506 \r\n\r\n/opt/conda/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)\r\n    931             # we only care about session the first pass thru\r\n    932             pickler._session = False\r\n--> 933         StockPickler.save_dict(pickler, obj)\r\n    934         log.info(\"# D2\")\r\n    935     return\r\n\r\n/opt/conda/lib/python3.7/pickle.py in save_dict(self, obj)\r\n    857 \r\n    858         self.memoize(obj)\r\n--> 859         self._batch_setitems(obj.items())\r\n    860 \r\n    861     dispatch[dict] = save_dict\r\n\r\n/opt/conda/lib/python3.7/pickle.py in _batch_setitems(self, items)\r\n    883                 for k, v in tmp:\r\n    884                     save(k)\r\n--> 885                     save(v)\r\n    886                 write(SETITEMS)\r\n    887             elif n:\r\n\r\n/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    547 \r\n    548         # Save the reduce() output and finally memoize the object\r\n--> 549         self.save_reduce(obj=obj, *rv)\r\n    550 \r\n    551     def persistent_id(self, obj):\r\n\r\n/opt/conda/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)\r\n    660 \r\n    661         if state is not None:\r\n--> 662             save(state)\r\n    663             write(BUILD)\r\n    664 \r\n\r\n/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    502         f = self.dispatch.get(t)\r\n    503         if f is not None:\r\n--> 504             f(self, obj) # Call unbound method with explicit self\r\n    505             return\r\n    506 \r\n\r\n/opt/conda/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)\r\n    931             # we only care about session the first pass thru\r\n    932             pickler._session = False\r\n--> 933         StockPickler.save_dict(pickler, obj)\r\n    934         log.info(\"# D2\")\r\n    935     return\r\n\r\n/opt/conda/lib/python3.7/pickle.py in save_dict(self, obj)\r\n    857 \r\n    858         self.memoize(obj)\r\n--> 859         self._batch_setitems(obj.items())\r\n    860 \r\n    861     dispatch[dict] = save_dict\r\n\r\n/opt/conda/lib/python3.7/pickle.py in _batch_setitems(self, items)\r\n    883                 for k, v in tmp:\r\n    884                     save(k)\r\n--> 885                     save(v)\r\n    886                 write(SETITEMS)\r\n    887             elif n:\r\n\r\n/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    522             reduce = getattr(obj, \"__reduce_ex__\", None)\r\n    523             if reduce is not None:\r\n--> 524                 rv = reduce(self.proto)\r\n    525             else:\r\n    526                 reduce = getattr(obj, \"__reduce__\", None)\r\n\r\nTypeError: can't pickle Tokenizer objects\r\n```\r\n\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 707017791,
    "title": "load_dataset from local squad.py, raise error: TypeError: 'NoneType' object is not callable ",
    "dateCreated": "2020-09-23T03:53:36Z",
    "dateModified": "2020-09-23T03:53:36Z",
    "description": "\r\nversion: 1.0.2\r\n\r\n```\r\ntrain_dataset  = datasets.load_dataset('squad') \r\n```\r\n\r\nThe above code can works. However, when I download the squad.py from your server, and saved as `my_squad.py` to local. I run followings raise errors.\r\n```\r\ntrain_dataset  = datasets.load_dataset('./my_squad.py')                                                                                                \r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-28-25a84b4d1581> in <module>\r\n----> 1 train_dataset  = nlp.load_dataset('./my_squad.py')\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)\r\n    602         hash=hash,\r\n    603         features=features,\r\n--> 604         **config_kwargs,\r\n    605     )\r\n    606 \r\n\r\nTypeError: 'NoneType' object is not callable\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 706732636,
    "title": "Created dataset card snli.md",
    "dateCreated": "2020-09-22T22:29:37Z",
    "dateModified": "2020-09-22T22:29:37Z",
    "description": "First draft of a dataset card using the SNLI corpus as an example.\r\n\r\nThis is mostly based on the [Google Doc draft](https://docs.google.com/document/d/1dKPGP-dA2W0QoTRGfqQ5eBp0CeSsTy7g2yM8RseHtos/edit), but I added a few sections and moved some things around. \r\n\r\n- I moved **Who Was Involved** to follow **Language**, both because I thought the authors should be presented more towards the front and because I think it makes sense to present the speakers close to the language so it doesn't have to be repeated.\r\n\r\n- I created a section I called **Data Characteristics** by pulling some things out of the other sections. I was thinking that this would be more about the language use in context of the specific task construction. That name isn't very descriptive though and could probably be improved.\r\n-- Domain and language type out of **Language**. I particularly wanted to keep the Language section as simple and as abstracted from the task as possible.\r\n-- 'How was the data collected' out of **Who Was Involved** \r\n-- Normalization out of **Features/Dataset Structure** \r\n-- I also added an annotation process section.\r\n\r\n- I kept the **Features** section mostly the same as the Google Doc, but I renamed it **Dataset Structure** to more clearly separate it from the language use, and added some links to the documentation pages. \r\n\r\n- I also kept **Tasks Supported**,  **Known Limitations**, and **Licensing Information** mostly the same. Looking at it again though, maybe **Tasks Supported** should come before **Data Characteristics**?\r\n\r\nThe trickiest part about writing a dataset card for the SNLI corpus specifically is that it's built on datasets which are themselves built on datasets so I had to dig in a lot of places to find information. I think this will be easier with other datasets and once there is more uptake of dataset cards so they can just link to each other. (Maybe that needs to be an added section?)\r\n\r\nI also made an effort not to repeat information across the sections or to refer to a previous section if the information was relevant in a later one. Is there too much repetition still?",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 706689866,
    "title": "Created dataset card snli.md",
    "dateCreated": "2020-09-22T21:00:17Z",
    "dateModified": "2020-09-22T21:00:17Z",
    "description": "First draft of a dataset card using the SNLI corpus as an example",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 706465936,
    "title": "Replace pa.OSFile by open",
    "dateCreated": "2020-09-22T15:05:59Z",
    "dateModified": "2020-09-22T15:05:59Z",
    "description": "It should fix #643 ",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 706324032,
    "title": "add openwebtext",
    "dateCreated": "2020-09-22T12:05:22Z",
    "dateModified": "2020-09-22T12:05:22Z",
    "description": "This adds [The OpenWebText Corpus](https://skylion007.github.io/OpenWebTextCorpus/), which is a clean and large text corpus for nlp pretraining. It is an open source effort to reproduce OpenAI\u2019s WebText dataset used by GPT-2, and it is also needed to reproduce ELECTRA.\r\n\r\nIt solves #132 .\r\n\r\n### Besides dataset building script, I made some changes to the library.\r\n\r\n1. Extract large amount of compressed files with multi processing\r\nI add a `num_proc` argument to `DownloadManager.extract` and pass this `num_proc` to `map_nested`. So I can decompress 20 thousands compressed files faster. `num_proc` I add is default to `None`, so it shouldn't break any other thing.\r\n\r\n2. In `cached_path`, I change the order to deal with different kind of compressed files (zip, tar, gzip)\r\nBecause there is no way to 100% detect a file is a zip file (see [this](https://stackoverflow.com/questions/18194688/how-can-i-determine-if-a-file-is-a-zip-file)), I found it wrongly detect `'./datasets/downloads/extracted/58764bd6898fa339b25d92e7fbbc3d8dbf64fb504edff1a30a1d7d99d1561027/openwebtext/urlsf_subset13-630_data.xz'` as a zip and try decompress it with zip, sure it will get error. So I made it detect wheter the file is tar or gzip first and detect zip in the last.\r\n\r\n3. `MockDownloadManager.extract`\r\nCuz I pass `num_proc` to `DownloadManager.extract`, I also have to make `MockDownloadManager.extract` to accept extra keywork arguments. So I make it `extract(path, *args, **kwargs)`, but just return the path as original implementation.\r\n\r\n**Note**: If there is better way for points mentioned above, thought I would like to help, unless we can solve point4 (make dataset building fast), I may not be able to afford rebuild the dataset again because of change of the dataset script (Building the dataset cost me 4 days). \r\n\r\n### There is something I think we can improve\r\n\r\n4. Long time to decompress compressed files\r\nEven I decompress those 20 thousands compressed files with 12 process on my 16 core 3.x Ghz server. It still took about 3 ~ 4days to complete dataset building. Most of time spent on decompress those files.\r\n\r\n### Info about the source data\r\nThe source data is an  tar.xz file with following structure, files/directory beyond compressed file is what can we get after decompress it.\r\n```\r\nopenwebtext.tar.xz\r\n  |__ openwebtext\r\n         |__subset000.xz\r\n         |     |__ ....txt\r\n         |     |__ ....txt\r\n         |     ...\r\n         |__ subset001.xz\r\n         |\r\n         ....\r\n```\r\nAnd this the structure of dummy data, same as the original one.\r\n```\r\ndummy_data.zip\r\n  |__ dummy_data\r\n         |__ openwebtext\r\n              |__fake_subset-1_data-dirxz # actually it is a directory\r\n              |     |__ ....txt\r\n              |     |__ ....txt\r\n              |__ fake_subset-2_data-dirxz\r\n                    |__ ....txt\r\n                    |__ ....txt\r\n```",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 706231506,
    "title": "Keep new columns in transmit format",
    "dateCreated": "2020-09-22T09:47:23Z",
    "dateModified": "2020-09-22T09:47:23Z",
    "description": "When a dataset is formatted with a list of columns that `__getitem__` should return, then calling `map` to add new columns doesn't add the new columns to this list. \r\n\r\nIt caused `KeyError` issues in #620 \r\n\r\nI changed the logic to add those new columns to the list that `__getitem__` should return.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 706206247,
    "title": "Fix squad metric's Features",
    "dateCreated": "2020-09-22T09:09:52Z",
    "dateModified": "2020-09-22T09:09:52Z",
    "description": "Resolves issue [657](https://github.com/huggingface/datasets/issues/657).",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 706204383,
    "title": "Squad Metric Description & Feature Mismatch",
    "dateCreated": "2020-09-22T09:07:00Z",
    "dateModified": "2020-09-22T09:07:00Z",
    "description": "The [description](https://github.com/huggingface/datasets/blob/master/metrics/squad/squad.py#L39) doesn't mention `answer_start` in squad. However the `datasets.features` require [it](https://github.com/huggingface/datasets/blob/master/metrics/squad/squad.py#L68). It's also not used in the evaluation.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 705736319,
    "title": "Use multiprocess from pathos for multiprocessing",
    "dateCreated": "2020-09-21T16:12:19Z",
    "dateModified": "2020-09-21T16:12:19Z",
    "description": "[Multiprocess](https://github.com/uqfoundation/multiprocess) (from the [pathos](https://github.com/uqfoundation/pathos) project) allows to use lambda functions in multiprocessed map.\r\nIt was suggested to use it by @kandorm.\r\n\r\nWe're already using dill which is its only dependency.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 705672208,
    "title": "added Winogrande debiased subset",
    "dateCreated": "2020-09-21T14:51:08Z",
    "dateModified": "2020-09-21T14:51:08Z",
    "description": "The [Winogrande](https://arxiv.org/abs/1907.10641) paper mentions a `debiased` subset that wasn't in the first release; this PR adds it.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 705511058,
    "title": "Allow empty inputs in metrics",
    "dateCreated": "2020-09-21T11:26:36Z",
    "dateModified": "2020-09-21T11:26:36Z",
    "description": "There was an arrow error when trying to compute a metric with empty inputs. The error was occurring when reading the arrow file, before calling metric._compute.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 705482391,
    "title": "handle data alteration when trying type",
    "dateCreated": "2020-09-21T10:41:49Z",
    "dateModified": "2020-09-21T10:41:49Z",
    "description": "Fix #649 \r\n\r\nThe bug came from the type inference that didn't handle a weird case in Pyarrow.\r\nIndeed this code runs without error but alters the data in arrow:\r\n```python\r\nimport pyarrow as pa\r\n\r\ntype = pa.struct({\"a\": pa.struct({\"b\": pa.string()})})\r\narray_with_altered_data = pa.array([{\"a\": {\"b\": \"foo\", \"c\": \"bar\"}}] * 10, type=type)\r\nprint(array_with_altered_data[0].as_py())\r\n# {'a': {'b': 'foo'}} -> the sub-field \"c\" is missing\r\n```\r\n(I don't know if this is intended in pyarrow tbh)\r\n\r\nWe didn't take this case into account during type inference. By default it was keeping old features and maybe alter data.\r\nTo fix that I added a line that checks that the first element of the array is not altered.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 705390850,
    "title": "handle connection error in download_prepared_from_hf_gcs",
    "dateCreated": "2020-09-21T08:21:11Z",
    "dateModified": "2020-09-21T08:21:11Z",
    "description": "Fix #647 ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 705212034,
    "title": "Problem with JSON dataset format",
    "dateCreated": "2020-09-20T23:57:14Z",
    "dateModified": "2020-09-20T23:57:14Z",
    "description": "I have a local json dataset with the following form.\r\n\r\n{\r\n    'id01234': {'key1': value1, 'key2': value2, 'key3': value3},\r\n    'id01235': {'key1': value1, 'key2': value2, 'key3': value3},\r\n    .\r\n    .\r\n    .\r\n    'id09999': {'key1': value1, 'key2': value2, 'key3': value3}\r\n}\r\nNote that instead of a list of records it's basically a dictionary of key value pairs with the keys being the record_ids and the values being the corresponding record.\r\n\r\nReading this with json:\r\n\r\n```\r\ndata = datasets.load('json', data_files='path_to_local.json')\r\n```\r\nThrows an error and asks me to chose a field. What's the right way to handle this?",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 704861844,
    "title": "dummy data testing can't test datasets using `dl_manager.extract` in `_split_generators`",
    "dateCreated": "2020-09-19T11:07:03Z",
    "dateModified": "2020-09-19T11:07:03Z",
    "description": "Hi, I recently want to add a dataset whose source data is like this\r\n```\r\nopenwebtext.tar.xz\r\n  |__ openwebtext\r\n         |__subset000.xz\r\n         |     |__ ....txt\r\n         |     |__ ....txt\r\n         |     ...\r\n         |__ subset001.xz\r\n         |\r\n         ....\r\n```\r\nSo I wrote `openwebtext.py` like this\r\n```\r\n def _split_generators(self, dl_manager):\r\n        dl_dir = dl_manager.download_and_extract(_URL)\r\n        owt_dir = os.path.join(dl_dir, 'openwebtext')\r\n        subset_xzs = [\r\n            os.path.join(owt_dir, file_name) for file_name in os.listdir(owt_dir) if file_name.endswith('xz') # filter out ...xz.lock\r\n        ]\r\n        ex_dirs = dl_manager.extract(subset_xzs, num_proc=round(os.cpu_count()*0.75))\r\n        nested_txt_files = [ \r\n          [ \r\n            os.path.join(ex_dir,txt_file_name) for txt_file_name in os.listdir(ex_dir) if txt_file_name.endswith('txt')\r\n          ] for ex_dir in ex_dirs\r\n        ]\r\n        txt_files = chain(*nested_txt_files)\r\n        return [\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.TRAIN, gen_kwargs={\"txt_files\": txt_files}\r\n            ),\r\n        ]\r\n```\r\nAll went good, I can load and use real openwebtext, except when I try to test with dummy data. The problem is  `MockDownloadManager.extract` do nothing, so `ex_dirs = dl_manager.extract(subset_xzs)` won't decompress `subset_xxx.xz`s for me.\r\n\r\nHow should I do ? Or you can modify `MockDownloadManager` to make it like a real `DownloadManager` ?",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 704838415,
    "title": "Inconsistent behavior in map",
    "dateCreated": "2020-09-19T08:41:12Z",
    "dateModified": "2020-09-19T08:41:12Z",
    "description": "I'm observing inconsistent behavior when applying .map(). This happens specifically when I'm incrementally adding onto a feature that is a nested dictionary. Here's a simple example that reproduces the problem.\r\n\r\n```python\r\nimport datasets\r\n\r\n# Dataset with a single feature called 'field' consisting of two examples\r\ndataset = datasets.Dataset.from_dict({'field': ['a', 'b']})\r\nprint(dataset[0])\r\n# outputs\r\n{'field': 'a'}\r\n\r\n# Map this dataset to create another feature called 'otherfield', which is a dictionary containing a key called 'capital'\r\ndataset = dataset.map(lambda example: {'otherfield': {'capital': example['field'].capitalize()}})\r\nprint(dataset[0])\r\n# output is okay\r\n{'field': 'a', 'otherfield': {'capital': 'A'}}\r\n\r\n# Now I want to map again to modify 'otherfield', by adding another key called 'append_x' to the dictionary under 'otherfield'\r\nprint(dataset.map(lambda example: {'otherfield': {'append_x': example['field'] + 'x'}})[0])\r\n# printing out the first example after applying the map shows that the new key 'append_x' doesn't get added\r\n# it also messes up the value stored at 'capital'\r\n{'field': 'a', 'otherfield': {'capital': None}}\r\n\r\n# Instead, I try to do the same thing by using a different mapped fn\r\nprint(dataset.map(lambda example:  {'otherfield': {'append_x': example['field'] + 'x', 'capital': example['otherfield']['capital']}})[0])\r\n# this preserves the value under capital, but still no 'append_x'\r\n{'field': 'a', 'otherfield': {'capital': 'A'}}\r\n\r\n# Instead, I try to pass 'otherfield' to remove_columns\r\nprint(dataset.map(lambda example:  {'otherfield': {'append_x': example['field'] + 'x', 'capital': example['otherfield']['capital']}}, remove_columns=['otherfield'])[0])\r\n# this still doesn't fix the problem\r\n{'field': 'a', 'otherfield': {'capital': 'A'}}\r\n\r\n# Alternately, here's what happens if I just directly map both 'capital' and 'append_x' on a fresh dataset.\r\n\r\n# Recreate the dataset\r\ndataset = datasets.Dataset.from_dict({'field': ['a', 'b']})\r\n# Now map the entire 'otherfield' dict directly, instead of incrementally as before\r\nprint(dataset.map(lambda example:  {'otherfield': {'append_x': example['field'] + 'x', 'capital': example['field'].capitalize()}})[0])\r\n# This looks good!\r\n{'field': 'a', 'otherfield': {'append_x': 'ax', 'capital': 'A'}}\r\n```\r\n\r\nThis might be a new issue, because I didn't see this behavior in the `nlp` library. \r\n\r\nAny help is appreciated!",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 704753123,
    "title": "offset overflow when multiprocessing batched map on large datasets.",
    "dateCreated": "2020-09-19T02:15:11Z",
    "dateModified": "2020-09-19T02:15:11Z",
    "description": "It only happened when \"multiprocessing\" + \"batched\" + \"large dataset\" at the same time.\r\n\r\n```\r\ndef bprocess(examples):\r\n  examples['len'] = []\r\n  for text in examples['text']:\r\n    examples['len'].append(len(text))\r\n  return examples\r\nwiki.map(brpocess, batched=True, num_proc=8)\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nRemoteTraceback                           Traceback (most recent call last)\r\nRemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/home/yisiang/miniconda3/envs/ml/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 153, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/yisiang/datasets/src/datasets/fingerprint.py\", line 163, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 1486, in _map_single\r\n    batch = self[i : i + batch_size]\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 1071, in __getitem__\r\n    format_kwargs=self._format_kwargs,\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 972, in _getitem\r\n    data_subset = self._data.take(indices_array)\r\n  File \"pyarrow/table.pxi\", line 1145, in pyarrow.lib.Table.take\r\n  File \"/home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/pyarrow/compute.py\", line 268, in take\r\n    return call_function('take', [data, indices], options)\r\n  File \"pyarrow/_compute.pyx\", line 298, in pyarrow._compute.call_function\r\n  File \"pyarrow/_compute.pyx\", line 192, in pyarrow._compute.Function.call\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nArrowInvalid                              Traceback (most recent call last)\r\n in \r\n     30   owt = datasets.load_dataset('/home/yisiang/datasets/datasets/openwebtext/openwebtext.py', cache_dir='./datasets')['train']\r\n     31   print('load/create data from OpenWebText Corpus for ELECTRA')\r\n---> 32   e_owt = ELECTRAProcessor(owt, apply_cleaning=False).map(cache_file_name=f\"electra_owt_{c.max_length}.arrow\")\r\n     33   dsets.append(e_owt)\r\n     34 \r\n\r\n~/Reexamine_Attention/electra_pytorch/_utils/utils.py in map(self, **kwargs)\r\n    126       writer_batch_size=10**4,\r\n    127       num_proc=num_proc,\r\n--> 128       **kwargs\r\n    129     )\r\n    130 \r\n\r\n~/hugdatafast/hugdatafast/transform.py in my_map(self, *args, **kwargs)\r\n     21     if not cache_file_name.endswith('.arrow'): cache_file_name += '.arrow'\r\n     22     if '/' not in cache_file_name: cache_file_name = os.path.join(self.cache_directory(), cache_file_name)\r\n---> 23   return self.map(*args, cache_file_name=cache_file_name, **kwargs)\r\n     24 \r\n     25 @patch\r\n\r\n~/datasets/src/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\r\n   1285                 logger.info(\"Spawning {} processes\".format(num_proc))\r\n   1286                 results = [pool.apply_async(self.__class__._map_single, kwds=kwds) for kwds in kwds_per_shard]\r\n-> 1287                 transformed_shards = [r.get() for r in results]\r\n   1288                 logger.info(\"Concatenating {} shards from multiprocessing\".format(num_proc))\r\n   1289                 result = concatenate_datasets(transformed_shards)\r\n\r\n~/datasets/src/datasets/arrow_dataset.py in (.0)\r\n   1285                 logger.info(\"Spawning {} processes\".format(num_proc))\r\n   1286                 results = [pool.apply_async(self.__class__._map_single, kwds=kwds) for kwds in kwds_per_shard]\r\n-> 1287                 transformed_shards = [r.get() for r in results]\r\n   1288                 logger.info(\"Concatenating {} shards from multiprocessing\".format(num_proc))\r\n   1289                 result = concatenate_datasets(transformed_shards)\r\n\r\n~/miniconda3/envs/ml/lib/python3.7/multiprocessing/pool.py in get(self, timeout)\r\n    655             return self._value\r\n    656         else:\r\n--> 657             raise self._value\r\n    658 \r\n    659     def _set(self, i, obj):\r\n\r\nArrowInvalid: offset overflow while concatenating arrays\r\n```",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 704734764,
    "title": "Cannot download dataset_info.json",
    "dateCreated": "2020-09-19T01:35:15Z",
    "dateModified": "2020-09-19T01:35:15Z",
    "description": "I am running my job on a cloud server where does not provide for connections from the standard compute nodes to outside resources. Hence, when I use `dataset.load_dataset()` to load data, I got an error like this:\r\n\r\n```\r\nConnectionError: Couldn't reach https://storage.googleapis.com/huggingface-nlp/cache/datasets/text/default-53ee3045f07ba8ca/0.0.0/dataset_info.json\r\n```\r\n\r\nI tried to open this link manually, but I cannot access this file. How can I download this file and pass it through `dataset.load_dataset()` manually?\r\n\r\nVersions:\r\nPython version 3.7.3\r\nPyTorch version 1.6.0\r\nTensorFlow version 2.3.0\r\ndatasets version: 1.0.1 \r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 704607371,
    "title": "Fix docs typos",
    "dateCreated": "2020-09-18T19:32:27Z",
    "dateModified": "2020-09-18T19:32:27Z",
    "description": "This PR fixes few typos in the docs and the error in the code snippet in the set_format section in docs/source/torch_tensorflow.rst. `torch.utils.data.Dataloader` expects padded batches so it throws an error due to not being able to stack the unpadded tensors. If we follow the Quick tour from the docs where they add the `truncation=True, padding='max_length'` arguments to the tokenizer before passing data to Dataloader, we can easily fix the issue. ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 704542234,
    "title": "Don't use take on dataset table in pyarrow 1.0.x",
    "dateCreated": "2020-09-18T17:31:34Z",
    "dateModified": "2020-09-18T17:31:34Z",
    "description": "Fix #615 ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 704534501,
    "title": "Better windows support",
    "dateCreated": "2020-09-18T17:17:36Z",
    "dateModified": "2020-09-18T17:17:36Z",
    "description": "There are a few differences in the behavior of python and pyarrow on windows.\r\n\r\nFor example there are restrictions when accessing/deleting files that are open\r\n\r\nFix #590 ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 704477164,
    "title": "Caching processed dataset at wrong folder",
    "dateCreated": "2020-09-18T15:41:26Z",
    "dateModified": "2020-09-18T15:41:26Z",
    "description": "Hi guys, I run this on my Colab (PRO):\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('text', data_files='/content/corpus.txt', cache_dir='/content/drive/My Drive', split='train')\r\n\r\ndef encode(examples):\r\n  return tokenizer(examples['text'], truncation=True, padding='max_length')\r\n\r\ndataset = dataset.map(encode, batched=True)\r\n```\r\nThe file is about 4 GB, so I cannot process it on the Colab HD because there is no enough space. So I decided to mount my Google Drive fs and do it on it.\r\nThe dataset is cached in the right place but by processing it (applying `encode` function) seems to use a different folder because Colab HD starts to grow and it crashes when it should be done in the Drive fs.\r\n\r\nWhat gets me crazy, it prints it is processing/encoding the dataset in the right folder:\r\n```\r\nTesting the mapped function outputs\r\nTesting finished, running the mapping function on the dataset\r\nCaching processed dataset at /content/drive/My Drive/text/default-ad3e69d6242ee916/0.0.0/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7/cache-b16341780a59747d.arrow\r\n```",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 704397499,
    "title": "Rename wnut fields",
    "dateCreated": "2020-09-18T13:51:31Z",
    "dateModified": "2020-09-18T13:51:31Z",
    "description": "As mentioned in #641 it would be cool to have it follow the naming of the other NER datasets",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 704373940,
    "title": "Add Polyglot-NER Dataset",
    "dateCreated": "2020-09-18T13:21:44Z",
    "dateModified": "2020-09-18T13:21:44Z",
    "description": "Adds the [Polyglot-NER dataset](https://sites.google.com/site/rmyeid/projects/polylgot-ner) with named entity tags for 40 languages. I include separate configs for each language as well as a `combined` config which lumps them all together.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 704311758,
    "title": "Make shuffle compatible with temp_seed",
    "dateCreated": "2020-09-18T11:38:58Z",
    "dateModified": "2020-09-18T11:38:58Z",
    "description": "This code used to return different dataset at each run\r\n```python\r\nimport dataset as ds\r\n\r\ndataset = ...\r\n\r\nwith ds.temp_seed(42):\r\n    shuffled = dataset.shuffle()\r\n```\r\n\r\nNow it returns the same one since the seed is set",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 704217963,
    "title": "Update glue QQP checksum",
    "dateCreated": "2020-09-18T09:08:15Z",
    "dateModified": "2020-09-18T09:08:15Z",
    "description": "Fix #638 ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 704146956,
    "title": "GLUE/QQP dataset: NonMatchingChecksumError",
    "dateCreated": "2020-09-18T07:09:10Z",
    "dateModified": "2020-09-18T07:09:10Z",
    "description": "Hi @lhoestq , I know you are busy and there are also other important issues. But if this is easy to be fixed, I am shamelessly wondering if you can give me some help , so I can evaluate my models and restart with my developing cycle asap. \ud83d\ude1a\r\n\r\ndatasets version: editable install of master at 9/17\r\n\r\n`datasets.load_dataset('glue','qqp', cache_dir='./datasets')`\r\n\r\n```\r\nDownloading and preparing dataset glue/qqp (download: 57.73 MiB, generated: 107.02 MiB, post-processed: Unknown size, total: 164.75 MiB) to ./datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n in \r\n----> 1 datasets.load_dataset('glue','qqp', cache_dir='./datasets')\r\n\r\n~/datasets/src/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)\r\n    609         download_config=download_config,\r\n    610         download_mode=download_mode,\r\n--> 611         ignore_verifications=ignore_verifications,\r\n    612     )\r\n    613 \r\n\r\n~/datasets/src/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    467                     if not downloaded_from_gcs:\r\n    468                         self._download_and_prepare(\r\n--> 469                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    470                         )\r\n    471                     # Sync info\r\n\r\n~/datasets/src/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    527         if verify_infos:\r\n    528             verify_checksums(\r\n--> 529                 self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n    530             )\r\n    531 \r\n\r\n~/datasets/src/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     37     if len(bad_urls) > 0:\r\n     38         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 39         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     40     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     41 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip']\r\n```",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 703539909,
    "title": "Add MATINF",
    "dateCreated": "2020-09-17T12:24:53Z",
    "dateModified": "2020-09-17T12:24:53Z",
    "description": "",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 702883989,
    "title": "Consistent ner features",
    "dateCreated": "2020-09-16T15:56:25Z",
    "dateModified": "2020-09-16T15:56:25Z",
    "description": "As discussed in #613 , this PR aims at making NER feature names consistent across datasets.\r\n\r\nI changed the feature names of LinCE and XTREME/PAN-X",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 702822439,
    "title": "Loglevel",
    "dateCreated": "2020-09-16T14:37:53Z",
    "dateModified": "2020-09-16T14:37:53Z",
    "description": "Continuation of #618 ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 702676041,
    "title": "Add ConLL-2000 dataset",
    "dateCreated": "2020-09-16T11:14:11Z",
    "dateModified": "2020-09-16T11:14:11Z",
    "description": "Adds ConLL-2000 dataset used for text chunking. See https://www.clips.uantwerpen.be/conll2000/chunking/ for details and [motivation](https://github.com/huggingface/transformers/pull/7041#issuecomment-692710948) behind this PR",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 702440484,
    "title": "Load large text file for LM pre-training resulting in OOM",
    "dateCreated": "2020-09-16T04:33:15Z",
    "dateModified": "2020-09-16T04:33:15Z",
    "description": "I tried to pretrain Longformer using transformers and datasets. But I got OOM issues with loading a large text file. My script is almost like this:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n@dataclass\r\nclass DataCollatorForDatasetsLanguageModeling(DataCollatorForLanguageModeling):\r\n    \"\"\"\r\n    Data collator used for language modeling based on DataCollatorForLazyLanguageModeling\r\n    - collates batches of tensors, honoring their tokenizer's pad_token\r\n    - preprocesses batches for masked language modeling\r\n    \"\"\"\r\n\r\n    block_size: int = 512\r\n\r\n    def __call__(self, examples: List[dict]) -> Dict[str, torch.Tensor]:\r\n        examples = [example['text'] for example in examples]\r\n        batch, attention_mask = self._tensorize_batch(examples)\r\n        if self.mlm:\r\n            inputs, labels = self.mask_tokens(batch)\r\n            return {\"input_ids\": inputs, \"labels\": labels}\r\n        else:\r\n            labels = batch.clone().detach()\r\n            if self.tokenizer.pad_token_id is not None:\r\n                labels[labels == self.tokenizer.pad_token_id] = -100\r\n            return {\"input_ids\": batch, \"labels\": labels}\r\n\r\n    def _tensorize_batch(self, examples: List[str]) -> Tuple[torch.Tensor, torch.Tensor]:\r\n\r\n        if self.tokenizer._pad_token is None:\r\n            raise ValueError(\r\n                \"You are attempting to pad samples but the tokenizer you are using\"\r\n                f\" ({self.tokenizer.__class__.__name__}) does not have one.\"\r\n            )\r\n\r\n        tensor_examples = self.tokenizer.batch_encode_plus(\r\n            [ex for ex in examples if ex],\r\n            max_length=self.block_size,\r\n            return_tensors=\"pt\",\r\n            pad_to_max_length=True,\r\n            return_attention_mask=True,\r\n            truncation=True,\r\n        )\r\n\r\n        input_ids, attention_mask = tensor_examples[\"input_ids\"], tensor_examples[\"attention_mask\"]\r\n        return input_ids, attention_mask\r\n\r\ndataset = load_dataset('text', data_files='train.txt',cache_dir=\"./\", , split='train')\r\ndata_collator = DataCollatorForDatasetsLanguageModeling(tokenizer=tokenizer, mlm=True, \r\n                      mlm_probability=0.15, block_size=tokenizer.max_len)\r\ntrainer = Trainer(model=model, args=args, data_collator=data_collator,\r\n                      train_dataset=train_dataset, prediction_loss_only=True, )\r\ntrainer.train(model_path=model_path)\r\n```\r\nThis train.txt is about 1.1GB and has 90k lines where each line is a sequence of 4k words. \r\nDuring training, the memory usage increased fast as the following graph and resulted in OOM before the finish of training.\r\n\r\n![image](https://user-images.githubusercontent.com/29704017/93292112-5576b280-f817-11ea-8da2-b2db9bf35665.png)\r\n\r\nCould you please give me any suggestions on why this happened and how to fix it?\r\nThanks. ",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 702358124,
    "title": "Fix typos in the loading datasets docs",
    "dateCreated": "2020-09-16T00:27:41Z",
    "dateModified": "2020-09-16T00:27:41Z",
    "description": "This PR fixes two typos in the loading datasets docs, one of them being a broken link to the `load_dataset` function.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 701711255,
    "title": "Fix text delimiter",
    "dateCreated": "2020-09-15T08:08:42Z",
    "dateModified": "2020-09-15T08:08:42Z",
    "description": "I changed the delimiter in the `text` dataset script.\r\nIt should fix the `pyarrow.lib.ArrowInvalid: CSV parse error` from #622 \r\n\r\nI changed the delimiter to an unused ascii character that is not present in text files : `\\b`",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 701636350,
    "title": "Text dataset not working with large files",
    "dateCreated": "2020-09-15T06:02:36Z",
    "dateModified": "2020-09-15T06:02:36Z",
    "description": "```\r\nTraceback (most recent call last):\r\n  File \"examples/language-modeling/run_language_modeling.py\", line 333, in <module>\r\n    main()\r\n  File \"examples/language-modeling/run_language_modeling.py\", line 262, in main\r\n    get_dataset(data_args, tokenizer=tokenizer, cache_dir=model_args.cache_dir) if training_args.do_train else None\r\n  File \"examples/language-modeling/run_language_modeling.py\", line 144, in get_dataset\r\n    dataset = load_dataset(\"text\", data_files=file_path, split='train+test')\r\n  File \"/home/ksjae/.local/lib/python3.7/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/home/ksjae/.local/lib/python3.7/site-packages/datasets/builder.py\", line 469, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/ksjae/.local/lib/python3.7/site-packages/datasets/builder.py\", line 546, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/ksjae/.local/lib/python3.7/site-packages/datasets/builder.py\", line 888, in _prepare_split\r\n    for key, table in utils.tqdm(generator, unit=\" tables\", leave=False, disable=not_verbose):\r\n  File \"/home/ksjae/.local/lib/python3.7/site-packages/tqdm/std.py\", line 1129, in __iter__\r\n    for obj in iterable:\r\n  File \"/home/ksjae/.cache/huggingface/modules/datasets_modules/datasets/text/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7/text.py\", line 104, in _generate_tables\r\n    convert_options=self.config.convert_options,\r\n  File \"pyarrow/_csv.pyx\", line 714, in pyarrow._csv.read_csv\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\n```\r\n\r\n**pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries (try to increase block size?)**\r\n\r\nIt gives the same message for both 200MB, 10GB .tx files but not for 700MB file.\r\nCan't upload due to size & copyright problem. sorry.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 701517550,
    "title": "straddling object straddles two block boundaries",
    "dateCreated": "2020-09-15T00:30:46Z",
    "dateModified": "2020-09-15T00:30:46Z",
    "description": "I am trying to read json data (it's an array with lots of dictionaries) and getting block boundaries issue as below : \r\n\r\nI tried calling read_json with readOptions but no luck .\r\n\r\n```\r\ntable = json.read_json(fn)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pyarrow/_json.pyx\", line 246, in pyarrow._json.read_json\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries (try to increase block size?)\r\n```\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 701496053,
    "title": "Update docs links in the contribution guideline",
    "dateCreated": "2020-09-14T23:27:19Z",
    "dateModified": "2020-09-14T23:27:19Z",
    "description": "Fixed the `add a dataset` and `share a dataset` links in the contribution guideline to refer to the new docs website.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 701411661,
    "title": "fix (#619) MLQA features names",
    "dateCreated": "2020-09-14T20:41:59Z",
    "dateModified": "2020-09-14T20:41:59Z",
    "description": "Fixed the features names as suggested in (#619) in the `_generate_examples` and `_info` methods in the MLQA loading script and also changed the names in the `dataset_infos.json` file.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 701352605,
    "title": "Update GLUE URLs (now hosted on FB)",
    "dateCreated": "2020-09-14T19:05:39Z",
    "dateModified": "2020-09-14T19:05:39Z",
    "description": "NYU is switching dataset hosting from Google to FB. This PR closes https://github.com/huggingface/datasets/issues/608 and is necessary for https://github.com/jiant-dev/jiant/issues/161. This PR updates the data URLs based on changes made in https://github.com/nyu-mll/jiant/pull/1112.\r\n\r\nNote: rebased on huggingface/datasets",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 701057799,
    "title": "dtype of tensors should be preserved",
    "dateCreated": "2020-09-14T12:38:05Z",
    "dateModified": "2020-09-14T12:38:05Z",
    "description": "After switching to `datasets` my model just broke. After a weekend of debugging, the issue was that my model could not handle the double that the Dataset provided, as it expected a float (but didn't give a warning, which seems a [PyTorch issue](https://discuss.pytorch.org/t/is-it-required-that-input-and-hidden-for-gru-have-the-same-dtype-float32/96221)). \r\n\r\nAs a user I did not expect this bug. I have a `map` function that I call on the Dataset that looks like this:\r\n\r\n```python\r\ndef preprocess(sentences: List[str]):\r\n    token_ids = [[vocab.to_index(t) for t in s.split()] for s in sentences]\r\n\r\n    sembeddings = stransformer.encode(sentences)\r\n    print(sembeddings.dtype)\r\n    return {\"input_ids\": token_ids, \"sembedding\": sembeddings}\r\n```\r\n\r\nGiven a list of `sentences` (`List[str]`), it converts those into token_ids on the one hand (list of lists of ints; `List[List[int]]`) and into sentence embeddings on the other (Tensor of dtype `torch.float32`). That means that I actually set the column \"sembedding\" to a tensor that I as a user expect to be a float32.\r\n\r\nIt appears though that behind the scenes, this tensor is converted into a **list**. I did not find this documented anywhere but I might have missed it. From a user's perspective this is incredibly important though, because it means you cannot do any data_type or tensor casting yourself in a mapping function! Furthermore, this can lead to issues, as was my case. \r\n\r\nMy model expected float32 precision, which I thought `sembedding` was because that is what `stransformer.encode` outputs. But behind the scenes this tensor is first cast to a list, and when we then set its format, as below, this column is cast not to float32 but to double precision float64.\r\n\r\n```python\r\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"sembedding\"])\r\n```\r\n\r\nThis happens because apparently there is an intermediate step of casting to a **numpy** array (?) **whose dtype creation/deduction is different from torch dtypes** (see the snippet below).  As you can see, this means that the dtype is not preserved: if I got it right, the dataset goes from torch.float32 -> list -> float64 (numpy) -> torch.float64. \r\n\r\n```python\r\nimport torch\r\nimport numpy as np\r\n\r\nl = [-0.03010837361216545, -0.035979013890028, -0.016949838027358055]\r\ntorch_tensor = torch.tensor(l)\r\nnp_array = np.array(l)\r\nnp_to_torch = torch.from_numpy(np_array)\r\n\r\nprint(torch_tensor.dtype)\r\n# torch.float32\r\nprint(np_array.dtype)\r\n# float64\r\nprint(np_to_torch.dtype)\r\n# torch.float64\r\n```\r\n\r\nThis might lead to unwanted behaviour. I understand that the whole library is probably built around casting from numpy to other frameworks, so this might be difficult to solve. Perhaps `set_format` should include a `dtypes` option where for each input column the user can specify the wanted precision.\r\n\r\nThe alternative is that the user needs to cast manually after loading data from the dataset but that does not seem user-friendly, makes the dataset less portable, and might use more space in memory as well as on disk than is actually needed.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 700541628,
    "title": "Add learningq dataset",
    "dateCreated": "2020-09-13T10:20:27Z",
    "dateModified": "2020-09-13T10:20:27Z",
    "description": "Hi, \r\n\r\nThank you again for this amazing repo. \r\n\r\nWould it be possible for y'all to add the LearningQ dataset - https://github.com/AngusGLChen/LearningQ ? \r\n\r\n",
    "status": "open",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 700235308,
    "title": "Custom feature types in `load_dataset` from CSV",
    "dateCreated": "2020-09-12T13:21:34Z",
    "dateModified": "2020-09-12T13:21:34Z",
    "description": "I am trying to load a local file with the `load_dataset` function and I want to predefine the feature types with the `features` argument. However, the types are always the same independent of the value of `features`. \r\n\r\nI am working with the local files from the emotion dataset. To get the data you can use the following code:\r\n\r\n```Python\r\nfrom pathlib import Path\r\nimport wget\r\n\r\nEMOTION_PATH = Path(\"./data/emotion\")\r\nDOWNLOAD_URLS = [\r\n    \"https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt?dl=1\",\r\n    \"https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt?dl=1\",\r\n    \"https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt?dl=1\",\r\n]\r\n\r\nif not Path.is_dir(EMOTION_PATH):\r\n     Path.mkdir(EMOTION_PATH)\r\nfor url in DOWNLOAD_URLS:\r\n     wget.download(url, str(EMOTION_PATH))\r\n```\r\n\r\nThe first five lines of the train set are:\r\n```\r\ni didnt feel humiliated;sadness\r\ni can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake;sadness\r\nim grabbing a minute to post i feel greedy wrong;anger\r\ni am ever feeling nostalgic about the fireplace i will know that it is still on the property;love\r\ni am feeling grouchy;anger\r\n```\r\n\r\nHere the code to reproduce the issue:\r\n```Python\r\nfrom datasets import Features, Value, ClassLabel, load_dataset\r\n\r\nclass_names = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\r\nemotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})\r\nfile_dict = {'train': EMOTION_PATH/'train.txt'}\r\n\r\ndataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)\r\n```\r\n\r\n**Observed behaviour:**\r\n```Python\r\ndataset['train'].features\r\n```\r\n```Python\r\n{'text': Value(dtype='string', id=None),\r\n 'label': Value(dtype='string', id=None)}\r\n```\r\n**Expected behaviour:**\r\n```Python\r\ndataset['train'].features\r\n```\r\n```Python\r\n{'text': Value(dtype='string', id=None),\r\n 'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], names_file=None, id=None)}\r\n```\r\n\r\n**Things I've tried:**\r\n- deleting the cache\r\n- trying other types such as `int64`\r\n\r\nAm I missing anything? Thanks for any pointer in the right direction.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 700225826,
    "title": "load_dataset for text files not working",
    "dateCreated": "2020-09-12T12:49:28Z",
    "dateModified": "2020-09-12T12:49:28Z",
    "description": "Trying the following snippet, I get different problems on Linux and Windows.\r\n\r\n\r\n```python\r\ndataset = load_dataset(\"text\", data_files=\"data.txt\")\r\n# or \r\ndataset = load_dataset(\"text\", data_files=[\"data.txt\"])\r\n```\r\n\r\n(ps [This example](https://huggingface.co/docs/datasets/loading_datasets.html#json-files) shows that you can use a string as input for data_files, but the signature is `Union[Dict, List]`.)\r\n\r\nThe problem on Linux is that the script crashes with a CSV error (even though it isn't a CSV file). On Windows the script just seems to freeze or get stuck after loading the config file.\r\n\r\nLinux stack trace:\r\n```\r\nPyTorch version 1.6.0+cu101 available.\r\nChecking /home/bram/.cache/huggingface/datasets/b1d50a0e74da9a7b9822cea8ff4e4f217dd892e09eb14f6274a2169e5436e2ea.30c25842cda32b0540d88b7195147decf9671ee442f4bc2fb6ad74016852978e.py for additional imports.\r\nFound main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at /home/bram/.cache/huggingface/modules/datasets_modules/datasets/text\r\nFound specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at /home/bram/.cache/huggingface/modules/datasets_modules/datasets/text/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7\r\nFound script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py to /home/bram/.cache/huggingface/modules/datasets_modules/datasets/text/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7/text.py\r\nCouldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/dataset_infos.json\r\nFound metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at /home/bram/.cache/huggingface/modules/datasets_modules/datasets/text/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7/text.json\r\nUsing custom data configuration default\r\nGenerating dataset text (/home/bram/.cache/huggingface/datasets/text/default-0907112cc6cd2a38/0.0.0/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7)\r\nDownloading and preparing dataset text/default-0907112cc6cd2a38 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/bram/.cache/huggingface/datasets/text/default-0907112cc6cd2a38/0.0.0/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7...\r\nDataset not on Hf google storage. Downloading and preparing it from source\r\nDownloading took 0.0 min\r\nChecksum Computation took 0.0 min\r\nUnable to verify checksums.\r\nGenerating split train\r\nTraceback (most recent call last):\r\n  File \"/home/bram/Python/projects/dutch-simplification/utils.py\", line 45, in prepare_data\r\n    dataset = load_dataset(\"text\", data_files=dataset_f)\r\n  File \"/home/bram/.local/share/virtualenvs/dutch-simplification-NcpPZtDF/lib/python3.8/site-packages/datasets/load.py\", line 608, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/bram/.local/share/virtualenvs/dutch-simplification-NcpPZtDF/lib/python3.8/site-packages/datasets/builder.py\", line 468, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/bram/.local/share/virtualenvs/dutch-simplification-NcpPZtDF/lib/python3.8/site-packages/datasets/builder.py\", line 546, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/bram/.local/share/virtualenvs/dutch-simplification-NcpPZtDF/lib/python3.8/site-packages/datasets/builder.py\", line 888, in _prepare_split\r\n    for key, table in utils.tqdm(generator, unit=\" tables\", leave=False, disable=not_verbose):\r\n  File \"/home/bram/.local/share/virtualenvs/dutch-simplification-NcpPZtDF/lib/python3.8/site-packages/tqdm/std.py\", line 1130, in __iter__\r\n    for obj in iterable:\r\n  File \"/home/bram/.cache/huggingface/modules/datasets_modules/datasets/text/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7/text.py\", line 100, in _generate_tables\r\n    pa_table = pac.read_csv(\r\n  File \"pyarrow/_csv.pyx\", line 714, in pyarrow._csv.read_csv\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 1 columns, got 2\r\n```\r\n\r\nWindows just seems to get stuck. Even with a tiny dataset of 10 lines, it has been stuck for 15 minutes already at this message:\r\n\r\n```\r\nChecking C:\\Users\\bramv\\.cache\\huggingface\\datasets\\b1d50a0e74da9a7b9822cea8ff4e4f217dd892e09eb14f6274a2169e5436e2ea.30c25842cda32b0540d88b7195147decf9671ee442f4bc2fb6ad74016852978e.py for additional imports.\r\nFound main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at C:\\Users\\bramv\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\text\r\nFound specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at C:\\Users\\bramv\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\text\\7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7\r\nFound script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py to C:\\Users\\bramv\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\text\\7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7\\text.py\r\nCouldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text\\dataset_infos.json\r\nFound metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at C:\\Users\\bramv\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\text\\7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7\\text.json\r\nUsing custom data configuration default\r\n```\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 700171097,
    "title": "[docs] Index: The native emoji looks kinda ugly in large size",
    "dateCreated": "2020-09-12T09:48:40Z",
    "dateModified": "2020-09-12T09:48:40Z",
    "description": "",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 699815135,
    "title": "map/filter multiprocessing raises errors and corrupts datasets",
    "dateCreated": "2020-09-11T22:30:06Z",
    "dateModified": "2020-09-11T22:30:06Z",
    "description": "After upgrading to the 1.0 started seeing errors in my data loading script after enabling multiprocessing.\r\n\r\n```python\r\n    ...\r\n    ner_ds_dict = ner_ds.train_test_split(test_size=test_pct, shuffle=True, seed=seed)\r\n    ner_ds_dict[\"validation\"] = ner_ds_dict[\"test\"]\r\n    rel_ds_dict = rel_ds.train_test_split(test_size=test_pct, shuffle=True, seed=seed)\r\n    rel_ds_dict[\"validation\"] = rel_ds_dict[\"test\"]\r\n    return ner_ds_dict, rel_ds_dict\r\n```\r\n\r\nThe first train_test_split, `ner_ds`/`ner_ds_dict`, returns a `train` and `test` split that are iterable.\r\nThe second, `rel_ds`/`rel_ds_dict` in this case, returns a Dataset dict that has rows but if selected from or sliced into into returns an empty dictionary. eg `rel_ds_dict['train'][0] == {}` and `rel_ds_dict['train'][0:100] == {}`.\r\n\r\nOk I think I know the problem -- the rel_ds was mapped though a mapper with `num_proc=12`. If I remove `num_proc`. The dataset loads.\r\n\r\nI also see errors with other map and filter functions when `num_proc` is set.\r\n\r\n```\r\nDone writing 67 indices in 536 bytes .\r\nDone writing 67 indices in 536 bytes .\r\nFatal Python error: PyCOND_WAIT(gil_cond) failed\r\n```",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 699733612,
    "title": "Mistakes in MLQA features names",
    "dateCreated": "2020-09-11T20:46:23Z",
    "dateModified": "2020-09-11T20:46:23Z",
    "description": "I think the following features in MLQA shouldn't be named the way they are:\r\n1. `questions` (should be `question`)\r\n2. `ids` (should be `id`)\r\n3. `start` (should be `answer_start`)\r\n\r\nThe reasons I'm suggesting these features be renamed are:\r\n* To make them consistent with other QA datasets like SQuAD, XQuAD, TyDiQA etc. and hence make it easier to concatenate multiple QA datasets.\r\n* The features names are not the same as the ones provided in the original MLQA datasets (it uses the names I suggested).\r\n\r\nI know these columns can be renamed using  using `Dataset.rename_column_`, `questions` and `ids` can be easily renamed but `start` on the other hand is annoying to rename since it's nested inside the feature `answers`.\r\n",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 699684831,
    "title": "sync logging utils with transformers",
    "dateCreated": "2020-09-11T19:46:13Z",
    "dateModified": "2020-09-11T19:46:13Z",
    "description": "sync the docs/code with the recent changes in transformers' `logging` utils:\r\n1. change the default level to `WARNING`\r\n2. add `DATASETS_VERBOSITY` env var\r\n3. expand docs",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 699472596,
    "title": "Compare different Rouge implementations  ",
    "dateCreated": "2020-09-11T15:49:32Z",
    "dateModified": "2020-09-11T15:49:32Z",
    "description": "I used RougeL implementation provided in `datasets` [here](https://github.com/huggingface/datasets/blob/master/metrics/rouge/rouge.py) and it gives numbers that match those reported in the pegasus paper but very different from those reported in other papers, [this](https://arxiv.org/pdf/1909.03186.pdf) for example.\r\nCan you make sure the google-research implementation you are using matches the official perl implementation? \r\nThere are a couple of python wrappers around the perl implementation, [this](https://pypi.org/project/pyrouge/) has been commonly used, and [this](https://github.com/pltrdy/files2rouge) is used in fairseq). \r\nThere's also a python reimplementation [here](https://github.com/pltrdy/rouge) but its RougeL numbers are way off. \r\n",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 699462293,
    "title": "UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors",
    "dateCreated": "2020-09-11T15:39:16Z",
    "dateModified": "2020-09-11T15:39:16Z",
    "description": "I am trying out the library and want to load in pickled data with `from_dict`. In that dict, one column `text` should be tokenized and the other (an embedding vector) should be retained. All other columns should be removed. When I eventually try to set the format for the columns with `set_format` I am getting this strange Userwarning without a stack trace:\r\n\r\n> Set __getitem__(key) output type to torch for ['input_ids', 'sembedding'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\r\n> C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\datasets\\arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\r\n>   return torch.tensor(x, **format_kwargs)\r\n\r\nThe first one might not be related to the warning, but it is odd that it is shown, too. It is unclear whether that is something that I should do or something that that the program is doing at that moment.\r\n\r\nSnippet:\r\n```\r\n    dataset = Dataset.from_dict(torch.load(\"data/dummy.pt.pt\"))\r\n    print(dataset)\r\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\r\n    keys_to_retain = {\"input_ids\", \"sembedding\"}\r\n    dataset = dataset.map(lambda example: tokenizer(example[\"text\"], padding='max_length'), batched=True)\r\n    dataset.remove_columns_(set(dataset.column_names) - keys_to_retain)\r\n\r\n    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"sembedding\"])\r\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=2)\r\n\r\n    print(next(iter(dataloader)))\r\n```\r\n\r\nPS: the input type for `remove_columns_` should probably be an Iterable rather than just a List.",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 699410773,
    "title": "Offset overflow when slicing a big dataset with an array of indices in Pyarrow >= 1.0.0",
    "dateCreated": "2020-09-11T14:50:38Z",
    "dateModified": "2020-09-11T14:50:38Z",
    "description": "How to reproduce:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nwiki = load_dataset(\"wikipedia\", \"20200501.en\", split=\"train\")\r\nwiki[[0]]\r\n\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\n<ipython-input-13-381aedc9811b> in <module>\r\n----> 1 wikipedia[[0]]\r\n\r\n~/Desktop/hf/nlp/src/datasets/arrow_dataset.py in __getitem__(self, key)\r\n   1069             format_columns=self._format_columns,\r\n   1070             output_all_columns=self._output_all_columns,\r\n-> 1071             format_kwargs=self._format_kwargs,\r\n   1072         )\r\n   1073 \r\n\r\n~/Desktop/hf/nlp/src/datasets/arrow_dataset.py in _getitem(self, key, format_type, format_columns, output_all_columns, format_kwargs)\r\n   1037                 )\r\n   1038             else:\r\n-> 1039                 data_subset = self._data.take(indices_array)\r\n   1040 \r\n   1041             if format_type is not None:\r\n\r\n~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.take()\r\n\r\n~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/compute.py in take(data, indices, boundscheck)\r\n    266     \"\"\"\r\n    267     options = TakeOptions(boundscheck)\r\n--> 268     return call_function('take', [data, indices], options)\r\n    269 \r\n    270 \r\n\r\n~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/_compute.pyx in pyarrow._compute.call_function()\r\n\r\n~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/_compute.pyx in pyarrow._compute.Function.call()\r\n\r\n~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: offset overflow while concatenating arrays\r\n```\r\n\r\nIt seems to work fine with small datasets or with pyarrow 0.17.1",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 699177110,
    "title": "[doc] Update deploy.sh",
    "dateCreated": "2020-09-11T11:06:13Z",
    "dateModified": "2020-09-11T11:06:13Z",
    "description": "",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 699117070,
    "title": "Add CoNLL-2003 shared task dataset",
    "dateCreated": "2020-09-11T10:02:30Z",
    "dateModified": "2020-09-11T10:02:30Z",
    "description": "Please consider adding CoNLL-2003 shared task dataset as it's beneficial for token classification tasks. The motivation behind this PR is the [PR](https://github.com/huggingface/transformers/pull/7041) in the transformers project. This dataset would be not only useful for the usual run-of-the-mill NER tasks but also for syntactic chunking and part-of-speech (POS) tagging. ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 699008644,
    "title": "add multi-proc to dataset dict",
    "dateCreated": "2020-09-11T08:18:13Z",
    "dateModified": "2020-09-11T08:18:13Z",
    "description": "Add multi-proc to `DatasetDict`",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 698863988,
    "title": "ArrowCapacityError: List array cannot contain more than 2147483646 child elements, have 2147483648",
    "dateCreated": "2020-09-11T05:29:12Z",
    "dateModified": "2020-09-11T05:29:12Z",
    "description": "Hi, I'm trying to load a dataset from Dataframe, but I get the error:\r\n```bash\r\n---------------------------------------------------------------------------\r\nArrowCapacityError                        Traceback (most recent call last)\r\n<ipython-input-7-146b6b495963> in <module>\r\n----> 1 dataset = Dataset.from_pandas(emb)\r\n\r\n~/miniconda3/envs/dev/lib/python3.7/site-packages/nlp/arrow_dataset.py in from_pandas(cls, df, features, info, split)\r\n    223         info.features = features\r\n    224         pa_table: pa.Table = pa.Table.from_pandas(\r\n--> 225             df=df, schema=pa.schema(features.type) if features is not None else None\r\n    226         )\r\n    227         return cls(pa_table, info=info, split=split)\r\n\r\n~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_pandas()\r\n\r\n~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/pandas_compat.py in dataframe_to_arrays(df, schema, preserve_index, nthreads, columns, safe)\r\n    591         for i, maybe_fut in enumerate(arrays):\r\n    592             if isinstance(maybe_fut, futures.Future):\r\n--> 593                 arrays[i] = maybe_fut.result()\r\n    594 \r\n    595     types = [x.type for x in arrays]\r\n\r\n~/miniconda3/envs/dev/lib/python3.7/concurrent/futures/_base.py in result(self, timeout)\r\n    426                 raise CancelledError()\r\n    427             elif self._state == FINISHED:\r\n--> 428                 return self.__get_result()\r\n    429 \r\n    430             self._condition.wait(timeout)\r\n\r\n~/miniconda3/envs/dev/lib/python3.7/concurrent/futures/_base.py in __get_result(self)\r\n    382     def __get_result(self):\r\n    383         if self._exception:\r\n--> 384             raise self._exception\r\n    385         else:\r\n    386             return self._result\r\n\r\n~/miniconda3/envs/dev/lib/python3.7/concurrent/futures/thread.py in run(self)\r\n     55 \r\n     56         try:\r\n---> 57             result = self.fn(*self.args, **self.kwargs)\r\n     58         except BaseException as exc:\r\n     59             self.future.set_exception(exc)\r\n\r\n~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/pandas_compat.py in convert_column(col, field)\r\n    557 \r\n    558         try:\r\n--> 559             result = pa.array(col, type=type_, from_pandas=True, safe=safe)\r\n    560         except (pa.ArrowInvalid,\r\n    561                 pa.ArrowNotImplementedError,\r\n\r\n~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib.array()\r\n\r\n~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib._ndarray_to_array()\r\n\r\n~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowCapacityError: List array cannot contain more than 2147483646 child elements, have 2147483648\r\n```\r\nMy code is :\r\n```python\r\nfrom nlp import Dataset\r\ndataset = Dataset.from_pandas(emb)\r\n```",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 698349388,
    "title": "Load text file for RoBERTa pre-training. ",
    "dateCreated": "2020-09-10T18:41:38Z",
    "dateModified": "2020-09-10T18:41:38Z",
    "description": "I migrate my question from https://github.com/huggingface/transformers/pull/4009#issuecomment-690039444\r\n\r\nI tried to train a Roberta from scratch using transformers. But I got OOM issues with loading a large text file. \r\nAccording to the suggestion from @thomwolf , I tried to implement `datasets` to load my text file. This test.txt is a simple sample where each line is a sentence.\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('text', data_files='test.txt',cache_dir=\"./\")\r\ndataset.set_format(type='torch',columns=[\"text\"])\r\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\r\nnext(iter(dataloader))\r\n```\r\n\r\nBut dataload cannot yield sample and error is:\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-12-388aca337e2f> in <module>\r\n----> 1 next(iter(dataloader))\r\n\r\n/Library/Python/3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)\r\n    361 \r\n    362     def __next__(self):\r\n--> 363         data = self._next_data()\r\n    364         self._num_yielded += 1\r\n    365         if self._dataset_kind == _DatasetKind.Iterable and \\\r\n\r\n/Library/Python/3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)\r\n    401     def _next_data(self):\r\n    402         index = self._next_index()  # may raise StopIteration\r\n--> 403         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n    404         if self._pin_memory:\r\n    405             data = _utils.pin_memory.pin_memory(data)\r\n\r\n/Library/Python/3.7/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)\r\n     42     def fetch(self, possibly_batched_index):\r\n     43         if self.auto_collation:\r\n---> 44             data = [self.dataset[idx] for idx in possibly_batched_index]\r\n     45         else:\r\n     46             data = self.dataset[possibly_batched_index]\r\n\r\n/Library/Python/3.7/site-packages/torch/utils/data/_utils/fetch.py in <listcomp>(.0)\r\n     42     def fetch(self, possibly_batched_index):\r\n     43         if self.auto_collation:\r\n---> 44             data = [self.dataset[idx] for idx in possibly_batched_index]\r\n     45         else:\r\n     46             data = self.dataset[possibly_batched_index]\r\n\r\nKeyError: 0\r\n```\r\n\r\n`dataset.set_format(type='torch',columns=[\"text\"])` returns a log says:\r\n```\r\nSet __getitem__(key) output type to torch for ['text'] columns (when key is int or slice) and don't output other (un-formatted) columns.\r\n```\r\n\r\nI noticed the dataset is `DatasetDict({'train': Dataset(features: {'text': Value(dtype='string', id=None)}, num_rows: 44)})`.\r\nEach sample can be accessed by `dataset[\"train\"][\"text\"]` instead of `dataset[\"text\"]`. \r\n\r\nCould you please give me any suggestions on how to modify this code to load the text file?\r\n\r\nVersions:\r\nPython version 3.7.3\r\nPyTorch version 1.6.0 \r\nTensorFlow version 2.3.0 \r\ndatasets version: 1.0.1",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 698323989,
    "title": "Update GLUE URLs (now hosted on FB)",
    "dateCreated": "2020-09-10T18:16:32Z",
    "dateModified": "2020-09-10T18:16:32Z",
    "description": "NYU is switching dataset hosting from Google to FB. This PR closes https://github.com/huggingface/datasets/issues/608 and is necessary for https://github.com/jiant-dev/jiant/issues/161. This PR updates the data URLs based on changes made in https://github.com/nyu-mll/jiant/pull/1112.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 698291156,
    "title": "Don't use the old NYU GLUE dataset URLs",
    "dateCreated": "2020-09-10T17:47:02Z",
    "dateModified": "2020-09-10T17:47:02Z",
    "description": "NYU is switching dataset hosting from Google to FB. Initial changes to `datasets` are in https://github.com/jeswan/nlp/commit/b7d4a071d432592ded971e30ef73330529de25ce. What tests do you suggest I run before opening a PR?\r\n\r\nSee: https://github.com/jiant-dev/jiant/issues/161 and https://github.com/nyu-mll/jiant/pull/1112",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 698094442,
    "title": "Add transmit_format wrapper and tests",
    "dateCreated": "2020-09-10T15:03:50Z",
    "dateModified": "2020-09-10T15:03:50Z",
    "description": "Same as #605 but using a decorator on-top of dataset transforms that are not in place",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 698050442,
    "title": "Quick fix :)",
    "dateCreated": "2020-09-10T14:32:06Z",
    "dateModified": "2020-09-10T14:32:06Z",
    "description": "`nlp` => `datasets`",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 697887401,
    "title": "[Datasets] Transmit format to children",
    "dateCreated": "2020-09-10T12:30:18Z",
    "dateModified": "2020-09-10T12:30:18Z",
    "description": "Transmit format to children obtained when processing a dataset.\r\n\r\nAdded a test.\r\n\r\nWhen concatenating datasets, if the formats are disparate, the concatenated dataset has a format reset to defaults.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 697774581,
    "title": "Update bucket prefix",
    "dateCreated": "2020-09-10T11:01:13Z",
    "dateModified": "2020-09-10T11:01:13Z",
    "description": "cc @julien-c ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 697758750,
    "title": "Set scripts version to master",
    "dateCreated": "2020-09-10T10:47:44Z",
    "dateModified": "2020-09-10T10:47:44Z",
    "description": "By default the scripts version is master, so that if the library is installed with \r\n```\r\npip install git+http://github.com/huggingface/nlp.git\r\n```\r\nor\r\n```\r\ngit clone http://github.com/huggingface/nlp.git\r\npip install -e ./nlp\r\n```\r\n\r\nwill use the latest scripts, and not the ones from the previous version.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 697636605,
    "title": "apply offset to indices in multiprocessed map",
    "dateCreated": "2020-09-10T08:54:30Z",
    "dateModified": "2020-09-10T08:54:30Z",
    "description": "Fix #597 \r\n\r\nI fixed the indices by applying an offset.\r\nI added the case to our tests to make sure it doesn't happen again.\r\n\r\nI also added the message proposed by @thomwolf in #597 \r\n\r\n```python\r\n>>> d.select(range(10)).map(fn, with_indices=True, batched=True, num_proc=2, load_from_cache_file=False)\r\nDone writing 10 indices in 80 bytes .\r\nTesting the mapped function outputs\r\n[0, 1]\r\nTesting finished, running the mapping function on the dataset\r\nDone writing 5 indices in 41 bytes .\r\nDone writing 5 indices in 41 bytes .\r\nSpawning 2 processes\r\n[0, 1, 2, 3, 4]\r\n[5, 6, 7, 8, 9]\r\n#0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 377.90ba/s]\r\n#1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 378.92ba/s]\r\nConcatenating 2 shards from multiprocessing\r\n\r\n# Dataset(features: {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None), 'text': Value(dtype='string', id=None)}, num_rows: 10)\r\n```",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 697574848,
    "title": "check if trasnformers has PreTrainedTokenizerBase",
    "dateCreated": "2020-09-10T07:54:56Z",
    "dateModified": "2020-09-10T07:54:56Z",
    "description": "Fix #598 ",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 697496913,
    "title": "Pickling error when loading dataset",
    "dateCreated": "2020-09-10T06:28:08Z",
    "dateModified": "2020-09-10T06:28:08Z",
    "description": "Hi,\r\n\r\nI modified line 136 in the original [run_language_modeling.py](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) as:\r\n\r\n```\r\n# line 136: return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=args.block_size)\r\ndataset = load_dataset(\"text\", data_files=file_path, split=\"train\")\r\ndataset = dataset.map(lambda ex: tokenizer(ex[\"text\"], add_special_tokens=True,\r\n                                        truncation=True, max_length=args.block_size), batched=True)\r\ndataset.set_format(type='torch', columns=['input_ids'])\r\nreturn dataset\r\n```\r\n\r\nWhen I run this with transformers (3.1.0) and nlp (0.4.0), I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"src/run_language_modeling.py\", line 319, in <module>\r\n    main()\r\n  File \"src/run_language_modeling.py\", line 248, in main\r\n    get_dataset(data_args, tokenizer=tokenizer, cache_dir=model_args.cache_dir) if training_args.do_train else None\r\n  File \"src/run_language_modeling.py\", line 139, in get_dataset\r\n    dataset = dataset.map(lambda ex: tokenizer(ex[\"text\"], add_special_tokens=True, truncation=True, max_length=args.block_size), batched=True)\r\n  File \"/data/nlp/src/nlp/arrow_dataset.py\", line 1136, in map\r\n    new_fingerprint=new_fingerprint,\r\n  File \"/data/nlp/src/nlp/fingerprint.py\", line 158, in wrapper\r\n    self._fingerprint, transform, kwargs_for_fingerprint\r\n  File \"/data/nlp/src/nlp/fingerprint.py\", line 105, in update_fingerprint\r\n    hasher.update(transform_args[key])\r\n  File \"/data/nlp/src/nlp/fingerprint.py\", line 57, in update\r\n    self.m.update(self.hash(value).encode(\"utf-8\"))\r\n  File \"/data/nlp/src/nlp/fingerprint.py\", line 53, in hash\r\n    return cls.hash_default(value)\r\n  File \"/data/nlp/src/nlp/fingerprint.py\", line 46, in hash_default\r\n    return cls.hash_bytes(dumps(value))\r\n  File \"/data/nlp/src/nlp/utils/py_utils.py\", line 362, in dumps\r\n    dump(obj, file)\r\n  File \"/data/nlp/src/nlp/utils/py_utils.py\", line 339, in dump\r\n    Pickler(file, recurse=True).dump(obj)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/site-packages/dill/_dill.py\", line 446, in dump\r\n    StockPickler.dump(self, obj)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 409, in dump\r\n    self.save(obj)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/site-packages/dill/_dill.py\", line 1438, in save_function\r\n    obj.__dict__, fkwdefaults), obj=obj)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 610, in save_reduce\r\n    save(args)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 751, in save_tuple\r\n    save(element)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 736, in save_tuple\r\n    save(element)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/site-packages/dill/_dill.py\", line 1170, in save_cell\r\n    pickler.save_reduce(_create_cell, (f,), obj=obj)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 610, in save_reduce\r\n    save(args)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 736, in save_tuple\r\n    save(element)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 521, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 605, in save_reduce\r\n    save(cls)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/site-packages/dill/_dill.py\", line 1365, in save_type\r\n    obj.__bases__, _dict), obj=obj)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 610, in save_reduce\r\n    save(args)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 751, in save_tuple\r\n    save(element)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/site-packages/dill/_dill.py\", line 933, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/site-packages/dill/_dill.py\", line 933, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 507, in save\r\n    self.save_global(obj, rv)\r\n  File \"/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py\", line 927, in save_global\r\n    (obj, module_name, name))\r\n_pickle.PicklingError: Can't pickle typing.Union[str, NoneType]: it's not the same object as typing.Union\r\n```",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 697377786,
    "title": "Add MATINF dataset",
    "dateCreated": "2020-09-10T03:31:09Z",
    "dateModified": "2020-09-10T03:31:09Z",
    "description": "@lhoestq The command to create metadata failed. I guess it's because the zip is not downloaded from a remote address? How to solve that? Also the CI fails and I don't know how to fix that :(",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 697156501,
    "title": "The current version of the package on github has an error when loading dataset",
    "dateCreated": "2020-09-09T21:03:23Z",
    "dateModified": "2020-09-09T21:03:23Z",
    "description": "Instead of downloading the package from pip, downloading the version from source will result in an error when loading dataset (the pip version is completely fine):\r\n\r\nTo recreate the error: \r\nFirst, installing nlp directly from source:\r\n```\r\ngit clone https://github.com/huggingface/nlp.git\r\ncd nlp\r\npip install -e .\r\n```\r\nThen run:\r\n```\r\nfrom nlp import load_dataset\r\ndataset = load_dataset('wikitext', 'wikitext-2-v1',split = 'train') \r\n```\r\nwill give error:\r\n\r\n```\r\n>>> dataset = load_dataset('wikitext', 'wikitext-2-v1',split = 'train')\r\nChecking /home/zeyuy/.cache/huggingface/datasets/84a754b488511b109e2904672d809c041008416ae74e38f9ee0c80a8dffa1383.2e21f48d63b5572d19c97e441fbb802257cf6a4c03fbc5ed8fae3d2c2273f59e.py for additional imports.\r\nFound main folder for dataset https://raw.githubusercontent.com/huggingface/nlp/0.4.0/datasets/wikitext/wikitext.py at /home/zeyuy/.cache/huggingface/modules/nlp_modules/datasets/wikitext\r\nFound specific version folder for dataset https://raw.githubusercontent.com/huggingface/nlp/0.4.0/datasets/wikitext/wikitext.py at /home/zeyuy/.cache/huggingface/modules/nlp_modules/datasets/wikitext/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d\r\nFound script file from https://raw.githubusercontent.com/huggingface/nlp/0.4.0/datasets/wikitext/wikitext.py to /home/zeyuy/.cache/huggingface/modules/nlp_modules/datasets/wikitext/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d/wikitext.py\r\nFound dataset infos file from https://raw.githubusercontent.com/huggingface/nlp/0.4.0/datasets/wikitext/dataset_infos.json to /home/zeyuy/.cache/huggingface/modules/nlp_modules/datasets/wikitext/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d/dataset_infos.json\r\nFound metadata file for dataset https://raw.githubusercontent.com/huggingface/nlp/0.4.0/datasets/wikitext/wikitext.py at /home/zeyuy/.cache/huggingface/modules/nlp_modules/datasets/wikitext/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d/wikitext.json\r\nLoading Dataset Infos from /home/zeyuy/.cache/huggingface/modules/nlp_modules/datasets/wikitext/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d\r\nOverwrite dataset info from restored data version.\r\nLoading Dataset info from /home/zeyuy/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d\r\nReusing dataset wikitext (/home/zeyuy/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d)\r\nConstructing Dataset for split train, from /home/zeyuy/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/load.py\", line 600, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications)\r\n  File \"/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/builder.py\", line 611, in as_dataset\r\n    datasets = utils.map_nested(\r\n  File \"/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/utils/py_utils.py\", line 216, in map_nested\r\n    return function(data_struct)\r\n  File \"/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/builder.py\", line 631, in _build_single_dataset\r\n    ds = self._as_dataset(\r\n  File \"/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/builder.py\", line 704, in _as_dataset\r\n    return Dataset(**dataset_kwargs)\r\n  File \"/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/arrow_dataset.py\", line 188, in __init__\r\n    self._fingerprint = generate_fingerprint(self)\r\n  File \"/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/fingerprint.py\", line 91, in generate_fingerprint\r\n    hasher.update(key)\r\n  File \"/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/fingerprint.py\", line 57, in update\r\n    self.m.update(self.hash(value).encode(\"utf-8\"))\r\n  File \"/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/fingerprint.py\", line 53, in hash\r\n    return cls.hash_default(value)\r\n  File \"/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/fingerprint.py\", line 46, in hash_default\r\n    return cls.hash_bytes(dumps(value))\r\n  File \"/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/utils/py_utils.py\", line 361, in dumps\r\n    with _no_cache_fields(obj):\r\n  File \"/home/zeyuy/miniconda3/lib/python3.8/contextlib.py\", line 113, in __enter__\r\n    return next(self.gen)\r\n  File \"/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/utils/py_utils.py\", line 348, in _no_cache_fields\r\n    if isinstance(obj, tr.PreTrainedTokenizerBase) and hasattr(obj, \"cache\") and isinstance(obj.cache, dict):\r\nAttributeError: module 'transformers' has no attribute 'PreTrainedTokenizerBase'\r\n\r\n```\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 697112029,
    "title": "Indices incorrect with multiprocessing",
    "dateCreated": "2020-09-09T19:50:56Z",
    "dateModified": "2020-09-09T19:50:56Z",
    "description": "When `num_proc` > 1, the indices argument passed to the map function is incorrect:\r\n\r\n```python\r\nd = load_dataset('imdb', split='test[:1%]')\r\n\r\ndef fn(x, inds):\r\n    print(inds)\r\n    return x\r\n\r\nd.select(range(10)).map(fn, with_indices=True, batched=True)\r\n# [0, 1]\r\n# [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n\r\nd.select(range(10)).map(fn, with_indices=True, batched=True, num_proc=2)\r\n# [0, 1]\r\n# [0, 1]\r\n# [0, 1, 2, 3, 4]\r\n# [0, 1, 2, 3, 4]\r\n```\r\n\r\nAs you can see, the subset passed to each thread is indexed from 0 to N which doesn't reflect their positions in `d`.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 696928139,
    "title": "[style/quality] Moving to isort 5.0.0 + style/quality on datasets and metrics",
    "dateCreated": "2020-09-09T15:47:21Z",
    "dateModified": "2020-09-09T15:47:21Z",
    "description": "Move the repo to isort 5.0.0.\r\n\r\nAlso start testing style/quality on datasets and metrics.\r\n\r\nSpecific rule: we allow F401 (unused imports) in metrics to be able to add imports to detect early on missing dependencies.\r\nMaybe we could add this in datasets but while cleaning this I've seen many example of really unused imports in dataset so maybe it's better to have it as a line-by-line nova instead of a general rule like in metrics.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 696892304,
    "title": "`Dataset`/`DatasetDict` has no attribute 'save_to_disk'",
    "dateCreated": "2020-09-09T15:01:52Z",
    "dateModified": "2020-09-09T15:01:52Z",
    "description": "Hi,\r\n\r\nAs the title indicates, both `Dataset` and `DatasetDict` classes don't seem to have the `save_to_disk` method.  While the file [`arrow_dataset.py`](https://github.com/huggingface/nlp/blob/34bf0b03bfe03e7f77b8fec1cd48f5452c4fc7c1/src/nlp/arrow_dataset.py) in the repo here has the method, the file `arrow_dataset.py` which is saved after `pip install nlp -U` in my `conda` environment DOES NOT contain the `save_to_disk` method. I even tried `pip install git+https://github.com/huggingface/nlp.git ` and still no luck. Do I need to install the library in another way?",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 696816893,
    "title": "Fix germeval url",
    "dateCreated": "2020-09-09T13:29:35Z",
    "dateModified": "2020-09-09T13:29:35Z",
    "description": "Continuation of #593 but without the dummy data hack",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 696679182,
    "title": "GermEval 2014: new download urls",
    "dateCreated": "2020-09-09T10:07:29Z",
    "dateModified": "2020-09-09T10:07:29Z",
    "description": "Hi,\r\n\r\nunfortunately, the download links for the GermEval 2014 dataset have changed: they're now located on a Google Drive.\r\n\r\nI changed the URLs and bump version from 1.0.0 to 2.0.0.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 696619986,
    "title": "Test in memory and on disk",
    "dateCreated": "2020-09-09T08:59:30Z",
    "dateModified": "2020-09-09T08:59:30Z",
    "description": "I added test parameters to do every test both in memory and on disk.\r\nI also found a bug in concatenate_dataset thanks to the new tests and fixed it.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 696530413,
    "title": "fix #589 (backward compat)",
    "dateCreated": "2020-09-09T07:33:13Z",
    "dateModified": "2020-09-09T07:33:13Z",
    "description": "Fix #589",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 696501827,
    "title": "The process cannot access the file because it is being used by another process (windows)",
    "dateCreated": "2020-09-09T07:01:36Z",
    "dateModified": "2020-09-09T07:01:36Z",
    "description": "Hi, I consistently get the following error when developing in my PC (windows 10):\r\n\r\n```\r\n    train_dataset = train_dataset.map(convert_to_features, batched=True)\r\n  File \"C:\\Users\\saareliad\\AppData\\Local\\Continuum\\miniconda3\\envs\\py38\\lib\\site-packages\\nlp\\arrow_dataset.py\", line 970, in map\r\n    shutil.move(tmp_file.name, cache_file_name)\r\n  File \"C:\\Users\\saareliad\\AppData\\Local\\Continuum\\miniconda3\\envs\\py38\\lib\\shutil.py\", line 803, in move\r\n    os.unlink(src)\r\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\saareliad\\\\.cache\\\\huggingface\\\\datasets\\\\squad\\\\plain_text\\\\1.0.0\\\\408a8fa46a1e2805445b793f1022e743428ca739a34809fce872f0c7f17b44ab\\\\tmpsau1bep1'\r\n\r\n```",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 696488447,
    "title": "Cannot use nlp.load_dataset text, AttributeError: module 'nlp.utils' has no attribute 'logging'",
    "dateCreated": "2020-09-09T06:46:53Z",
    "dateModified": "2020-09-09T06:46:53Z",
    "description": "\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.7/site-packages/nlp/load.py\", line 533, in load_dataset\r\n    builder_cls = import_main_class(module_path, dataset=True)\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.7/site-packages/nlp/load.py\", line 61, in import_main_class\r\n    module = importlib.import_module(module_path)\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.7/site-packages/nlp/datasets/text/5dc629379536c4037d9c2063e1caa829a1676cf795f8e030cd90a537eba20c08/text.py\", line 9, in <module>\r\n    logger = nlp.utils.logging.get_logger(__name__)\r\nAttributeError: module 'nlp.utils' has no attribute 'logging'\r\n```\r\n\r\nOccurs on the following code, or any code including the load_dataset('text'):\r\n```\r\ndataset = load_dataset(\"text\", data_files=file_path, split=\"train\")\r\ndataset = dataset.map(lambda ex: tokenizer(ex[\"text\"], add_special_tokens=True,\r\n                                        truncation=True, max_length=args.block_size), batched=True)\r\ndataset.set_format(type='torch', columns=['input_ids'])\r\nreturn dataset\r\n```",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 695249809,
    "title": "Support pathlike obj in load dataset ",
    "dateCreated": "2020-09-07T16:13:21Z",
    "dateModified": "2020-09-07T16:13:21Z",
    "description": "Fix #582 \r\n\r\n(I recreated the PR, I got an issue with git)",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 695246018,
    "title": "Support pathlike obj in load dataset",
    "dateCreated": "2020-09-07T16:09:16Z",
    "dateModified": "2020-09-07T16:09:16Z",
    "description": "Fix #582 ",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 695237999,
    "title": "Better message when data files is empty",
    "dateCreated": "2020-09-07T15:59:57Z",
    "dateModified": "2020-09-07T15:59:57Z",
    "description": "Fix #581 ",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 695191209,
    "title": "Fix select for pyarrow < 1.0.0",
    "dateCreated": "2020-09-07T15:02:52Z",
    "dateModified": "2020-09-07T15:02:52Z",
    "description": "Fix #583 ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 695186652,
    "title": "Use github versioning",
    "dateCreated": "2020-09-07T14:58:15Z",
    "dateModified": "2020-09-07T14:58:15Z",
    "description": "Right now dataset scripts and metrics are downloaded from S3 which is in sync with master. It means that it's not currently possible to pin the dataset/metric script version.\r\n\r\nTo fix that I changed the download url from S3 to github, and adding a `version` parameter in `load_dataset` and `load_metric` to pin a certain version of the lib, as in #562 ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 695166265,
    "title": "ArrowIndexError on Dataset.select",
    "dateCreated": "2020-09-07T14:36:29Z",
    "dateModified": "2020-09-07T14:36:29Z",
    "description": "If the indices table consists in several chunks, then `dataset.select` results in an `ArrowIndexError` error for pyarrow < 1.0.0\r\n\r\nExample:\r\n\r\n```python\r\nfrom nlp import load_dataset\r\n\r\nmnli = load_dataset(\"glue\", \"mnli\", split=\"train\")\r\nshuffled = mnli.shuffle(seed=42)\r\nmnli.select(list(range(len(mnli))))\r\n```\r\n\r\nraises:\r\n```python\r\n---------------------------------------------------------------------------\r\nArrowIndexError                           Traceback (most recent call last)\r\n<ipython-input-64-006a5d38d418> in <module>\r\n----> 1 mnli.shuffle(seed=42).select(list(range(len(mnli))))\r\n\r\n~/Desktop/hf/nlp/src/nlp/fingerprint.py in wrapper(*args, **kwargs)\r\n    161             # Call actual function\r\n    162 \r\n--> 163             out = func(self, *args, **kwargs)\r\n    164 \r\n    165             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n~/Desktop/hf/nlp/src/nlp/arrow_dataset.py in select(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\r\n   1653         if self._indices is not None:\r\n   1654             if PYARROW_V0:\r\n-> 1655                 indices_array = self._indices.column(0).chunk(0).take(indices_array)\r\n   1656             else:\r\n   1657                 indices_array = self._indices.column(0).take(indices_array)\r\n\r\n~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib.Array.take()\r\n\r\n~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowIndexError: take index out of bounds\r\n```\r\n\r\nThis is because the `take` method is only done on the first chunk which only contains 1000 elements by default (mnli has ~400 000 elements).\r\n\r\nShall we change that to use \r\n```python\r\npa.concat_tables(self._indices._indices.slice(i, 1) for i in indices_array)\r\n```\r\ninstead of `take` ? @thomwolf ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 695126456,
    "title": "Allow for PathLike objects",
    "dateCreated": "2020-09-07T13:54:51Z",
    "dateModified": "2020-09-07T13:54:51Z",
    "description": "Using PathLike objects as input for `load_dataset` does not seem to work. The following will throw an error.\r\n\r\n```python\r\nfiles = list(Path(r\"D:\\corpora\\yourcorpus\").glob(\"*.txt\"))\r\ndataset = load_dataset(\"text\", data_files=files)\r\n```\r\n\r\nTraceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/dev/python/dutch-simplification/main.py\", line 7, in <module>\r\n    dataset = load_dataset(\"text\", data_files=files)\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\nlp\\load.py\", line 548, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\nlp\\builder.py\", line 470, in download_and_prepare\r\n    self._save_info()\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\nlp\\builder.py\", line 564, in _save_info\r\n    self.info.write_to_directory(self._cache_dir)\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\nlp\\info.py\", line 149, in write_to_directory\r\n    self._dump_info(f)\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\nlp\\info.py\", line 156, in _dump_info\r\n    file.write(json.dumps(asdict(self)).encode(\"utf-8\"))\r\n  File \"c:\\users\\bramv\\appdata\\local\\programs\\python\\python38\\lib\\json\\__init__.py\", line 231, in dumps\r\n    return _default_encoder.encode(obj)\r\n  File \"c:\\users\\bramv\\appdata\\local\\programs\\python\\python38\\lib\\json\\encoder.py\", line 199, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"c:\\users\\bramv\\appdata\\local\\programs\\python\\python38\\lib\\json\\encoder.py\", line 257, in iterencode\r\n    return _iterencode(o, 0)\r\nTypeError: keys must be str, int, float, bool or None, not WindowsPath\r\n```\r\n\r\nWe have to cast to a string explicitly to make this work. It would be nicer if we could actually use PathLike objects.\r\n\r\n```python\r\nfiles = [str(f) for f in Path(r\"D:\\corpora\\wablieft\").glob(\"*.txt\")]\r\n```\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 695120517,
    "title": "Better error message when input file does not exist",
    "dateCreated": "2020-09-07T13:47:59Z",
    "dateModified": "2020-09-07T13:47:59Z",
    "description": "In the following scenario, when `data_files` is an empty list, the stack trace and error message could be improved. This can probably be solved by checking for each file whether it actually exists and/or whether the argument is not false-y.\r\n\r\n```python\r\ndataset = load_dataset(\"text\", data_files=[])\r\n```\r\n\r\nExample error trace.\r\n\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset text/default-d18f9b6611eb8e16 (download: Unknown size, generated: Unknown size, post-processed: Unknown sizetotal: Unknown size) to C:\\Users\\bramv\\.cache\\huggingface\\datasets\\text\\default-d18f9b6611eb8e16\\0.0.0\\3a79870d85f1982d6a2af884fde86a71c771747b4b161fd302d28ad22adf985b...\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\nlp\\builder.py\", line 424, in incomplete_dir\r\n    yield tmp_dir\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\nlp\\builder.py\", line 462, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\nlp\\builder.py\", line 537, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\nlp\\builder.py\", line 813, in _prepare_split\r\n    num_examples, num_bytes = writer.finalize()\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\nlp\\arrow_writer.py\", line 217, in finalize\r\n    self.pa_writer.close()\r\nAttributeError: 'NoneType' object has no attribute 'close'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/dev/python/dutch-simplification/main.py\", line 7, in <module>\r\n    dataset = load_dataset(\"text\", data_files=files)\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\nlp\\load.py\", line 548, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\nlp\\builder.py\", line 470, in download_and_prepare\r\n    self._save_info()\r\n  File \"c:\\users\\bramv\\appdata\\local\\programs\\python\\python38\\lib\\contextlib.py\", line 131, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\dutch-simplification-nbNdqK9u\\lib\\site-packages\\nlp\\builder.py\", line 430, in incomplete_dir\r\n    shutil.rmtree(tmp_dir)\r\n  File \"c:\\users\\bramv\\appdata\\local\\programs\\python\\python38\\lib\\shutil.py\", line 737, in rmtree\r\n    return _rmtree_unsafe(path, onerror)\r\n  File \"c:\\users\\bramv\\appdata\\local\\programs\\python\\python38\\lib\\shutil.py\", line 615, in _rmtree_unsafe\r\n    onerror(os.unlink, fullname, sys.exc_info())\r\n  File \"c:\\users\\bramv\\appdata\\local\\programs\\python\\python38\\lib\\shutil.py\", line 613, in _rmtree_unsafe\r\n    os.unlink(fullname)\r\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\bramv\\\\.cache\\\\huggingface\\\\datasets\\\\text\\\\default-d18f9b6611eb8e16\\\\0.0.0\\\\3a79870d85f1982d6a2af884fde86a71c771747b4b161fd302d28ad22adf985b.incomplete\\\\text-train.arrow'\r\n```",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 694954551,
    "title": "nlp re-creates already-there caches when using a script, but not within a shell",
    "dateCreated": "2020-09-07T10:23:50Z",
    "dateModified": "2020-09-07T10:23:50Z",
    "description": "`nlp` keeps creating new caches for the same file when launching `filter` from a script, and behaves correctly from within the shell.\r\n\r\nExample: try running\r\n\r\n```\r\nimport nlp\r\n\r\nhans_easy_data = nlp.load_dataset('hans', split=\"validation\").filter(lambda x: x['label'] == 0)\r\nhans_hard_data = nlp.load_dataset('hans', split=\"validation\").filter(lambda x: x['label'] == 1)\r\n```\r\n\r\ntwice. If launched from a `file.py` script, the cache will be re-created the second time. If launched as 3 shell/`ipython` commands, `nlp` will correctly re-use the cache.\r\nAs observed with @lhoestq.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 694947599,
    "title": "Doc metrics",
    "dateCreated": "2020-09-07T10:15:24Z",
    "dateModified": "2020-09-07T10:15:24Z",
    "description": "Adding documentation on metrics loading/using/sharing",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 694849940,
    "title": "Add CommonGen Dataset",
    "dateCreated": "2020-09-07T08:17:17Z",
    "dateModified": "2020-09-07T08:17:17Z",
    "description": "CC Authors:\r\n@yuchenlin @MichaelZhouwang",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 694607148,
    "title": "Some languages in wikipedia dataset are not loading",
    "dateCreated": "2020-09-07T01:16:29Z",
    "dateModified": "2020-09-07T01:16:29Z",
    "description": "Hi,\r\n\r\nI am working with the `wikipedia` dataset and I have a script that goes over 92 of the available languages in that dataset. So far I have detected that `ar`, `af`, `an` are not loading. Other languages like `fr` and `en` are working fine. Here's how I am loading them:\r\n\r\n```\r\nimport nlp\r\n\r\nlangs = ['ar'. 'af', 'an']\r\n\r\nfor lang in langs:\r\n    data = nlp.load_dataset('wikipedia', f'20200501.{lang}', beam_runner='DirectRunner', split='train') \r\n    print(lang, len(data))\r\n```\r\n\r\nHere's what I see for 'ar' (it gets stuck there):\r\n```\r\nDownloading and preparing dataset wikipedia/20200501.ar (download: Unknown size, generated: Unknown size, post-processed: Unknown sizetotal: Unknown size) to /home/gaguilar/.cache/huggingface/datasets/wikipedia/20200501.ar/1.0.0/7be7f4324255faf70687be8692de57cf79197afdc33ff08d6a04ed602df32d50...\r\n```\r\n\r\nNote that those languages are indeed in the list of expected languages. Any suggestions on how to work around this? Thanks!",
    "status": "open",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 694348645,
    "title": "Fix the code block in doc",
    "dateCreated": "2020-09-06T11:40:55Z",
    "dateModified": "2020-09-06T11:40:55Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 693691611,
    "title": "Couldn't reach certain URLs and for the ones that can be reached, code just blocks after downloading.",
    "dateCreated": "2020-09-04T21:46:25Z",
    "dateModified": "2020-09-04T21:46:25Z",
    "description": "Hi,\r\n\r\nI'm following the [quick tour](https://huggingface.co/nlp/quicktour.html) and tried to load the glue dataset:\r\n```\r\n>>> from nlp import load_dataset\r\n>>> dataset = load_dataset('glue', 'mrpc', split='train')\r\n```\r\n\r\nHowever, this ran into a `ConnectionError` saying it could not reach the URL (just pasting the last few lines):\r\n```\r\n\r\n/net/vaosl01/opt/NFS/su0/miniconda3/envs/hf/lib/python3.7/site-packages/nlp/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)\r\n    354                 \" to False.\"\r\n    355             )\r\n--> 356         raise ConnectionError(\"Couldn't reach {}\".format(url))\r\n    357 \r\n    358     # From now on, connected is True.\r\n\r\nConnectionError: Couldn't reach https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc\r\n```\r\n\r\nI tried glue with cola and sst2. I got the same error, just instead of mrpc in the URL, it was replaced with cola and sst2.\r\n\r\nSince this was not working, I thought I'll try another dataset. So I tried downloading the imdb dataset:\r\n```\r\nds = load_dataset('imdb', split='train')\r\n```\r\nThis downloads the data, but it just blocks after that:\r\n```\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.56k/4.56k [00:00<00:00, 1.38MB/s]\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.07k/2.07k [00:00<00:00, 1.15MB/s]\r\nDownloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.06 MiB, post-processed: Unknown sizetotal: 207.28 MiB) to /net/vaosl01/opt/NFS/su0/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743...\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 84.1M/84.1M [00:07<00:00, 11.1MB/s]\r\n```\r\n\r\nI checked the folder `$HF_HOME/datasets/downloads/extracted/<id>/aclImdb`. This folder is constantly growing in size. When I navigated to the train folder within, there was no file. However, the test folder seemed to be populating. The last time I checked it was 327M. I thought the Imdb dataset was smaller than that. My questions are:\r\n1. Why is it still blocking? Is it still downloading?\r\n2. I specified split as train, so why is the test folder being populated?\r\n3. I read somewhere that after downloading, `nlp` converts the text files into some sort of `arrow` files, which will also take a while. Is this also happening here?\r\n\r\nThanks.\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 693364853,
    "title": "Add modules cache",
    "dateCreated": "2020-09-04T16:30:03Z",
    "dateModified": "2020-09-04T16:30:03Z",
    "description": "As discusses in #554 , we should use a module cache directory outside of the python packages directory since we may not have write permissions.\r\n\r\nI added a new HF_MODULES_PATH directory that is added to the python path when doing `import nlp`.\r\nIn this directory, a module `nlp_modules` is created so that datasets can be added to `nlp_modules.datasets` and metrics to `nlp_modules.metrics`. `nlp_modules` doesn't exist on Pypi.\r\n\r\nIf someone using cloudpickle still wants to have the downloaded dataset/metrics scripts to be inside the nlp directory, it is still possible to change the environment variable HF_MODULES_CACHE to be a path inside the nlp lib.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 693091790,
    "title": "Faster caching for text dataset",
    "dateCreated": "2020-09-04T11:58:34Z",
    "dateModified": "2020-09-04T11:58:34Z",
    "description": "As mentioned in #546 and #548 , hashing `data_files` contents to get the cache directory name for a text dataset can take a long time.\r\n\r\nTo make it faster I changed the hashing so that it takes into account the `path` and the `last modified timestamp` of each data file, instead of iterating through the content of each file to get a hash.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 692598231,
    "title": "Add CLUE Benchmark (11 datasets)",
    "dateCreated": "2020-09-04T01:57:40Z",
    "dateModified": "2020-09-04T01:57:40Z",
    "description": "Add 11 tasks of [CLUE](https://github.com/CLUEbenchmark/CLUE).",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 692109287,
    "title": "Serialization",
    "dateCreated": "2020-09-03T16:21:38Z",
    "dateModified": "2020-09-03T16:21:38Z",
    "description": "I added `save` and `load` method to serialize/deserialize a dataset object in a folder.\r\nIt moves the arrow files there (or write them if the tables were in memory), and saves the pickle state in a json file `state.json`, except the info that are in a separate file `dataset_info.json`.\r\n\r\nExample:\r\n\r\n```python\r\nimport nlp\r\n\r\nsquad = nlp.load_dataset(\"squad\", split=\"train\")\r\nsquad.save(\"tmp/squad\")\r\nsquad = nlp.Dataset.load(\"tmp/squad\")\r\n```\r\n\r\n`ls tmp/squad`\r\n```\r\ndataset_info.json squad-train.arrow state.json\r\n```\r\n\r\n`cat tmp/squad/state.json`\r\n```json\r\n{\r\n  \"_data\": null,\r\n  \"_data_files\": [\r\n    {\r\n      \"filename\": \"squad-train.arrow\",\r\n      \"skip\": 0,\r\n      \"take\": 87599\r\n    }\r\n  ],\r\n  \"_fingerprint\": \"61f452797a686bc1\",\r\n  \"_format_columns\": null,\r\n  \"_format_kwargs\": {},\r\n  \"_format_type\": null,\r\n  \"_indexes\": {},\r\n  \"_indices\": null,\r\n  \"_indices_data_files\": [],\r\n  \"_inplace_history\": [\r\n    {\r\n      \"transforms\": []\r\n    }\r\n  ],\r\n  \"_output_all_columns\": false,\r\n  \"_split\": \"train\"\r\n}\r\n```\r\n\r\n`cat tmp/squad/dataset_info.json`\r\n```json\r\n{\r\n  \"builder_name\": \"squad\",\r\n  \"citation\": \"@article{2016arXiv160605250R,\\n       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\\n                 Konstantin and {Liang}, Percy},\\n        title = \\\"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\\\",\\n      journal = {arXiv e-prints},\\n         year = 2016,\\n          eid = {arXiv:1606.05250},\\n        pages = {arXiv:1606.05250},\\narchivePrefix = {arXiv},\\n       eprint = {1606.05250},\\n}\\n\",\r\n  \"config_name\": \"plain_text\",\r\n  \"dataset_size\": 89789763,\r\n  \"description\": \"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\\n\",\r\n  \"download_checksums\": {\r\n    \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\": {\r\n      \"checksum\": \"95aa6a52d5d6a735563366753ca50492a658031da74f301ac5238b03966972c9\",\r\n      \"num_bytes\": 4854279\r\n    },\r\n    \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\": {\r\n      \"checksum\": \"3527663986b8295af4f7fcdff1ba1ff3f72d07d61a20f487cb238a6ef92fd955\",\r\n      \"num_bytes\": 30288272\r\n    }\r\n  },\r\n  \"download_size\": 35142551,\r\n  \"features\": {\r\n    \"answers\": {\r\n      \"_type\": \"Sequence\",\r\n      \"feature\": {\r\n        \"answer_start\": {\r\n          \"_type\": \"Value\",\r\n          \"dtype\": \"int32\",\r\n          \"id\": null\r\n        },\r\n        \"text\": {\r\n          \"_type\": \"Value\",\r\n          \"dtype\": \"string\",\r\n          \"id\": null\r\n        }\r\n      },\r\n      \"id\": null,\r\n      \"length\": -1\r\n    },\r\n    \"context\": {\r\n      \"_type\": \"Value\",\r\n      \"dtype\": \"string\",\r\n      \"id\": null\r\n    },\r\n    \"id\": {\r\n      \"_type\": \"Value\",\r\n      \"dtype\": \"string\",\r\n      \"id\": null\r\n    },\r\n    \"question\": {\r\n      \"_type\": \"Value\",\r\n      \"dtype\": \"string\",\r\n      \"id\": null\r\n    },\r\n    \"title\": {\r\n      \"_type\": \"Value\",\r\n      \"dtype\": \"string\",\r\n      \"id\": null\r\n    }\r\n  },\r\n  \"homepage\": \"https://rajpurkar.github.io/SQuAD-explorer/\",\r\n  \"license\": \"\",\r\n  \"post_processed\": {\r\n    \"features\": null,\r\n    \"resources_checksums\": {\r\n      \"train\": {},\r\n      \"train[:10%]\": {}\r\n    }\r\n  },\r\n  \"post_processing_size\": 0,\r\n  \"size_in_bytes\": 124932314,\r\n  \"splits\": {\r\n    \"train\": {\r\n      \"dataset_name\": \"squad\",\r\n      \"name\": \"train\",\r\n      \"num_bytes\": 79317110,\r\n      \"num_examples\": 87599\r\n    },\r\n    \"validation\": {\r\n      \"dataset_name\": \"squad\",\r\n      \"name\": \"validation\",\r\n      \"num_bytes\": 10472653,\r\n      \"num_examples\": 10570\r\n    }\r\n  },\r\n  \"supervised_keys\": null,\r\n  \"version\": {\r\n    \"description\": \"New split API (https://tensorflow.org/datasets/splits)\",\r\n    \"major\": 1,\r\n    \"minor\": 0,\r\n    \"nlp_version_to_prepare\": null,\r\n    \"patch\": 0,\r\n    \"version_str\": \"1.0.0\"\r\n  }\r\n}\r\n```",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 691846397,
    "title": "add reuters21578 dataset",
    "dateCreated": "2020-09-03T10:25:47Z",
    "dateModified": "2020-09-03T10:25:47Z",
    "description": "Reopen a PR this the merge.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 691832720,
    "title": "Revert \"add reuters21578 dataset\"",
    "dateCreated": "2020-09-03T10:06:16Z",
    "dateModified": "2020-09-03T10:06:16Z",
    "description": "Reverts huggingface/nlp#471",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 691638656,
    "title": "`metric.compute` throws `ArrowInvalid` error",
    "dateCreated": "2020-09-03T04:56:57Z",
    "dateModified": "2020-09-03T04:56:57Z",
    "description": "I get the following error with `rouge.compute`. It happens only with distributed training, and it occurs randomly I can't easily reproduce it. This is using `nlp==0.4.0`\r\n\r\n```\r\n  File \"/home/beltagy/trainer.py\", line 92, in validation_step\r\n    rouge_scores = rouge.compute(predictions=generated_str, references=gold_str, rouge_types=['rouge2', 'rouge1', 'rougeL'])\r\n  File \"/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/nlp/metric.py\", line 224, in compute\r\n    self.finalize(timeout=timeout)\r\n  File \"/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/nlp/metric.py\", line 213, in finalize\r\n    self.data = Dataset(**reader.read_files(node_files))\r\n  File \"/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/nlp/arrow_reader.py\", line 217, in read_files\r\n    dataset_kwargs = self._read_files(files=files, info=self._info, original_instructions=original_instructions)\r\n  File \"/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/nlp/arrow_reader.py\", line 162, in _read_files\r\n    pa_table: pa.Table = self._get_dataset_from_filename(f_dict)\r\n  File \"/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/nlp/arrow_reader.py\", line 276, in _get_dataset_from_filename\r\n    f = pa.ipc.open_stream(mmap)\r\n  File \"/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/pyarrow/ipc.py\", line 173, in open_stream\r\n    return RecordBatchStreamReader(source)\r\n  File \"/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/pyarrow/ipc.py\", line 64, in __init__\r\n    self._open(source)\r\n  File \"pyarrow/ipc.pxi\", line 469, in pyarrow.lib._RecordBatchStreamReader._open\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Tried reading schema message, was null or length 0\r\n```",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 691430245,
    "title": "Fix BLEURT metrics for backward compatibility",
    "dateCreated": "2020-09-02T21:22:35Z",
    "dateModified": "2020-09-02T21:22:35Z",
    "description": "Fix #565",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 691160208,
    "title": "Remove logger pickling to fix gg colab issues",
    "dateCreated": "2020-09-02T16:16:21Z",
    "dateModified": "2020-09-02T16:16:21Z",
    "description": "A `logger` objects are not picklable in google colab, contrary to `logger` objects in jupyter notebooks or in python shells.\r\nIt creates some issues in google colab right now.\r\n\r\nIndeed by calling any `Dataset` method, the fingerprint update pickles the transform function, and as the logger comes with it, it results in an error (full stacktrace [here](http://pastebin.fr/64330)):\r\n\r\n```python\r\n/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/socket.cpython-36m-x86_64-linux-gnu.so in zmq.backend.cython.socket.Socket.__reduce_cython__()\r\n\r\nTypeError: no default __reduce__ due to non-trivial __cinit__\r\n```\r\n\r\nTo fix that I no longer dump the transform (`_map_single`, `select`, etc.), but the full name only (`nlp.arrow_dataset.Dataset._map_single`, `nlp.arrow_dataset.Dataset.select`, etc.)",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 691039121,
    "title": "No module named 'nlp.logging'",
    "dateCreated": "2020-09-02T13:49:50Z",
    "dateModified": "2020-09-02T13:49:50Z",
    "description": "Hi, I am using nlp version 0.4.0. Trying to use bleurt as an eval metric, however, the bleurt script imports nlp.logging which creates the following error. What am I missing?\r\n\r\n```\r\n>>> import nlp\r\n2020-09-02 13:47:09.210310: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n>>> bleurt = nlp.load_metric(\"bleurt\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/melody/anaconda3/envs/transformers/lib/python3.6/site-packages/nlp/load.py\", line 443, in load_metric\r\n    metric_cls = import_main_class(module_path, dataset=False)\r\n  File \"/home/melody/anaconda3/envs/transformers/lib/python3.6/site-packages/nlp/load.py\", line 61, in import_main_class\r\n    module = importlib.import_module(module_path)\r\n  File \"/home/melody/anaconda3/envs/transformers/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/melody/anaconda3/envs/transformers/lib/python3.6/site-packages/nlp/metrics/bleurt/43448cf2959ea81d3ae0e71c5c8ee31dc15eed9932f197f5f50673cbcecff2b5/bleurt.py\", line 20, in <module>\r\n    from nlp.logging import get_logger\r\nModuleNotFoundError: No module named 'nlp.logging'\r\n```\r\n\r\nJust to show once again that I can't import the logging module:\r\n\r\n```\r\n>>> import nlp\r\n2020-09-02 13:48:38.190621: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n>>> nlp.__version__\r\n'0.4.0'\r\n>>> from nlp.logging import get_logger\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'nlp.logging'\r\n```",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 691000020,
    "title": "Wait for writing in distributed metrics",
    "dateCreated": "2020-09-02T12:58:50Z",
    "dateModified": "2020-09-02T12:58:50Z",
    "description": "There were CI bugs where a distributed metric would try to read all the files in process 0 while the other processes haven't started writing.\r\n\r\nTo fix that I added a custom locking mechanism that waits for the file to exist before trying to read it",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 690908674,
    "title": "[Large datasets] Speed up download and processing",
    "dateCreated": "2020-09-02T10:31:54Z",
    "dateModified": "2020-09-02T10:31:54Z",
    "description": "Various improvements to speed-up creation and processing of large scale datasets.\r\n\r\nCurrently:\r\n- distributed downloads\r\n- remove etag from datafiles hashes to spare a request when restarting a failed download",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 690907604,
    "title": "[Reproductibility] Allow to pin versions of datasets/metrics",
    "dateCreated": "2020-09-02T10:30:13Z",
    "dateModified": "2020-09-02T10:30:13Z",
    "description": "Repurpose the `version` attribute in datasets and metrics to let the user pin a specific version of datasets and metric scripts:\r\n```\r\ndataset = nlp.load_dataset('squad', version='1.0.0')\r\nmetric = nlp.load_metric('squad', version='1.0.0')\r\n```\r\n\r\nNotes:\r\n- version number are the release version of the library\r\n- currently only possible for canonical datasets/metrics, ie. integrated in the GitHub repo of the library",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 690871415,
    "title": "Made `share_dataset` more readable",
    "dateCreated": "2020-09-02T09:34:48Z",
    "dateModified": "2020-09-02T09:34:48Z",
    "description": "",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 690488764,
    "title": "Using custom DownloadConfig results in an error",
    "dateCreated": "2020-09-01T22:23:02Z",
    "dateModified": "2020-09-01T22:23:02Z",
    "description": "## Version / Environment\r\n\r\nUbuntu 18.04\r\nPython 3.6.8\r\nnlp 0.4.0\r\n\r\n## Description\r\n\r\nLoading `imdb` dataset works fine when when I don't specify any `download_config` argument. When I create a custom `DownloadConfig` object and pass it to the `nlp.load_dataset` function, this results in an error.\r\n\r\n## How to reproduce\r\n\r\n### Example without DownloadConfig --> works\r\n\r\n```python\r\nimport os\r\n\r\nos.environ[\"HF_HOME\"] = \"/data/hf-test-without-dl-config-01/\"\r\n\r\nimport logging\r\nimport nlp\r\n\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\nif __name__ == \"__main__\":\r\n    imdb = nlp.load_dataset(path=\"imdb\")\r\n```\r\n\r\n### Example with DownloadConfig --> doesn't work\r\n\r\n```python\r\nimport os\r\n\r\nos.environ[\"HF_HOME\"] = \"/data/hf-test-with-dl-config-01/\"\r\n\r\nimport logging\r\nimport nlp\r\nfrom nlp.utils import DownloadConfig\r\n\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\nif __name__ == \"__main__\":\r\n    download_config = DownloadConfig()\r\n    imdb = nlp.load_dataset(path=\"imdb\", download_config=download_config)\r\n```\r\n\r\nError traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/.../example_with_dl_config.py\", line 13, in <module>\r\n    imdb = nlp.load_dataset(path=\"imdb\", download_config=download_config)\r\n  File \"/.../python3.6/python3.6/site-packages/nlp/load.py\", line 549, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"/.../python3.6/python3.6/site-packages/nlp/builder.py\", line 463, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/.../python3.6/python3.6/site-packages/nlp/builder.py\", line 518, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/.../python3.6/python3.6/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743/imdb.py\", line 86, in _split_generators\r\n    arch_path = dl_manager.download_and_extract(_DOWNLOAD_URL)\r\n  File \"/.../python3.6/python3.6/site-packages/nlp/utils/download_manager.py\", line 220, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/.../python3.6/python3.6/site-packages/nlp/utils/download_manager.py\", line 158, in download\r\n    self._record_sizes_checksums(url_or_urls, downloaded_path_or_paths)\r\n  File \"/.../python3.6/python3.6/site-packages/nlp/utils/download_manager.py\", line 108, in _record_sizes_checksums\r\n    self._recorded_sizes_checksums[url] = get_size_checksum_dict(path)\r\n  File \"/.../python3.6/python3.6/site-packages/nlp/utils/info_utils.py\", line 79, in get_size_checksum_dict\r\n    with open(path, \"rb\") as f:\r\nIsADirectoryError: [Errno 21] Is a directory: '/data/hf-test-with-dl-config-01/datasets/extracted/b6802c5b61824b2c1f7dbf7cda6696b5f2e22214e18d171ce1ed3be90c931ce5'\r\n```\r\n\r\n",
    "status": "open",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 690411263,
    "title": "Adding the KILT knowledge source and tasks",
    "dateCreated": "2020-09-01T20:05:13Z",
    "dateModified": "2020-09-01T20:05:13Z",
    "description": "This adds Wikipedia pre-processed for KILT, as well as the task data. Only the question IDs are provided for TriviaQA, but they can easily be mapped back with:\r\n```\r\nimport nlp\r\n\r\nkilt_wikipedia = nlp.load_dataset('kilt_wikipedia')\r\n\r\nkilt_tasks = nlp.load_dataset('kilt_tasks')\r\ntriviaqa = nlp.load_dataset('trivia_qa', 'unfiltered.nocontext')\r\ntriviaqa_map = {}\r\nfor k in ['train', 'validation', 'test']:\r\n    triviaqa_map = dict([(q_id, i) for i, q_id in enumerate(triviaqa[k]['question_id'])])\r\n    kilt_tasks[k + '_triviaqa'] = kilt_tasks[k + '_triviaqa'].filter(lambda x: x['id'] in triviaqa_map)\r\n    kilt_tasks[k + '_triviaqa'].map(lambda x: {'input': triviaqa[split][triviaqa_map[x['id']]]['question']})\r\n```\r\n\r\nIt would be great to have the dataset by Monday, which is when the paper should land on Arxiv and @fabiopetroni is planning on tweeting about the paper and `facebookresearch` repository for the datasett",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 690318105,
    "title": "Rerun pip install -e",
    "dateCreated": "2020-09-01T17:24:39Z",
    "dateModified": "2020-09-01T17:24:39Z",
    "description": "Hopefully it fixes the github actions",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 690220135,
    "title": "Fix a few typos",
    "dateCreated": "2020-09-01T15:03:24Z",
    "dateModified": "2020-09-01T15:03:24Z",
    "description": "",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 690218423,
    "title": "Add DailyDialog",
    "dateCreated": "2020-09-01T15:01:15Z",
    "dateModified": "2020-09-01T15:01:15Z",
    "description": "http://yanran.li/dailydialog.html\r\n\r\nhttps://arxiv.org/pdf/1710.03957.pdf\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 690197725,
    "title": "Upgrade pip in benchmark github action",
    "dateCreated": "2020-09-01T14:37:26Z",
    "dateModified": "2020-09-01T14:37:26Z",
    "description": "It looks like it fixes the `import nlp` issue we have",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 690173214,
    "title": "nlp downloads to its module path",
    "dateCreated": "2020-09-01T14:06:14Z",
    "dateModified": "2020-09-01T14:06:14Z",
    "description": "I am trying to package `nlp` for Nix, because it is now an optional dependency for `transformers`. The problem that I encounter is that the `nlp` library downloads to the module path, which is typically not writable in most package management systems:\r\n\r\n```>>> import nlp\r\n>>> squad_dataset = nlp.load_dataset('squad')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/nix/store/2yhik0hhqayksmkkfb0ylqp8cf5wa5wp-python3-3.8.5-env/lib/python3.8/site-packages/nlp/load.py\", line 530, in load_dataset\r\n    module_path, hash = prepare_module(path, download_config=download_config, dataset=True)\r\n  File \"/nix/store/2yhik0hhqayksmkkfb0ylqp8cf5wa5wp-python3-3.8.5-env/lib/python3.8/site-packages/nlp/load.py\", line 329, in prepare_module\r\n    os.makedirs(main_folder_path, exist_ok=True)\r\n  File \"/nix/store/685kq8pyhrvajah1hdsfn4q7gm3j4yd4-python3-3.8.5/lib/python3.8/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nOSError: [Errno 30] Read-only file system: '/nix/store/2yhik0hhqayksmkkfb0ylqp8cf5wa5wp-python3-3.8.5-env/lib/python3.8/site-packages/nlp/datasets/squad'\r\n```\r\n\r\nDo you have any suggested workaround for this issue?\r\n\r\nPerhaps overriding the default value for `force_local_path` of `prepare_module`?",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 690143182,
    "title": "[Fix GitHub Actions] test adding tmate",
    "dateCreated": "2020-09-01T13:28:03Z",
    "dateModified": "2020-09-01T13:28:03Z",
    "description": "",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 690079429,
    "title": "Add multiprocessing",
    "dateCreated": "2020-09-01T11:56:17Z",
    "dateModified": "2020-09-01T11:56:17Z",
    "description": "Adding multiprocessing to `.map`\r\n\r\nIt works in 3 steps:\r\n- shard the dataset in `num_proc` shards\r\n- spawn one process per shard and call `map` on them\r\n- concatenate the resulting datasets\r\n\r\nExample of usage:\r\n\r\n```python\r\nfrom nlp import load_dataset\r\n\r\ndataset = load_dataset(\"squad\", split=\"train\")\r\n\r\ndef function(x):\r\n    return {\"lowered\": x.lower()}\r\n\r\nprocessed = d.map(\r\n    function,\r\n    input_columns=[\"context\"],\r\n    num_proc=4,\r\n    cache_file_name=\"playground/tmp.arrow\",\r\n    load_from_cache_file=False\r\n)\r\n```\r\n\r\nHere it writes 4 files  depending on the process rank:\r\n- `playground/tmp_00000_of_00004.arrow`\r\n- `playground/tmp_00001_of_00004.arrow`\r\n- `playground/tmp_00002_of_00004.arrow`\r\n- `playground/tmp_00003_of_00004.arrow`\r\n\r\nThe suffix format can be specified by the user.\r\n\r\nIf the `cache_file_name` is not specified, it writes into separated files depending on the fingerprint, as usual.\r\n\r\nI still need to:\r\n- write tests for this\r\n- try to improve the logging (currently it shows 4 progress bars, but if one finishes before the others, then the following messages are written over the progress bars)\r\n",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 690034762,
    "title": "added HANS dataset",
    "dateCreated": "2020-09-01T10:42:02Z",
    "dateModified": "2020-09-01T10:42:02Z",
    "description": "Adds the [HANS](https://github.com/tommccoy1/hans) dataset to evaluate NLI systems.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 689775914,
    "title": "[BUGFIX] Solving mismatched checksum issue for the LinCE dataset (#539)",
    "dateCreated": "2020-09-01T03:27:03Z",
    "dateModified": "2020-09-01T03:27:03Z",
    "description": "Hi,\r\n\r\nI have added the updated `dataset_infos.json` file for the LinCE benchmark. This update is to fix the mismatched checksum bug #539 for one of the datasets in the LinCE benchmark. To update the file, I run this command from the nlp root directory:\r\n\r\n```\r\npython nlp-cli test ./datasets/lince --save_infos --all_configs\r\n```\r\n\r\n**NOTE**: I needed to  change [this line](https://github.com/huggingface/nlp/blob/master/src/nlp/commands/dummy_data.py#L8) from: `from .utils.logging import get_logger` to `from nlp.utils.logging import get_logger`, otherwise the script was not able to import `get_logger`. However, I did not include that in this PR since that could have been just my environment (and another PR could be fixing this already if it is actually an issue).",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 689766465,
    "title": "Fix bleurt logging import",
    "dateCreated": "2020-09-01T03:01:25Z",
    "dateModified": "2020-09-01T03:01:25Z",
    "description": "Bleurt started throwing an error in some code we have.\r\nThis looks like the fix but...\r\n\r\nIt's also unnerving that even a prebuilt docker image with pinned versions can be working 1 day and then fail the next (especially for production systems).\r\n\r\nAny way for us to pin your metrics code so that they are guaranteed not to to change and possibly fail on repository changes?\r\n\r\nThanks (and also for your continued work on the lib...)",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 689285996,
    "title": "[Breaking] Switch text loading to multi-threaded PyArrow loading",
    "dateCreated": "2020-08-31T15:15:41Z",
    "dateModified": "2020-08-31T15:15:41Z",
    "description": "Test if we can get better performances for large-scale text datasets by using multi-threaded text file loading based on Apache Arrow multi-threaded CSV loader.\r\n\r\nIf it works ok, it would fix #546.\r\n\r\n**Breaking change**:\r\nThe text lines now do not include final line-breaks anymore.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 689268589,
    "title": "[Distributed] Making loading distributed datasets a bit safer",
    "dateCreated": "2020-08-31T14:51:34Z",
    "dateModified": "2020-08-31T14:51:34Z",
    "description": "Add some file-locks during dataset loading",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 689186526,
    "title": "Very slow data loading on large dataset",
    "dateCreated": "2020-08-31T12:57:23Z",
    "dateModified": "2020-08-31T12:57:23Z",
    "description": "I made a simple python script to check the NLP library speed, which loads 1.1 TB of textual data.\r\nIt has been 8 hours and still, it is on the loading steps.\r\nIt does work when the text dataset size is small about  1 GB, but it doesn't scale.\r\nIt also uses a single thread during the data loading step.\r\n\r\n```\r\ntrain_files = glob.glob(\"xxx/*.txt\",recursive=True)\r\nrandom.shuffle(train_files)\r\n\r\nprint(train_files)\r\n\r\ndataset = nlp.load_dataset('text', \r\n                           data_files=train_files,\r\n                           name=\"customDataset\",\r\n                           version=\"1.0.0\",\r\n                           cache_dir=\"xxx/nlp\")\r\n```\r\n\r\nIs there something that I am missing ?",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 689138878,
    "title": "New release coming up for this library",
    "dateCreated": "2020-08-31T11:37:38Z",
    "dateModified": "2020-08-31T11:37:38Z",
    "description": "Hi all,\r\nA few words on the roadmap for this library.\r\n\r\nThe next release will be a big one and is planed at the end of this week.\r\n\r\nIn addition to the support for indexed datasets (useful for non-parametric models like REALM, RAG, DPR, knn-LM and many other fast dataset retrieval technics), it will:\r\n- have support for multi-modal datasets\r\n- include various significant improvements on speed for standard processing (map, shuffling, ...)\r\n- have a better support for metrics (better caching, and a robust API) and a bigger focus on reproductibility\r\n- change the name to the final name (voted by the community): `datasets`\r\n- be the 1.0.0 release as we think the API will be mostly stabilized from now on",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 689062519,
    "title": "[Distributed] Fix load_dataset error when multiprocessing + add test",
    "dateCreated": "2020-08-31T09:30:10Z",
    "dateModified": "2020-08-31T09:30:10Z",
    "description": "Fix #543 + add test",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 688644407,
    "title": "nlp.load_dataset is not safe for multi processes when loading from local files",
    "dateCreated": "2020-08-30T03:20:34Z",
    "dateModified": "2020-08-30T03:20:34Z",
    "description": "Loading from local files, e.g., `dataset = nlp.load_dataset('csv', data_files=['file_1.csv', 'file_2.csv'])`\r\nconcurrently from multiple processes, will raise `FileExistsError` from builder's line 430, https://github.com/huggingface/nlp/blob/6655008c738cb613c522deb3bd18e35a67b2a7e5/src/nlp/builder.py#L423-L438\r\n\r\nLikely because multiple processes step into download_and_prepare, https://github.com/huggingface/nlp/blob/6655008c738cb613c522deb3bd18e35a67b2a7e5/src/nlp/load.py#L550-L554\r\n\r\nThis can happen when launching distributed training with commands like `python -m torch.distributed.launch --nproc_per_node 4` on a new collection of files never loaded before.\r\n\r\nI can create a PR that puts in some file locks. It would be helpful if I can be informed of the convention for naming and placement of the lock.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 688555036,
    "title": "Add TensorFlow example",
    "dateCreated": "2020-08-29T15:39:27Z",
    "dateModified": "2020-08-29T15:39:27Z",
    "description": "Update the Quick Tour documentation in order to add the TensorFlow equivalent source code for the classification example. Now it is possible to select either the code in PyTorch or in TensorFlow in the Quick tour.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 688521224,
    "title": "Best practices for training tokenizers with nlp",
    "dateCreated": "2020-08-29T12:06:49Z",
    "dateModified": "2020-08-29T12:06:49Z",
    "description": "Hi, thank you for developing this library. \r\n\r\nWhat do you think are the best practices for training tokenizers using `nlp`?  In the document and examples, I could only find pre-trained tokenizers used.",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 688475884,
    "title": "[BUGFIX] Fix Race Dataset Checksum bug",
    "dateCreated": "2020-08-29T07:00:10Z",
    "dateModified": "2020-08-29T07:00:10Z",
    "description": "In #537 I noticed that there was a bug in checksum checking when I have tried to download the race dataset. The reason for this is that the current preprocessing was just considering the `high school` data and it was ignoring the `middle` one. This PR just fixes it :)\r\n\r\nMoreover, I have added some descriptions.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 688323602,
    "title": "[Dataset] `NonMatchingChecksumError` due to an update in the LinCE benchmark data",
    "dateCreated": "2020-08-28T19:55:51Z",
    "dateModified": "2020-08-28T19:55:51Z",
    "description": "Hi,\r\n\r\nThere is a `NonMatchingChecksumError` error for the `lid_msaea` (language identification for Modern Standard Arabic - Egyptian Arabic) dataset from the LinCE benchmark due to a minor update on that dataset. \r\n\r\nHow can I update the checksum of the library to solve this issue? The error is below and it also appears in the [nlp viewer](https://huggingface.co/nlp/viewer/?dataset=lince&config=lid_msaea):\r\n\r\n```python\r\nimport nlp\r\nnlp.load_dataset('lince', 'lid_msaea')\r\n```\r\n\r\nOutput:\r\n```\r\nNonMatchingChecksumError: ['https://ritual.uh.edu/lince/libaccess/eyJ1c2VybmFtZSI6ICJodWdnaW5nZmFjZSBubHAiLCAidXNlcl9pZCI6IDExMSwgImVtYWlsIjogImR1bW15QGVtYWlsLmNvbSJ9/lid_msaea.zip']\r\nTraceback:\r\nFile \"/home/sasha/streamlit/lib/streamlit/ScriptRunner.py\", line 322, in _run_script\r\n    exec(code, module.__dict__)\r\nFile \"/home/sasha/nlp-viewer/run.py\", line 196, in <module>\r\n    dts, fail = get(str(option.id), str(conf_option.name) if conf_option else None)\r\nFile \"/home/sasha/streamlit/lib/streamlit/caching.py\", line 591, in wrapped_func\r\n    return get_or_create_cached_value()\r\nFile \"/home/sasha/streamlit/lib/streamlit/caching.py\", line 575, in get_or_create_cached_value\r\n    return_value = func(*args, **kwargs)\r\nFile \"/home/sasha/nlp-viewer/run.py\", line 150, in get\r\n    builder_instance.download_and_prepare()\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/nlp/builder.py\", line 432, in download_and_prepare\r\n    download_config.force_download = download_mode == FORCE_REDOWNLOAD\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/nlp/builder.py\", line 469, in _download_and_prepare\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/nlp/utils/info_utils.py\", line 36, in verify_checksums\r\n    raise NonMatchingChecksumError(str(bad_urls))\r\n```\r\n\r\nThank you in advance!\r\n\r\n@lhoestq ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 688015912,
    "title": "[logging] Add centralized logging - Bump-up cache loads to warnings",
    "dateCreated": "2020-08-28T11:42:29Z",
    "dateModified": "2020-08-28T11:42:29Z",
    "description": "Add a `nlp.logging` module to set the global logging level easily. The verbosity level also controls the tqdm bars (disabled when set higher than INFO).\r\n\r\nYou can use:\r\n```\r\nnlp.logging.set_verbosity(verbosity: int)\r\nnlp.logging.set_verbosity_info()\r\nnlp.logging.set_verbosity_warning()\r\nnlp.logging.set_verbosity_debug()\r\nnlp.logging.set_verbosity_error()\r\nnlp.logging.get_verbosity() -> int\r\n```\r\nAnd use the levels:\r\n```\r\nnlp.logging.CRITICAL\r\nnlp.logging.DEBUG\r\nnlp.logging.ERROR\r\nnlp.logging.FATAL\r\nnlp.logging.INFO\r\nnlp.logging.NOTSET\r\nnlp.logging.WARN\r\nnlp.logging.WARNING\r\n```",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 687614699,
    "title": "[Dataset] RACE dataset Checksums error",
    "dateCreated": "2020-08-27T23:58:16Z",
    "dateModified": "2020-08-27T23:58:16Z",
    "description": "Hi there, I just would like to use this awesome lib to perform a dataset fine-tuning on RACE dataset. I have performed the following steps:\r\n\r\n```\r\ndataset = nlp.load_dataset(\"race\")\r\nlen(dataset[\"train\"]), len(dataset[\"validation\"])\r\n```\r\n\r\nBut then I got the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-15-8bf7603ce0ed> in <module>\r\n----> 1 dataset = nlp.load_dataset(\"race\")\r\n      2 len(dataset[\"train\"]), len(dataset[\"validation\"])\r\n\r\n~/miniconda3/envs/masters/lib/python3.8/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    546 \r\n    547     # Download and prepare data\r\n--> 548     builder_instance.download_and_prepare(\r\n    549         download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n    550     )\r\n\r\n~/miniconda3/envs/masters/lib/python3.8/site-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    460                         logger.info(\"Dataset not on Hf google storage. Downloading and preparing it from source\")\r\n    461                 if not downloaded_from_gcs:\r\n--> 462                     self._download_and_prepare(\r\n    463                         dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    464                     )\r\n\r\n~/miniconda3/envs/masters/lib/python3.8/site-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    519         # Checksums verification\r\n    520         if verify_infos:\r\n--> 521             verify_checksums(\r\n    522                 self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n    523             )\r\n\r\n~/miniconda3/envs/masters/lib/python3.8/site-packages/nlp/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     36     if len(bad_urls) > 0:\r\n     37         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 38         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     39     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     40 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['http://www.cs.cmu.edu/~glai1/data/race/RACE.tar.gz']\r\n```",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 687378332,
    "title": "Fingerprint",
    "dateCreated": "2020-08-27T16:27:09Z",
    "dateModified": "2020-08-27T16:27:09Z",
    "description": "This PR is a continuation of #513 , in which many in-place functions were introduced or updated (cast_, flatten_) etc.\r\nHowever the caching didn't handle these changes. Indeed the caching took into account only the previous cache file name of the table, and not the possible in-place transforms of the table.\r\n\r\nTo fix that, I added the concept of dataset fingerprint, that is updated after each transform (in place or not), and stored inside the table metadata.\r\n\r\nWhen a dataset is created, an initial fingerprint is computed. If the dataset is memory-mapped, then the fingerprint generator doesn't read the table and only looks at the filename. However if the table is in-memory, then the fingerprint generator reads the content of the table using a batched non-crypto hashing.\r\n\r\nI added a utility class to compute hashes of arbitrary python objects in `fingerprint.py` : `Hasher`. The API is close to standard hashing tools (`.update`, `.hexdigest`). It also supports custom hashing functions depending on object types using a registry like pickle. I added a custom hashing function to hash a `pa.Table` in a batched way, and also for `nlp.DatasetInfo` to leverage its json serialization feature.\r\n\r\nNote about this PR:\r\nThis is a draft PR because #513 needs to be merged first.\r\nThe diff that is shown is for branches fingerprint -> indices (and not master, for now)",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 686238315,
    "title": "Benchmarks",
    "dateCreated": "2020-08-26T11:21:26Z",
    "dateModified": "2020-08-26T11:21:26Z",
    "description": "Adding some benchmarks with DVC/CML\r\n\r\nTo add a new tracked benchmark:\r\n- create a new python benchmarking script in `./benchmarks/`. The script can use the utilities in `./benchmarks/utils.py` and should output a JSON file with results in `./benchmarks/results/`.\r\n- add a new pipeline stage in [dvc.yaml](./dvc.yaml) with the name of your new benchmark.\r\n\r\nThat's it",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 686115912,
    "title": "`list_datasets()` is broken.",
    "dateCreated": "2020-08-26T08:19:01Z",
    "dateModified": "2020-08-26T08:19:01Z",
    "description": "version = '0.4.0'\r\n\r\n`list_datasets()` is broken. It results in the following error : \r\n\r\n```\r\nIn [3]: nlp.list_datasets()\r\nOut[3]: ---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n~/.virtualenvs/san-lgUCsFg_/lib/python3.8/site-packages/IPython/core/formatters.py in __call__(self, obj)\r\n    700                 type_pprinters=self.type_printers,\r\n    701                 deferred_pprinters=self.deferred_printers)\r\n--> 702             printer.pretty(obj)\r\n    703             printer.flush()\r\n    704             return stream.getvalue()\r\n\r\n~/.virtualenvs/san-lgUCsFg_/lib/python3.8/site-packages/IPython/lib/pretty.py in pretty(self, obj)\r\n    375                 if cls in self.type_pprinters:\r\n    376                     # printer registered in self.type_pprinters\r\n--> 377                     return self.type_pprinters[cls](obj, self, cycle)\r\n    378                 else:\r\n    379                     # deferred printer\r\n\r\n~/.virtualenvs/san-lgUCsFg_/lib/python3.8/site-packages/IPython/lib/pretty.py in inner(obj, p, cycle)\r\n    553                 p.text(',')\r\n    554                 p.breakable()\r\n--> 555             p.pretty(x)\r\n    556         if len(obj) == 1 and type(obj) is tuple:\r\n    557             # Special case for 1-item tuples.\r\n\r\n~/.virtualenvs/san-lgUCsFg_/lib/python3.8/site-packages/IPython/lib/pretty.py in pretty(self, obj)\r\n    392                         if cls is not object \\\r\n    393                                 and callable(cls.__dict__.get('__repr__')):\r\n--> 394                             return _repr_pprint(obj, self, cycle)\r\n    395\r\n    396             return _default_pprint(obj, self, cycle)\r\n\r\n~/.virtualenvs/san-lgUCsFg_/lib/python3.8/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)\r\n    698     \"\"\"A pprint that just redirects to the normal repr function.\"\"\"\r\n    699     # Find newlines and replace them with p.break_()\r\n--> 700     output = repr(obj)\r\n    701     lines = output.splitlines()\r\n    702     with p.group():\r\n\r\n~/.virtualenvs/san-lgUCsFg_/lib/python3.8/site-packages/nlp/hf_api.py in __repr__(self)\r\n    110\r\n    111     def __repr__(self):\r\n--> 112         single_line_description = self.description.replace(\"\\n\", \"\")\r\n    113         return f\"nlp.ObjectInfo(id='{self.id}', description='{single_line_description}', files={self.siblings})\"\r\n    114\r\n\r\nAttributeError: 'NoneType' object has no attribute 'replace'\r\n```",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 685585914,
    "title": "Fix ArrayXD for pyarrow 0.17.1 by using non fixed length list arrays",
    "dateCreated": "2020-08-25T15:32:44Z",
    "dateModified": "2020-08-25T15:32:44Z",
    "description": "It should fix the CI problems in #513 ",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 685540614,
    "title": "File exists error when used with TPU",
    "dateCreated": "2020-08-25T14:36:38Z",
    "dateModified": "2020-08-25T14:36:38Z",
    "description": "Hi,\r\n\r\nI'm getting a \"File exists\" error when I use [text dataset](https://github.com/huggingface/nlp/tree/master/datasets/text) for pre-training a RoBERTa model using `transformers` (3.0.2) and `nlp`(0.4.0) on a VM with TPU (v3-8).\r\n\r\nI modified [line 131 in the original `run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py#L131) as follows:\r\n\r\n```python\r\n# line 131: return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=args.block_size)\r\ndataset = load_dataset(\"text\", data_files=file_path, split=\"train\")\r\ndataset = dataset.map(lambda ex: tokenizer(ex[\"text\"], add_special_tokens=True,\r\n                                        truncation=True, max_length=args.block_size), batched=True)\r\ndataset.set_format(type='torch', columns=['input_ids'])\r\nreturn dataset\r\n```\r\n\r\nWhen I run this with [`xla_spawn.py`](https://github.com/huggingface/transformers/blob/master/examples/xla_spawn.py), I get the following error (it produces one message per core in TPU, which I believe is fine).\r\n\r\nIt seems the current version doesn't take into account distributed training processes as in [this example](https://github.com/huggingface/transformers/blob/a573777901e662ec2e565be312ffaeedef6effec/src/transformers/data/datasets/language_modeling.py#L35-L38)?\r\n\r\n```\r\n08/25/2020 13:59:41 - WARNING - nlp.builder -   Using custom data configuration default\r\n08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)\r\n08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)\r\n08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)\r\n08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)\r\n08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)\r\n08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)\r\n08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)\r\n08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)\r\nDownloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/\r\n447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...\r\nDownloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/\r\n447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...\r\nDownloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/\r\n447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...\r\nDownloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/\r\n447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...\r\nDownloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/\r\n447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...\r\nDownloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/\r\n447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...\r\nException in device=TPU:6: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'\r\nException in device=TPU:4: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'\r\nException in device=TPU:1: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'\r\nDownloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/\r\n447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...\r\nException in device=TPU:7: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'\r\nException in device=TPU:3: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'\r\nDownloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/\r\n447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...\r\nException in device=TPU:2: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'\r\nException in device=TPU:0: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'\r\nTraceback (most recent call last):\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 231, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 231, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 231, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 300, in _mp_fn\r\n    main()\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 300, in _mp_fn\r\n    main()\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 300, in _mp_fn\r\n    main()\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 240, in main\r\n    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 240, in main\r\n    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 240, in main\r\n    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 134, in get_dataset\r\n    dataset = load_dataset(\"text\", data_files=file_path, split=\"train\")\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py\", line 546, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 134, in get_dataset\r\n    dataset = load_dataset(\"text\", data_files=file_path, split=\"train\")\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 134, in get_dataset\r\n    dataset = load_dataset(\"text\", data_files=file_path, split=\"train\")\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py\", line 450, in download_and_prepare\r\n    with incomplete_dir(self._cache_dir) as tmp_data_dir:\r\nTraceback (most recent call last):\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py\", line 546, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py\", line 546, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 231, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py\", line 450, in download_and_prepare\r\n    with incomplete_dir(self._cache_dir) as tmp_data_dir:\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py\", line 422, in incomplete_dir\r\n    os.makedirs(tmp_dir)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py\", line 450, in download_and_prepare\r\n    with incomplete_dir(self._cache_dir) as tmp_data_dir:\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 300, in _mp_fn\r\n      main()\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/os.py\", line 220, in makedirs\r\n    mkdir(name, mode)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 240, in main\r\n    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py\", line 422, in incomplete_dir\r\n    os.makedirs(tmp_dir)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 231, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py\", line 422, in incomplete_dir\r\n    os.makedirs(tmp_dir)\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 134, in get_dataset\r\n    dataset = load_dataset(\"text\", data_files=file_path, split=\"train\")\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/os.py\", line 220, in makedirs\r\n    mkdir(name, mode)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py\", line 546, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\nFileExistsError: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 300, in _mp_fn\r\n    main()\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py\", line 450, in download_and_prepare\r\n    with incomplete_dir(self._cache_dir) as tmp_data_dir:\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/os.py\", line 220, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 240, in main\r\n    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\nFileExistsError: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 134, in get_dataset\r\n    dataset = load_dataset(\"text\", data_files=file_path, split=\"train\")\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py\", line 422, in incomplete_dir\r\n    os.makedirs(tmp_dir)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py\", line 546, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/os.py\", line 220, in makedirs\r\n    mkdir(name, mode)\r\n      File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py\", line 450, in download_and_prepare\r\n    with incomplete_dir(self._cache_dir) as tmp_data_dir:\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\nFileExistsError: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py\", line 422, in incomplete_dir\r\n    os.makedirs(tmp_dir)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/os.py\", line 220, in makedirs\r\n    mkdir(name, mode)\r\nTraceback (most recent call last):\r\nFileExistsError: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'\r\nTraceback (most recent call last):\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 231, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 300, in _mp_fn\r\n    main()\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 240, in main\r\n    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 134, in get_dataset\r\n    dataset = load_dataset(\"text\", data_files=file_path, split=\"train\")\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 231, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py\", line 546, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py\", line 450, in download_and_prepare\r\n    with incomplete_dir(self._cache_dir) as tmp_data_dir:\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 300, in _mp_fn\r\n    main()\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 240, in main\r\n    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\r\n  File \"/home/*****/huggingface_roberta/run_language_modeling.py\", line 134, in get_dataset\r\n    dataset = load_dataset(\"text\", data_files=file_path, split=\"train\")\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py\", line 546, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py\", line 450, in download_and_prepare\r\n    with incomplete_dir(self._cache_dir) as tmp_data_dir:\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py\", line 422, in incomplete_dir\r\n    os.makedirs(tmp_dir)\r\n  File \"/anaconda3/envs/torch-xla-1.6/lib/python3.6/os.py\", line 220, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'\r\n```\r\n\r\n",
    "status": "open",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 685291036,
    "title": "add concatenate_datasets to the docs",
    "dateCreated": "2020-08-25T08:40:05Z",
    "dateModified": "2020-08-25T08:40:05Z",
    "description": "",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 684825612,
    "title": "use ragged tensor by default",
    "dateCreated": "2020-08-24T17:06:15Z",
    "dateModified": "2020-08-24T17:06:15Z",
    "description": "I think it's better if it's clear whether the returned tensor is ragged or not when the type is set to tensorflow.\r\nPreviously it was a tensor (not ragged) if numpy could stack the output (which can change depending on the batch of example you take), which make things difficult to handle, as it may sometimes return a ragged tensor and sometimes not.\r\n\r\nTherefore I reverted this behavior to always return a ragged tensor as we used to do.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 684797157,
    "title": "Add MLSUM",
    "dateCreated": "2020-08-24T16:18:35Z",
    "dateModified": "2020-08-24T16:18:35Z",
    "description": "Hello (again :) !), \r\n\r\nSo, I started a new branch because of a [rebase issue](https://github.com/huggingface/nlp/pull/463), sorry for the mess. \r\n\r\nHowever, the command `pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_real_dataset_mlsum` still fails because there is no default language dataset : the script throws an error as a specific config language is necessary. \r\n\r\nI think that setting a default language would be a bad workaround for this so I kept it as it is. Putting all the train files across languages together would also be a bad idea because of the size. \r\n\r\nThanks for your help, \r\n\r\nRachel\r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 684673673,
    "title": "fix missing variable names in docs",
    "dateCreated": "2020-08-24T13:31:48Z",
    "dateModified": "2020-08-24T13:31:48Z",
    "description": "fix #524 ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 684632930,
    "title": "Fix config used for slow test on real dataset",
    "dateCreated": "2020-08-24T12:39:34Z",
    "dateModified": "2020-08-24T12:39:34Z",
    "description": "As noticed in #470, #474, #476, #504 , the slow test `test_load_real_dataset` couldn't run on datasets that require config parameters.\r\n\r\nTo fix that I replaced it with one test with the first config of BUILDER_CONFIGS `test_load_real_dataset`, and another test that runs all of the configs in BUILDER_CONFIGS `test_load_real_dataset_all_configs`",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 684615455,
    "title": "Returning None instead of \"python\" if dataset is unformatted",
    "dateCreated": "2020-08-24T12:10:35Z",
    "dateModified": "2020-08-24T12:10:35Z",
    "description": "Following the discussion on Slack, this small fix ensures that calling `dataset.set_format(type=dataset.format[\"type\"])` works properly. Slightly breaking as calling `dataset.format` when the dataset is unformatted will return `None` instead of `python`.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 683875483,
    "title": "wmt download speed example",
    "dateCreated": "2020-08-21T23:29:06Z",
    "dateModified": "2020-08-21T23:29:06Z",
    "description": "Continuing from the slack 1.0 roadmap thread w @lhoestq , I realized the slow downloads is only a thing sometimes. Here are a few examples, I suspect there are multiple issues. All commands were run from the same gcp us-central-1f machine.\r\n\r\n```\r\nimport nlp\r\nnlp.load_dataset('wmt16', 'de-en')\r\n```\r\nDownloads at 49.1 KB/S\r\n\r\nWhereas \r\n```\r\npip install gdown # download from google drive\r\n!gdown https://drive.google.com/uc?id=1iO7um-HWoNoRKDtw27YUSgyeubn9uXqj\r\n```\r\nDownloads at 127 MB/s. (The file is a copy of wmt-en-de raw).\r\n\r\n\r\n```\r\nnlp.load_dataset('wmt16', 'ro-en')\r\n```\r\ngoes at 27 MB/s, much faster. \r\n\r\nif we wget the same data from s3 is the same download speed, but \u00bc the file size:\r\n```\r\nwget https://s3.amazonaws.com/datasets.huggingface.co/translation/wmt_en_ro_packed_200_rand.tgz\r\n```\r\n\r\nFinally,\r\n```\r\nnlp.load_dataset('wmt19', 'zh-en')\r\n```\r\nStarts fast, but broken. (duplicate of #493 )\r\n\r\n\r\n",
    "status": "open",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 683686359,
    "title": "Some docs are missing parameter names",
    "dateCreated": "2020-08-21T16:47:34Z",
    "dateModified": "2020-08-21T16:47:34Z",
    "description": "See https://huggingface.co/nlp/master/package_reference/main_classes.html#nlp.Dataset.map. I believe this is because the parameter names are enclosed in backticks in the docstrings, maybe it's an old docstring format that doesn't work with the current Sphinx version.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 682573232,
    "title": "Speed up Tokenization by optimizing cast_to_python_objects",
    "dateCreated": "2020-08-20T09:42:02Z",
    "dateModified": "2020-08-20T09:42:02Z",
    "description": "I changed how `cast_to_python_objects` works to make it faster.\r\nIt is used to cast numpy/pytorch/tensorflow/pandas objects to python lists, and it works recursively.\r\n\r\nTo avoid iterating over possibly long lists, it first checks if the first element that is not None has to be casted.\r\nIf the first element needs to be casted, then all the elements of the list will be casted, otherwise they'll stay the same.\r\nThis trick allows to cast objects that contain tokenizers outputs without iterating over every single token for example.\r\n\r\nSpeed improvement:\r\n\r\n\r\n```python\r\nimport transformers\r\nimport nlp\r\n\r\ntok = transformers.BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\r\ntxt = [\"a \" * 512] * 1000\r\ndataset = nlp.Dataset.from_dict({\"txt\": txt})\r\n\r\n# Tokenization using .map is now faster. Previously it was taking 3.5s\r\n%time _ = dataset.map(lambda x: tok(x[\"txt\"]), batched=True, load_from_cache_file=False)\r\n# 450ms\r\n\r\n# for comparison\r\n%time _ = tok(txt)\r\n# 280ms\r\n\r\n```",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 682478833,
    "title": "dictionnary typo in docs",
    "dateCreated": "2020-08-20T07:11:05Z",
    "dateModified": "2020-08-20T07:11:05Z",
    "description": "Many places dictionary is spelled dictionnary, not sure if its on purpose or not.\r\nFixed in this pr:  \r\nhttps://github.com/huggingface/nlp/pull/521 ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 682477648,
    "title": "Fix dictionnary (dictionary) typo",
    "dateCreated": "2020-08-20T07:09:02Z",
    "dateModified": "2020-08-20T07:09:02Z",
    "description": "This error happens many times I'm thinking maybe its spelled like this on purpose?",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 682264839,
    "title": "Transform references for sacrebleu",
    "dateCreated": "2020-08-20T00:26:55Z",
    "dateModified": "2020-08-20T00:26:55Z",
    "description": "Currently it is impossible to use sacrebleu when len(predictions) != the number of references per prediction (very uncommon), due to a strange format expected by sacrebleu. If one passes in the data to `nlp.metric.compute()` in sacrebleu format, `nlp` throws an error due to mismatching lengths between predictions and references. If one uses a more standard format where predictions and references are lists of the same length, sacrebleu throws an error.\r\n\r\nThis PR transforms reference data in a more standard format into the [unusual format](https://github.com/mjpost/sacreBLEU#using-sacrebleu-from-python) expected by sacrebleu.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 682193882,
    "title": "[BUG] Metrics throwing new error on master since 0.4.0",
    "dateCreated": "2020-08-19T21:29:15Z",
    "dateModified": "2020-08-19T21:29:15Z",
    "description": "The following error occurs when passing in references of type `List[List[str]]` to metrics like bleu.\r\nWasn't happening on 0.4.0 but happening now on master.\r\n\r\n```\r\n  File \"/usr/local/lib/python3.7/site-packages/nlp/metric.py\", line 226, in compute\r\n    self.add_batch(predictions=predictions, references=references)\r\n  File \"/usr/local/lib/python3.7/site-packages/nlp/metric.py\", line 242, in add_batch\r\n    batch = self.info.features.encode_batch(batch)\r\n  File \"/usr/local/lib/python3.7/site-packages/nlp/features.py\", line 527, in encode_batch\r\n    encoded_batch[key] = [encode_nested_example(self[key], cast_to_python_objects(obj)) for obj in column]\r\n  File \"/usr/local/lib/python3.7/site-packages/nlp/features.py\", line 527, in <listcomp>\r\n    encoded_batch[key] = [encode_nested_example(self[key], cast_to_python_objects(obj)) for obj in column]\r\n  File \"/usr/local/lib/python3.7/site-packages/nlp/features.py\", line 456, in encode_nested_example\r\n    raise ValueError(\"Got a string but expected a list instead: '{}'\".format(obj))\r\n```",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 682131165,
    "title": "[METRICS, breaking] Refactor caching behavior, pickle/cloudpickle metrics and dataset, add tests on metrics",
    "dateCreated": "2020-08-19T19:43:08Z",
    "dateModified": "2020-08-19T19:43:08Z",
    "description": "Move the acquisition of the filelock at a later stage during metrics processing so it can be pickled/cloudpickled after instantiation.\r\n\r\nAlso add some tests on pickling, concurrent but separate metric instances and concurrent and distributed metric instances.\r\n\r\nChanges significantly the caching behavior for the metrics:\r\n- if the metric is used in a non-distributed setup (most common case) we try to find a free cache file using UUID instead of asking for an `experiment_id` if we can't lock the cache file this allows to use several instances of the same metrics in parallel.\r\n- if the metrics is used in a distributed setup we ask for an `experiment_id` if we can't lock the cache file (because all the nodes need to have related cache file names for the final sync.\r\n- after the computation, we free the locks and delete all the cache files.\r\n\r\nBreaking: Some arguments for Metrics initialization have been removed for simplicity (`version`...) and some have been renamed for consistency with the rest of the library (`in_memory` => `keep_in_memory`).\r\n\r\nAlso remove the `_has_transformers` detection in utils to avoid importing transformers everytime during loading.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 681896944,
    "title": "add MLDoc dataset",
    "dateCreated": "2020-08-19T14:41:59Z",
    "dateModified": "2020-08-19T14:41:59Z",
    "description": "Hi,\r\n\r\nI am recommending that someone add MLDoc, a multilingual news topic classification dataset.\r\n\r\n- Here's a link to the Github: https://github.com/facebookresearch/MLDoc\r\n- and the paper: http://www.lrec-conf.org/proceedings/lrec2018/pdf/658.pdf\r\n\r\nLooks like the dataset contains news stories in multiple languages that can be classified into four hierarchical groups: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social) and MCAT (Markets). There are 13 languages: Dutch, French, German, Chinese, Japanese, Russian, Portuguese, Spanish, Latin American Spanish, Italian, Danish, Norwegian, and Swedish",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 681846032,
    "title": "[Breaking] Rename formated to formatted",
    "dateCreated": "2020-08-19T13:35:23Z",
    "dateModified": "2020-08-19T13:35:23Z",
    "description": "`formated` is not correct but `formatted` is",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 681845619,
    "title": "Fix batched map for formatted dataset",
    "dateCreated": "2020-08-19T13:34:50Z",
    "dateModified": "2020-08-19T13:34:50Z",
    "description": "If you had a dataset formatted as numpy for example, and tried to do a batched map, then it would crash because one of the elements from the inputs was missing for unchanged columns (ex: batch of length 999 instead of 1000).\r\nThe happened during the creation of the `pa.Table`, since columns had different lengths.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 681256348,
    "title": "dataset.shuffle(keep_in_memory=True) is never allowed",
    "dateCreated": "2020-08-18T18:47:40Z",
    "dateModified": "2020-08-18T18:47:40Z",
    "description": "As of commit ef4aac2, the usage of the parameter `keep_in_memory=True` is never possible: `dataset.select(keep_in_memory=True)`\r\n\r\nThe commit added the lines\r\n```python\r\n# lines 994-996 in src/nlp/arrow_dataset.py\r\n       assert (\r\n            not keep_in_memory or cache_file_name is None\r\n        ), \"Please use either `keep_in_memory` or `cache_file_name` but not both.\"\r\n```\r\n\r\nThis affects both `shuffle()` as `select()` is a sub-routine, and `map()` that has the same check. \r\n\r\nI'd love to fix this myself, but unsure what the intention of the assert is given the rest of the logic in the function concerning `ccache_file_name` and `keep_in_memory`.",
    "status": "open",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 681215612,
    "title": "[speedup] Use indices mappings instead of deepcopy for all the samples reordering methods",
    "dateCreated": "2020-08-18T17:36:02Z",
    "dateModified": "2020-08-18T17:36:02Z",
    "description": "Use an indices mapping instead of rewriting the dataset for all the samples re-ordering/selection methods (`select`, `sort`, `shuffle`, `shard`, `train_test_split`).\r\n\r\nAdded a `flatten_indices` method which copy the dataset to a new table to remove the indices mapping with tests.\r\n\r\nAll the samples re-ordering/selection methods should be a lot faster. The downside is that iterating on very large batch of the dataset might be a little slower when we have changed the order of the samples since with in these case we use `pyarrow.Table.take` instead of `pyarrow.Table.slice`. There is no free lunch but the speed of iterating over the dataset is rarely the bottleneck.\r\n\r\n*Backward breaking change*: the `cache_file_name` argument in all the samples re-ordering/selection methods (`select`, `sort`, `shuffle`, `shard`, `train_test_split`) is now called `indices_cache_file_name` on purpose to make it explicit to the user that this caching file is used for caching the indices mapping and not the dataset itself.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 681137164,
    "title": "Delete CONTRIBUTING.md",
    "dateCreated": "2020-08-18T15:33:25Z",
    "dateModified": "2020-08-18T15:33:25Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 681055553,
    "title": "dataset.shuffle() and select() resets format. Intended?",
    "dateCreated": "2020-08-18T13:46:01Z",
    "dateModified": "2020-08-18T13:46:01Z",
    "description": "Calling `dataset.shuffle()` or `dataset.select()` on a dataset resets its format set by `dataset.set_format()`. Is this intended or an oversight?\r\n\r\nWhen working on quite large datasets that require a lot of preprocessing I find it convenient to save the processed dataset to file using `torch.save(\"dataset.pt\")`. Later loading the dataset object using `torch.load(\"dataset.pt\")`, which conserves the defined format before saving. \r\nI do shuffling and selecting (for controlling dataset size) after loading the data from .pt-file, as it's convenient whenever you train multiple models with varying sizes of the same dataset. \r\n\r\nThe obvious workaround for this is to set the format again after using `dataset.select()` or `dataset.shuffle()`.\r\n\r\n_I guess this is more of a discussion on the design philosophy of the functions. Please let me know if this is not the right channel for these kinds of discussions or if they are not wanted at all!_\r\n\r\n####  How to reproduce:\r\n\r\n```python\r\nimport nlp\r\nfrom transformers import T5Tokenizer\r\n\r\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\r\ndef create_features(batch):\r\n    context_encoding = tokenizer.batch_encode_plus(batch[\"context\"])\r\n    return {\"input_ids\": context_encoding[\"input_ids\"]}\r\n\r\ndataset = nlp.load_dataset(\"cosmos_qa\", split=\"train\")\r\ndataset = dataset.map(create_features, batched=True)\r\ndataset.set_format(type=\"torch\", columns=[\"input_ids\"])\r\ndataset[0]\r\n# {'input_ids': tensor([ 1804,  3525,  1602,  ...   0,     0])}\r\n\r\ndataset = dataset.shuffle()\r\ndataset[0]\r\n# {'id': '3Q9(...)20', 'context': \"Good Old War an (...) play ?', 'answer0': 'None of the above choices .', 'answer1': 'This person likes music and likes to see the show , they will see other bands play .', (...) 'input_ids': [1804, 3525, 1602, ... , 0, 0]}\r\n\r\n```",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 680823644,
    "title": "Version of numpy to use the library",
    "dateCreated": "2020-08-18T08:59:13Z",
    "dateModified": "2020-08-18T08:59:13Z",
    "description": "Thank you so much for your excellent work! I would like to use nlp library in my project. While importing nlp, I am receiving the following error `AttributeError: module 'numpy.random' has no attribute 'Generator'` Numpy version in my project is 1.16.0. May I learn which numpy version is used for the nlp library.\r\n\r\nThanks in advance.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 679711585,
    "title": "Converting TensorFlow dataset example",
    "dateCreated": "2020-08-16T08:05:20Z",
    "dateModified": "2020-08-16T08:05:20Z",
    "description": "Hi,\r\nI want to use TensorFlow datasets with this repo, I noticed you made some conversion script,\r\ncan you give a simple example of using it?\r\n\r\nThanks\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 679705734,
    "title": "TypeError: Receiver() takes no arguments",
    "dateCreated": "2020-08-16T07:18:16Z",
    "dateModified": "2020-08-16T07:18:16Z",
    "description": "I am trying to load a wikipedia data set\r\n\r\n```\r\nimport nlp\r\nfrom nlp import load_dataset\r\n\r\ndataset = load_dataset(\"wikipedia\", \"20200501.en\", split=\"train\", cache_dir=data_path, beam_runner='DirectRunner')\r\n#dataset = load_dataset('wikipedia', '20200501.sv', cache_dir=data_path, beam_runner='DirectRunner')\r\n```\r\n\r\nThis fails in the apache beam runner. \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:/ML/wikiembedding/gpt2_sv.py\", line 36, in <module>\r\n    dataset = load_dataset(\"wikipedia\", \"20200501.en\", split=\"train\", cache_dir=my_cache_dir, beam_runner='DirectRunner')\r\n  File \"C:\\Users\\seto\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nlp\\load.py\", line 548, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"C:\\Users\\seto\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nlp\\builder.py\", line 462, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"C:\\Users\\seto\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\nlp\\builder.py\", line 969, in _download_and_prepare\r\n    pipeline_results = pipeline.run()\r\n  File \"C:\\Users\\seto\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\apache_beam\\pipeline.py\", line 534, in run\r\n    return self.runner.run_pipeline(self, self._options)\r\n....\r\n  File \"C:\\Users\\seto\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\apache_beam\\runners\\worker\\bundle_processor.py\", line 218, in process_encoded\r\n    self.output(decoded_value)\r\n  File \"C:\\Users\\seto\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\apache_beam\\runners\\worker\\operations.py\", line 332, in output\r\n    cython.cast(Receiver, self.receivers[output_index]).receive(windowed_value)\r\n  File \"C:\\Users\\seto\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\Cython\\Shadow.py\", line 167, in cast\r\n    return type(*args)\r\nTypeError: Receiver() takes no arguments\r\n\r\n```\r\n\r\nThis is run on a Windows 10 machine with python 3.8. I get the same error loading the swedish wikipedia dump.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 679400683,
    "title": "Errors when I use ",
    "dateCreated": "2020-08-14T21:03:57Z",
    "dateModified": "2020-08-14T21:03:57Z",
    "description": "I tried the following example code from https://huggingface.co/deepset/roberta-base-squad2 and got errors \r\nI am using **transformers 3.0.2** code .\r\n\r\n\r\nfrom transformers.pipelines import pipeline\r\nfrom transformers.modeling_auto import AutoModelForQuestionAnswering\r\nfrom transformers.tokenization_auto import AutoTokenizer\r\n\r\nmodel_name = \"deepset/roberta-base-squad2\"\r\n\r\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\r\nQA_input = {\r\n    'question': 'Why is model conversion important?',\r\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\r\n}\r\nres = nlp(QA_input)\r\n\r\nThe errors are :\r\n\r\nres = nlp(QA_input)\r\n  File \".local/lib/python3.6/site-packages/transformers/pipelines.py\", line 1316, in __call__\r\n    for s, e, score in zip(starts, ends, scores)\r\n  File \".local/lib/python3.6/site-packages/transformers/pipelines.py\", line 1316, in <listcomp>\r\n    for s, e, score in zip(starts, ends, scores)\r\nKeyError: 0\r\n\r\n",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 679164788,
    "title": "fix dataset.map for function without outputs",
    "dateCreated": "2020-08-14T13:40:22Z",
    "dateModified": "2020-08-14T13:40:22Z",
    "description": "As noticed in #505 , giving a function that doesn't return anything in `.map` raises an error because of an unreferenced variable.\r\nI fixed that and added tests.\r\n\r\nThanks @avloss for reporting",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 678791400,
    "title": "tmp_file referenced before assignment",
    "dateCreated": "2020-08-13T23:27:33Z",
    "dateModified": "2020-08-13T23:27:33Z",
    "description": "Just learning about this library - so might've not set up all the flags correctly, but was getting this error about \"tmp_file\".",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 678756211,
    "title": "Added downloading to Hyperpartisan news detection",
    "dateCreated": "2020-08-13T21:53:46Z",
    "dateModified": "2020-08-13T21:53:46Z",
    "description": "Following the discussion on Slack and #349, I've updated the hyperpartisan dataset to pull directly from Zenodo rather than manual install, which should make this dataset much more accessible. Many thanks to @johanneskiesel !\r\n\r\nCurrently doesn't pass `test_load_real_dataset` - I'm using `self.config.name` which is `default` in this test. Might be related to #474",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 678726538,
    "title": "CompGuessWhat?! 0.2.0",
    "dateCreated": "2020-08-13T20:51:26Z",
    "dateModified": "2020-08-13T20:51:26Z",
    "description": "We updated some metadata information associated with the dataset. In addition, we've updated the `create_dummy_data.py` script to generate data samples for the dataset. ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 678546070,
    "title": "Fix tokenizers caching",
    "dateCreated": "2020-08-13T15:53:37Z",
    "dateModified": "2020-08-13T15:53:37Z",
    "description": "I've found some cases where the caching didn't work properly for tokenizers:\r\n\r\n1. if a tokenizer has a regex pattern, then the caching would be inconsistent across sessions\r\n2. if a tokenizer has a cache attribute that changes after some calls, the the caching would not work after cache updates\r\n3. if a tokenizer is used inside a function, the caching of this function would result in the same cache file for different tokenizers\r\n4. if `unique_no_split_tokens`'s attribute is not the same across sessions (after loading a tokenizer) then the caching could be inconsistent\r\n\r\nTo fix that, this is what I did:\r\n\r\n1. register a specific `save_regex` function for pickle that makes regex dumps deterministic\r\n2. ignore cache attribute of some tokenizers before dumping\r\n3. enable recursive dump by default for all dumps\r\n4. make `unique_no_split_tokens` deterministic in https://github.com/huggingface/transformers/pull/6461\r\n\r\nI also added tests to make sure that tokenizers hashing works as expected.\r\nIn the future we should find a way to test if hashing also works across session (maybe using two CI jobs ? or by hardcoding a tokenizer's hash ?)",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 677952893,
    "title": "Caching doesn't work for map (non-deterministic)",
    "dateCreated": "2020-08-12T20:20:07Z",
    "dateModified": "2020-08-12T20:20:07Z",
    "description": "The caching functionality doesn't work reliably when tokenizing a dataset. Here's a small example to reproduce it. \r\n\r\n```python\r\nimport nlp\r\nimport transformers\r\n\r\ndef main():\r\n    ds = nlp.load_dataset(\"reddit\", split=\"train[:500]\")\r\n\r\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\r\n\r\n    def convert_to_features(example_batch):\r\n        input_str = example_batch[\"body\"]\r\n        encodings = tokenizer(input_str, add_special_tokens=True, truncation=True)\r\n        return encodings\r\n\r\n    ds = ds.map(convert_to_features, batched=True)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nRoughly 3/10 times, this example recomputes the tokenization.\r\n\r\nIs this expected behaviour?",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 677841708,
    "title": "Use hnsw in wiki_dpr",
    "dateCreated": "2020-08-12T16:58:07Z",
    "dateModified": "2020-08-12T16:58:07Z",
    "description": "The HNSW faiss index is much faster that regular Flat index.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 677709938,
    "title": "Narrativeqa (with full text)",
    "dateCreated": "2020-08-12T13:49:43Z",
    "dateModified": "2020-08-12T13:49:43Z",
    "description": "Following the uploading of the full text data in #309, I've added the full text to the narrativeqa dataset.\r\n\r\nFew notes:\r\n- Had some encoding issues using the default `open` so am using `open(encoding=\"latin-1\"...` which seems to fix it. Looks fine.\r\n-  Can't get the dummy data to work. Currently putting stuff at: \r\n    ```\r\n    dummy\r\n    |---- 0.0.0\r\n            |- dummy_data.zip\r\n                |-master.zip\r\n                |   |- narrativeqa-master\r\n                |       |- documents.csv\r\n                |       |- qaps.csv\r\n                |       |- third_party ......\r\n                | \r\n                | - narrativeqa_full_text.zip\r\n                |       | - 001.content\r\n                |       | - ....\r\n    ```\r\n    Not sure what I'm messing up here (probably something obvious).",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 677597479,
    "title": "dont use beam fs to save info for local cache dir",
    "dateCreated": "2020-08-12T11:00:00Z",
    "dateModified": "2020-08-12T11:00:00Z",
    "description": "If the cache dir is local, then we shouldn't use beam's filesystem to save the dataset info\r\n\r\nFix #490 \r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 677057116,
    "title": "skip header in PAWS-X",
    "dateCreated": "2020-08-11T17:26:25Z",
    "dateModified": "2020-08-11T17:26:25Z",
    "description": "This should fix #485 \r\n\r\nI also updated the `dataset_infos.json` file that is used to verify the integrity of the generated splits (the number of examples was reduced by one).\r\n\r\nNote that there are new fields in `dataset_infos.json` introduced in the latest release 0.4.0 corresponding to post processing info. I removed them in this case when I ran `nlp-cli ./datasets/xtreme --save_infos` to keep backward compatibility (versions 0.3.0 can't load these fields).\r\n\r\nI think I'll change the logic so that `nlp-cli test` doesn't create these fields for dataset with no post processing",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 677016998,
    "title": "fix bad type in overflow check",
    "dateCreated": "2020-08-11T16:24:58Z",
    "dateModified": "2020-08-11T16:24:58Z",
    "description": "When writing an arrow file and inferring the features, the overflow check could fail if the first example had a `null` field.\r\nThis is because we were not using the inferred features to do this check, and we could end up with arrays that don't match because of a type mismatch (`null` vs `string` for example).\r\n\r\nThis should fix #482",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 676959289,
    "title": "stack vectors in pytorch and tensorflow",
    "dateCreated": "2020-08-11T15:12:53Z",
    "dateModified": "2020-08-11T15:12:53Z",
    "description": "When the format of a dataset is set to pytorch or tensorflow, and if the dataset has vectors in it, they were not stacked together as tensors when calling `dataset[i:i + batch_size][column]` or `dataset[column]`.\r\n\r\nI added support for stacked tensors for both pytorch and tensorflow.\r\nFor ragged tensors, they are stacked only for tensorflow as pytorch doesn't support ragged tensors.\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 676886955,
    "title": "Fix numpy stacking",
    "dateCreated": "2020-08-11T13:40:30Z",
    "dateModified": "2020-08-11T13:40:30Z",
    "description": "When getting items using a column name as a key, numpy arrays were not stacked.\r\nI fixed that and added some tests.\r\n\r\nThere is another issue that still needs to be fixed though: when getting items using a column name as a key, pytorch tensors are not stacked (it outputs a list of tensors). This PR should help with the to fix this issue.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 676527351,
    "title": "Fix wmt zh-en url",
    "dateCreated": "2020-08-11T02:14:52Z",
    "dateModified": "2020-08-11T02:14:52Z",
    "description": "I verified that\r\n```\r\nwget https://stuncorpusprod.blob.core.windows.net/corpusfiles/UNv1.0.en-zh.tar.gz.00\r\n```\r\nruns in 2 minutes.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 676495064,
    "title": "nlp.Features does not distinguish between nullable and non-nullable types in PyArrow schema",
    "dateCreated": "2020-08-11T00:27:46Z",
    "dateModified": "2020-08-11T00:27:46Z",
    "description": "Here's the code I'm trying to run:\r\n\r\n```python\r\ndset_wikipedia = nlp.load_dataset(\"wikipedia\", \"20200501.en\", split=\"train\", cache_dir=args.cache_dir)\r\ndset_wikipedia.drop(columns=[\"title\"])\r\ndset_wikipedia.features.pop(\"title\")\r\ndset_books = nlp.load_dataset(\"bookcorpus\", split=\"train\", cache_dir=args.cache_dir)\r\ndset = nlp.concatenate_datasets([dset_wikipedia, dset_books])\r\n```\r\n\r\nThis fails because they have different schemas, despite having identical features.\r\n\r\n```python\r\nassert dset_wikipedia.features == dset_books.features # True\r\nassert dset_wikipedia._data.schema == dset_books._data.schema # False\r\n```\r\n\r\nThe Wikipedia dataset has 'text: string', while the BookCorpus dataset has 'text: string not null'. Currently I hack together a working schema match with the following line, but it would be better if this was handled in Features themselves.\r\n\r\n```python\r\ndset_wikipedia._data = dset_wikipedia.data.cast(dset_books._data.schema)\r\n```\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 676486275,
    "title": "No 0.4.0 release on GitHub",
    "dateCreated": "2020-08-10T23:59:57Z",
    "dateModified": "2020-08-10T23:59:57Z",
    "description": "0.4.0 was released on PyPi, but not on GitHub. This means [the documentation](https://huggingface.co/nlp/) is still displaying from 0.3.0, and that there's no tag to easily clone the 0.4.0 version of the repo.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 676482242,
    "title": "Loading preprocessed Wikipedia dataset requires apache_beam",
    "dateCreated": "2020-08-10T23:46:50Z",
    "dateModified": "2020-08-10T23:46:50Z",
    "description": "Running \r\n\r\n`nlp.load_dataset(\"wikipedia\", \"20200501.en\", split=\"train\", dir=\"/tmp/wikipedia\")`\r\n\r\ngives an error if apache_beam is not installed, stemming from\r\n\r\nhttps://github.com/huggingface/nlp/blob/38eb2413de54ee804b0be81781bd65ac4a748ced/src/nlp/builder.py#L981-L988\r\n\r\nThis succeeded without the dependency in version 0.3.0. This seems like an unnecessary dependency to process some dataset info if you're using the already-preprocessed version. Could it be removed?",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 676456257,
    "title": "ug",
    "dateCreated": "2020-08-10T22:33:03Z",
    "dateModified": "2020-08-10T22:33:03Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 676299993,
    "title": "issues with downloading datasets for wmt16 and wmt19",
    "dateCreated": "2020-08-10T17:32:51Z",
    "dateModified": "2020-08-10T17:32:51Z",
    "description": "I  have encountered multiple issues while trying to:\r\n```\r\nimport nlp\r\ndataset = nlp.load_dataset('wmt16', 'ru-en')\r\nmetric = nlp.load_metric('wmt16')\r\n```\r\n1. I had to do `pip install -e \".[dev]\" ` on master, currently released nlp didn't work (sorry, didn't save the error) - I went back to the released version and now it worked. So it must have been some outdated dependencies that  `pip install -e \".[dev]\" ` fixed.\r\n\r\n2. it was downloading at 60kbs - almost 5 hours to get the dataset. It was downloading all pairs and not just the one I asked for. \r\n\r\nI tried the same code with `wmt19` in parallel and it took a few secs to download and it only fetched data for the requested pair. (but it failed too, see below)\r\n\r\n3. my machine has crushed and when I retried I got:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./download.py\", line 9, in <module>\r\n    dataset = nlp.load_dataset('wmt16', 'ru-en')\r\n  File \"/mnt/nvme1/code/huggingface/nlp-master/src/nlp/load.py\", line 549, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"/mnt/nvme1/code/huggingface/nlp-master/src/nlp/builder.py\", line 449, in download_and_prepare\r\n    with incomplete_dir(self._cache_dir) as tmp_data_dir:\r\n  File \"/home/stas/anaconda3/envs/main/lib/python3.7/contextlib.py\", line 112, in __enter__\r\n    return next(self.gen)\r\n  File \"/mnt/nvme1/code/huggingface/nlp-master/src/nlp/builder.py\", line 422, in incomplete_dir\r\n    os.makedirs(tmp_dir)\r\n  File \"/home/stas/anaconda3/envs/main/lib/python3.7/os.py\", line 221, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '/home/stas/.cache/huggingface/datasets/wmt16/ru-en/1.0.0/4d8269cdd971ed26984a9c0e4a158e0c7afc8135fac8fb8ee43ceecf38fd422d.incomplete'\r\n```\r\nit can't handle resumes. but neither allows a new start. Had to delete it manually.\r\n\r\n4. and finally when it downloaded the dataset, it then failed to fetch the metrics:\r\n```\r\nTraceback (most recent call last):\r\n  File \"./download.py\", line 15, in <module>\r\n    metric = nlp.load_metric('wmt16')\r\n  File \"/mnt/nvme1/code/huggingface/nlp-master/src/nlp/load.py\", line 442, in load_metric\r\n    module_path, hash = prepare_module(path, download_config=download_config, dataset=False)\r\n  File \"/mnt/nvme1/code/huggingface/nlp-master/src/nlp/load.py\", line 258, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/mnt/nvme1/code/huggingface/nlp-master/src/nlp/utils/file_utils.py\", line 198, in cached_path\r\n    local_files_only=download_config.local_files_only,\r\n  File \"/mnt/nvme1/code/huggingface/nlp-master/src/nlp/utils/file_utils.py\", line 356, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https://s3.amazonaws.com/datasets.huggingface.co/nlp/metrics/wmt16/wmt16.py\r\n```\r\n\r\n5. If I run the same code with `wmt19`, it fails too:\r\n\r\n```\r\nConnectionError: Couldn't reach https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-ru.tar.gz\r\n```",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 676143029,
    "title": "Fix elasticsearch result ids returning as strings",
    "dateCreated": "2020-08-10T13:37:11Z",
    "dateModified": "2020-08-10T13:37:11Z",
    "description": "I am using the latest elasticsearch binary and master of nlp. For me elasticsearch searches failed because the resultant \"id_\" returned for searches are strings, but our library assumes them to be integers.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 675649034,
    "title": "Bookcorpus data contains pretokenized text",
    "dateCreated": "2020-08-09T06:53:24Z",
    "dateModified": "2020-08-09T06:53:24Z",
    "description": "It seem that the bookcoprus data downloaded through the library was pretokenized with NLTK's Treebank tokenizer, which changes the text in incompatible ways to how, for instance, BERT's wordpiece tokenizer works. For example, \"didn't\" becomes \"did\" + \"n't\", and double quotes are changed to `` and '' for start and end quotes, respectively.\r\n\r\nOn my own projects, I just run the data through NLTK's TreebankWordDetokenizer to reverse the tokenization (as best as possible). I think it would be beneficial to apply this transformation directly on your remote cached copy of the dataset. If you choose to do so, I would also suggest to use my fork of NLTK that fixes several bugs in their detokenizer (I've opened a pull-request, but they've yet to respond): https://github.com/nltk/nltk/pull/2575",
    "status": "open",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 675595393,
    "title": "PAWS dataset first item is header",
    "dateCreated": "2020-08-08T22:05:25Z",
    "dateModified": "2020-08-08T22:05:25Z",
    "description": "```\r\nimport nlp\r\ndataset = nlp.load_dataset('xtreme', 'PAWS-X.en')\r\ndataset['test'][0]\r\n```\r\n\r\nprints the following\r\n\r\n```\r\n{'label': 'label', 'sentence1': 'sentence1', 'sentence2': 'sentence2'}\r\n```\r\n\r\ndataset['test'][0] should probably be the first item in the dataset, not just a dictionary mapping the column names to themselves. Probably just need to ignore the first row in the dataset by default or something like that.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 675088983,
    "title": "update mirror for RT dataset",
    "dateCreated": "2020-08-07T15:25:45Z",
    "dateModified": "2020-08-07T15:25:45Z",
    "description": "",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 675080694,
    "title": "rotten tomatoes movie review dataset taken down",
    "dateCreated": "2020-08-07T15:12:01Z",
    "dateModified": "2020-08-07T15:12:01Z",
    "description": "In an interesting twist of events, the individual who created the movie review seems to have left Cornell, and their webpage has been removed, along with the movie review dataset (http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz). It's not downloadable anymore.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 674851147,
    "title": "Bugs : dataset.map() is frozen on ELI5",
    "dateCreated": "2020-08-07T08:23:35Z",
    "dateModified": "2020-08-07T08:23:35Z",
    "description": "Hi Huggingface Team!\r\n\r\nThank you guys once again for this amazing repo.\r\n\r\nI have tried to prepare ELI5 to train with T5, based on [this wonderful notebook of Suraj Patil](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb) \r\n\r\nHowever, when I run `dataset.map()` on ELI5 to prepare `input_text, target_text`, `dataset.map` is **frozen** in the first hundreds examples. On the contrary, this works totally fine on SQUAD (80,000 examples). Both `nlp` version 0.3.0 and 0.4.0 cause frozen process . Also try various `pyarrow` versions from 0.16.0 / 0.17.0 / 1.0.0 also have the same frozen process.\r\n\r\nReproducible code can be found on [this colab notebook ](https://colab.research.google.com/drive/14wttOTv3ky74B_c0kv5WrbgQjCF2fYQk?usp=sharing), where I also show that the same mapping function works fine on SQUAD, so the problem is likely due to ELI5 somehow.\r\n\r\n----------------------------------------\r\n**More Info :** instead of `map`, if I run `for` loop and apply function by myself, there's no error and can finish within 10 seconds. However, `nlp dataset` is immutable (I couldn't manually assign a new key-value to `dataset `object)\r\n\r\nI also notice that SQUAD texts are quite clean while ELI5 texts contain many special characters, not sure if this is the cause ?",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 674567389,
    "title": "Apply utf-8 encoding to all datasets",
    "dateCreated": "2020-08-06T20:02:09Z",
    "dateModified": "2020-08-06T20:02:09Z",
    "description": "## Description\r\nThis PR applies utf-8 encoding for all instances of `with open(...) as f` to all Python files in `datasets/`. As suggested by @thomwolf in #468 , we use regular expressions and the following function\r\n\r\n```python\r\ndef apply_encoding_on_file_open(filepath: str):\r\n    \"\"\"Apply UTF-8 encoding for all instances where a non-binary file is opened.\"\"\"\r\n    \r\n    with open(filepath, 'r', encoding='utf-8') as input_file:\r\n        regexp = re.compile(r\"(?!.*\\b(?:encoding|rb|w|wb|w+|wb+|ab|ab+)\\b)(?<=\\s)(open)\\((.*)\\)\")\r\n        input_text = input_file.read()\r\n        match = regexp.search(input_text)\r\n        \r\n        if match:\r\n            output = regexp.sub(lambda m: m.group()[:-1]+', encoding=\"utf-8\")', input_text)\r\n            with open(filepath, 'w', encoding='utf-8') as output_file:\r\n                output_file.write(output)\r\n```\r\n\r\nto perform the replacement. \r\n\r\nNote:\r\n\r\n1. I excluded all _**binary files**_ from the search since it's possible some objects are opened for which the encoding doesn't make sense. Please correct me if I'm wrong and I'll tweak the regexp accordingly\r\n2. There were two edge cases where the regexp failed (e.g. two `open` instances on a single line), but I decided to just fix these manually in the interest of time.\r\n3. I only applied the replacement to files in `datasets/`. Let me know if this should be extended to other places like `metrics/`\r\n4. I have implemented a unit test that should catch missing encodings in future CI runs\r\n\r\nCloses #468  and possibly #347 ",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 674245959,
    "title": "Column indexing hotfix",
    "dateCreated": "2020-08-06T11:37:05Z",
    "dateModified": "2020-08-06T11:37:05Z",
    "description": "As observed for example in #469 , currently `__getitem__` does not convert the data to the dataset format when indexing by column. This is a hotfix that imitates functional 0.3.0. code. In the future it'd probably be nice to have a test there.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 673905407,
    "title": "add METEOR metric",
    "dateCreated": "2020-08-05T23:13:00Z",
    "dateModified": "2020-08-05T23:13:00Z",
    "description": "Added the METEOR metric. Can be used like this:\r\n\r\n```python\r\nimport nlp\r\nmeteor = nlp.load_metric('metrics/meteor')\r\nmeteor.compute([\"some string\", \"some string\"], [\"some string\", \"some similar string\"])\r\n# {'meteor': 0.6411637931034483}\r\nmeteor.add(\"some string\", \"some string\")\r\nmeteor.add('some string\", \"some similar string\")\r\nmeteor.compute()\r\n# {'meteor': 0.6411637931034483}\r\n```\r\n\r\nUses [NLTK's implementation](https://www.nltk.org/api/nltk.translate.html#module-nltk.translate.meteor_score), [(source)](https://github.com/nltk/nltk/blob/develop/nltk/translate/meteor_score.py)",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 673178317,
    "title": "Export TFRecord to GCP bucket",
    "dateCreated": "2020-08-05T01:08:32Z",
    "dateModified": "2020-08-05T01:08:32Z",
    "description": "Previously, I was writing TFRecords manually to GCP bucket with : `with tf.io.TFRecordWriter('gs://my_bucket/x.tfrecord')`\r\n\r\nSince `0.4.0` is out with the `export()` function, I tried it. But it seems TFRecords cannot be directly written to GCP bucket.\r\n\r\n`dataset.export('local.tfrecord')` works fine,  \r\nbut `dataset.export('gs://my_bucket/x.tfrecord')` does not work. \r\n\r\nThere is no error message, I just can't find the file on my bucket...\r\n\r\n---\r\n\r\nLooking at the code, `nlp` is using `tf.data.experimental.TFRecordWriter`, while I was using `tf.io.TFRecordWriter`.  \r\n\r\n**What's the difference between those 2 ? How can I write TFRecords files directly to GCP bucket ?**\r\n\r\n@jarednielsen @lhoestq ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 673142143,
    "title": "Overview.ipynb throws exceptions with nlp 0.4.0",
    "dateCreated": "2020-08-04T23:18:15Z",
    "dateModified": "2020-08-04T23:18:15Z",
    "description": "with nlp 0.4.0, the TensorFlow example in Overview.ipynb throws the following exceptions:\r\n\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-5-48907f2ad433> in <module>\r\n----> 1 features = {x: train_tf_dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.max_len]) for x in columns[:3]}\r\n      2 labels = {\"output_1\": train_tf_dataset[\"start_positions\"].to_tensor(default_value=0, shape=[None, 1])}\r\n      3 labels[\"output_2\"] = train_tf_dataset[\"end_positions\"].to_tensor(default_value=0, shape=[None, 1])\r\n      4 tfdataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)\r\n\r\n<ipython-input-5-48907f2ad433> in <dictcomp>(.0)\r\n----> 1 features = {x: train_tf_dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.max_len]) for x in columns[:3]}\r\n      2 labels = {\"output_1\": train_tf_dataset[\"start_positions\"].to_tensor(default_value=0, shape=[None, 1])}\r\n      3 labels[\"output_2\"] = train_tf_dataset[\"end_positions\"].to_tensor(default_value=0, shape=[None, 1])\r\n      4 tfdataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)\r\n\r\nAttributeError: 'numpy.ndarray' object has no attribute 'to_tensor'",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 672991854,
    "title": "CheckList",
    "dateCreated": "2020-08-04T18:32:05Z",
    "dateModified": "2020-08-04T18:32:05Z",
    "description": "Sorry for the large pull request.\r\n- Added checklists as datasets. I can't run `test_load_real_dataset` (see #474), but I can load the datasets successfully as shown in the example notebook\r\n- Added a checklist wrapper ",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 672884595,
    "title": "misc. bugs and quality of life",
    "dateCreated": "2020-08-04T15:32:29Z",
    "dateModified": "2020-08-04T15:32:29Z",
    "description": "A few misc. bugs and QOL improvements that I've come across in using the library. Let me know if you don't like any of them and I can adjust/remove them.\r\n\r\n1. Printing datasets without a description field throws an error when formatting the `single_line_description`. This fixes that, and also adds some formatting to the repr to make it slightly more readable.\r\n```\r\n>>> print(list_datasets()[0])\r\nnlp.ObjectInfo(\r\n\tid='aeslc',\r\n\tdescription='A collection of email messages of employees in the Enron Corporation.There are two features:  - email_body: email body text.  - subject_line: email subject text.',\r\n\tfiles=[nlp.S3Object('aeslc.py'), nlp.S3Object('dataset_infos.json'), nlp.S3Object('dummy/1.0.0/dummy_data-zip-extracted/dummy_data/AESLC-master/enron_subject_line/dev/allen-p_inbox_29.subject'), nlp.S3Object('dummy/1.0.0/dummy_data-zip-extracted/dummy_data/AESLC-master/enron_subject_line/test/allen-p_inbox_24.subject'), nlp.S3Object('dummy/1.0.0/dummy_data-zip-extracted/dummy_data/AESLC-master/enron_subject_line/train/allen-p_inbox_20.subject'), nlp.S3Object('dummy/1.0.0/dummy_data.zip'), nlp.S3Object('urls_checksums/checksums.txt')]\r\n)\r\n```\r\n\r\n2. Add id-only option to `list_datasets` and `list_metrics` to allow the user to easily print out just the names of the datasets & metrics. I often found myself annoyed that this took so many strokes to do.\r\n\r\n```python\r\n[dataset.id for dataset in list_datasets()] # before\r\nlist_datasets(id_only=True) # after\r\n```\r\n\r\n3. Fix null-seed randomization caching. When using `train_test_split` and `shuffle`, the computation was being cached even without a seed or generator being passed. The result was that calling `.shuffle` more than once on the same dataset didn't do anything without passing a distinct seed or generator. Likewise with `train_test_split`.\r\n\r\n4. Indexing by iterables of bool. I added support for passing an iterable of type bool to `_getitem` as a numpy/pandas-like indexing method. Let me know if you think it's redundant with `filter` (I know it's not optimal memory-wise), but I think it's nice to have as a lightweight alternative to do simple things without having to create a copy of the entire dataset, e.g.\r\n\r\n```python\r\ndataset[dataset['label'] == 0] # numpy-like bool indexing to look at instances with labels of 0\r\n```\r\n\r\n5. Add an `input_column` argument to `map` and `filter`, which allows you to filter/map on a particular column rather than passing the whole dict to the function. Also adds `fn_kwargs` to be passed to the function. I think these together make mapping much cleaner in many cases such as mono-column tokenization:\r\n\r\n```python\r\n# before\r\ndataset = dataset.map(lambda batch: tokenizer(batch[\"text\"])\r\n# after\r\ndataset = dataset.map(tokenizer, input_column=\"text\")\r\ndataset = dataset.map(tokenizer, input_column=\"text\", fn_kwargs={\"truncation\": True, \"padding\": True})\r\n```",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 672407330,
    "title": "test_load_real_dataset when config has BUILDER_CONFIGS that matter",
    "dateCreated": "2020-08-03T23:46:36Z",
    "dateModified": "2020-08-03T23:46:36Z",
    "description": "It a dataset has custom `BUILDER_CONFIGS` with non-keyword arguments (or keyword arguments with non default values), the config is not loaded during the test and causes an error.\r\nI think the problem is that `test_load_real_dataset` calls `load_dataset` with `data_dir=temp_data_dir` ([here](https://github.com/huggingface/nlp/blob/master/tests/test_dataset_common.py#L200)). This causes [this line](https://github.com/huggingface/nlp/blob/master/src/nlp/builder.py#L201) to always be false because `config_kwargs` is not `None`. [This line](https://github.com/huggingface/nlp/blob/master/src/nlp/builder.py#L222) will be run instead, which doesn't use `BUILDER_CONFIGS`.\r\n\r\nFor an example, you can try running the test for lince:\r\n` RUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_real_dataset_lince`\r\nwhich yields\r\n> E           TypeError: __init__() missing 3 required positional arguments: 'colnames', 'classes', and 'label_column'",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 672007247,
    "title": "add DoQA dataset (ACL 2020)",
    "dateCreated": "2020-08-03T11:26:52Z",
    "dateModified": "2020-08-03T11:26:52Z",
    "description": "add DoQA dataset (ACL 2020) http://ixa.eus/node/12931",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 672000745,
    "title": "add crd3 dataset",
    "dateCreated": "2020-08-03T11:15:02Z",
    "dateModified": "2020-08-03T11:15:02Z",
    "description": "opening new PR for CRD3 dataset (ACL2020) to fix the circle CI problems",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 671996423,
    "title": "add reuters21578 dataset",
    "dateCreated": "2020-08-03T11:07:14Z",
    "dateModified": "2020-08-03T11:07:14Z",
    "description": "new PR to add the reuters21578 dataset and fix the circle CI problems.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 671952276,
    "title": "Adding IWSLT 2017  dataset.",
    "dateCreated": "2020-08-03T09:52:39Z",
    "dateModified": "2020-08-03T09:52:39Z",
    "description": "Created a [IWSLT 2017](https://sites.google.com/site/iwsltevaluation2017/TED-tasks) dataset script for the *multilingual data*.\r\n\r\n```\r\nBilingual data:  {Arabic, German, French, Japanese, Korean, Chinese} <-> English\r\nMultilingual data:   German, English, Italian, Dutch, Romanian. (Any pair)\r\n```\r\n\r\nI'm unsure how to handle bilingual vs multilingual. Given `nlp` architecture a Config option seems to be the way to go, however, it might be a bit confusing to have different language pairs with different option. Using just language pairs is not viable as English to German exists in both.\r\n\r\nAny opinion on how that should be done ?\r\nEDIT: I decided to just omit de-en from multilingual as it's only a subset of the bilingual one. That way only language pairs exist.\r\nEDIT : Could be interesting for #438 ",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 671876963,
    "title": "invalid data type 'str' at _convert_outputs in arrow_dataset.py",
    "dateCreated": "2020-08-03T07:48:29Z",
    "dateModified": "2020-08-03T07:48:29Z",
    "description": "I trying to build multi label text classifier model using Transformers lib. \r\n\r\nI'm using Transformers NLP to load the data set, while calling trainer.train() method. It throws the following error \r\n\r\nFile \"C:\\***\\arrow_dataset.py\", line 343, in _convert_outputs\r\n    v = command(v)\r\nTypeError: new(): invalid data type 'str'\r\n\r\nI'm using pyarrow 1.0.0.  And I have simple custom data set with Text and Integer Label.  \r\nEx: Data\r\n Text ,     Label  #Column Header\r\n I'm facing an Network issue, 1\r\n I forgot my password, 2\r\n\r\nError StackTrace:\r\n\r\nFile \"C:\\**\\transformers\\trainer.py\", line 492, in train\r\n    for step, inputs in enumerate(epoch_iterator):\r\n  File \"C:\\**\\tqdm\\std.py\", line 1104, in __iter__\r\n    for obj in iterable:\r\n  File \"C:\\**\\torch\\utils\\data\\dataloader.py\", line 345, in __next__\r\n    data = self._next_data()\r\n  File \"C:\\**\\torch\\utils\\data\\dataloader.py\", line 385, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"C:\\**\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"C:\\**\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"C:\\**\\nlp\\arrow_dataset.py\", line 414, in __getitem__\r\n    output_all_columns=self._output_all_columns,\r\n  File \"C:\\**\\nlp\\arrow_dataset.py\", line 403, in _getitem\r\n    outputs, format_type=format_type, format_columns=format_columns, output_all_columns=output_all_columns\r\n  File \"C:\\**\\nlp\\arrow_dataset.py\", line 343, in _convert_outputs\r\n    v = command(v)\r\nTypeError: new(): invalid data type 'str'\r\n \r\n",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 671622441,
    "title": "UnicodeDecodeError while loading PAN-X task of XTREME dataset",
    "dateCreated": "2020-08-02T14:05:10Z",
    "dateModified": "2020-08-02T14:05:10Z",
    "description": "Hi \ud83e\udd17  team!\r\n\r\n## Description of the problem\r\nI'm running into a `UnicodeDecodeError` while trying to load the PAN-X subset the XTREME dataset: \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nUnicodeDecodeError                        Traceback (most recent call last)\r\n<ipython-input-5-1d61f439b843> in <module>\r\n----> 1 dataset = load_dataset(\"xtreme\", \"PAN-X.en\", data_dir='./data')\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    528     ignore_verifications = ignore_verifications or save_infos\r\n    529     # Download/copy dataset processing script\r\n--> 530     module_path, hash = prepare_module(path, download_config=download_config, dataset=True)\r\n    531 \r\n    532     # Get dataset builder class from the processing script\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/load.py in prepare_module(path, download_config, dataset, force_local_path, **download_kwargs)\r\n    265 \r\n    266     # Download external imports if needed\r\n--> 267     imports = get_imports(local_path)\r\n    268     local_imports = []\r\n    269     library_imports = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/load.py in get_imports(file_path)\r\n    156     lines = []\r\n    157     with open(file_path, mode=\"r\") as f:\r\n--> 158         lines.extend(f.readlines())\r\n    159 \r\n    160     logger.info(\"Checking %s for additional imports.\", file_path)\r\n\r\n/usr/lib/python3.6/encodings/ascii.py in decode(self, input, final)\r\n     24 class IncrementalDecoder(codecs.IncrementalDecoder):\r\n     25     def decode(self, input, final=False):\r\n---> 26         return codecs.ascii_decode(input, self.errors)[0]\r\n     27 \r\n     28 class StreamWriter(Codec,codecs.StreamWriter):\r\n\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 111: ordinal not in range(128)\r\n```\r\n\r\n## Steps to reproduce\r\nInstall from nlp's master branch\r\n```python\r\npip install git+https://github.com/huggingface/nlp.git\r\n```\r\nthen run\r\n```python\r\nfrom nlp import load_dataset\r\n# AmazonPhotos.zip is located in data/\r\ndataset = load_dataset(\"xtreme\", \"PAN-X.en\", data_dir='./data')\r\n```\r\n\r\n## OS / platform details\r\n\r\n- `nlp` version: latest from master\r\n- Platform: Linux-4.15.0-72-generic-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyTorch version (GPU?): 1.4.0 (True)\r\n- Tensorflow version (GPU?): 2.1.0 (True)\r\n- Using GPU in script?: True\r\n- Using distributed or parallel set-up in script?: False\r\n\r\n## Proposed solution\r\nEither change [line 762](https://github.com/huggingface/nlp/blob/7ada00b1d62f94eee22a7df38c6b01e3f27194b7/datasets/xtreme/xtreme.py#L762) in `xtreme.py` to include UTF-8 encoding:\r\n\r\n```\r\n# old\r\nwith open(filepath) as f\r\n# new\r\nwith open(filepath, encoding='utf-8') as f\r\n```\r\n\r\nor raise a warning that suggests setting the locale explicitly, e.g.\r\n```python\r\nimport locale\r\nlocale.setlocale(locale.LC_ALL, 'C.UTF-8')\r\n```\r\nI have a preference for the first solution. Let me know if you agree and I'll be happy to implement the simple fix!",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 671580010,
    "title": "DOCS: Fix typo",
    "dateCreated": "2020-08-02T08:59:37Z",
    "dateModified": "2020-08-02T08:59:37Z",
    "description": "Fix typo from dictionnary -> dictionary",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 670766891,
    "title": "[METRICS] Various improvements on metrics",
    "dateCreated": "2020-08-01T11:03:45Z",
    "dateModified": "2020-08-01T11:03:45Z",
    "description": "- Disallow the use of positional arguments to avoid `predictions` vs `references` mistakes\r\n- Allow to directly feed numpy/pytorch/tensorflow/pandas objects in metrics",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 669889779,
    "title": "Keep features after transform",
    "dateCreated": "2020-07-31T14:43:21Z",
    "dateModified": "2020-07-31T14:43:21Z",
    "description": "When applying a transform like `map`, some features were lost (and inferred features were used).\r\nIt was the case for ClassLabel, Translation, etc.\r\n\r\nTo fix that, I did some modifications in the `ArrowWriter`:\r\n\r\n- added the `update_features` parameter. When it's `True`, then the features specified by the user (if any) can be updated with inferred features if their type don't match. `map` transform sets `update_features=True` when writing to cache file or buffer. Features won't change by default in `map`.\r\n\r\n- added the `with_metadata` parameter. If `True`, the `features` (after update) will be written inside the metadata of the schema in this format:\r\n```\r\n{\r\n    \"huggingface\": {\"features\" : <serialized Features exactly like dataset_info.json>}\r\n} \r\n```\r\nThen, once a dataset is instantiated without info/features, these metadata are used to set the features of the dataset.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 669767381,
    "title": "Add rename, remove and cast in-place operations",
    "dateCreated": "2020-07-31T12:30:21Z",
    "dateModified": "2020-07-31T12:30:21Z",
    "description": "Add a bunch of in-place operation leveraging the Arrow back-end to rename and remove columns and cast to new features without using the more expensive `map` method.\r\n\r\nThese methods are added to `Dataset` as well as `DatasetDict`.\r\n\r\nAdded tests for these new methods and add the methods to the doc.\r\n\r\nNaming follows the new pattern with a trailing underscore indicating in-place methods.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 669735455,
    "title": "Add dataset/mlsum",
    "dateCreated": "2020-07-31T11:50:52Z",
    "dateModified": "2020-07-31T11:50:52Z",
    "description": "New pull request that should correct the previous errors. \r\n\r\nThe load_real_data stills fails because it is looking for a default dataset URL that does not exists, this does not happen when loading the dataset with load_dataset",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 669715547,
    "title": "add DoQA (ACL 2020) dataset",
    "dateCreated": "2020-07-31T11:25:56Z",
    "dateModified": "2020-07-31T11:25:56Z",
    "description": "adds DoQA (ACL 2020) dataset",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 669703508,
    "title": "Doqa",
    "dateCreated": "2020-07-31T11:11:12Z",
    "dateModified": "2020-07-31T11:11:12Z",
    "description": "add DoQA (ACL 2020) dataset",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 669585256,
    "title": "Fix KeyboardInterrupt in map and bad indices in select",
    "dateCreated": "2020-07-31T08:57:15Z",
    "dateModified": "2020-07-31T08:57:15Z",
    "description": "If you interrupted a map function while it was writing, the cached file was not discarded.\r\nTherefore the next time you called map, it was loading an incomplete arrow file.\r\n\r\nWe had the same issue with select if there was a bad indice at one point.\r\n\r\nTo fix that I used temporary files that are renamed once everything is finished.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 669545437,
    "title": "[Breaking] Update Dataset and DatasetDict API",
    "dateCreated": "2020-07-31T08:11:33Z",
    "dateModified": "2020-07-31T08:11:33Z",
    "description": "This PR contains a few breaking changes so it's probably good to keep it for the next (major) release:\r\n- rename the `flatten`, `drop` and `dictionary_encode_column` methods in `flatten_`, `drop_` and `dictionary_encode_column_` to indicate that these methods have in-place effects as discussed in #166. From now on we should keep the convention of having a trailing underscore for methods which have an in-place effet. I also adopt the conversion of not returning the (self) dataset for these methods. This is different than what PyTorch does for instance (`model.to()` is in-place but return the self model) but I feel like it's a safer approach in terms of UX.\r\n- remove the `dataset.columns` property which returns a low-level Apache Arrow object and should not be used by users. Similarly, remove `dataset. nbytes` which we don't really want to expose in this bare-bone format.\r\n- add a few more properties and methods to `DatasetDict`",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 668972666,
    "title": "Install CoVal metric from github",
    "dateCreated": "2020-07-30T16:59:25Z",
    "dateModified": "2020-07-30T16:59:25Z",
    "description": "Changed the import statements in `coval.py` to direct the user to install the original package from github if it's not already installed (the warning will only display properly after merging [PR455](https://github.com/huggingface/nlp/pull/455))\r\n\r\nAlso changed the function call to use named rather than positional arguments.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 668898386,
    "title": "add set_format to DatasetDict + tests",
    "dateCreated": "2020-07-30T15:53:20Z",
    "dateModified": "2020-07-30T15:53:20Z",
    "description": "Add the `set_format` and `formated_as` and `reset_format` to `DatasetDict`.\r\nAdd tests to these for `Dataset` and `DatasetDict`.\r\nFix some bugs uncovered by the tests for `pandas` formating.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 668723785,
    "title": "add crd3(ACL 2020) dataset",
    "dateCreated": "2020-07-30T13:28:35Z",
    "dateModified": "2020-07-30T13:28:35Z",
    "description": "This PR adds the **Critical Role Dungeons and Dragons Dataset** published at ACL 2020",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 668037965,
    "title": "Add bleurt",
    "dateCreated": "2020-07-29T18:08:32Z",
    "dateModified": "2020-07-29T18:08:32Z",
    "description": "This PR adds the BLEURT metric to the library.\r\n\r\nThe BLEURT `Metric` downloads a TF checkpoint corresponding to its `config_name` at creation (in the `_info` function). Default is set to `bleurt-base-128`.\r\n\r\nNote that the default in the original package is `bleurt-tiny-128`, but they throw a warning and recommend using `bleurt-base-128` instead. I think it's safer to have our users have a functioning metric when they call the default behavior, we'll address discrepancies in the issues/discussions if it comes up.\r\n\r\nIn addition to the BLEURT file, `load.py` was changed so we can ask users to pip install the required packages from git when they have a `setup.py` but are not on PyPL\r\n\r\ncc @ankparikh @tsellam",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 668011577,
    "title": "Create SECURITY.md",
    "dateCreated": "2020-07-29T17:23:34Z",
    "dateModified": "2020-07-29T17:23:34Z",
    "description": "",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 667728247,
    "title": "add builder tests",
    "dateCreated": "2020-07-29T10:22:07Z",
    "dateModified": "2020-07-29T10:22:07Z",
    "description": "I added `as_dataset` and `download_and_prepare` to the tests",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 667498295,
    "title": "Guardian authorship dataset",
    "dateCreated": "2020-07-29T02:23:57Z",
    "dateModified": "2020-07-29T02:23:57Z",
    "description": "A new dataset: Guardian news articles for authorship attribution\r\n\r\n**tests passed:**\r\npython nlp-cli dummy_data datasets/guardian_authorship --save_infos --all_configs\r\n\r\nRUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_dataset_all_configs_guardian_authorship\r\n\r\n**Tests failed:**\r\nReal data: RUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_real_dataset_guardian_authorship\r\noutput: __init__() missing 3 required positional arguments: 'train_folder', 'valid_folder', and 'tes...' \r\n\r\nRemarks: This is the init function of my class. I am not sure why it passes in both my tests and with nlp-cli, but fails here. By the way, I ran this command with another 2 datasets and they failed:\r\n* _glue - OSError: Cannot find data file.\r\n*_newsgroup - FileNotFoundError: Local file datasets/newsgroup/dummy/18828_comp.graphics/3.0.0/dummy_data.zip doesn't exist\r\n\r\nThank you for letting us contribute to such a huge and important library! \r\n\r\nEDIT:\r\nI was able to fix the dummy_data issue. This dataset has around 14 configurations. I was testing with only 2, but their versions were not in a sequence, they were V1.0.0 and V.12.0.0. It seems that the testing code generates testes for all the versions from 0 to MAX, and was testing for versions (and dummy_data.zip files) that do not exist. I fixed that by changing the versions to 1 and 2.\r\n\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 667210468,
    "title": "Fix csv/json/txt cache dir",
    "dateCreated": "2020-07-28T16:30:51Z",
    "dateModified": "2020-07-28T16:30:51Z",
    "description": "The cache dir for csv/json/txt datasets was always the same. This is an issue because it should be different depending on the data files provided by the user.\r\n\r\nTo fix that, I added a line that use the hash of the data files provided by the user to define the cache dir.\r\n\r\nThis should fix #444 ",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 667074120,
    "title": "add sogou_news",
    "dateCreated": "2020-07-28T13:29:10Z",
    "dateModified": "2020-07-28T13:29:10Z",
    "description": "This PR adds the sogou news dataset\r\n#353 ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 666898923,
    "title": "add reuters21578 dataset",
    "dateCreated": "2020-07-28T08:58:12Z",
    "dateModified": "2020-07-28T08:58:12Z",
    "description": "This PR adds the `Reuters_21578` dataset https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html  \r\n#353 \r\n\r\nThe datasets is a lit of `.sgm` files which are a bit different from xml file indeed `xml.etree` couldn't be used to read files. I consider them as text file (to avoid using external library)  and read line by line (maybe there is a better way to do, happy to get your opinion on it)\r\n\r\nIn the Readme file 3 ways to split the dataset are given.:\r\n\r\n- The Modified Lewis (\"ModLewis\") Split: train, test and unused-set\r\n\r\n- The Modified Apte (\"ModApte\") Split : train, test and unused-set\r\n\r\n- The Modified Hayes (\"ModHayes\") Split: train and test\r\n\r\nHere I consider the last one as the readme file highlight that this split provides the ability to compare results with those of the 2 first splits.\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 666893443,
    "title": "add aws load metric test",
    "dateCreated": "2020-07-28T08:50:22Z",
    "dateModified": "2020-07-28T08:50:22Z",
    "description": "Following issue #445\r\nAdded a test to recognize import errors of all metrics",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 666842115,
    "title": "[BugFix] fix wrong import of DEFAULT_TOKENIZER",
    "dateCreated": "2020-07-28T07:41:10Z",
    "dateModified": "2020-07-28T07:41:10Z",
    "description": "Fixed the path to `DEFAULT_TOKENIZER`\r\n#445",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 666837351,
    "title": "[BugFix] fix wrong import of DEFAULT_TOKENIZER",
    "dateCreated": "2020-07-28T07:32:47Z",
    "dateModified": "2020-07-28T07:32:47Z",
    "description": "Fixed the path to `DEFAULT_TOKENIZER`\r\n#445 ",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 666836658,
    "title": "DEFAULT_TOKENIZER import error in sacrebleu",
    "dateCreated": "2020-07-28T07:31:30Z",
    "dateModified": "2020-07-28T07:31:30Z",
    "description": "Latest Version 0.3.0\r\n\r\nWhen loading the metric \"sacrebleu\" there is an import error due to the wrong path\r\n![image](https://user-images.githubusercontent.com/5303103/88633063-2c5e5f00-d0bd-11ea-8ca8-4704dc975433.png)\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 666280842,
    "title": "Keep loading old file even I specify a new file in load_dataset",
    "dateCreated": "2020-07-27T13:08:06Z",
    "dateModified": "2020-07-27T13:08:06Z",
    "description": "I used load a file called 'a.csv' by \r\n```\r\ndataset = load_dataset('csv', data_file='./a.csv')\r\n```\r\nAnd after a while, I tried to load another csv called 'b.csv'\r\n```\r\ndataset = load_dataset('csv', data_file='./b.csv')\r\n```\r\nHowever, the new dataset seems to remain the old 'a.csv' and not loading new csv file.\r\n\r\nEven worse, after I load a.csv, the load_dataset function keeps loading the 'a.csv' afterward. \r\n\r\nIs this a cache problem?\r\n",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 666246716,
    "title": "Cannot unpickle saved .pt dataset with torch.save()/load()",
    "dateCreated": "2020-07-27T12:13:37Z",
    "dateModified": "2020-07-27T12:13:37Z",
    "description": "Saving a formatted torch dataset to file using `torch.save()`. Loading the same file fails during unpickling:\r\n\r\n```python\r\n>>> import torch\r\n>>> import nlp\r\n\r\n>>> squad = nlp.load_dataset(\"squad.py\", split=\"train\")\r\n>>> squad\r\nDataset(features: {'source_text': Value(dtype='string', id=None), 'target_text': Value(dtype='string', id=None)}, num_rows: 87599)\r\n>>> squad = squad.map(create_features, batched=True)\r\n>>> squad.set_format(type=\"torch\", columns=[\"source_ids\", \"target_ids\", \"attention_mask\"])\r\n>>> torch.save(squad, \"squad.pt\")\r\n\r\n>>> squad_pt = torch.load(\"squad.pt\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\", line 593, in load\r\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n  File \"/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\", line 773, in _legacy_load\r\n    result = unpickler.load()\r\n  File \"/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/nlp/splits.py\", line 493, in __setitem__\r\n    raise ValueError(\"Cannot add elem. Use .add() instead.\")\r\nValueError: Cannot add elem. Use .add() instead.\r\n```\r\nwhere `create_features` is a function that tokenizes the data using `batch_encode_plus` and returns a Dict with `input_ids`, `target_ids` and `attention_mask`. \r\n```python\r\ndef create_features(batch):\r\n    source_text_encoding = tokenizer.batch_encode_plus(\r\n        batch[\"source_text\"],\r\n        max_length=max_source_length,\r\n        pad_to_max_length=True,\r\n        truncation=True)\r\n\r\n    target_text_encoding = tokenizer.batch_encode_plus(\r\n        batch[\"target_text\"],\r\n        max_length=max_target_length,\r\n        pad_to_max_length=True,\r\n        truncation=True)\r\n\r\n    features = {\r\n        \"source_ids\": source_text_encoding[\"input_ids\"],\r\n        \"target_ids\": target_text_encoding[\"input_ids\"],\r\n        \"attention_mask\": source_text_encoding[\"attention_mask\"]\r\n    }\r\n\r\n    return features\r\n```\r\n\r\nI found a similar issue in [issue 5267 in the huggingface/transformers repo](https://github.com/huggingface/transformers/issues/5267) which was solved by downgrading to `nlp==0.2.0`. That did not solve this problem, however. ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 666201810,
    "title": "[Suggestion] Glue Diagnostic Data with Labels ",
    "dateCreated": "2020-07-27T10:59:58Z",
    "dateModified": "2020-07-27T10:59:58Z",
    "description": "Hello! First of all, thanks for setting up this useful project!\r\n\r\nI've just realised you provide the the [Glue Diagnostics Data](https://huggingface.co/nlp/viewer/?dataset=glue&config=ax) without labels, indicating in the `GlueConfig` that you've only a test set.\r\n\r\nYet, the data with labels is available, too (see also [here](https://gluebenchmark.com/diagnostics#introduction)):\r\n\r\nhttps://www.dropbox.com/s/ju7d95ifb072q9f/diagnostic-full.tsv?dl=1 \r\n\r\nHave you considered incorporating it?",
    "status": "open",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 666148413,
    "title": "Add features parameter in load dataset",
    "dateCreated": "2020-07-27T09:50:01Z",
    "dateModified": "2020-07-27T09:50:01Z",
    "description": "Added `features` argument in `nlp.load_dataset`.\r\nIf they don't match the data type, it raises a `ValueError`.\r\n\r\nIt's a draft PR because #440 needs to be merged first.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 666116823,
    "title": "Fix user specified features in map",
    "dateCreated": "2020-07-27T09:04:26Z",
    "dateModified": "2020-07-27T09:04:26Z",
    "description": "`.map` didn't keep the user specified features because of an issue in the writer.\r\nThe writer used to overwrite the user specified features with inferred features.\r\n\r\nI also added tests to make sure it doesn't happen again.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 665964673,
    "title": "Issues: Adding a FAISS or Elastic Search index to a Dataset",
    "dateCreated": "2020-07-27T04:25:17Z",
    "dateModified": "2020-07-27T04:25:17Z",
    "description": "It seems the DPRContextEncoder, DPRContextEncoderTokenizer cited[ in this documentation](https://huggingface.co/nlp/faiss_and_ea.html) is not implemented ? It didnot work with the standard nlp installation . Also, I couldn't find or use it with the latest nlp install from github in Colab.  Is  there any dependency on the latest PyArrow 1.0.0 ? Is it yet to be made generally available ?",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 665865490,
    "title": "New Datasets: IWSLT15+, ITTB",
    "dateCreated": "2020-07-26T21:43:04Z",
    "dateModified": "2020-07-26T21:43:04Z",
    "description": "**Links:**\r\n[iwslt](https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/datasets/iwslt.html)\r\nDon't know if that link is up to date.\r\n\r\n[ittb](http://www.cfilt.iitb.ac.in/iitb_parallel/)\r\n**Motivation**: replicate mbart finetuning results (table below)\r\n![image](https://user-images.githubusercontent.com/6045025/88490093-0c1c8c00-cf67-11ea-960d-8dcaad2aa8eb.png)\r\n\r\n\r\nFor future readers, we already have the following language pairs in the wmt namespaces:\r\n\r\n```\r\nwmt14: ['cs-en', 'de-en', 'fr-en', 'hi-en', 'ru-en']\r\nwmt15: ['cs-en', 'de-en', 'fi-en', 'fr-en', 'ru-en']\r\nwmt16: ['cs-en', 'de-en', 'fi-en', 'ro-en', 'ru-en', 'tr-en']\r\nwmt17: ['cs-en', 'de-en', 'fi-en', 'lv-en', 'ru-en', 'tr-en', 'zh-en']\r\nwmt18: ['cs-en', 'de-en', 'et-en', 'fi-en', 'kk-en', 'ru-en', 'tr-en', 'zh-en']\r\nwmt19: ['cs-en', 'de-en', 'fi-en', 'gu-en', 'kk-en', 'lt-en', 'ru-en', 'zh-en', 'fr-de']\r\n```",
    "status": "open",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 665597176,
    "title": "Fix XTREME PAN-X loading",
    "dateCreated": "2020-07-25T14:44:57Z",
    "dateModified": "2020-07-25T14:44:57Z",
    "description": "Hi \ud83e\udd17 \r\nIn response to the discussion in #425 @lewtun and I made some fixes to the repo. In the original XTREME implementation the PAN-X dataset for named entity recognition loaded each word/tag pair as a single row and the sentence relation was lost. With the fix each row contains the list of all words in a single sentence and their NER tags. This is also in agreement with the [NER example](https://github.com/huggingface/transformers/tree/master/examples/token-classification) in the transformers repo.\r\n\r\nWith the fix the output of the dataset should look as follows:\r\n```python\r\n>>> dataset = load_dataset(\"xtreme\", \"PAN-X.en\", data_dir='./data')\r\n>>> dataset['train'][0]\r\n{'words': ['R.H.', 'Saunders', '(', 'St.', 'Lawrence', 'River', ')', '(', '968', 'MW', ')'],\r\n 'ner_tags': ['B-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O'],\r\n 'langs': ['en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en']}\r\n```",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 665582167,
    "title": "Google Colab - load_dataset - PyArrow exception",
    "dateCreated": "2020-07-25T13:05:20Z",
    "dateModified": "2020-07-25T13:05:20Z",
    "description": "With latest PyArrow 1.0.0 installed, I get the following exception   . Restarting colab has the same issue\r\n\r\nImportWarning: To use `nlp`, the module `pyarrow>=0.16.0` is required, and the current version of `pyarrow` doesn't match this condition. If you are running this in a Google Colab, you should probably just restart the runtime to use the right version of `pyarrow`.\r\n\r\nThe error goes only when I install version 0.16.0 \r\ni.e.  !pip install pyarrow==0.16.0",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 665507141,
    "title": "ImportWarning for pyarrow 1.0.0",
    "dateCreated": "2020-07-25T03:44:39Z",
    "dateModified": "2020-07-25T03:44:39Z",
    "description": "The following PR raised ImportWarning at `pyarrow ==1.0.0` https://github.com/huggingface/nlp/pull/265/files",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 665477638,
    "title": "Fixed check for pyarrow",
    "dateCreated": "2020-07-25T00:16:53Z",
    "dateModified": "2020-07-25T00:16:53Z",
    "description": "Fix check for pyarrow in __init__.py.  Previously would raise an error for pyarrow >= 1.0.0",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 665311025,
    "title": "How to reuse functionality of a (generic) dataset?",
    "dateCreated": "2020-07-24T17:27:37Z",
    "dateModified": "2020-07-24T17:27:37Z",
    "description": "I have written a generic dataset for corpora created with the Brat annotation tool ([specification](https://brat.nlplab.org/standoff.html), [dataset code](https://github.com/ArneBinder/nlp/blob/brat/datasets/brat/brat.py)). Now I wonder how to use that to create specific dataset instances. What's the recommended way to reuse formats and loading functionality for datasets with a common format?\r\n\r\nIn my case, it took a bit of time to create the Brat dataset and I think others would appreciate to not have to think about that again. Also, I assume there are other formats (e.g. conll) that are widely used, so having this would really ease dataset onboarding and adoption of the library.",
    "status": "open",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 665234340,
    "title": "Fix handling of config files while loading datasets from multiple processes",
    "dateCreated": "2020-07-24T15:10:57Z",
    "dateModified": "2020-07-24T15:10:57Z",
    "description": "When loading shards on several processes, each process upon loading the dataset will overwrite dataset_infos.json in <package path>/datasets/<dataset name>/<hash>/dataset_infos.json. It does so every time, even when the target file already exists and is identical. Because multiple processes rewrite the same file in parallel, it creates a race condition when a process tries to load the file, often resulting in a JSON decoding exception because the file is only partially written.\r\n\r\nThis pull requests partially address this by comparing if the files are already identical before copying over the downloaded copy to the cached destination. There's still a race condition, but now it's less likely to occur if some basic precautions are taken by the library user, e.g., download all datasets to cache before spawning multiple processes.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 665044416,
    "title": "Specify split post processing + Add post processing resources downloading",
    "dateCreated": "2020-07-24T09:29:19Z",
    "dateModified": "2020-07-24T09:29:19Z",
    "description": "Previously if you tried to do\r\n\r\n```python\r\nfrom nlp import load_dataset\r\nwiki = load_dataset(\"wiki_dpr\", \"psgs_w100_with_nq_embeddings\", split=\"train[:100]\", with_index=True)\r\n```\r\nThen you'd get an error `Index size should match Dataset size...`\r\nThis was because it was trying to use the full index (21M elements).\r\n\r\nTo fix that I made it so post processing resources can be named according to the split.\r\n\r\nI'm going to add tests on post processing too.\r\n\r\nNote that the CI will fail as I added a new argument in `_post_processing_resources`: the AWS version of wiki_dpr fails, and there's also an error telling that it is not synced (it'll be synced once it's merged):\r\n```\r\n=========================== short test summary info ============================\r\nFAILED tests/test_dataset_common.py::AWSDatasetTest::test_load_dataset_wiki_dpr\r\nFAILED tests/test_hf_gcp.py::TestDatasetSynced::test_script_synced_with_s3_wiki_dpr\r\n```\r\n\r\nEDIT: I did a change to ignore the script hash to locate the arrow files on GCS, so I removed the sync test. It was there just because of the hash logic for files on GCS",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 664583837,
    "title": "add DatasetDict",
    "dateCreated": "2020-07-23T15:43:49Z",
    "dateModified": "2020-07-23T15:43:49Z",
    "description": "## Add DatasetDict\r\n\r\n### Overview\r\n\r\nWhen you call `load_dataset` it can return a dictionary of datasets if there are several splits (train/test for example).\r\nIf you wanted to apply dataset transforms you had to iterate over each split and apply the transform.\r\n\r\nInstead of returning a dict, it now returns a `nlp.DatasetDict` object which inherits from dict and contains the same data as before, except that now users can call dataset transforms directly from the output, and they'll be applied on each split.\r\n\r\nBefore:\r\n```python\r\nfrom nlp import load_dataset\r\n\r\nsquad = load_dataset(\"squad\")\r\nprint(squad.keys())\r\n# dict_keys(['train', 'validation'])\r\nsquad = {\r\n    split_name: dataset.map(my_func) for split_name, dataset in squad.items()\r\n}\r\nprint(squad.keys())\r\n# dict_keys(['train', 'validation'])\r\n```\r\n\r\nNow:\r\n```python\r\nfrom nlp import load_dataset\r\n\r\nsquad = load_dataset(\"squad\")\r\nprint(squad.keys())\r\n# dict_keys(['train', 'validation'])\r\nsquad = squad.map(my_func)\r\nprint(squad.keys())\r\n# dict_keys(['train', 'validation'])\r\n```\r\n\r\n### Dataset transforms\r\n\r\n`nlp.DatasetDict` implements the following dataset transforms:\r\n- map\r\n- filter\r\n- sort\r\n- shuffle\r\n\r\n### Arguments\r\n\r\nThe arguments of the methods are the same except for split-specific arguments like `cache_file_name`.\r\nFor such arguments, the expected input is a dictionary `{split_name: argument_value}`\r\nIt concerns:\r\n- `cache_file_name` in map, filter, sort, shuffle\r\n- `seed` and `generator` in shuffle",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 664412137,
    "title": "mlsum",
    "dateCreated": "2020-07-23T11:52:39Z",
    "dateModified": "2020-07-23T11:52:39Z",
    "description": "Hello, \r\n\r\nThe tests for the load_real_data fail, as there is no default language subset to download it looks for a file that does not exist. This bug does not happen when using the load_dataset function, as it asks you to specify a language if you do not, so I submit this PR anyway. The dataset is avalaible on : https://gitlab.lip6.fr/scialom/mlsum_data",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 664367086,
    "title": "fix concatenate_datasets",
    "dateCreated": "2020-07-23T10:30:59Z",
    "dateModified": "2020-07-23T10:30:59Z",
    "description": "`concatenate_datatsets` used to test that the different`nlp.Dataset.schema` match, but this attribute was removed in #423 ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 664341623,
    "title": "Allow sequence features for beam + add processed Natural Questions",
    "dateCreated": "2020-07-23T09:52:41Z",
    "dateModified": "2020-07-23T09:52:41Z",
    "description": "## Allow Sequence features for Beam Datasets + add Natural Questions\r\n\r\n### The issue\r\n\r\nThe steps of beam datasets processing is the following:\r\n- download the source files and send them in a remote storage (gcs)\r\n- process the files using a beam runner (dataflow)\r\n- save output in remote storage (gcs)\r\n- convert output to arrow in remote storage (gcs)\r\n\r\nHowever it wasn't possible to process `natural_questions` because apache beam's processing outputs parquet files, and it's not yet possible to read parquet files with list features.\r\n\r\n### The proposed solution\r\n\r\nTo allow sequence features for beam I added a workaround that serializes the values using `json.dumps`, so that we end up with strings instead of the original features. Then when the arrow file is created, the serialized objects are transformed back to normal with `json.loads`. Not sure if there's a better way to do it.\r\n\r\n### Natural Questions\r\n\r\nI was able to process NQ with it, and so I added the json infos file in this PR too.\r\nThe processed arrow files are also stored in gcs.\r\nIt allows you to load NQ with\r\n\r\n```python\r\nfrom nlp import load_dataset\r\nnq = load_dataset(\"natural_questions\")  # download the 90GB arrow files from gcs and return the dataset\r\n```\r\n\r\n### Tests\r\n\r\nI added a test case to make sure it works as expected.\r\nNote that the CI will fail because I am updating `natural_questions.py`: it's not synced with the script on S3. It will be synced as soon as this PR is merged.\r\n```\r\n=========================== short test summary info ============================\r\nFAILED tests/test_hf_gcp.py::TestDatasetOnHfGcp::test_script_synced_with_s3_natural_questions/default\r\n```",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 664203897,
    "title": "[FEATURE REQUEST] Multiprocessing with for dataset.map, dataset.filter",
    "dateCreated": "2020-07-23T05:00:41Z",
    "dateModified": "2020-07-23T05:00:41Z",
    "description": "It would be nice to be able to speed up `dataset.map` or `dataset.filter`. Perhaps this is as easy as sharding the dataset sending each shard to a process/thread/dask pool and using the new `nlp.concatenate_dataset()` function to join them all together?",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 664029848,
    "title": "Correct data structure for PAN-X task in XTREME dataset?",
    "dateCreated": "2020-07-22T20:29:20Z",
    "dateModified": "2020-07-22T20:29:20Z",
    "description": "Hi \ud83e\udd17  team!\r\n\r\n## Description of the problem\r\nThanks to the fix from #416 I am now able to load the NER task in the XTREME dataset as follows:\r\n\r\n```python\r\nfrom nlp import load_dataset\r\n# AmazonPhotos.zip is located in data/\r\ndataset = load_dataset(\"xtreme\", \"PAN-X.en\", data_dir='./data')\r\ndataset_train = dataset['train']\r\n```\r\n\r\nHowever, I am not sure that `load_dataset()` is returning the correct data structure for NER. \r\n\r\nCurrently, every row in `dataset_train` is of the form\r\n```python\r\n{'word': str, 'ner_tag': str, 'lang': str}\r\n```\r\nbut I think we actually want something like\r\n```python\r\n{'words': List[str], 'ner_tags': List[str], 'langs': List[str]}\r\n```\r\nso that each row corresponds to a _sequence_ of words associated with each example. With the current data structure I do not think it is possible to transform `dataset_train` into a form suitable for training because we do not know the boundaries between examples.\r\n\r\nIndeed, [this line](https://github.com/google-research/xtreme/blob/522434d1aece34131d997a97ce7e9242a51a688a/third_party/utils_tag.py#L58) in the XTREME repo, processes the texts as lists of sentences, tags, and languages.\r\n\r\n## Proposed solution\r\nReplace\r\n```python\r\nwith open(filepath) as f:\r\n    data = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\r\n    for id_, row in enumerate(data):\r\n        if row:\r\n            lang, word = row[0].split(\":\")[0], row[0].split(\":\")[1]\r\n            tag = row[1]\r\n            yield id_, {\"word\": word, \"ner_tag\": tag, \"lang\": lang}\r\n```\r\nfrom  [these lines](https://github.com/huggingface/nlp/blob/ce7d3a1d630b78fe27188d1706f3ea980e8eec43/datasets/xtreme/xtreme.py#L881-L887) of the `_generate_examples()` function with something like\r\n\r\n```python\r\nguid_index = 1\r\nwith open(filepath, encoding=\"utf-8\") as f:\r\n    words = []\r\n    ner_tags = []\r\n    langs = []\r\n    for line in f:\r\n        if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\r\n            if words:\r\n                yield guid_index, {\"words\": words, \"ner_tags\": ner_tags, \"langs\": langs}\r\n                guid_index += 1\r\n                words = []\r\n                ner_tags = []\r\n        else:\r\n            # pan-x data is tab separated\r\n            splits = line.split(\"\\t\")\r\n            # strip out en: prefix\r\n            langs.append(splits[0][:2])\r\n            words.append(splits[0][3:])\r\n            if len(splits) > 1:\r\n                labels.append(splits[-1].replace(\"\\n\", \"\"))\r\n            else:\r\n                # examples have no label in test set\r\n                labels.append(\"O\")\r\n```\r\nIf you agree, me or @lvwerra would be happy to implement this and create a PR.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 663858552,
    "title": "Web of science",
    "dateCreated": "2020-07-22T15:38:31Z",
    "dateModified": "2020-07-22T15:38:31Z",
    "description": "this PR adds the WebofScience dataset\r\n#353 ",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 663079359,
    "title": "Change features vs schema logic",
    "dateCreated": "2020-07-21T14:52:47Z",
    "dateModified": "2020-07-21T14:52:47Z",
    "description": "## New logic for `nlp.Features` in datasets\r\n\r\nPreviously, it was confusing to have `features` and pyarrow's `schema` in `nlp.Dataset`.\r\nHowever `features` is supposed to be the front-facing object to define the different fields of a dataset, while `schema` is only used to write arrow files.\r\n\r\nChanges:\r\n- Remove `schema` field in `nlp.Dataset`\r\n- Make `features` the source of truth to read/write examples\r\n- `features` can no longer be `None` in `nlp.Dataset`\r\n- Update `features` after each dataset transform such as `nlp.Dataset.map`\r\n\r\nTodo: change the tests to take these changes into account",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 663028497,
    "title": "- Corrected encoding for IMDB.",
    "dateCreated": "2020-07-21T13:46:59Z",
    "dateModified": "2020-07-21T13:46:59Z",
    "description": "The preparation phase (after the download phase) crashed on windows because of charmap encoding not being able to decode certain characters. This change suggested in Issue #347 fixes it for the IMDB dataset.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 662213864,
    "title": "Style change",
    "dateCreated": "2020-07-20T20:08:29Z",
    "dateModified": "2020-07-20T20:08:29Z",
    "description": "make quality and make style ran on scripts",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 662029782,
    "title": "Better handle nested features",
    "dateCreated": "2020-07-20T16:44:13Z",
    "dateModified": "2020-07-20T16:44:13Z",
    "description": "Changes:\r\n- added arrow schema to features conversion (it's going to be useful to fix #342 )\r\n- make flatten handle deep features (useful for tfrecords conversion in #339 )\r\n- add tests for flatten and features conversions\r\n- the reader now returns the kwargs to instantiate a Dataset (fix circular dependencies)",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 661974747,
    "title": "EmoContext dataset add",
    "dateCreated": "2020-07-20T15:48:45Z",
    "dateModified": "2020-07-20T15:48:45Z",
    "description": "EmoContext Dataset add\r\n\r\nSigned-off-by: lordtt13 <thakurtanmay72@yahoo.com>",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 661914873,
    "title": "Addition of google drive links to dl_manager",
    "dateCreated": "2020-07-20T14:52:02Z",
    "dateModified": "2020-07-20T14:52:02Z",
    "description": "Hello there, I followed the template to create a download script of my own, which works fine for me, although I had to shun the dl_manager because it was downloading nothing from the drive links and instead use gdown.\r\n\r\nThis is the script for me:\r\n\r\n```python\r\nclass EmoConfig(nlp.BuilderConfig):\r\n    \"\"\"BuilderConfig for SQUAD.\"\"\"\r\n\r\n    def __init__(self, **kwargs):\r\n        \"\"\"BuilderConfig for EmoContext.\r\n    Args:\r\n      **kwargs: keyword arguments forwarded to super.\r\n    \"\"\"\r\n        super(EmoConfig, self).__init__(**kwargs)\r\n\r\n_TEST_URL = \"https://drive.google.com/file/d/1Hn5ytHSSoGOC4sjm3wYy0Dh0oY_oXBbb/view?usp=sharing\"\r\n_TRAIN_URL = \"https://drive.google.com/file/d/12Uz59TYg_NtxOy7SXraYeXPMRT7oaO7X/view?usp=sharing\"\r\n\r\nclass EmoDataset(nlp.GeneratorBasedBuilder):\r\n    \"\"\" SemEval-2019 Task 3: EmoContext Contextual Emotion Detection in Text. Version 1.0.0 \"\"\"\r\n\r\n    VERSION = nlp.Version(\"1.0.0\")\r\n    force = False\r\n\r\n    def _info(self):\r\n        return nlp.DatasetInfo(\r\n            description=_DESCRIPTION,\r\n            features=nlp.Features(\r\n                {\r\n                    \"text\": nlp.Value(\"string\"),\r\n                    \"label\": nlp.features.ClassLabel(names=[\"others\", \"happy\", \"sad\", \"angry\"]),\r\n                }\r\n            ),\r\n            supervised_keys=None,\r\n            homepage=\"https://www.aclweb.org/anthology/S19-2005/\",\r\n            citation=_CITATION,\r\n        )\r\n    \r\n    def _get_drive_url(self, url):\r\n        base_url = 'https://drive.google.com/uc?id='\r\n        split_url = url.split('/')\r\n        return base_url + split_url[5]\r\n    \r\n    def _split_generators(self, dl_manager):\r\n        \"\"\"Returns SplitGenerators.\"\"\"\r\n        if(not os.path.exists(\"emo-train.json\") or self.force):\r\n            gdown.download(self._get_drive_url(_TRAIN_URL), \"emo-train.json\", quiet = True)\r\n        if(not os.path.exists(\"emo-test.json\") or self.force):\r\n            gdown.download(self._get_drive_url(_TEST_URL), \"emo-test.json\", quiet = True)\r\n        return [\r\n            nlp.SplitGenerator(\r\n                name=nlp.Split.TRAIN,\r\n                gen_kwargs={\r\n                    \"filepath\": \"emo-train.json\",\r\n                    \"split\": \"train\",\r\n                },\r\n            ),\r\n            nlp.SplitGenerator(\r\n                name=nlp.Split.TEST,\r\n                gen_kwargs={\"filepath\": \"emo-test.json\", \"split\": \"test\"},\r\n            ),\r\n        ]\r\n\r\n    def _generate_examples(self, filepath, split):\r\n        \"\"\" Yields examples. \"\"\"\r\n        with open(filepath, 'rb') as f:\r\n            data = json.load(f)\r\n            for id_, text, label in zip(data[\"text\"].keys(), data[\"text\"].values(), data[\"Label\"].values()):\r\n                yield id_, {\r\n                    \"text\": text,\r\n                    \"label\": label,\r\n                }\r\n```\r\n\r\nCan someone help me in adding gdrive links to be used with default dl_manager or adding gdown as another dl_manager, because I'd like to add this dataset to nlp's official database.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 661804054,
    "title": "Fix docstrins multiple metrics instances",
    "dateCreated": "2020-07-20T13:08:59Z",
    "dateModified": "2020-07-20T13:08:59Z",
    "description": "We change the docstrings of `nlp.Metric.compute`, `nlp.Metric.add` and `nlp.Metric.add_batch` depending on which metric is instantiated. However we had issues when instantiating multiple metrics (docstrings were duplicated).\r\n\r\nThis should fix #304 ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 661635393,
    "title": "Fix xtreme panx directory",
    "dateCreated": "2020-07-20T10:09:17Z",
    "dateModified": "2020-07-20T10:09:17Z",
    "description": "Fix #412 ",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 660687076,
    "title": "Something is wrong with WMT 19 kk-en dataset",
    "dateCreated": "2020-07-19T08:18:51Z",
    "dateModified": "2020-07-19T08:18:51Z",
    "description": "The translation in the `train` set does not look right:\r\n\r\n```\r\n>>>import nlp\r\n>>>from nlp import load_dataset\r\n>>>dataset = load_dataset('wmt19', 'kk-en')\r\n>>>dataset[\"train\"][\"translation\"][0]\r\n{'kk': 'Trumpian Uncertainty', 'en': '\u0422\u0440\u0430\u043c\u043f\u0442\u044b\u049b \u0431\u0435\u043b\u0433\u0456\u0441\u0456\u0437\u0434\u0456\u043a'}\r\n>>>dataset[\"validation\"][\"translation\"][0]\r\n{'kk': '\u0410\u049b\u0448\u0430-\u043d\u0435\u0441\u0438\u0435 \u0441\u0430\u044f\u0441\u0430\u0442\u044b\u043d\u044b\u04a3 \u0441\u0446\u0435\u043d\u0430\u0440\u0438\u0439\u0456\u043d \u049b\u0430\u0439\u0442\u0430 \u0436\u0430\u0437\u0441\u0430\u049b', 'en': 'Rewriting the Monetary-Policy Script'}\r\n```",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 660654013,
    "title": "from_dict delete?",
    "dateCreated": "2020-07-19T07:08:36Z",
    "dateModified": "2020-07-19T07:08:36Z",
    "description": "AttributeError: type object 'Dataset' has no attribute 'from_dict'",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 660063655,
    "title": "Is there a way to download only NQ dev?",
    "dateCreated": "2020-07-18T10:28:23Z",
    "dateModified": "2020-07-18T10:28:23Z",
    "description": "Maybe I missed that in the docs, but is there a way to only download the dev set of natural questions (~1 GB)? \r\nAs we want to benchmark QA models on different datasets, I would like to avoid downloading the 41GB of training data. \r\n\r\nI tried\r\n```\r\ndataset = nlp.load_dataset('natural_questions', split=\"validation\", beam_runner=\"DirectRunner\")\r\n```\r\nBut this still triggered a big download of presumably the whole dataset. Is there any way of doing this or are splits / slicing options only available after downloading?\r\n\r\nThanks!",
    "status": "open",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 660047139,
    "title": "Unable to load XTREME dataset from disk",
    "dateCreated": "2020-07-18T09:55:00Z",
    "dateModified": "2020-07-18T09:55:00Z",
    "description": "Hi \ud83e\udd17  team!\r\n\r\n## Description of the problem\r\nFollowing the [docs](https://huggingface.co/nlp/loading_datasets.html?highlight=xtreme#manually-downloading-files) I'm trying to load the `PAN-X.fr` dataset from the [XTREME](https://github.com/google-research/xtreme) benchmark.\r\n\r\nI have manually downloaded the `AmazonPhotos.zip` file from [here](https://www.amazon.com/clouddrive/share/d3KGCRCIYwhKJF0H3eWA26hjg2ZCRhjpEQtDL70FSBN?_encoding=UTF8&%2AVersion%2A=1&%2Aentries%2A=0&mgh=1) and am running into a `FileNotFoundError` when I point to the location of the dataset.\r\n\r\nAs far as I can tell, the problem is that `AmazonPhotos.zip` decompresses to `panx_dataset` and `load_dataset()` is not looking in the correct path:\r\n\r\n```\r\n# path where load_dataset is looking for fr.tar.gz\r\n/root/.cache/huggingface/datasets/9b8c4f1578e45cb2539332c79738beb3b54afbcd842b079cabfd79e3ed6704f6/\r\n# path where it actually exists\r\n/root/.cache/huggingface/datasets/9b8c4f1578e45cb2539332c79738beb3b54afbcd842b079cabfd79e3ed6704f6/panx_dataset/\r\n```\r\n\r\n## Steps to reproduce the problem\r\n\r\n1. Manually download the XTREME benchmark from [here](https://www.amazon.com/clouddrive/share/d3KGCRCIYwhKJF0H3eWA26hjg2ZCRhjpEQtDL70FSBN?_encoding=UTF8&%2AVersion%2A=1&%2Aentries%2A=0&mgh=1)\r\n\r\n2. Run the following code snippet\r\n```python\r\nfrom nlp import load_dataset\r\n# AmazonPhotos.zip is in the root of the folder\r\ndataset = load_dataset(\"xtreme\", \"PAN-X.fr\", data_dir='./')\r\n```\r\n\r\n3. Here is the stack trace\r\n```\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-4-26786bb5fa93> in <module>\r\n----> 1 dataset = load_dataset(\"xtreme\", \"PAN-X.fr\", data_dir='./')\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    522         download_mode=download_mode,\r\n    523         ignore_verifications=ignore_verifications,\r\n--> 524         save_infos=save_infos,\r\n    525     )\r\n    526 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    430                 verify_infos = not save_infos and not ignore_verifications\r\n    431                 self._download_and_prepare(\r\n--> 432                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    433                 )\r\n    434                 # Sync info\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    464         split_dict = SplitDict(dataset_name=self.name)\r\n    465         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 466         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    467         # Checksums verification\r\n    468         if verify_infos:\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/datasets/xtreme/b8c2ed3583a7a7ac60b503576dfed3271ac86757628897e945bd329c43b8a746/xtreme.py in _split_generators(self, dl_manager)\r\n    725             panx_dl_dir = dl_manager.extract(panx_path)\r\n    726             lang = self.config.name.split(\".\")[1]\r\n--> 727             lang_folder = dl_manager.extract(os.path.join(panx_dl_dir, lang + \".tar.gz\"))\r\n    728             return [\r\n    729                 nlp.SplitGenerator(\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/utils/download_manager.py in extract(self, path_or_paths)\r\n    196         \"\"\"\r\n    197         return map_nested(\r\n--> 198             lambda path: cached_path(path, extract_compressed_file=True, force_extract=False), path_or_paths,\r\n    199         )\r\n    200 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_tuple)\r\n    170                 return tuple(mapped)\r\n    171     # Singleton\r\n--> 172     return function(data_struct)\r\n    173 \r\n    174 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/utils/download_manager.py in <lambda>(path)\r\n    196         \"\"\"\r\n    197         return map_nested(\r\n--> 198             lambda path: cached_path(path, extract_compressed_file=True, force_extract=False), path_or_paths,\r\n    199         )\r\n    200 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/utils/file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    203     elif urlparse(url_or_filename).scheme == \"\":\r\n    204         # File, but it doesn't exist.\r\n--> 205         raise FileNotFoundError(\"Local file {} doesn't exist\".format(url_or_filename))\r\n    206     else:\r\n    207         # Something unknown\r\n\r\nFileNotFoundError: Local file /root/.cache/huggingface/datasets/9b8c4f1578e45cb2539332c79738beb3b54afbcd842b079cabfd79e3ed6704f6/fr.tar.gz doesn't exist\r\n```\r\n\r\n## OS and hardware\r\n```\r\n- `nlp` version: 0.3.0\r\n- Platform: Linux-4.15.0-72-generic-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyTorch version (GPU?): 1.4.0 (True)\r\n- Tensorflow version (GPU?): 2.1.0 (True)\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n```",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 659393398,
    "title": "Sbf",
    "dateCreated": "2020-07-17T16:19:45Z",
    "dateModified": "2020-07-17T16:19:45Z",
    "description": "This PR adds the Social Bias Frames Dataset (ACL 2020) .\r\ndataset homepage: https://homes.cs.washington.edu/~msap/social-bias-frames/",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 659242871,
    "title": "20newsgroup",
    "dateCreated": "2020-07-17T13:07:57Z",
    "dateModified": "2020-07-17T13:07:57Z",
    "description": "Add 20Newsgroup dataset.\r\n#353 ",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 659128611,
    "title": "train_test_split error: 'dict' object has no attribute 'deepcopy'",
    "dateCreated": "2020-07-17T10:36:28Z",
    "dateModified": "2020-07-17T10:36:28Z",
    "description": "`train_test_split` is giving me an error when I try and call it:\r\n\r\n`'dict' object has no attribute 'deepcopy'`\r\n\r\n## To reproduce\r\n\r\n```\r\ndataset = load_dataset('glue', 'mrpc', split='train')\r\ndataset = dataset.train_test_split(test_size=0.2)\r\n```\r\n\r\n## Full Stacktrace\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-12-feb740dbec9a> in <module>\r\n      1 dataset = load_dataset('glue', 'mrpc', split='train')\r\n----> 2 dataset = dataset.train_test_split(test_size=0.2)\r\n\r\n~/anaconda3/envs/fastai2_me/lib/python3.7/site-packages/nlp/arrow_dataset.py in train_test_split(self, test_size, train_size, shuffle, seed, generator, keep_in_memory, load_from_cache_file, train_cache_file_name, test_cache_file_name, writer_batch_size)\r\n   1032                     \"writer_batch_size\": writer_batch_size,\r\n   1033                 }\r\n-> 1034                 train_kwargs = cache_kwargs.deepcopy()\r\n   1035                 train_kwargs[\"split\"] = \"train\"\r\n   1036                 test_kwargs = cache_kwargs.deepcopy()\r\n\r\nAttributeError: 'dict' object has no attribute 'deepcopy'\r\n```",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 659064144,
    "title": "Add tests datasets gcp",
    "dateCreated": "2020-07-17T09:23:27Z",
    "dateModified": "2020-07-17T09:23:27Z",
    "description": "Some datasets are available on our google cloud storage in arrow format, so that the users don't need to process the data.\r\nThese tests make sure that they're always available. It also makes sure that their scripts are in sync between S3 and the repo.\r\nThis should avoid future issues like #407 ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 658672736,
    "title": "MissingBeamOptions for Wikipedia 20200501.en",
    "dateCreated": "2020-07-16T23:48:03Z",
    "dateModified": "2020-07-16T23:48:03Z",
    "description": "There may or may not be a regression for the pre-processed Wikipedia dataset. This was working fine 10 commits ago (without having Apache Beam available):\r\n\r\n```\r\nnlp.load_dataset('wikipedia', \"20200501.en\", split='train')\r\n```\r\n\r\nAnd now, having pulled master, I get:\r\n\r\n```\r\nDownloading and preparing dataset wikipedia/20200501.en (download: 16.99 GiB, generated: 17.07 GiB, total: 34.06 GiB) to /home/hltcoe/mgordon/.cache/huggingface/datasets/wikipedia/20200501.en/1.0.0/76b0b2747b679bb0ee7a1621e50e5a6378477add0c662668a324a5bc07d516dd...\r\nTraceback (most recent call last):\r\n  File \"scripts/download.py\", line 11, in <module>\r\n    fire.Fire(download_pretrain)\r\n  File \"/home/hltcoe/mgordon/.conda/envs/huggingface/lib/python3.6/site-packages/fire/core.py\", line 138, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/hltcoe/mgordon/.conda/envs/huggingface/lib/python3.6/site-packages/fire/core.py\", line 468, in _Fire\r\n    target=component.__name__)\r\n  File \"/home/hltcoe/mgordon/.conda/envs/huggingface/lib/python3.6/site-packages/fire/core.py\", line 672, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"scripts/download.py\", line 6, in download_pretrain\r\n    nlp.load_dataset('wikipedia', \"20200501.en\", split='train')\r\n  File \"/exp/mgordon/nlp/src/nlp/load.py\", line 534, in load_dataset\r\n    save_infos=save_infos,\r\n  File \"/exp/mgordon/nlp/src/nlp/builder.py\", line 460, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/exp/mgordon/nlp/src/nlp/builder.py\", line 870, in _download_and_prepare\r\n    \"\\n\\t`{}`\".format(usage_example)\r\nnlp.builder.MissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, S\r\npark, etc. More information about Apache Beam runners at https://beam.apache.org/documentation/runners/capability-matrix/\r\nIf you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory).\r\nExample of usage:\r\n        `load_dataset('wikipedia', '20200501.en', beam_runner='DirectRunner')`\r\n```",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 658581764,
    "title": "Faster Shuffling?",
    "dateCreated": "2020-07-16T21:21:53Z",
    "dateModified": "2020-07-16T21:21:53Z",
    "description": "Consider shuffling bookcorpus:\r\n\r\n```\r\ndataset = nlp.load_dataset('bookcorpus', split='train')\r\ndataset.shuffle()\r\n```\r\nAccording to tqdm, this will take around 2.5 hours on my machine to complete (even with the faster version of select from #405). I've also tried with `keep_in_memory=True` and `writer_batch_size=1000`.\r\n\r\nBut I can also just write the lines to a text file:\r\n\r\n```\r\nbatch_size = 100000\r\nwith open('tmp.txt', 'w+') as out_f:\r\n    for i in tqdm(range(0, len(dataset), batch_size)):\r\n        batch = dataset[i:i+batch_size]['text']\r\n        print(\"\\n\".join(batch), file=out_f)\r\n```\r\n\r\nWhich completes in a couple minutes, followed by `shuf tmp.txt > tmp2.txt` which completes in under a minute. And finally,\r\n\r\n```\r\ndataset = nlp.load_dataset('text', data_files='tmp2.txt')\r\n```\r\n\r\nWhich completes in under 10 minutes. I read up on Apache Arrow this morning, and it seems like the columnar data format is not especially well-suited to shuffling rows, since moving items around requires a lot of book-keeping. \r\n\r\nIs shuffle inherently slow, or am I just using it wrong? And if it is slow, would it make sense to try converting the data to a row-based format on disk and then shuffling? (Instead of calling select with a random permutation, as is currently done.)",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 658580192,
    "title": "Make select() faster by batching reads",
    "dateCreated": "2020-07-16T21:19:45Z",
    "dateModified": "2020-07-16T21:19:45Z",
    "description": "Here's a benchmark:\r\n\r\n```\r\ndataset = nlp.load_dataset('bookcorpus', split='train')\r\n\r\nstart = time.time()\r\ndataset.select(np.arange(1000), reader_batch_size=1, load_from_cache_file=False)\r\nend = time.time()\r\nprint(f'{end - start}')\r\n\r\nstart = time.time()\r\ndataset.select(np.arange(1000), reader_batch_size=1000, load_from_cache_file=False)\r\nend = time.time()\r\nprint(f'{end - start}')\r\n```\r\n\r\nWithout batching, select takes around 1.27 seconds. With batching, it takes around 0.01 seconds. The slowness was upsetting me because dataset.shuffle() was supposed to take ~27 hours for bookcorpus. Now with the fix it takes ~2.5 hours (which still is pretty slow, but I'll open a separate issue for that).",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 658400987,
    "title": "Add seed in metrics",
    "dateCreated": "2020-07-16T17:27:05Z",
    "dateModified": "2020-07-16T17:27:05Z",
    "description": "With #361 we noticed that some metrics were not deterministic.\r\nIn this PR I allow the user to specify numpy's seed when instantiating a metric with `load_metric`.\r\nThe seed is set only when `compute` is called, and reset afterwards.\r\n\r\nMoreover when calling `compute` with the same metric instance (i.e. same experiment_id), the metric will always return the same results given the same inputs. This is the case even if the seed is was not specified by the user, as the previous seed is going to be reused.\r\n\r\nHowever, instantiating twice a metric (two different experiments) without specifying a seed can create different results.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 658325756,
    "title": "return python objects instead of arrays by default",
    "dateCreated": "2020-07-16T15:51:52Z",
    "dateModified": "2020-07-16T15:51:52Z",
    "description": "We were using to_pandas() to convert from arrow types, however it returns numpy arrays instead of python lists.\r\nI fixed it by using to_pydict/to_pylist instead.\r\n\r\nFix #387 \r\nIt was mentioned in https://github.com/huggingface/transformers/issues/5729\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 658001288,
    "title": "Search qa",
    "dateCreated": "2020-07-16T09:00:10Z",
    "dateModified": "2020-07-16T09:00:10Z",
    "description": "add SearchQA dataset\r\n\r\n#336 ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 657996252,
    "title": "add web_questions",
    "dateCreated": "2020-07-16T08:54:59Z",
    "dateModified": "2020-07-16T08:54:59Z",
    "description": "add Web Question dataset\r\n#336 \r\n\r\nMaybe @patrickvonplaten you can help with the dummy_data structure? it still broken",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 657975600,
    "title": "Web questions",
    "dateCreated": "2020-07-16T08:28:29Z",
    "dateModified": "2020-07-16T08:28:29Z",
    "description": "add the WebQuestion dataset\r\n#336 ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 657841433,
    "title": "Spelling mistake",
    "dateCreated": "2020-07-16T04:37:58Z",
    "dateModified": "2020-07-16T04:37:58Z",
    "description": "In \"Formatting the dataset\" part, \"The two toehr modifications...\" should be \"The two other modifications...\" ,the word \"other\" wrong spelled as \"toehr\".",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 657511962,
    "title": "Add inline links",
    "dateCreated": "2020-07-15T17:04:04Z",
    "dateModified": "2020-07-15T17:04:04Z",
    "description": "Add inline links to `Contributing.md`",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 657510856,
    "title": "Add contiguous sharding",
    "dateCreated": "2020-07-15T17:02:58Z",
    "dateModified": "2020-07-15T17:02:58Z",
    "description": "This makes dset.shard() play nice with nlp.concatenate_datasets(). When I originally wrote the shard() method, I was thinking about a distributed training scenario, but https://github.com/huggingface/nlp/pull/389 also uses it for splitting the dataset for distributed preprocessing.\r\n\r\nUsage:\r\n```\r\nnlp.concatenate_datasets([dset.shard(n, i, contiguous=True) for i in range(n)])\r\n```",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 657477952,
    "title": "Fix memory issue when doing select",
    "dateCreated": "2020-07-15T16:15:04Z",
    "dateModified": "2020-07-15T16:15:04Z",
    "description": "We were passing the `nlp.Dataset` object to get the hash for the new dataset's file name.\r\nFix #395 ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 657454983,
    "title": "Memory issue when doing select",
    "dateCreated": "2020-07-15T15:43:38Z",
    "dateModified": "2020-07-15T15:43:38Z",
    "description": "As noticed in #389, the following code loads the entire wikipedia in memory.\r\n\r\n```python\r\nimport nlp\r\nw = nlp.load_dataset(\"wikipedia\", \"20200501.en\", split=\"train\")\r\nw.select([0])\r\n```\r\n\r\nThis is caused by [this line](https://github.com/huggingface/nlp/blob/master/src/nlp/arrow_dataset.py#L626) for some reason, that tries to serialize the function with all the wikipedia data with it.\r\n\r\nIt's not the case with `.map` or `.filter`.\r\nHowever functions that are based on `.select` like `.shuffle`, `.shard`, `.train_test_split`, `.sort` are affected.\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 657425548,
    "title": "Remove remaining nested dict",
    "dateCreated": "2020-07-15T15:05:52Z",
    "dateModified": "2020-07-15T15:05:52Z",
    "description": "This PR deletes the remaining unnecessary nested dict \r\n#378 ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 657330911,
    "title": "Fix extracted files directory for the DownloadManager",
    "dateCreated": "2020-07-15T12:59:55Z",
    "dateModified": "2020-07-15T12:59:55Z",
    "description": "The cache dir was often cluttered by extracted files because of the download manager.\r\n\r\nFor downloaded files, we are using the `downloads` directory to make things easier to navigate, but extracted files were still placed at the root of the cache directory. To fix that I changed the directory for extracted files to cache_dir/downloads/extracted.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 657313738,
    "title": "Style change detection",
    "dateCreated": "2020-07-15T12:32:14Z",
    "dateModified": "2020-07-15T12:32:14Z",
    "description": "Another [PAN task](https://pan.webis.de/clef20/pan20-web/style-change-detection.html). This time about identifying when the style/author changes in documents.\r\n\r\n- There's the possibility of adding the [PAN19](https://zenodo.org/record/3577602) and PAN18 style change detection tasks too (these are datasets whose labels are a subset of PAN20's). These would probably make more sense as separate datasets (like wmt is now)\r\n- I've converted the integer 0,1 values to a boolean\r\n- Using manually downloaded data again. This might be changed at some point following the discussion in https://github.com/huggingface/nlp/pull/349.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 656991432,
    "title": "\ud83c\udf1f [Metric Request] WOOD score",
    "dateCreated": "2020-07-15T01:16:37Z",
    "dateModified": "2020-07-15T01:16:37Z",
    "description": "WOOD score paper : https://arxiv.org/pdf/2007.06898.pdf\r\n\r\nAbstract :\r\n\r\n>Models that surpass human performance on several popular benchmarks display significant degradation in performance on exposure to Out of Distribution (OOD) data. Recent research has shown that models overfit to spurious biases and \u2018hack\u2019 datasets, in lieu of learning generalizable features like humans. In order to stop the inflation in model performance \u2013 and thus overestimation in AI systems\u2019 capabilities \u2013 we propose a simple and novel evaluation metric, WOOD Score, that encourages generalization during evaluation.",
    "status": "open",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 656956384,
    "title": "Concatenate datasets",
    "dateCreated": "2020-07-14T23:24:37Z",
    "dateModified": "2020-07-14T23:24:37Z",
    "description": "I'm constructing the \"WikiBooks\" dataset, which is a concatenation of Wikipedia & BookCorpus. So I implemented the `Dataset.from_concat()` method, which concatenates two datasets with the same schema.\r\n\r\nThis would also be useful if someone wants to pretrain on a large generic dataset + their own custom dataset. Not in love with the method name, so would love to hear suggestions.\r\n\r\nUsage:\r\n```python\r\nfrom nlp import Dataset, load_dataset\r\n\r\ndata1, data2 = {\"id\": [0, 1, 2]}, {\"id\": [3, 4, 5]}\r\ndset1, dset2 = Dataset.from_dict(data1), Dataset.from_dict(data2)\r\ndset_concat = Dataset.from_concat([dset1, dset2])\r\nprint(dset_concat)\r\n# Dataset(schema: {'id': 'int64'}, num_rows: 6)\r\n```",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 656921768,
    "title": "Fix pickling of SplitDict",
    "dateCreated": "2020-07-14T21:53:39Z",
    "dateModified": "2020-07-14T21:53:39Z",
    "description": "It would be nice to pickle and unpickle Datasets, as done in [this tutorial](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb). Example:\r\n\r\n```\r\nwiki = nlp.load_dataset('wikipedia', split='train')\r\ndef sentencize(examples):\r\n    ...\r\n\r\nwiki = wiki.map(sentencize, batched=True)\r\ntorch.save(wiki, 'sentencized_wiki_dataset.pt')\r\n```\r\n\r\nHowever, upon unpickling the dataset via torch.load(...), this error is raised:\r\n\r\n```\r\nValueError(\"Cannot add elem. Use .add() instead.\")\r\n```\r\nOn line [492 of splits.py](https://github.com/huggingface/nlp/blob/master/src/nlp/splits.py#L492). This is because SplitDict subclasses dict, and pickle treats [dicts specially](https://github.com/huggingface/nlp/blob/master/src/nlp/splits.py#L492). Pickle expects access to `dict.__setitem__`, but this is disallowed by the class.\r\n\r\nThe workaround is to provide an explicit interface for pickle to call when pickling and unpickling, thereby avoiding the use of `__setitem__`.\r\n\r\nTesting:\r\n- Manually pickled and unpickled a modified wikipedia dataset.\r\n- Ran `make style`\r\n\r\nI would be happy to run any other tests, but I couldn't find any in the contributing guidelines.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 656707497,
    "title": "\ud83d\udc1b [Dataset] Cannot download wmt14, wmt15 and wmt17",
    "dateCreated": "2020-07-14T15:36:41Z",
    "dateModified": "2020-07-14T15:36:41Z",
    "description": "1. I try downloading `wmt14`, `wmt15`, `wmt17`, `wmt19` with the following code:\r\n```\r\nnlp.load_dataset('wmt14','de-en')\r\nnlp.load_dataset('wmt15','de-en')\r\nnlp.load_dataset('wmt17','de-en')\r\nnlp.load_dataset('wmt19','de-en')\r\n```\r\nThe code runs but the download speed is **extremely slow**, the same behaviour is not observed on `wmt16` and `wmt18`\r\n\r\n2. When trying to download `wmt17 zh-en`, I got the following error:\r\n> ConnectionError: Couldn't reach https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-zh.tar.gz",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 656361357,
    "title": "Conversion through to_pandas output numpy arrays for lists instead of python objects",
    "dateCreated": "2020-07-14T06:24:01Z",
    "dateModified": "2020-07-14T06:24:01Z",
    "description": "In a related question, the conversion through to_pandas output numpy arrays for the lists instead of python objects.\r\n\r\nHere is an example:\r\n```python\r\n>>> dataset._data.slice(key, 1).to_pandas().to_dict(\"list\")\r\n{'sentence1': ['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .'], 'sentence2': ['Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'], 'label': [1], 'idx': [0], 'input_ids': [array([  101,  7277,  2180,  5303,  4806,  1117,  1711,   117,  2292,\r\n        1119,  1270,   107,  1103,  7737,   107,   117,  1104,  9938,\r\n        4267, 12223, 21811,  1117,  2554,   119,   102])], 'token_type_ids': [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0])], 'attention_mask': [array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n       1, 1, 1])]}\r\n>>> type(dataset._data.slice(key, 1).to_pandas().to_dict(\"list\")['input_ids'][0])\r\n<class 'numpy.ndarray'>\r\n>>> dataset._data.slice(key, 1).to_pydict()\r\n{'sentence1': ['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .'], 'sentence2': ['Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'], 'label': [1], 'idx': [0], 'input_ids': [[101, 7277, 2180, 5303, 4806, 1117, 1711, 117, 2292, 1119, 1270, 107, 1103, 7737, 107, 117, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\r\n```",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 655839067,
    "title": "Update dataset loading and features - Add TREC dataset",
    "dateCreated": "2020-07-13T13:10:18Z",
    "dateModified": "2020-07-13T13:10:18Z",
    "description": "This PR:\r\n- add a template for a new dataset script\r\n- update the caching structure so that the path to the cached data files is also a function of the dataset loading script hash. This way when you update a loading script the data will be automatically updated instead of falling back to the previous version (which is usually a outdated). This makes it in particular easier to iterate when writing a new dataset loading script.\r\n- fix a bug in the `ClassLabel` feature and make it more flexible so that its methods `str2int` and `int2str` can also accept list, numpy arrays and PyTorch/TensorFlow tensors.\r\n- add the TREC-6 dataset",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 655663997,
    "title": "Remove unnecessary nested dict",
    "dateCreated": "2020-07-13T08:46:23Z",
    "dateModified": "2020-07-13T08:46:23Z",
    "description": "This PR is removing unnecessary nested dictionary used in some datasets. For now the following datasets are updated:\r\n\r\n- MLQA\r\n\r\n- RACE\r\n\r\nWill be adding more if necessary.\r\n\r\n#378 ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 655291201,
    "title": "Adding the Linguistic Code-switching Evaluation (LinCE) benchmark",
    "dateCreated": "2020-07-11T22:35:20Z",
    "dateModified": "2020-07-11T22:35:20Z",
    "description": "Hi,\r\n\r\nFirst of all, this library is really cool! Thanks for putting all of this together! \r\n\r\nThis PR contains the [Linguistic Code-switching Evaluation (LinCE) benchmark](https://ritual.uh.edu/lince). As described in the official website (FAQ):\r\n\r\n> 1. Why do we need LinCE?\r\n>LinCE brings 10 code-switching datasets together for 4 tasks and 4 language pairs with 5 leaderboards in a single evaluation platform. We examined each dataset and fixed major issues on the partitions (or even define official partitions) with a comprehensive stratification method (see our paper for more details).\r\n>Besides, we believe that online benchmarks like LinCE bring steady research progress and allow to compare state-of-the-art models at the pace of the progress in NLP. We expect to benefit greatly the code-switching community with this benchmark.\r\n\r\n\r\nThe data comes from social media and here's the summary table of tasks per language pair:\r\n\r\n| Language Pairs                         | LID | POS | NER | SA |\r\n|----------------------------------------|-----|-----|-----|----|\r\n| Spanish-English                        | \u2705  | \u2705  | \u2705  | \u2705 |\r\n| Hindi-English                          | \u2705  | \u2705  | \u2705  |    |\r\n| Modern Standard Arabic-Egyptian Arabic | \u2705  |     | \u2705  |    |\r\n| Nepali-English                         | \u2705  |     |     |    |\r\n\r\nThe tasks are as follows:\r\n* LID: token-level language identification\r\n* POS: part-of-speech tagging\r\n* NER: named entity recognition\r\n* SA: sentiment analysis\r\n\r\nWith the exception of MSA-EA, the rest of the datasets contain token-level LID labels.\r\n\r\n## Usage\r\n\r\nFor Spanish-English LID, we can load the data as follows:\r\n```\r\nimport nlp\r\n\r\ndata = nlp.load_dataset('./datasets/lince/lince.py', 'lid_spaeng')\r\n\r\nfor split in data:\r\n    print(data[split])\r\n```\r\n\r\nHere's the output:\r\n```\r\nDataset(schema: {'idx': 'int32', 'tokens': 'list<item: string>', 'lid': 'list<item: string>'}, num_rows: 21030)\r\nDataset(schema: {'idx': 'int32', 'tokens': 'list<item: string>', 'lid': 'list<item: string>'}, num_rows: 3332)\r\nDataset(schema: {'idx': 'int32', 'tokens': 'list<item: string>', 'lid': 'list<item: string>'}, num_rows: 8289)\r\n```\r\n\r\nHere's the list of shortcut names for every dataset available in LinCE:\r\n* `lid_spaeng`\r\n* `lid_hineng`\r\n* `lid_nepeng`\r\n* `lid_msaea`\r\n* `pos_spaeng`\r\n* `pos_hineng`\r\n* `ner_spaeng`\r\n* `ner_hineng`\r\n* `ner_msaea`\r\n* `sa_spaeng`\r\n\r\n\r\nAll the numbers match with Table 3 in the LinCE [paper](https://www.aclweb.org/anthology/2020.lrec-1.223.pdf). Also, note that the MSA-EA datasets use the Persian script while the other datasets use the Roman script.\r\n\r\n\r\n## Features\r\n\r\nHere is how the features look in the case of language identification (LID) tasks:\r\n\r\n| LID Feature          | Type          | Description                               |\r\n|----------------------|---------------|-------------------------------------------|\r\n| `idx`                | `int`       | Dataset index of current sentence         |\r\n| `tokens`             | `list<str>` | List of tokens (string) of a sentence     |\r\n| `lid`                | `list<str>` | List of LID labels (string) of a sentence |\r\n\r\nFor part-of-speech (POS) tagging:\r\n\r\n| POS Feature          | Type          | Description                               |\r\n|----------------------|---------------|-------------------------------------------|\r\n| `idx`                | `int`       | Dataset index of current sentence         |\r\n| `tokens`             | `list<str>` | List of tokens (string) of a sentence     |\r\n| `lid`                | `list<str>` | List of LID labels (string) of a sentence |\r\n| `pos`                | `list<str>` | List of POS tags (string) of a sentence   |\r\n\r\nFor named entity recognition (NER):\r\n\r\n| NER Feature          | Type          | Description                               |\r\n|----------------------|---------------|-------------------------------------------|\r\n| `idx`                | `int`       | Dataset index of current sentence         |\r\n| `tokens`             | `list<str>` | List of tokens (string) of a sentence     |\r\n| `lid`                | `list<str>` | List of LID labels (string) of a sentence |\r\n| `ner`                | `list<str>` | List of NER labels (string) of a sentence |\r\n\r\n**NOTE**: the MSA-EA NER dataset does not contain the `lid` feature.\r\n\r\nFor sentiment analysis (SA):\r\n\r\n| SA Feature          | Type        | Description                               |\r\n|---------------------|-------------|-------------------------------------------|\r\n| `idx`               | `int`       | Dataset index of current sentence         |\r\n| `tokens`            | `list<str>` | List of tokens (string) of a sentence     |\r\n| `lid`               | `list<str>` | List of LID labels (string) of a sentence |\r\n| `sa`                | `str`       | Sentiment label (string) of a sentence    |\r\n\r\n\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 655290482,
    "title": "1080",
    "dateCreated": "2020-07-11T22:29:07Z",
    "dateModified": "2020-07-11T22:29:07Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 655277119,
    "title": "NLp",
    "dateCreated": "2020-07-11T20:50:14Z",
    "dateModified": "2020-07-11T20:50:14Z",
    "description": "",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 655226316,
    "title": "[dataset] Structure of MLQA seems unecessary nested",
    "dateCreated": "2020-07-11T15:16:08Z",
    "dateModified": "2020-07-11T15:16:08Z",
    "description": "The features of the MLQA dataset comprise several nested dictionaries with a single element inside (for `questions` and `ids`): https://github.com/huggingface/nlp/blob/master/datasets/mlqa/mlqa.py#L90-L97\r\n\r\nShould we keep this @mariamabarham @patrickvonplaten? Was this added for compatibility with tfds?\r\n\r\n```python\r\n            features=nlp.Features(\r\n                {\r\n                    \"context\": nlp.Value(\"string\"),\r\n                    \"questions\": nlp.features.Sequence({\"question\": nlp.Value(\"string\")}),\r\n                    \"answers\": nlp.features.Sequence(\r\n                        {\"text\": nlp.Value(\"string\"), \"answer_start\": nlp.Value(\"int32\"),}\r\n                    ),\r\n                    \"ids\": nlp.features.Sequence({\"idx\": nlp.Value(\"string\")})\r\n```",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 655215790,
    "title": "Iyy!!!",
    "dateCreated": "2020-07-11T14:11:07Z",
    "dateModified": "2020-07-11T14:11:07Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 655047826,
    "title": "to_pandas conversion doesn't always work",
    "dateCreated": "2020-07-10T21:33:31Z",
    "dateModified": "2020-07-10T21:33:31Z",
    "description": "For some complex nested types, the conversion from Arrow to python dict through pandas doesn't seem to be possible.\r\n\r\nHere is an example using the official SQUAD v2 JSON file.\r\n\r\nThis example was found while investigating #373.\r\n\r\n```python\r\n>>> squad = load_dataset('json', data_files={nlp.Split.TRAIN: [\"./train-v2.0.json\"]}, download_mode=nlp.GenerateMode.FORCE_REDOWNLOAD, version=\"1.0.0\", field='data')\r\n>>> squad['train']\r\nDataset(schema: {'title': 'string', 'paragraphs': 'list<item: struct<qas: list<item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>>, context: string>>'}, num_rows: 442)\r\n>>> squad['train'][0]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/thomwolf/Documents/GitHub/datasets/src/nlp/arrow_dataset.py\", line 589, in __getitem__\r\n    format_kwargs=self._format_kwargs,\r\n  File \"/Users/thomwolf/Documents/GitHub/datasets/src/nlp/arrow_dataset.py\", line 529, in _getitem\r\n    outputs = self._unnest(self._data.slice(key, 1).to_pandas().to_dict(\"list\"))\r\n  File \"pyarrow/array.pxi\", line 559, in pyarrow.lib._PandasConvertible.to_pandas\r\n  File \"pyarrow/table.pxi\", line 1367, in pyarrow.lib.Table._to_pandas\r\n  File \"/Users/thomwolf/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/pandas_compat.py\", line 766, in table_to_blockmanager\r\n    blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)\r\n  File \"/Users/thomwolf/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/pandas_compat.py\", line 1101, in _table_to_blocks\r\n    list(extension_columns.keys()))\r\n  File \"pyarrow/table.pxi\", line 881, in pyarrow.lib.table_to_blocks\r\n  File \"pyarrow/error.pxi\", line 105, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowNotImplementedError: Not implemented type for Arrow list to pandas: struct<qas: list<item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>>, context: string>\r\n```\r\n\r\ncc @lhoestq would we have a way to detect this from the schema maybe?\r\n\r\nHere is the schema for this pretty complex JSON:\r\n```python\r\n>>> squad['train'].schema\r\ntitle: string\r\nparagraphs: list<item: struct<qas: list<item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>>, context: string>>\r\n  child 0, item: struct<qas: list<item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>>, context: string>\r\n      child 0, qas: list<item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>>\r\n          child 0, item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>\r\n              child 0, question: string\r\n              child 1, id: string\r\n              child 2, answers: list<item: struct<text: string, answer_start: int64>>\r\n                  child 0, item: struct<text: string, answer_start: int64>\r\n                      child 0, text: string\r\n                      child 1, answer_start: int64\r\n              child 3, is_impossible: bool\r\n              child 4, plausible_answers: list<item: struct<text: string, answer_start: int64>>\r\n                  child 0, item: struct<text: string, answer_start: int64>\r\n                      child 0, text: string\r\n                      child 1, answer_start: int64\r\n      child 1, context: string\r\n```",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 655023307,
    "title": "TypeError when computing bertscore",
    "dateCreated": "2020-07-10T20:37:44Z",
    "dateModified": "2020-07-10T20:37:44Z",
    "description": "Hi, \r\n\r\nI installed nlp 0.3.0 via pip, and my python version is 3.7.\r\nWhen I tried to compute bertscore with the code:\r\n```\r\nimport nlp \r\nbertscore = nlp.load_metric('bertscore')  \r\n# load hyps and refs \r\n...\r\nprint (bertscore.compute(hyps, refs, lang='en'))\r\n```\r\n\r\nI got the following error.\r\n```\r\nTraceback (most recent call last):\r\n  File \"bert_score_evaluate.py\", line 16, in <module>\r\n    print (bertscore.compute(hyps, refs, lang='en'))\r\n  File \"/home/willywsm/anaconda3/envs/torcher/lib/python3.7/site-packages/nlp/metric.py\", line 200, in compute\r\n    output = self._compute(predictions=predictions, references=references, **metrics_kwargs)\r\n  File \"/home/willywsm/anaconda3/envs/torcher/lib/python3.7/site-packages/nlp/metrics/bertscore/fb176889831bf0ce995ed197edc94b2e9a83f647a869bb8c9477dbb2d04d0f08/bertscore.py\", line 105, in _compute\r\n    hashcode = bert_score.utils.get_hash(model_type, num_layers, idf, rescale_with_baseline)\r\nTypeError: get_hash() takes 3 positional arguments but 4 were given\r\n```\r\n\r\nIt seems like there is something wrong with get_hash() function?",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 654895066,
    "title": "Add dataset post processing for faiss indexes",
    "dateCreated": "2020-07-10T16:25:59Z",
    "dateModified": "2020-07-10T16:25:59Z",
    "description": "# Post processing of datasets for faiss indexes\r\n\r\nNow that we can have datasets with embeddings (see `wiki_pr` for example), we can allow users to load the dataset + get the Faiss index that comes with it to do nearest neighbors queries.\r\n\r\n## Implementation proposition\r\n\r\n- Faiss indexes have to be added to the `nlp.Dataset` object, and therefore it's in a different scope that what are doing the `_split_generators` and `_generate_examples` methods of `nlp.DatasetBuilder`. Therefore I added a new method for post processing of the `nlp.Dataset` object called `_post_process` (name could change)\r\n- The role of `_post_process` is to apply dataset transforms (filter/map etc.) or indexing functions (add_faiss_index) to modify/enrich the `nlp.Dataset` object. It is not part of the `download_and_prepare` process (that is focused on arrow files creation) so the post processing is run inside the `as_dataset` method.\r\n- `_post_process` can generate new files (cached files from dataset transforms or serialized faiss indexes) and their names are defined by `_post_processing_resources`\r\n- as we know what are the post processing resources, we can download them automatically from google storage instead of computing them if they're available (as we do for arrow files)\r\n\r\nI'd happy to discuss these choices !\r\n\r\n## The `wiki_dpr` index\r\n\r\nIt takes 1h20 and ~7GB of memory to compute. The final index is 1.42GB and takes ~1.5GB of memory.\r\nThis is pretty cool given that a naive flat index would take 170GB of memory to store the 21M vectors of dim 768.\r\nI couldn't use directly the Faiss `index_factory` as I needed to set the metric to inner product.\r\n\r\n## Example of usage\r\n\r\n```python\r\nimport nlp\r\ndset = nlp.load_dataset(\r\n    \"wiki_dpr\",\r\n    \"psgs_w100_with_nq_embeddings\",\r\n    split=\"train\",\r\n    with_index=True\r\n)\r\nprint(len(dset), dset.list_indexes())  # (21015300, ['embeddings'])\r\n```\r\n\r\n(it also works with the dataset configuration without the embeddings because I added the index file in google storage for this one too)\r\n\r\n## Demo\r\n\r\nYou can also check a demo on google colab that shows how to use it with the DPRQuestionEncoder from transformers:\r\nhttps://colab.research.google.com/drive/1FakNU8W5EPMcWff7iP1H6REg3XSS0YLp?usp=sharing\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 654845133,
    "title": "Segmentation fault when loading local JSON dataset as of #372",
    "dateCreated": "2020-07-10T15:04:25Z",
    "dateModified": "2020-07-10T15:04:25Z",
    "description": "The last issue was closed (#369) once the #372 update was merged. However, I'm still not able to load a SQuAD formatted JSON file. Instead of the previously recorded pyarrow error, I now get a segmentation fault. \r\n\r\n```\r\ndataset = nlp.load_dataset('json', data_files={nlp.Split.TRAIN: [\"./datasets/train-v2.0.json\"]}, field='data')\r\n```\r\ncauses\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/XXX/.cache/huggingface/datasets/json/default/0.0.0...\r\n0 tables [00:00, ? tables/s]Segmentation fault (core dumped)\r\n```\r\nwhere `./datasets/train-v2.0.json` is downloaded directly from https://rajpurkar.github.io/SQuAD-explorer/.\r\nThis is consistent with other SQuAD-formatted JSON files.\r\n\r\nWhen attempting to load the dataset again, I get the following:\r\n```\r\nUsing custom data configuration default\r\nTraceback (most recent call last):\r\n  File \"dataloader.py\", line 6, in <module>\r\n    'json', data_files={nlp.Split.TRAIN: [\"./datasets/train-v2.0.json\"]}, field='data')\r\n  File \"/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/load.py\", line 524, in load_dataset\r\n    save_infos=save_infos,\r\n  File \"/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/builder.py\", line 382, in download_and_prepare\r\n    with incomplete_dir(self._cache_dir) as tmp_data_dir:\r\n  File \"/home/XXX/.conda/envs/torch/lib/python3.7/contextlib.py\", line 112, in __enter__\r\n    return next(self.gen)\r\n  File \"/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/builder.py\", line 368, in incomplete_dir\r\n    os.makedirs(tmp_dir)\r\n  File \"/home/XXX/.conda/envs/torch/lib/python3.7/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '/home/XXX/.cache/huggingface/datasets/json/default/0.0.0.incomplete'\r\n```\r\n\r\n(Not sure if you wanted this in the previous issue #369 or not as it was closed.)",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 654774420,
    "title": "Make the json script more flexible",
    "dateCreated": "2020-07-10T13:15:15Z",
    "dateModified": "2020-07-10T13:15:15Z",
    "description": "Fix https://github.com/huggingface/nlp/issues/359\r\nFix https://github.com/huggingface/nlp/issues/369\r\n\r\nJSON script now can accept JSON files containing a single dict with the records as a list in one attribute to the dict (previously it only accepted JSON files containing records as rows of dicts in the file).\r\n\r\nIn this case, you should indicate using `field=XXX` the name of the field in the JSON structure which contains the records you want to load. The records can be a dict of lists or a list of dicts.\r\n\r\nE.g. to load the SQuAD dataset JSON (without using the `squad` specific dataset loading script), in which the data rows are in the `data` field of the JSON dict, you can do:\r\n```python\r\nfrom nlp import load_dataset\r\ndataset = load_dataset('json', data_files='/PATH/TO/JSON', field='data')\r\n```",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 654668242,
    "title": "Fix cached file path for metrics with different config names",
    "dateCreated": "2020-07-10T10:02:24Z",
    "dateModified": "2020-07-10T10:02:24Z",
    "description": "The config name was not taken into account to build the cached file path.\r\nIt should fix #368 ",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 654304193,
    "title": "Allow indexing Dataset via np.ndarray",
    "dateCreated": "2020-07-09T19:43:15Z",
    "dateModified": "2020-07-09T19:43:15Z",
    "description": "",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 654186890,
    "title": "can't load local dataset: pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries",
    "dateCreated": "2020-07-09T16:16:53Z",
    "dateModified": "2020-07-09T16:16:53Z",
    "description": "Trying to load a local SQuAD-formatted dataset (from a JSON file, about 60MB):\r\n```\r\ndataset = nlp.load_dataset(path='json', data_files={nlp.Split.TRAIN: [\"./path/to/file.json\"]})\r\n```\r\ncauses\r\n```\r\nTraceback (most recent call last):\r\n  File \"dataloader.py\", line 9, in <module>\r\n    [\"./path/to/file.json\"]})\r\n  File \"/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/load.py\", line 524, in load_dataset\r\n    save_infos=save_infos,\r\n  File \"/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/builder.py\", line 432, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/builder.py\", line 483, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/builder.py\", line 719, in _prepare_split\r\n    for key, table in utils.tqdm(generator, unit=\" tables\", leave=False):\r\n  File \"/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/tqdm/std.py\", line 1129, in __iter__\r\n    for obj in iterable:\r\n  File \"/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/datasets/json/88c1bc5c68489f7eda549ed05a5a738527c613b3e7a4ee3524d9d233353a949b/json.py\", line 53, in _generate_tables\r\n    file, read_options=self.config.pa_read_options, parse_options=self.config.pa_parse_options,\r\n  File \"pyarrow/_json.pyx\", line 191, in pyarrow._json.read_json\r\n  File \"pyarrow/error.pxi\", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries (try to increase block size?)\r\n```\r\n\r\nI haven't been able to find any reports of this specific pyarrow error here or elsewhere. ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 654087251,
    "title": "load_metric can't acquire lock anymore",
    "dateCreated": "2020-07-09T14:04:09Z",
    "dateModified": "2020-07-09T14:04:09Z",
    "description": "I can't load metric (glue) anymore after an error in a previous run. I even removed the whole cache folder `/home/XXX/.cache/huggingface/`, and the issue persisted. What are the steps to fix this?\r\n\r\n    Traceback (most recent call last):\r\n      File \"/home/XXX/miniconda3/envs/ML-DL-py-3.7/lib/python3.7/site-packages/nlp/metric.py\", line 101, in __init__\r\n        self.filelock.acquire(timeout=1)\r\n      File \"/home/XXX/miniconda3/envs/ML-DL-py-3.7/lib/python3.7/site-packages/filelock.py\", line 278, in acquire\r\n        raise Timeout(self._lock_file)\r\n    filelock.Timeout: The file lock '/home/XXX/.cache/huggingface/metrics/glue/1.0.0/1-glue-0.arrow.lock' could not be acquired.\r\n\r\n    During handling of the above exception, another exception occurred:\r\n\r\n    Traceback (most recent call last):\r\n      File \"examples_huggingface_nlp.py\", line 268, in <module>\r\n        main()\r\n      File \"examples_huggingface_nlp.py\", line 242, in main\r\n        dataset, metric = get_dataset_metric(glue_task)\r\n      File \"examples_huggingface_nlp.py\", line 77, in get_dataset_metric\r\n        metric = nlp.load_metric('glue', glue_config, experiment_id=1)\r\n      File \"/home/XXX/miniconda3/envs/ML-DL-py-3.7/lib/python3.7/site-packages/nlp/load.py\", line 440, in load_metric\r\n        **metric_init_kwargs,\r\n      File \"/home/XXX/miniconda3/envs/ML-DL-py-3.7/lib/python3.7/site-packages/nlp/metric.py\", line 104, in __init__\r\n        \"Cannot acquire lock, caching file might be used by another process, \"\r\n    ValueError: Cannot acquire lock, caching file might be used by another process, you should setup a unique 'experiment_id' for this run.\r\n    I0709 15:54:41.008838 139854118430464 filelock.py:318] Lock 139852058030936 released on /home/XXX/.cache/huggingface/metrics/glue/1.0.0/1-glue-0.arrow.lock\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 654012984,
    "title": "Update Xtreme to add PAWS-X es",
    "dateCreated": "2020-07-09T12:14:37Z",
    "dateModified": "2020-07-09T12:14:37Z",
    "description": "This PR adds the `PAWS-X.es` in the Xtreme dataset #362 ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 653954896,
    "title": "Add quora dataset",
    "dateCreated": "2020-07-09T10:34:22Z",
    "dateModified": "2020-07-09T10:34:22Z",
    "description": "Added the [Quora question pairs dataset](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs).\r\n\r\nImplementation Notes:\r\n- I used the original version provided on the quora website. There's also a [Kaggle competition](https://www.kaggle.com/c/quora-question-pairs) which has a nice train/test split but I can't find an easy way to download it.\r\n- I've made the questions into a list:\r\n    ```python\r\n    {\r\n        \"questions\": [\r\n            {\"id\":0, \"text\": \"Is this an example question?\"},\r\n            {\"id\":1, \"text\": \"Is this a sample question?\"},\r\n        ],\r\n    ...\r\n    }\r\n    ```\r\n    rather than:\r\n    ```python\r\n    {\r\n        \"question1\": \"Is this an example question?\",\r\n        \"question2\": \"Is this a sample question?\"\r\n        \"qid0\": 0\r\n        \"qid1\": 1\r\n    ...\r\n    }\r\n    ```\r\n    Not sure if this was the right call.\r\n- Can't find a good citation for this dataset",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 653845964,
    "title": "How to augment data ?",
    "dateCreated": "2020-07-09T07:52:37Z",
    "dateModified": "2020-07-09T07:52:37Z",
    "description": "Is there any clean way to augment data ?\r\n\r\nFor now my work-around is to use batched map, like this :\r\n\r\n```python\r\ndef aug(samples):\r\n    # Simply copy the existing data to have x2 amount of data\r\n    for k, v in samples.items():\r\n        samples[k].extend(v)\r\n    return samples\r\n\r\ndataset = dataset.map(aug, batched=True)\r\n```",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 653821597,
    "title": "add MS MARCO dataset",
    "dateCreated": "2020-07-09T07:11:19Z",
    "dateModified": "2020-07-09T07:11:19Z",
    "description": "This PR adds the MS MARCO dataset as requested in this issue #336. MS mARCO has multiple task including:\r\n\r\n- Passage and Document Retrieval\r\n\r\n- Keyphrase Extraction\r\n\r\n- QA and NLG\r\n\r\nThis PR only adds the 2 versions of the  QA and NLG task dataset which was realeased with the original paper here https://arxiv.org/pdf/1611.09268.pdf \r\n\r\nTests are failing because of the dummy data. I tried to fix it without success. Can you please have a look at it? @patrickvonplaten , @lhoestq ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 653821172,
    "title": "Adding support for  generic multi dimensional tensors and auxillary image data for multimodal datasets",
    "dateCreated": "2020-07-09T07:10:30Z",
    "dateModified": "2020-07-09T07:10:30Z",
    "description": "nlp/features.py:\r\n\r\nThe main factory class is MultiArray, every single time this class is called, a corresponding pyarrow extension array and type class is generated (and added to the list of globals for future use) for a given root data type and set of dimensions/shape. I provide examples on working with this in datasets/lxmert_pretraining_beta/test_multi_array.py\r\n\r\nsrc/nlp/arrow_writer.py\r\n\r\nI had to add a method for writing batches that include extension array types because despite having a unique class for each multidimensional array shape, pyarrow is unable to write any other \"array-like\" data class to a batch object unless it is of the type pyarrow.ExtensionType. The problem in this is that when writing multiple batches, the order of the schema and data to be written get mixed up (where the pyarrow datatype in the schema only refers to as ExtensionAray, but each ExtensionArray subclass has a different shape) ...  possibly I am missing something here and would be grateful if anyone else could take a look!\r\n\r\ndatasets/lxmert_pretraining_beta/lxmert_pretraining_beta.py & datasets/lxmert_pretraining_beta/to_arrow_data.py:\r\n\r\nI have begun adding the data from the original LXMERT paper (https://arxiv.org/abs/1908.07490)  hosted here: (https://github.com/airsplay/lxmert). The reason I am not pulling from the source of truth for each individual dataset is because it seems that there will also need to be functionality to aggregate multimodal datasets to create a pre-training corpus (:sleepy: ). \r\nFor now, this is just being used to test and run edge-cases for the MultiArray feature, so ive labeled it as \"beta_pretraining\"!\r\n\r\n(still working on the pretraining, just wanted to push out the new functionality sooner than later)",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 653766245,
    "title": "[dateset subset missing]  xtreme paws-x",
    "dateCreated": "2020-07-09T05:04:54Z",
    "dateModified": "2020-07-09T05:04:54Z",
    "description": "I tried nlp.load_dataset('xtreme', 'PAWS-X.es') but get the value error\r\nIt turns out that the subset for Spanish is missing\r\nhttps://github.com/google-research-datasets/paws/tree/master/pawsx",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 653757376,
    "title": "\ud83d\udc1b [Metrics] ROUGE is non-deterministic",
    "dateCreated": "2020-07-09T04:39:37Z",
    "dateModified": "2020-07-09T04:39:37Z",
    "description": "If I run the ROUGE metric 2 times, with same predictions / references, the scores are slightly different.\r\n\r\nRefer to [this Colab notebook](https://colab.research.google.com/drive/1wRssNXgb9ldcp4ulwj-hMJn0ywhDOiDy?usp=sharing) for reproducing the problem.\r\n\r\nExample of F-score for ROUGE-1, ROUGE-2, ROUGE-L in 2 differents run :\r\n\r\n> ['0.3350', '0.1470', '0.2329']\r\n['0.3358', '0.1451', '0.2332']\r\n\r\n---\r\n\r\nWhy ROUGE is not deterministic ?",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 653687176,
    "title": "[Feature request] Add dataset.ragged_map() function for many-to-many transformations",
    "dateCreated": "2020-07-09T01:04:43Z",
    "dateModified": "2020-07-09T01:04:43Z",
    "description": "`dataset.map()` enables one-to-one transformations. Input one example and output one example. This is helpful for tokenizing and cleaning individual lines.\r\n`dataset.filter()` enables one-to-(one-or-none) transformations. Input one example and output either zero/one example. This is helpful for removing portions from the dataset.\r\nHowever, some dataset transformations are many-to-many. Consider constructing BERT training examples from a dataset of sentences, where you map `[\"a\", \"b\", \"c\"] -> [\"a[SEP]b\", \"a[SEP]c\", \"b[SEP]c\", \"c[SEP]b\", ...]`\r\n\r\nI propose a more general `ragged_map()` method that takes in a batch of examples of length `N` and return a batch of examples `M`. This is different from the `map(batched=True)` method, which takes examples of length `N` and returns a batch of length `N`, processing individual examples in parallel. I don't have a clear vision of how this would be implemented efficiently and lazily, but would love to hear the community's feedback on this.\r\n\r\nMy specific use case is creating an end-to-end ELECTRA data pipeline. I would like to take the raw WikiText data and generate training examples from this using the `ragged_map()` method, then export to TFRecords and train quickly. This would be a reproducible pipeline with no bash scripts. Currently I'm relying on scripts like https://github.com/google-research/electra/blob/master/build_pretraining_dataset.py, which are less general.\r\n\r\n",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 653656279,
    "title": "ArrowBasedBuilder _prepare_split parse_schema breaks on nested structures",
    "dateCreated": "2020-07-08T23:24:05Z",
    "dateModified": "2020-07-08T23:24:05Z",
    "description": "I tried using the Json dataloader to load some JSON lines files. but get an exception in the parse_schema function.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\n\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-23-9aecfbee53bd> in <module>\r\n     55 from nlp import load_dataset\r\n     56 \r\n---> 57 ds = load_dataset(\"../text2struct/model/dataset_builder.py\", data_files=rel_datafiles)\r\n     58 \r\n     59 \r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    522         download_mode=download_mode,\r\n    523         ignore_verifications=ignore_verifications,\r\n--> 524         save_infos=save_infos,\r\n    525     )\r\n    526 \r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    430                 verify_infos = not save_infos and not ignore_verifications\r\n    431                 self._download_and_prepare(\r\n--> 432                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    433                 )\r\n    434                 # Sync info\r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    481             try:\r\n    482                 # Prepare split will record examples associated to the split\r\n--> 483                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    484             except OSError:\r\n    485                 raise OSError(\"Cannot find data file. \" + (self.manual_download_instructions or \"\"))\r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/builder.py in _prepare_split(self, split_generator)\r\n    736                     schema_dict[field.name] = Value(str(field.type))\r\n    737 \r\n--> 738         parse_schema(writer.schema, features)\r\n    739         self.info.features = Features(features)\r\n    740 \r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/builder.py in parse_schema(schema, schema_dict)\r\n    734                     parse_schema(field.type.value_type, schema_dict[field.name])\r\n    735                 else:\r\n--> 736                     schema_dict[field.name] = Value(str(field.type))\r\n    737 \r\n    738         parse_schema(writer.schema, features)\r\n\r\n<string> in __init__(self, dtype, id, _type)\r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/features.py in __post_init__(self)\r\n     55 \r\n     56     def __post_init__(self):\r\n---> 57         self.pa_type = string_to_arrow(self.dtype)\r\n     58 \r\n     59     def __call__(self):\r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/features.py in string_to_arrow(type_str)\r\n     32         if str(type_str + \"_\") not in pa.__dict__:\r\n     33             raise ValueError(\r\n---> 34                 f\"Neither {type_str} nor {type_str + '_'} seems to be a pyarrow data type. \"\r\n     35                 f\"Please make sure to use a correct data type, see: \"\r\n     36                 f\"https://arrow.apache.org/docs/python/api/datatypes.html#factory-functions\"\r\n\r\nValueError: Neither list<item: string> nor list<item: string>_ seems to be a pyarrow data type. Please make sure to use a correct data type, see: https://arrow.apache.org/docs/python/api/datatypes.html#factory-functions\r\n```\r\n\r\nIf I create the dataset imperatively, using a pyarrow table, the dataset is created correctly. If I override the `_prepare_split` method to avoid calling the validate schema, the dataset can load as well. ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 653645121,
    "title": "Starting to add some real doc",
    "dateCreated": "2020-07-08T22:53:03Z",
    "dateModified": "2020-07-08T22:53:03Z",
    "description": "Adding a lot of documentation for:\r\n- load a dataset\r\n- explore the dataset object\r\n- process data with the dataset\r\n- add a new dataset script\r\n- share a dataset script\r\n- full package reference\r\n\r\nThis version of the doc can be explored here: https://2219-250213286-gh.circle-artifacts.com/0/docs/_build/html/index.html\r\n\r\nAlso:\r\n- fix a bug in `train_test_split`\r\n- update the `csv` script\r\n- add a verbose argument to the dataset processing methods\r\n\r\nStill missing:\r\n- doc for the metrics\r\n- how to directly upload a community provided dataset with the CLI\r\n- clean up more docstrings\r\n- add the `features` argument to `load_dataset` (should be another PR)",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 653642292,
    "title": "Add hashes to cnn_dailymail",
    "dateCreated": "2020-07-08T22:45:21Z",
    "dateModified": "2020-07-08T22:45:21Z",
    "description": "The URL hashes are helpful for comparing results from other sources.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 653537388,
    "title": "Add text dataset",
    "dateCreated": "2020-07-08T19:21:53Z",
    "dateModified": "2020-07-08T19:21:53Z",
    "description": "Usage:\r\n\r\n```python\r\nfrom nlp import load_dataset\r\ndset = load_dataset(\"text\", data_files=\"/path/to/file.txt\")[\"train\"]\r\n```\r\n\r\n\r\nI created a dummy_data.zip which contains three files: `train.txt`, `test.txt`, `dev.txt`. Each of these contains two lines. It passes\r\n\r\n```bash\r\nRUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_dataset_all_configs_text\r\n```\r\n\r\nbut I would like a second set of eyes to ensure I did it right.\r\n",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 653451013,
    "title": "can't load SNLI dataset",
    "dateCreated": "2020-07-08T16:54:14Z",
    "dateModified": "2020-07-08T16:54:14Z",
    "description": "`nlp` seems to load `snli` from some URL based on nlp.stanford.edu. This subdomain is frequently down -- including right now, when I'd like to load `snli` in a Colab notebook, but can't.\r\n\r\nIs there a plan to move these datasets to huggingface servers for a more stable solution?\r\n\r\nBtw, here's the stack trace:\r\n\r\n```\r\nFile \"/content/nlp/src/nlp/builder.py\", line 432, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/content/nlp/src/nlp/builder.py\", line 466, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/content/nlp/src/nlp/datasets/snli/e417f6f2e16254938d977a17ed32f3998f5b23e4fcab0f6eb1d28784f23ea60d/snli.py\", line 76, in _split_generators\r\n    dl_dir = dl_manager.download_and_extract(_DATA_URL)\r\n  File \"/content/nlp/src/nlp/utils/download_manager.py\", line 217, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/content/nlp/src/nlp/utils/download_manager.py\", line 156, in download\r\n    lambda url: cached_path(url, download_config=self._download_config,), url_or_urls,\r\n  File \"/content/nlp/src/nlp/utils/py_utils.py\", line 190, in map_nested\r\n    return function(data_struct)\r\n  File \"/content/nlp/src/nlp/utils/download_manager.py\", line 156, in <lambda>\r\n    lambda url: cached_path(url, download_config=self._download_config,), url_or_urls,\r\n  File \"/content/nlp/src/nlp/utils/file_utils.py\", line 198, in cached_path\r\n    local_files_only=download_config.local_files_only,\r\n  File \"/content/nlp/src/nlp/utils/file_utils.py\", line 356, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https://nlp.stanford.edu/projects/snli/snli_1.0.zip\r\n```",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 653357617,
    "title": "More faiss control",
    "dateCreated": "2020-07-08T14:45:20Z",
    "dateModified": "2020-07-08T14:45:20Z",
    "description": "Allow users to specify a faiss index they created themselves, as sometimes indexes can be composite for examples",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 653250611,
    "title": "[Dataset requests] New datasets for Text Classification",
    "dateCreated": "2020-07-08T12:17:58Z",
    "dateModified": "2020-07-08T12:17:58Z",
    "description": "We are missing a few datasets for Text Classification which is an important field.\r\n\r\nNamely, it would be really nice to add:\r\n- TREC-6 dataset (see here for instance: https://pytorchnlp.readthedocs.io/en/latest/source/torchnlp.datasets.html#torchnlp.datasets.trec_dataset)  **[done]**\r\n- Yelp-5\r\n- Movie review (Movie Review (MR) dataset [156]) **[done (same as rotten_tomatoes)]**\r\n- SST (Stanford Sentiment Treebank) **[include in glue]**\r\n- Multi-Perspective Question Answering (MPQA) dataset **[require authentication (indeed manual download)]**\r\n- Amazon. This is a popular corpus of product reviews collected from the Amazon website [159]. It contains labels for both binary classification and multi-class (5-class) classification\r\n- 20 Newsgroups. The 20 Newsgroups dataset  **[done]**\r\n- Sogou News dataset **[done]**\r\n- Reuters news. The Reuters-21578 dataset [165] **[done]**\r\n- DBpedia. The DBpedia dataset [170]\r\n- Ohsumed. The Ohsumed collection [171] is a subset of the MEDLINE database\r\n- EUR-Lex. The EUR-Lex dataset\r\n- WOS. The Web Of Science (WOS) dataset **[done]**\r\n- PubMed. PubMed [173]\r\n- TREC-QA. TREC-QA\r\n- Quora. The Quora dataset [180]\r\n\r\nAll these datasets are cited in https://arxiv.org/abs/2004.03705",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 653128883,
    "title": "\ud83d\udc1b[BugFix]fix seqeval",
    "dateCreated": "2020-07-08T09:12:12Z",
    "dateModified": "2020-07-08T09:12:12Z",
    "description": "Fix seqeval process labels such as 'B', 'B-ARGM-LOC'",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 652424048,
    "title": "add pandas dataset",
    "dateCreated": "2020-07-07T15:38:07Z",
    "dateModified": "2020-07-07T15:38:07Z",
    "description": "Create a dataset from serialized pandas dataframes.\r\nUsage:\r\n```python\r\nfrom nlp import load_dataset\r\ndset = load_dataset(\"pandas\", data_files=\"df.pkl\")[\"train\"]\r\n```",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 652398691,
    "title": "add from_pandas and from_dict",
    "dateCreated": "2020-07-07T15:03:53Z",
    "dateModified": "2020-07-07T15:03:53Z",
    "description": "I added two new methods to the `Dataset` class:\r\n- `from_pandas()` to create a dataset from a pandas dataframe\r\n- `from_dict()` to create a dataset from a dictionary (keys = columns)\r\n\r\nIt uses the `pa.Table.from_pandas` and `pa.Table.from_pydict` funcitons to do so.\r\nIt is also possible to specify the features types via `features=...` if there are ambiguities (null/nan values), otherwise the arrow schema is infered from the data automatically by pyarrow.\r\n\r\nOne question that I have right now:\r\n+ Should we also add a `save()` method that would write the dataset on the disk ? Right now if we create a `Dataset` using those two new methods, the data are kept in RAM. Then to reload it we can call the `from_file()` method.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 652231571,
    "title": "Hyperpartisan news detection",
    "dateCreated": "2020-07-07T11:06:37Z",
    "dateModified": "2020-07-07T11:06:37Z",
    "description": "Adding the hyperpartisan news detection dataset from PAN. This contains news article text, labelled with whether they're hyper-partisan and why kinds of biases they display.\r\n\r\nImplementation notes:\r\n- As with many PAN tasks, the data is hosted on [Zenodo](https://zenodo.org/record/1489920) and must be requested before use. I've used the manual download stuff for this, although the dataset is provided under a Creative Commons Attribution 4.0 International License, so we could host a version if we wanted to?\r\n- The 'bias' attribute doesn't exist for the 'byarticle' configuration. I've added an empty string to the class labels to deal with this. Is there a more standard value for empty data?\r\n- Should we always subclass `nlp.BuilderConfig`?\r\n\r\n",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 652158308,
    "title": "Add OSCAR dataset",
    "dateCreated": "2020-07-07T09:22:07Z",
    "dateModified": "2020-07-07T09:22:07Z",
    "description": "I don't know if tests pass, when I run them it tries to download the whole corpus which is around 3.5TB compressed and I don't have that kind of space. I'll really need some help with it \ud83d\ude05 \r\n\r\nThanks!",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 652106567,
    "title": "'cp950' codec error from load_dataset('xtreme', 'tydiqa')",
    "dateCreated": "2020-07-07T08:14:23Z",
    "dateModified": "2020-07-07T08:14:23Z",
    "description": "![image](https://user-images.githubusercontent.com/50871412/86744744-67481680-c06c-11ea-8612-b77eba92a392.png)\r\n\r\nI guess the error is related to python source encoding issue that my PC is trying to decode the source code with wrong encoding-decoding tools, perhaps :\r\nhttps://www.python.org/dev/peps/pep-0263/\r\n\r\nI guess the error was triggered by the code \" module = importlib.import_module(module_path)\" at line 57 in the source code:  nlp/src/nlp/load.py / (https://github.com/huggingface/nlp/blob/911d5596f9b500e39af8642fe3d1b891758999c7/src/nlp/load.py#L51)\r\n\r\nAny ideas?\r\n\r\np.s. tried the same code on colab, that runs perfectly\r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 652044151,
    "title": "Add emotion dataset",
    "dateCreated": "2020-07-07T06:35:41Z",
    "dateModified": "2020-07-07T06:35:41Z",
    "description": "Hello \ud83e\udd17  team!\r\n\r\nI am trying to add an emotion classification dataset ([link](https://github.com/dair-ai/emotion_dataset)) to `nlp` but I am a bit stuck about what I should do when the URL for the dataset is not a ZIP file, but just a pickled `pandas.DataFrame` (see [here](https://www.dropbox.com/s/607ptdakxuh5i4s/merged_training.pkl)).\r\n\r\nWith the current implementation, running\r\n\r\n```bash\r\npython nlp-cli test datasets/emotion --save_infos --all_configs\r\n```\r\n\r\nthrows a `_pickle.UnpicklingError: invalid load key, '<'.` error (full stack trace below). The strange thing is that the path to the file does not carry the `.pkl` extension and instead appears to be some md5 hash (see the `FILE PATH` print statement in the stack trace).\r\n\r\nNote: I have checked that the `merged_training.pkl` file is not corrupted when I download it with `wget`. \r\n\r\nAny pointers on what I'm doing wrong would be greatly appreciated!\r\n\r\n**Stack trace**\r\n\r\n```\r\nINFO:nlp.load:Checking datasets/emotion/emotion.py for additional imports.\r\nINFO:filelock:Lock 140330435928512 acquired on datasets/emotion/emotion.py.lock\r\nINFO:nlp.load:Found main folder for dataset datasets/emotion/emotion.py at /Users/lewtun/git/nlp/src/nlp/datasets/emotion\r\nINFO:nlp.load:Creating specific version folder for dataset datasets/emotion/emotion.py at /Users/lewtun/git/nlp/src/nlp/datasets/emotion/59666994754d1b369228a749b695e377643d141fa98c6972be00407659788c7b\r\nINFO:nlp.load:Copying script file from datasets/emotion/emotion.py to /Users/lewtun/git/nlp/src/nlp/datasets/emotion/59666994754d1b369228a749b695e377643d141fa98c6972be00407659788c7b/emotion.py\r\nINFO:nlp.load:Couldn't find dataset infos file at datasets/emotion/dataset_infos.json\r\nINFO:nlp.load:Creating metadata file for dataset datasets/emotion/emotion.py at /Users/lewtun/git/nlp/src/nlp/datasets/emotion/59666994754d1b369228a749b695e377643d141fa98c6972be00407659788c7b/emotion.json\r\nINFO:filelock:Lock 140330435928512 released on datasets/emotion/emotion.py.lock\r\nINFO:nlp.builder:Generating dataset emotion (/Users/lewtun/.cache/huggingface/datasets/emotion/emotion/1.0.0)\r\nINFO:nlp.builder:Dataset not on Hf google storage. Downloading and preparing it from source\r\nDownloading and preparing dataset emotion/emotion (download: Unknown size, generated: Unknown size, total: Unknown size) to /Users/lewtun/.cache/huggingface/datasets/emotion/emotion/1.0.0...\r\nINFO:nlp.builder:Generating split train\r\n0 examples [00:00, ? examples/s]FILE PATH /Users/lewtun/.cache/huggingface/datasets/3615dcb52b7ba052ef63e1571894c4b67e8e12a6ab1ef2f756ec3c380bf48490\r\nTraceback (most recent call last):\r\n  File \"nlp-cli\", line 37, in <module>\r\n    service.run()\r\n  File \"/Users/lewtun/git/nlp/src/nlp/commands/test.py\", line 83, in run\r\n    builder.download_and_prepare(\r\n  File \"/Users/lewtun/git/nlp/src/nlp/builder.py\", line 431, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Users/lewtun/git/nlp/src/nlp/builder.py\", line 483, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/Users/lewtun/git/nlp/src/nlp/builder.py\", line 664, in _prepare_split\r\n    for key, record in utils.tqdm(generator, unit=\" examples\", total=split_info.num_examples, leave=False):\r\n  File \"/Users/lewtun/miniconda3/envs/nlp/lib/python3.8/site-packages/tqdm/std.py\", line 1129, in __iter__\r\n    for obj in iterable:\r\n  File \"/Users/lewtun/git/nlp/src/nlp/datasets/emotion/59666994754d1b369228a749b695e377643d141fa98c6972be00407659788c7b/emotion.py\", line 87, in _generate_examples\r\n    data = pickle.load(f)\r\n_pickle.UnpicklingError: invalid load key, '<'.\r\n```",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 651761201,
    "title": "Supporting documents in ELI5",
    "dateCreated": "2020-07-06T19:14:13Z",
    "dateModified": "2020-07-06T19:14:13Z",
    "description": "I was attempting to use the ELI5 dataset, when I realized that huggingface does not provide the supporting documents (the source documents from the common crawl). Without the supporting documents, this makes the dataset about as useful for my project as a block of cheese, or some other more apt metaphor.  According to facebook, the entire document collection is quite large. However, it would still be helpful to at least include a subset of the supporting documents i.e., having some data is better than having a block of cheese, in my case at least.\r\n\r\nIf you choose not to include them, it would be helpful to have documentation mentioning this specifically. It is especially confusing because the hf nlp ELI5 dataset has the key `'document'` but there are no documents to be found :(",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 651495246,
    "title": "Search qa",
    "dateCreated": "2020-07-06T12:23:16Z",
    "dateModified": "2020-07-06T12:23:16Z",
    "description": "This PR adds the Search QA dataset used in **SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine**. The dataset has the following config name:\r\n\r\n- raw_jeopardy: raw data\r\n\r\n- train_test_val:  which is the splitted version\r\n\r\n#336 ",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 651419630,
    "title": "Fix nested tensorflow format",
    "dateCreated": "2020-07-06T10:13:45Z",
    "dateModified": "2020-07-06T10:13:45Z",
    "description": "In #339 and #337 we are thinking about adding a way to export datasets to tfrecords.\r\n\r\nHowever I noticed that it was not possible to do `dset.set_format(\"tensorflow\")` on datasets with nested features like `squad`. I fixed that using a nested map operations to convert features to `tf.ragged.constant`.\r\n\r\nI also added tests on the `set_format` function.",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 651333194,
    "title": "Features should be updated when `map()` changes schema",
    "dateCreated": "2020-07-06T08:03:23Z",
    "dateModified": "2020-07-06T08:03:23Z",
    "description": "`dataset.map()` can change the schema and column names.\r\n\r\nWe should update the features in this case (with what is possible to infer).",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 650611969,
    "title": "add fever dataset",
    "dateCreated": "2020-07-03T13:53:07Z",
    "dateModified": "2020-07-03T13:53:07Z",
    "description": "This PR add the FEVER dataset https://fever.ai/ used in with the paper: FEVER: a large-scale dataset for Fact Extraction and VERification (https://arxiv.org/pdf/1803.05355.pdf).\r\n#336 ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 650533920,
    "title": "Update cfq.py",
    "dateCreated": "2020-07-03T11:23:19Z",
    "dateModified": "2020-07-03T11:23:19Z",
    "description": "Make the dataset name consistent with in the paper:  Compositional Freebase Question => Compositional Freebase Questions.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 650156468,
    "title": "Add dataset.export() to TFRecords",
    "dateCreated": "2020-07-02T19:26:27Z",
    "dateModified": "2020-07-02T19:26:27Z",
    "description": "Fixes https://github.com/huggingface/nlp/issues/337\r\n\r\nSome design decisions:\r\n\r\n- Simplified the function API to not handle sharding. It writes the entire dataset as a single TFRecord file. This simplifies the function logic and users can use other functions (`select`, `shard`, etc) to handle custom sharding or splitting.\r\n- Use `from_generator()` instead of `from_tensor_slices()` to address the memory issues discussed in https://github.com/huggingface/nlp/issues/315 and https://github.com/huggingface/nlp/issues/193.\r\n- Performs introspection using the values from `dataset.set_format()` to identify the TF datatypes. Currently it supports string, float, and int. If this should be extended for other datatypes, let me know.\r\n- There are quite a few helper functions required within the `export()` method. If these are better placed in a utils file somewhere, let me know.\r\n\r\nAlso, I noticed that \r\n```python\r\ndataset = dataset.select(indices)\r\ndataset.set_format(\"tensorflow\")\r\n# dataset._format_type is \"tensorflow\"\r\n```\r\ngives a different output than\r\n```python\r\ndataset.set_format(\"tensorflow\")\r\ndataset = dataset.select(indices)\r\n# dataset._format_type is None\r\n```\r\nThe latter loses the format of its parent dataset. Is there interest in making `set_format` a functional method that returns itself (can be chained), and that derived datasets maintain the format of their parent?",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 650057253,
    "title": "Run `make style`",
    "dateCreated": "2020-07-02T16:19:47Z",
    "dateModified": "2020-07-02T16:19:47Z",
    "description": "These files get changed when I run `make style` on an unrelated PR. Upstreaming these changes so development on a different branch can be easier.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 650035887,
    "title": "[Feature request] Export Arrow dataset to TFRecords",
    "dateCreated": "2020-07-02T15:47:12Z",
    "dateModified": "2020-07-02T15:47:12Z",
    "description": "The TFRecord generation process is error-prone and requires complex separate Python scripts to download and preprocess the data. I propose to combine the user-friendly features of `nlp` with the speed and efficiency of TFRecords. Sample API:\r\n\r\n```python\r\n# use these existing methods\r\nds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\r\nds = ds.map(lambda ex: tokenizer(ex))\r\nds.set_format(\"tensorflow\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"])\r\n# then add this method\r\nds.export(folder=\"/my/tfrecords\", prefix=\"myrecord\", num_shards=8, format=\"tfrecord\")\r\n```\r\nwhich would create files like so:\r\n```bash\r\n/my/tfrecords/myrecord_1.tfrecord\r\n/my/tfrecords/myrecord_2.tfrecord\r\n...\r\n```\r\n\r\nI would be happy to contribute this method. We could use a similar approach for PyTorch. Thoughts?",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 649914203,
    "title": "[Dataset requests] New datasets for Open Question Answering",
    "dateCreated": "2020-07-02T13:03:03Z",
    "dateModified": "2020-07-02T13:03:03Z",
    "description": "We are still a few datasets missing for Open-Question Answering which is currently a field in strong development.\r\n\r\nNamely, it would be really nice to add:\r\n- WebQuestions (Berant et al., 2013) [done]\r\n- CuratedTrec (Baudis et al. 2015) [not open-source]\r\n- MS-MARCO (NGuyen et al. 2016) [done]\r\n- SearchQA (Dunn et al. 2017) [done]\r\n- FEVER (Thorne et al. 2018) - [ done]\r\n\r\n \r\n\r\nAll these datasets are cited in http://arxiv.org/abs/2005.11401",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 649765179,
    "title": "BioMRC Dataset presented in BioNLP 2020 ACL Workshop",
    "dateCreated": "2020-07-02T09:03:41Z",
    "dateModified": "2020-07-02T09:03:41Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 649661791,
    "title": "Add dataset.shard() method",
    "dateCreated": "2020-07-02T06:05:19Z",
    "dateModified": "2020-07-02T06:05:19Z",
    "description": "Fixes https://github.com/huggingface/nlp/issues/312",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 649236516,
    "title": "fix variable name typo",
    "dateCreated": "2020-07-01T19:13:50Z",
    "dateModified": "2020-07-01T19:13:50Z",
    "description": "",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 649140135,
    "title": "Add wiki_dpr",
    "dateCreated": "2020-07-01T17:12:00Z",
    "dateModified": "2020-07-01T17:12:00Z",
    "description": "Presented in the [Dense Passage Retrieval paper](https://arxiv.org/pdf/2004.04906.pdf), this dataset consists in 21M passages from the english wikipedia along with their 768-dim embeddings computed using DPR's context encoder.\r\n\r\nNote on the implementation:\r\n- There are two configs: with and without the embeddings (73GB vs 14GB)\r\n- I used a non-fixed-size sequence of floats to describe the feature format of the embeddings. I wanted to use fixed-size sequences but I had issues with reading the arrow file afterwards (for example `dataset[0]` was crashing)\r\n- I added the case for lists of urls as input of the download_manager",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 648533199,
    "title": "Loading CNN/Daily Mail dataset produces `nlp.utils.info_utils.NonMatchingSplitsSizesError`",
    "dateCreated": "2020-06-30T22:21:33Z",
    "dateModified": "2020-06-30T22:21:33Z",
    "description": "```\r\n>>> import nlp\r\n>>> nlp.load_dataset('cnn_dailymail', '3.0.0')\r\nDownloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.26 GiB, total: 1.81 GiB) to /u/jm8wx/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0...\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/p/qdata/jm8wx/datasets/nlp/src/nlp/load.py\", line 520, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/p/qdata/jm8wx/datasets/nlp/src/nlp/builder.py\", line 431, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/p/qdata/jm8wx/datasets/nlp/src/nlp/builder.py\", line 488, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File \"/p/qdata/jm8wx/datasets/nlp/src/nlp/utils/info_utils.py\", line 70, in verify_splits\r\n    raise NonMatchingSplitsSizesError(str(bad_splits))\r\nnlp.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='test', num_bytes=49424491, num_examples=11490, dataset_name='cnn_dailymail'), 'recorded': SplitInfo(name='test', num_bytes=48931393, num_examples=11379, dataset_name='cnn_dailymail')}, {'expected': SplitInfo(name='train', num_bytes=1249178681, num_examples=287113, dataset_name='cnn_dailymail'), 'recorded': SplitInfo(name='train', num_bytes=1240618482, num_examples=285161, dataset_name='cnn_dailymail')}, {'expected': SplitInfo(name='validation', num_bytes=57149241, num_examples=13368, dataset_name='cnn_dailymail'), 'recorded': SplitInfo(name='validation', num_bytes=56637485, num_examples=13255, dataset_name='cnn_dailymail')}]\r\n```",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 648525720,
    "title": "Doc red",
    "dateCreated": "2020-06-30T22:05:31Z",
    "dateModified": "2020-06-30T22:05:31Z",
    "description": "Adding [DocRED](https://github.com/thunlp/DocRED) - a relation extraction dataset which tests document-level RE. A few implementation notes:\r\n\r\n- There are 2 separate versions of the training set - *annotated* and *distant*. Instead of `nlp.Split.Train` I've used the splits `\"train_annotated\"` and `\"train_distant\"` to reflect this.\r\n- As well as the relation id, the full relation name is mapped from `rel_info.json`\r\n- I renamed the 'h', 'r', 't' keys to 'head', 'relation' and 'tail' to make them more readable.\r\n- Used the fix from #319 to allow nested sequences of dicts.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 648446979,
    "title": "[Bug] FileLock dependency incompatible with filesystem",
    "dateCreated": "2020-06-30T19:45:31Z",
    "dateModified": "2020-06-30T19:45:31Z",
    "description": "I'm downloading a dataset successfully with\r\n`load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")`\r\n\r\nBut when I attempt to cache it on an external volume, it hangs indefinitely:\r\n`load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", cache_dir=\"/fsx\") # /fsx is an external volume mount`\r\n\r\nThe filesystem when hanging looks like this:\r\n```bash\r\n/fsx\r\n----downloads\r\n       ----94be...73.lock\r\n----wikitext\r\n       ----wikitext-2-raw\r\n             ----wikitext-2-raw-1.0.0.incomplete\r\n```\r\n\r\nIt appears that on this filesystem, the FileLock object is forever stuck in its \"acquire\" stage. I have verified that the issue lies specifically with the `filelock` dependency:\r\n```python\r\nopen(\"/fsx/hello.txt\").write(\"hello\") # succeeds\r\n\r\nfrom filelock import FileLock\r\nwith FileLock(\"/fsx/hello.lock\"):\r\n    open(\"/fsx/hello.txt\").write(\"hello\") # hangs indefinitely\r\n```\r\n\r\nHas anyone else run into this issue? I'd raise it directly on the FileLock repo, but that project appears abandoned with the last update over a year ago. Or if there's a solution that would remove the FileLock dependency from the project, I would appreciate that.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 648326841,
    "title": "Fork dataset",
    "dateCreated": "2020-06-30T16:42:53Z",
    "dateModified": "2020-06-30T16:42:53Z",
    "description": "We have a multi-task learning model training I'm trying to convert to using the Arrow-based nlp dataset. \r\n\r\nWe're currently training a custom TensorFlow model but the nlp paradigm should be a bridge for us to be able to use the wealth of pre-trained models in Transformers.\r\n\r\nOur preprocessing flow parses raw text and json with Entity and Relations annotations and creates 2 datasets for training a NER and Relations prediction heads.\r\n\r\nIs there some good way to \"fork\" dataset-\r\n\r\nEG\r\n\r\n1. text + json -> Dataset1\r\n1. Dataset1 -> DatasetNER\r\n1. Dataset1 -> DatasetREL\r\n\r\nor \r\n\r\n1. text + json -> Dataset1\r\n1. Dataset1 -> DatasetNER\r\n1. Dataset1 + DatasetNER -> DatasetREL\r\n\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 648312858,
    "title": "set seed for suffling tests",
    "dateCreated": "2020-06-30T16:21:34Z",
    "dateModified": "2020-06-30T16:21:34Z",
    "description": "Some tests were randomly failing because of a missing seed in a test for `train_test_split(shuffle=True)`",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 648126103,
    "title": "Large dataset in Squad2-format",
    "dateCreated": "2020-06-30T12:18:59Z",
    "dateModified": "2020-06-30T12:18:59Z",
    "description": "At the moment we are building an large question answering dataset and think about sharing it with the huggingface community.\r\nCaused the computing power we splitted it into multiple tiles, but they are all in the same format.\r\nRight now the most important facts about are this:\r\n- Contexts: 1.047.671\r\n- questions: 1.677.732\r\n- Answers: 6.742.406\r\n- unanswerable: 377.398\r\n\r\nIt is already cleaned\r\n\r\n<pre><code>\r\ntrain_data = [\r\n    {\r\n        'context': \"this is the context\",\r\n        'qas': [\r\n            {\r\n                'id': \"00002\",\r\n                'is_impossible': False,\r\n                'question': \"whats is this\",\r\n                'answers': [\r\n                    {\r\n                        'text': \"answer\",\r\n                        'answer_start': 0\r\n                    }\r\n                ]\r\n            },\r\n            {\r\n                'id': \"00003\",\r\n                'is_impossible': False,\r\n                'question': \"question2\",\r\n                'answers': [\r\n                    {\r\n                        'text': \"answer2\",\r\n                        'answer_start': 1\r\n                    }\r\n                ]\r\n            }\r\n        ]\r\n    }\r\n]\r\n</code></pre>\r\n\r\nCause it is growing every day we are thinking about an structure like this:\r\nWe host an Json file, containing all the download links and the script can load it dynamically.\r\nAt the moment it is around ~20GB\r\n\r\nAny advice how to handle this, or an ready to use template ?",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 647601592,
    "title": "Add SQuADShifts dataset",
    "dateCreated": "2020-06-29T19:11:16Z",
    "dateModified": "2020-06-29T19:11:16Z",
    "description": "This PR adds the four new variants of the SQuAD dataset used in [The Effect of Natural Distribution Shift on Question Answering Models](https://arxiv.org/abs/2004.14444) to facilitate evaluating model robustness to distribution shift.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 647525725,
    "title": "Error when calculating glue score",
    "dateCreated": "2020-06-29T16:53:48Z",
    "dateModified": "2020-06-29T16:53:48Z",
    "description": "I was trying glue score along with other metrics here. But glue gives me this error;\r\n\r\n```\r\nimport nlp\r\nglue_metric = nlp.load_metric('glue',name=\"cola\")\r\n\r\nglue_score = glue_metric.compute(predictions, references)\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-8-b9210a524504> in <module>()\r\n----> 1 glue_score = glue_metric.compute(predictions, references)\r\n\r\n6 frames\r\n/usr/local/lib/python3.6/dist-packages/nlp/metric.py in compute(self, predictions, references, timeout, **metrics_kwargs)\r\n    191         \"\"\"\r\n    192         if predictions is not None:\r\n--> 193             self.add_batch(predictions=predictions, references=references)\r\n    194         self.finalize(timeout=timeout)\r\n    195 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/metric.py in add_batch(self, predictions, references, **kwargs)\r\n    207         if self.writer is None:\r\n    208             self._init_writer()\r\n--> 209         self.writer.write_batch(batch)\r\n    210 \r\n    211     def add(self, prediction=None, reference=None, **kwargs):\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in write_batch(self, batch_examples, writer_batch_size)\r\n    155         if self.pa_writer is None:\r\n    156             self._build_writer(pa_table=pa.Table.from_pydict(batch_examples))\r\n--> 157         pa_table: pa.Table = pa.Table.from_pydict(batch_examples, schema=self._schema)\r\n    158         if writer_batch_size is None:\r\n    159             writer_batch_size = self.writer_batch_size\r\n\r\n/usr/local/lib/python3.6/dist-packages/pyarrow/types.pxi in __iter__()\r\n\r\n/usr/local/lib/python3.6/dist-packages/pyarrow/array.pxi in pyarrow.lib.asarray()\r\n\r\n/usr/local/lib/python3.6/dist-packages/pyarrow/array.pxi in pyarrow.lib.array()\r\n\r\n/usr/local/lib/python3.6/dist-packages/pyarrow/array.pxi in pyarrow.lib._sequence_to_array()\r\n\r\nTypeError: an integer is required (got type str)\r\n```\r\nI'm not sure whether I'm doing this wrong or whether it's an issue. I would like to know a workaround. Thank you.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 647521308,
    "title": "Add package path to sys when downloading package as github archive",
    "dateCreated": "2020-06-29T16:46:01Z",
    "dateModified": "2020-06-29T16:46:01Z",
    "description": "This fixes the `coval.py` metric so that imports within the downloaded module work correctly. We can use a similar trick to add the BLEURT metric (@ankparikh)\r\n\r\n@thomwolf not sure how you feel about adding to the `PYTHONPATH` from the script. This is the only way I could make it work with my understanding of `importlib` but there might be a more elegant method.\r\n\r\nThis PR fixes https://github.com/huggingface/nlp/issues/305",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 647483850,
    "title": "output nested dict in get_nearest_examples",
    "dateCreated": "2020-06-29T15:47:47Z",
    "dateModified": "2020-06-29T15:47:47Z",
    "description": "As we are using a columnar format like arrow as the backend for datasets, we expect to have a dictionary of columns when we slice a dataset like in this example:\r\n```python\r\nmy_examples = dataset[0:10]\r\nprint(type(my_examples))\r\n# >>> dict\r\nprint(my_examples[\"my_column\"][0]\r\n# >>> this is the first element of the column 'my_column'\r\n```\r\n\r\nTherefore I wanted to keep this logic when calling `get_nearest_examples` that returns the top 10 nearest examples:\r\n```python\r\ndataset.add_faiss_index(column=\"embeddings\")\r\nscores, examples = dataset.get_nearest_examples(\"embeddings\", query=my_numpy_embedding)\r\nprint(type(examples))\r\n# >>> dict\r\n```\r\n\r\nPreviously it was returning a list[dict]. It was the only place that was using this output format.\r\n\r\nTo make it work I had to implement `__getitem__(key)` where `key` is a list.\r\nThis is different from `.select` because `.select` is a dataset transform (it returns a new dataset object) while `__getitem__` is an extraction method (it returns python dictionaries).",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 647271526,
    "title": "ERROR:root:mwparserfromhell",
    "dateCreated": "2020-06-29T11:10:43Z",
    "dateModified": "2020-06-29T11:10:43Z",
    "description": "Hi,\r\n\r\nI am trying to download some wikipedia data but I got this error for spanish \"es\" (but there are maybe some others languages which have the same error I haven't tried all of them ).\r\n\r\n`ERROR:root:mwparserfromhell ParseError: This is a bug and should be reported. Info: C tokenizer exited with non-empty token stack.`\r\n\r\nThe code I have use was : \r\n`dataset = load_dataset('wikipedia', '20200501.es', beam_runner='DirectRunner')`\r\n\r\n",
    "status": "open",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 647188167,
    "title": "Blog Authorship Corpus, Non Matching Splits Sizes Error, nlp viewer",
    "dateCreated": "2020-06-29T07:36:35Z",
    "dateModified": "2020-06-29T07:36:35Z",
    "description": "Selecting `blog_authorship_corpus` in the nlp viewer throws the following error: \r\n\r\n```\r\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=610252351, num_examples=532812, dataset_name='blog_authorship_corpus'), 'recorded': SplitInfo(name='train', num_bytes=614706451, num_examples=535568, dataset_name='blog_authorship_corpus')}, {'expected': SplitInfo(name='validation', num_bytes=37500394, num_examples=31277, dataset_name='blog_authorship_corpus'), 'recorded': SplitInfo(name='validation', num_bytes=32553710, num_examples=28521, dataset_name='blog_authorship_corpus')}]\r\nTraceback:\r\nFile \"/home/sasha/streamlit/lib/streamlit/ScriptRunner.py\", line 322, in _run_script\r\n    exec(code, module.__dict__)\r\nFile \"/home/sasha/nlp-viewer/run.py\", line 172, in <module>\r\n    dts, fail = get(str(option.id), str(conf_option.name) if conf_option else None)\r\nFile \"/home/sasha/streamlit/lib/streamlit/caching.py\", line 591, in wrapped_func\r\n    return get_or_create_cached_value()\r\nFile \"/home/sasha/streamlit/lib/streamlit/caching.py\", line 575, in get_or_create_cached_value\r\n    return_value = func(*args, **kwargs)\r\nFile \"/home/sasha/nlp-viewer/run.py\", line 132, in get\r\n    builder_instance.download_and_prepare()\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/nlp/builder.py\", line 432, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/nlp/builder.py\", line 488, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/nlp/utils/info_utils.py\", line 70, in verify_splits\r\n    raise NonMatchingSplitsSizesError(str(bad_splits))\r\n```\r\n@srush @lhoestq ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 646792487,
    "title": "Nested sequences with dicts",
    "dateCreated": "2020-06-27T23:45:17Z",
    "dateModified": "2020-06-27T23:45:17Z",
    "description": "Am pretty much finished [adding a dataset](https://github.com/ghomasHudson/nlp/blob/DocRED/datasets/docred/docred.py) for [DocRED](https://github.com/thunlp/DocRED), but am getting an error when trying to add a nested `nlp.features.sequence(nlp.features.sequence({key:value,...}))`. \r\n\r\nThe original data is in this format:\r\n```python\r\n{\r\n  'title': \"Title of wiki page\",\r\n  'vertexSet': [\r\n                  [\r\n                    { 'name': \"mention_name\", \r\n                      'sent_id': \"mention in which sentence\", \r\n                      'pos': [\"postion of mention in a sentence\"], \r\n                      'type': \"NER_type\"},\r\n                    {another mention}\r\n                  ], \r\n                  [another entity]\r\n                ]\r\n    ...\r\n}\r\n```\r\nSo to represent this I've attempted to write:\r\n```\r\n...\r\nfeatures=nlp.Features({\r\n    \"title\": nlp.Value(\"string\"),\r\n    \"vertexSet\": nlp.features.Sequence(nlp.features.Sequence({\r\n        \"name\": nlp.Value(\"string\"),\r\n        \"sent_id\": nlp.Value(\"int32\"),\r\n        \"pos\": nlp.features.Sequence(nlp.Value(\"int32\")),\r\n        \"type\": nlp.Value(\"string\"),\r\n    })),\r\n    ...\r\n    }),\r\n...\r\n```\r\nThis is giving me the error:\r\n```\r\npyarrow.lib.ArrowTypeError: Could not convert [{'pos': [[0,2], [2,4], [3,5]], \"type\": [\"ORG\", \"ORG\", \"ORG\"], \"name\": [\"Lark Force\", \"Lark Force\", \"Lark Force\", \"sent_id\": [0, 3, 4]}..... with type list: was not a dict, tuple, or recognized null value for conversion to struct type\r\n```\r\nDo we expect the pyarrow stuff to break when doing this deeper nesting? I've checked that it still works when you do `nlp.features.Sequence(nlp.features.Sequence(nlp.Value(\"string\"))` or `nlp.features.Sequence({key:value,...})` just not nested sequences with a dict.\r\n\r\nIf it's not possible, I can always convert it to a shallower structure. I'd rather not change the DocRED authors' structure if I don't have to though.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 646682840,
    "title": "Multitask",
    "dateCreated": "2020-06-27T13:27:29Z",
    "dateModified": "2020-06-27T13:27:29Z",
    "description": "Following our discussion in #217, I've implemented a first working version of `MultiDataset`.\r\n\r\nThere's a function `build_multitask()` which takes either individual `nlp.Dataset`s or `dicts` of splits and constructs `MultiDataset`(s). I've added a notebook with example usage.\r\n\r\nI've implemented many of the `nlp.Dataset` methods (cache_files, columns, nbytes, num_columns, num_rows, column_names, schema, shape). Some of the other methods are complicated as they change the number of examples. These raise `NotImplementedError`s at the moment.\r\n\r\nThis will need some tests which I haven't written yet.\r\n\r\nThere's definitely room for improvements but I think the general approach is sound. ",
    "status": "open",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 646555384,
    "title": "Adding a dataset with multiple subtasks",
    "dateCreated": "2020-06-26T23:14:19Z",
    "dateModified": "2020-06-26T23:14:19Z",
    "description": "I intent to add the datasets of the MT Quality Estimation shared tasks to `nlp`. However, they have different subtasks -- such as word-level, sentence-level and document-level quality estimation, each of which having different language pairs, and some of the data reused in different subtasks.\r\n\r\nFor example, in [QE 2019,](http://www.statmt.org/wmt19/qe-task.html) we had the same English-Russian and English-German data for word-level and sentence-level QE. \r\n\r\nI suppose these datasets could have both their word and sentence-level labels inside `nlp.Features`; but what about other subtasks? Should they be considered a different dataset altogether?\r\n\r\nI read the discussion on #217 but the case of QE seems a lot simpler.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 646366450,
    "title": "add AG News dataset",
    "dateCreated": "2020-06-26T16:11:58Z",
    "dateModified": "2020-06-26T16:11:58Z",
    "description": "adds support for the AG-News topic classification dataset",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 645888943,
    "title": "[Question] Best way to batch a large dataset?",
    "dateCreated": "2020-06-25T22:30:20Z",
    "dateModified": "2020-06-25T22:30:20Z",
    "description": "I'm training on large datasets such as Wikipedia and BookCorpus. Following the instructions in [the tutorial notebook](https://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb), I see the following recommended for TensorFlow:\r\n\r\n```python\r\ntrain_tf_dataset = train_tf_dataset.filter(remove_none_values, load_from_cache_file=False)\r\ncolumns = ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\r\ntrain_tf_dataset.set_format(type='tensorflow', columns=columns)\r\nfeatures = {x: train_tf_dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.max_len]) for x in columns[:3]} \r\nlabels = {\"output_1\": train_tf_dataset[\"start_positions\"].to_tensor(default_value=0, shape=[None, 1])}\r\nlabels[\"output_2\"] = train_tf_dataset[\"end_positions\"].to_tensor(default_value=0, shape=[None, 1])\r\n### Question about this last line ###\r\ntfdataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)\r\n```\r\n\r\nThis code works for something like WikiText-2. However, scaling up to WikiText-103, the last line takes 5-10 minutes to run. I assume it is because tf.data.Dataset.from_tensor_slices() is pulling everything into memory, not lazily loading. This approach won't scale up to datasets 25x larger such as Wikipedia.\r\n\r\nSo I tried manual batching using `dataset.select()`:\r\n\r\n```python\r\nidxs = np.random.randint(len(dataset), size=bsz)\r\nbatch = dataset.select(idxs).map(lambda example: {\"input_ids\": tokenizer(example[\"text\"])})\r\ntf_batch = tf.constant(batch[\"ids\"], dtype=tf.int64)\r\n```\r\n\r\nThis appears to create a new Apache Arrow dataset with every batch I grab, and then tries to cache it. The runtime of `dataset.select([0, 1])` appears to be much worse than `dataset[:2]`. So using `select()` doesn't seem to be performant enough for a training loop.\r\n\r\nIs there a performant scalable way to lazily load batches of nlp Datasets?",
    "status": "open",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 645461174,
    "title": "Fixed singlular very minor spelling error",
    "dateCreated": "2020-06-25T10:45:59Z",
    "dateModified": "2020-06-25T10:45:59Z",
    "description": "An instance of \"independantly\" was changed to \"independently\". That's all.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 645390088,
    "title": "Add MWSC",
    "dateCreated": "2020-06-25T09:22:02Z",
    "dateModified": "2020-06-25T09:22:02Z",
    "description": "Adding the [Modified Winograd Schema Challenge](https://github.com/salesforce/decaNLP/blob/master/local_data/schema.txt) dataset which formed part of the [decaNLP](http://decanlp.com/) benchmark. Not sure how much use people would find for it it outside of the benchmark, but it is general purpose.\r\n\r\nCode is heavily borrowed from the [decaNLP repo](https://github.com/salesforce/decaNLP/blob/1e9605f246b9e05199b28bde2a2093bc49feeeaa/text/torchtext/datasets/generic.py#L773-L877).\r\n\r\nThere's a few (possibly overly opinionated) design choices I made:\r\n\r\n- I used the train/test/dev split [buried in the decaNLP code](https://github.com/salesforce/decaNLP/blob/1e9605f246b9e05199b28bde2a2093bc49feeeaa/text/torchtext/datasets/generic.py#L852-L855)\r\n- I split out each example into the 2 alternatives. Originally the data uses the format:\r\n    ```\r\n    The city councilmen refused the demonstrators a permit because they [feared/advocated] violence. \r\n    Who [feared/advocated] violence? \r\n    councilmen/demonstrators\r\n    ```\r\n    I split into the 2 variants:\r\n    ```\r\n    The city councilmen refused the demonstrators a permit because they feared violence. \r\n    Who feared violence? \r\n    councilmen/demonstrators\r\n    \r\n    The city councilmen refused the demonstrators a permit because they advocated violence. \r\n    Who advocated violence? \r\n    councilmen/demonstrators\r\n    ```\r\n    I can't see any use for having the options combined into a single example (splitting them is [the way decaNLP processes](https://github.com/salesforce/decaNLP/blob/1e9605f246b9e05199b28bde2a2093bc49feeeaa/text/torchtext/datasets/generic.py#L846-L850)) them. You can't train on both versions with them combined, and splitting the examples later would be a pain to do. I think [winogrande.py](https://github.com/huggingface/nlp/blob/master/datasets/winogrande/winogrande.py) presents the data in this way?\r\n\r\n- I've not used the decaNLP framing (appending the options to the question e.g. `Who feared violence? \r\n -- councilmen or demonstrators?`) but left it more generic by adding the options as a new key: `\"options\":[\"councilmen\",\"demonstrators\"]` This should be an easy thing to change using `map` if needed by a specific application.\r\n\r\nDataset is working as-is but if anyone has any thoughts/preferences on the design decisions here I'm definitely open to different choices.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 645025561,
    "title": "[Feature request] Add `shard()` method to dataset",
    "dateCreated": "2020-06-24T22:48:33Z",
    "dateModified": "2020-06-24T22:48:33Z",
    "description": "Currently, to shard a dataset into 10 pieces on different ranks, you can run\r\n\r\n```python\r\nrank = 3 # for example\r\nsize = 10\r\ndataset = nlp.load_dataset('wikitext', 'wikitext-2-raw-v1', split=f\"train[{rank*10}%:{(rank+1)*10}%]\")\r\n```\r\n\r\nHowever, this breaks down if you have a number of ranks that doesn't divide cleanly into 100, such as 64 ranks. Is there interest in adding a method shard() that looks like this?\r\n\r\n```python\r\nrank = 3\r\nsize = 64\r\ndataset = nlp.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\").shard(rank=rank, size=size)\r\n```\r\n\r\nTensorFlow has a similar API: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard. I'd be happy to contribute this code.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 645013131,
    "title": "Add qa_zre",
    "dateCreated": "2020-06-24T22:17:22Z",
    "dateModified": "2020-06-24T22:17:22Z",
    "description": "Adding the QA-ZRE dataset from [\"Zero-Shot Relation Extraction via Reading Comprehension\"](http://nlp.cs.washington.edu/zeroshot/).\r\n\r\nA common processing step seems to be replacing the `XXX` placeholder with the `subject`. I've left this out as it's something you could easily do with `map`.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 644806720,
    "title": "add wikisql",
    "dateCreated": "2020-06-24T18:00:35Z",
    "dateModified": "2020-06-24T18:00:35Z",
    "description": "Adding the [WikiSQL](https://github.com/salesforce/WikiSQL) dataset.\r\n\r\nInteresting things to note:\r\n- Have copied the function (`_convert_to_human_readable`) which converts the SQL query to a human-readable (string) format as this is what most people will want when actually using this dataset for NLP applications.\r\n- `conds` was originally a tuple but is converted to a dictionary to support differing types.\r\n\r\nWould be nice to add the logical_form metrics too at some point.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 644783822,
    "title": "Add narrative qa",
    "dateCreated": "2020-06-24T17:26:18Z",
    "dateModified": "2020-06-24T17:26:18Z",
    "description": "Test cases for dummy data don't pass\r\n\r\nOnly contains data for summaries (not whole story)",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 644195251,
    "title": "Specify utf-8 encoding for MRPC files",
    "dateCreated": "2020-06-23T22:44:36Z",
    "dateModified": "2020-06-23T22:44:36Z",
    "description": "Fixes #307, again probably a Windows-related issue.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 644187262,
    "title": "Specify encoding for MRPC",
    "dateCreated": "2020-06-23T22:24:49Z",
    "dateModified": "2020-06-23T22:24:49Z",
    "description": "Same as #242, but with MRPC: on Windows, I get a `UnicodeDecodeError` when I try to download the dataset:\r\n```python\r\ndataset = nlp.load_dataset('glue', 'mrpc')\r\n```\r\n\r\n```python\r\nDownloading and preparing dataset glue/mrpc (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Python\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0...\r\n---------------------------------------------------------------------------\r\nUnicodeDecodeError                        Traceback (most recent call last)\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\builder.py in incomplete_dir(dirname)\r\n    369                 try:\r\n--> 370                     yield tmp_dir\r\n    371                     if os.path.isdir(dirname):\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    430                 verify_infos = not save_infos and not ignore_verifications\r\n--> 431                 self._download_and_prepare(\r\n    432                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    482                 # Prepare split will record examples associated to the split\r\n--> 483                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    484             except OSError:\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\builder.py in _prepare_split(self, split_generator)\r\n    663         generator = self._generate_examples(**split_generator.gen_kwargs)\r\n--> 664         for key, record in utils.tqdm(generator, unit=\" examples\", total=split_info.num_examples, leave=False):\r\n    665             example = self.info.features.encode_example(record)\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\notebook.py in __iter__(self, *args, **kwargs)\r\n    217         try:\r\n--> 218             for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):\r\n    219                 # return super(tqdm...) will not catch exception\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\std.py in __iter__(self)\r\n   1128         try:\r\n-> 1129             for obj in iterable:\r\n   1130                 yield obj\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\datasets\\glue\\7fc58099eb3983a04c8dac8500b70d27e6eceae63ffb40d7900c977897bb58c6\\glue.py in _generate_examples(self, data_file, split, mrpc_files)\r\n    514             examples = self._generate_example_mrpc_files(mrpc_files=mrpc_files, split=split)\r\n--> 515             for example in examples:\r\n    516                 yield example[\"idx\"], example\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\datasets\\glue\\7fc58099eb3983a04c8dac8500b70d27e6eceae63ffb40d7900c977897bb58c6\\glue.py in _generate_example_mrpc_files(self, mrpc_files, split)\r\n    576                 reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\r\n--> 577                 for n, row in enumerate(reader):\r\n    578                     is_row_in_dev = [row[\"#1 ID\"], row[\"#2 ID\"]] in dev_ids\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\csv.py in __next__(self)\r\n    110             self.fieldnames\r\n--> 111         row = next(self.reader)\r\n    112         self.line_num = self.reader.line_num\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\encodings\\cp1252.py in decode(self, input, final)\r\n     22     def decode(self, input, final=False):\r\n---> 23         return codecs.charmap_decode(input,self.errors,decoding_table)[0]\r\n     24 \r\n\r\nUnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 1180: character maps to <undefined>\r\n```\r\nThe fix is the same: specify `utf-8` encoding when opening the file. The previous fix didn't work as MRPC's download process is different from the others in GLUE. \r\nI am going to propose a new PR :)",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 644176078,
    "title": "add pg19 dataset",
    "dateCreated": "2020-06-23T22:03:52Z",
    "dateModified": "2020-06-23T22:03:52Z",
    "description": "https://github.com/huggingface/nlp/issues/274\r\n\r\nAdd functioning PG19 dataset with dummy data\r\n\r\n`cos_e.py` was just auto-linted by `make style`",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 644148149,
    "title": "Importing downloaded package repository fails",
    "dateCreated": "2020-06-23T21:09:05Z",
    "dateModified": "2020-06-23T21:09:05Z",
    "description": "The `get_imports` function in `src/nlp/load.py` has a feature to download a package as a zip archive of the github repository and import functions from the unpacked directory. This is used for example in the `metrics/coval.py` file, and would be useful to add BLEURT (@ankparikh).\r\n\r\nCurrently however, the code seems to have trouble with imports within the package. For example:\r\n```\r\nimport nlp\r\ncoval = nlp.load_metric('coval')\r\n```\r\nyields:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/yacine/Code/nlp/src/nlp/load.py\", line 432, in load_metric\r\n    metric_cls = import_main_class(module_path, dataset=False)\r\n  File \"/home/yacine/Code/nlp/src/nlp/load.py\", line 57, in import_main_class\r\n    module = importlib.import_module(module_path)\r\n  File \"/home/yacine/anaconda3/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/yacine/Code/nlp/src/nlp/metrics/coval/a78807df33ac45edbb71799caf2b3b47e55df4fd690267808fe963a5e8b30952/coval.py\", line 21, in <module>\r\n    from .coval_backend.conll import reader  # From: https://github.com/ns-moosavi/coval\r\n  File \"/home/yacine/Code/nlp/src/nlp/metrics/coval/a78807df33ac45edbb71799caf2b3b47e55df4fd690267808fe963a5e8b30952/coval_backend/conll/reader.py\", line 2, in <module>\r\n    from conll import mention\r\nModuleNotFoundError: No module named 'conll'\r\n```\r\n\r\nNot sure what the fix would be there.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 644091970,
    "title": "Problem while printing doc string when instantiating multiple metrics.",
    "dateCreated": "2020-06-23T19:32:05Z",
    "dateModified": "2020-06-23T19:32:05Z",
    "description": "When I load more than one metric and try to print doc string of a particular metric,. It shows the doc strings of all imported metric one after the other which looks quite confusing and clumsy.\r\nAttached [Colab](https://colab.research.google.com/drive/13H0ZgyQ2se0mqJ2yyew0bNEgJuHaJ8H3?usp=sharing) Notebook for problem clarification..",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 643912464,
    "title": "allow to move files across file systems",
    "dateCreated": "2020-06-23T14:56:08Z",
    "dateModified": "2020-06-23T14:56:08Z",
    "description": "Users are allowed to use the `cache_dir` that they want.\r\nTherefore it can happen that we try to move files across filesystems.\r\nWe were using `os.rename` that doesn't allow that, so I changed some of them to `shutil.move`.\r\n\r\nThis should fix #301",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 643910418,
    "title": "Question - Sign Language Datasets",
    "dateCreated": "2020-06-23T14:53:40Z",
    "dateModified": "2020-06-23T14:53:40Z",
    "description": "An emerging field in NLP is SLP - sign language processing.\r\n\r\nI was wondering about adding datasets here, specifically because it's shaping up to be large and easily usable.\r\nThe metrics for sign language to text translation are the same.\r\n\r\nSo, what do you think about (me, or others) adding datasets here?\r\n\r\n\r\nAn example dataset would be [RWTH-PHOENIX-Weather 2014 T](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX-2014-T/)\r\nFor every item in the dataset, the data object includes:\r\n1. video_path - path to mp4 file\r\n2. pose_path - a path to `.pose` file with human pose landmarks\r\n3. openpose_path - a path to a `.json` file with human pose landmarks\r\n4. gloss - string\r\n5. text - string\r\n6. video_metadata - height, width, frames, framerate\r\n\r\n\r\n------\r\n\r\nTo make it a tad more complicated - what if sign language libraries add requirements to `nlp`? for example, sign language is commonly annotated using `ilex`, `eaf`, or `srt` files, which are all loadable as text, but there is no reason for the dataset to parse that file by itself, if libraries exist to do so.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 643763525,
    "title": "Setting cache_dir gives error on wikipedia download",
    "dateCreated": "2020-06-23T11:31:44Z",
    "dateModified": "2020-06-23T11:31:44Z",
    "description": "First of all thank you for a super handy library! I'd like to download large files to a specific drive so I set `cache_dir=my_path`. This works fine with e.g. imdb and squad. But on wikipedia I get an error:\r\n```\r\nnlp.load_dataset('wikipedia', '20200501.de', split = 'train', cache_dir=my_path)\r\n```\r\n```\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-2-23551344d7bc> in <module>\r\n      1 import nlp\r\n----> 2 nlp.load_dataset('wikipedia', '20200501.de', split = 'train', cache_dir=path)\r\n\r\n~/anaconda3/envs/fastai2/lib/python3.7/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    522         download_mode=download_mode,\r\n    523         ignore_verifications=ignore_verifications,\r\n--> 524         save_infos=save_infos,\r\n    525     )\r\n    526 \r\n\r\n~/anaconda3/envs/fastai2/lib/python3.7/site-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    385                     with utils.temporary_assignment(self, \"_cache_dir\", tmp_data_dir):\r\n    386                         reader = ArrowReader(self._cache_dir, self.info)\r\n--> 387                         reader.download_from_hf_gcs(self._cache_dir, self._relative_data_dir(with_version=True))\r\n    388                         downloaded_info = DatasetInfo.from_directory(self._cache_dir)\r\n    389                         self.info.update(downloaded_info)\r\n\r\n~/anaconda3/envs/fastai2/lib/python3.7/site-packages/nlp/arrow_reader.py in download_from_hf_gcs(self, cache_dir, relative_data_dir)\r\n    231             remote_dataset_info = os.path.join(remote_cache_dir, \"dataset_info.json\")\r\n    232             downloaded_dataset_info = cached_path(remote_dataset_info)\r\n--> 233             os.rename(downloaded_dataset_info, os.path.join(cache_dir, \"dataset_info.json\"))\r\n    234             if self._info is not None:\r\n    235                 self._info.update(self._info.from_directory(cache_dir))\r\n\r\nOSError: [Errno 18] Invalid cross-device link: '/home/local/NTU/nn/.cache/huggingface/datasets/025fa4fd4f04aaafc9e939260fbc8f0bb190ce14c61310c8ae1ddd1dcb31f88c.9637f367b6711a79ca478be55fe6989b8aea4941b7ef7adc67b89ff403020947' -> '/data/nn/nlp/wikipedia/20200501.de/1.0.0.incomplete/dataset_info.json'\r\n```",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 643688304,
    "title": "Fix bertscore references",
    "dateCreated": "2020-06-23T09:38:59Z",
    "dateModified": "2020-06-23T09:38:59Z",
    "description": "I added some type checking for metrics. There was an issue where a metric could interpret a string a a list. A `ValueError` is raised if a string is given instead of a list.\r\n\r\nMoreover I added support for both strings and lists of strings for `references` in `bertscore`, as it is the case in the original code.\r\n\r\nBoth ways work:\r\n```\r\nimport nlp\r\n\r\nscorer = nlp.load_metric(\"bertscore\")\r\nwith open(\"pred.txt\") as p, open(\"ref.txt\") as g:\r\n    for lp, lg in zip(p, g):\r\n        scorer.add(lp, [lg])\r\nscore = scorer.compute(lang=\"en\")\r\n```\r\n\r\n```\r\nimport nlp\r\n\r\nscorer = nlp.load_metric(\"bertscore\")\r\nwith open(\"pred.txt\") as p, open(\"ref.txt\") as g:\r\n    for lp, lg in zip(p, g):\r\n        scorer.add(lp, lg)\r\nscore = scorer.compute(lang=\"en\")\r\n```\r\n\r\nThis should fix #295 and #238 ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 643611557,
    "title": "remove some print in snli file",
    "dateCreated": "2020-06-23T07:46:06Z",
    "dateModified": "2020-06-23T07:46:06Z",
    "description": "This PR removes unwanted  `print` statements in some files such as `snli.py`",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 643603804,
    "title": "Add searchable datasets",
    "dateCreated": "2020-06-23T07:33:03Z",
    "dateModified": "2020-06-23T07:33:03Z",
    "description": "# Better support for Numpy format + Add Indexed Datasets\r\n\r\nI was working on adding Indexed Datasets but in the meantime I had to also add more support for Numpy arrays in the lib.\r\n\r\n## Better support for Numpy format\r\n\r\nNew features:\r\n- New fast method to convert Numpy arrays from Arrow structure (up to x100 speed up) using Pandas.\r\n- Allow to output Numpy arrays in batched `.map`, which was the only missing part to fully support Numpy arrays.\r\n\r\nPandas offers fast zero-copy Numpy arrays conversion from Arrow structures.\r\nUsing it we can speed up the reading of memory-mapped Numpy array stored in Arrow format.\r\n\r\nWith these changes you can easily compute embeddings of texts using `.map()`. For example:\r\n```python\r\ndef embed(text):\r\n    tokenized_example = tokenizer.encode(text, return_tensors=\"pt\")\r\n    embeddings = bert_encoder(tokenized_examples).numpy()\r\n    return embeddings\r\ndset_with_embeddings = dset.map(lambda example: {\"embeddings\": embed(example[\"text])})\r\n```\r\nAnd then reading the embeddings from the arrow format is be very fast.\r\n\r\nPS1: Note that right now only 1d arrays are supported.\r\nPS2: It seems possible to do without pandas but it will require more _trickery_.\r\nPS3: I did a simple benchmark with google colab that you can view here:\r\nhttps://colab.research.google.com/drive/1QlLTR6LRwYOKGJ-hTHmHyolE3wJzvfFg?usp=sharing\r\n\r\n## Add Indexed Datasets\r\n\r\nFor many retrieval tasks it is convenient to index a dataset to be able to run fast queries.\r\nFor example for models like DPR, REALM, RAG etc. that are models for Open Domain QA, the retrieval step is very important.\r\n\r\nTherefore I added two ways to add an index to a column of a dataset:\r\n1) You can index it using a Dense Index like Faiss. It is used to index vectors.\r\n    Faiss is a library for efficient similarity search and clustering of dense vectors.\r\n    It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM.\r\n2) You can index it using a Sparse Index like Elasticsearch. It is used to index text and run queries based on BM25 similarity.\r\n\r\nExample of usage:\r\n\r\n```python\r\nds = nlp.load_dataset('crime_and_punish', split='train')\r\nds_with_embeddings = ds.map(lambda example: {'embeddings': embed(example['line']}))  # `embed` outputs a `np.array`\r\nds_with_embeddings.add_vector_index(column='embeddings')\r\nscores, retrieved_examples = ds_with_embeddings.get_nearest(column='embeddings', query=embed('my new query'), k=10)\r\n```\r\n\r\n```python\r\nds = nlp.load_dataset('crime_and_punish', split='train')\r\nes_client = elasticsearch.Elasticsearch()\r\nds.add_text_index(column='line', es_client=es_client, index_name=\"my_es_index\")\r\nscores, retrieved_examples = ds.get_nearest(column='line', query='my new query', k=10)\r\n```\r\n\r\nPS4: Faiss allows to specify many options for the [index](https://github.com/facebookresearch/faiss/wiki/The-index-factory) and for [GPU settings](https://github.com/facebookresearch/faiss/wiki/Faiss-on-the-GPU). I made sure that the user has full control over those settings.\r\n\r\n## Tests\r\n\r\nI added tests for Faiss, Elasticsearch and indexed datasets.\r\nI had to edit the CI config because all the test scripts were not being run by CircleCI.\r\n\r\n------------------\r\n\r\nI'd be really happy to have some feedbacks :)",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 643444625,
    "title": "Error in Demo for Specific Datasets",
    "dateCreated": "2020-06-23T00:38:42Z",
    "dateModified": "2020-06-23T00:38:42Z",
    "description": "Selecting `natural_questions` or `newsroom` dataset in the online demo results in an error similar to the following.\r\n\r\n![image](https://user-images.githubusercontent.com/60150701/85347842-ac861900-b4ae-11ea-98c4-a53a00934783.png)\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 643423717,
    "title": "snli -1 labels",
    "dateCreated": "2020-06-22T23:33:30Z",
    "dateModified": "2020-06-22T23:33:30Z",
    "description": "I'm trying to train a model on the SNLI dataset. Why does it have so many -1 labels?\r\n```\r\nimport nlp\r\nfrom collections import Counter\r\ndata = nlp.load_dataset('snli')['train']\r\nprint(Counter(data['label']))\r\nCounter({0: 183416, 2: 183187, 1: 182764, -1: 785})\r\n```\r\n",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 643245412,
    "title": "Improve input warning for evaluation metrics",
    "dateCreated": "2020-06-22T17:28:57Z",
    "dateModified": "2020-06-22T17:28:57Z",
    "description": "Hi, \r\n\r\nI am the author of `bert_score`. Recently, we received [ an issue ](https://github.com/Tiiiger/bert_score/issues/62) reporting a problem in using `bert_score` from the `nlp` package (also see #238 in this repo).  After looking into this, I realized that the problem arises from the format `nlp.Metric` takes input. \r\n\r\nHere is a minimal example:\r\n```python\r\nimport nlp\r\n\r\nscorer = nlp.load_metric(\"bertscore\")\r\nwith open(\"pred.txt\") as p, open(\"ref.txt\") as g:\r\n    for lp, lg in zip(p, g):\r\n        scorer.add(lp, lg)\r\nscore = scorer.compute(lang=\"en\")\r\n```\r\n\r\nThe problem in the above code is that `scorer.add()` expects a list of strings as input for the references. As a result, the `scorer` here would take a list of characters in `lg` to be the references. The correct implementation would be calling\r\n```python\r\nscorer.add(lp, [lg])\r\n```\r\n\r\nI just want to raise this issue to you to prevent future user errors of a similar kind. I assume some simple type checking can prevent this from happening?\r\n\r\nThanks!",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 643181179,
    "title": "Cannot load arxiv dataset on MacOS?",
    "dateCreated": "2020-06-22T15:46:55Z",
    "dateModified": "2020-06-22T15:46:55Z",
    "description": "I am having trouble loading the `\"arxiv\"` config from the `\"scientific_papers\"` dataset on MacOS. When I try loading the dataset with:\r\n\r\n```python\r\narxiv = nlp.load_dataset(\"scientific_papers\", \"arxiv\")\r\n```\r\n\r\nI get the following stack trace:\r\n\r\n```bash\r\nJSONDecodeError                           Traceback (most recent call last)\r\n<ipython-input-2-8e00c55d5a59> in <module>\r\n----> 1 arxiv = nlp.load_dataset(\"scientific_papers\", \"arxiv\")\r\n\r\n~/miniconda3/envs/t2t/lib/python3.7/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    522         download_mode=download_mode,\r\n    523         ignore_verifications=ignore_verifications,\r\n--> 524         save_infos=save_infos,\r\n    525     )\r\n    526 \r\n\r\n~/miniconda3/envs/t2t/lib/python3.7/site-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    430                 verify_infos = not save_infos and not ignore_verifications\r\n    431                 self._download_and_prepare(\r\n--> 432                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    433                 )\r\n    434                 # Sync info\r\n\r\n~/miniconda3/envs/t2t/lib/python3.7/site-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    481             try:\r\n    482                 # Prepare split will record examples associated to the split\r\n--> 483                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    484             except OSError:\r\n    485                 raise OSError(\"Cannot find data file. \" + (self.manual_download_instructions or \"\"))\r\n\r\n~/miniconda3/envs/t2t/lib/python3.7/site-packages/nlp/builder.py in _prepare_split(self, split_generator)\r\n    662 \r\n    663         generator = self._generate_examples(**split_generator.gen_kwargs)\r\n--> 664         for key, record in utils.tqdm(generator, unit=\" examples\", total=split_info.num_examples, leave=False):\r\n    665             example = self.info.features.encode_example(record)\r\n    666             writer.write(example)\r\n\r\n~/miniconda3/envs/t2t/lib/python3.7/site-packages/tqdm/std.py in __iter__(self)\r\n   1106                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\r\n   1107 \r\n-> 1108         for obj in iterable:\r\n   1109             yield obj\r\n   1110             # Update and possibly print the progressbar.\r\n\r\n~/miniconda3/envs/t2t/lib/python3.7/site-packages/nlp/datasets/scientific_papers/107a416c0e1958cb846f5934b5aae292f7884a5b27e86af3f3ef1a093e058bbc/scientific_papers.py in _generate_examples(self, path)\r\n    114                 # \"section_names\": list[str], list of section names.\r\n    115                 # \"sections\": list[list[str]], list of sections (list of paragraphs)\r\n--> 116                 d = json.loads(line)\r\n    117                 summary = \"\\n\".join(d[\"abstract_text\"])\r\n    118                 # In original paper, <S> and </S> are not used in vocab during training\r\n\r\n~/miniconda3/envs/t2t/lib/python3.7/json/__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\r\n    346             parse_int is None and parse_float is None and\r\n    347             parse_constant is None and object_pairs_hook is None and not kw):\r\n--> 348         return _default_decoder.decode(s)\r\n    349     if cls is None:\r\n    350         cls = JSONDecoder\r\n\r\n~/miniconda3/envs/t2t/lib/python3.7/json/decoder.py in decode(self, s, _w)\r\n    335 \r\n    336         \"\"\"\r\n--> 337         obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n    338         end = _w(s, end).end()\r\n    339         if end != len(s):\r\n\r\n~/miniconda3/envs/t2t/lib/python3.7/json/decoder.py in raw_decode(self, s, idx)\r\n    351         \"\"\"\r\n    352         try:\r\n--> 353             obj, end = self.scan_once(s, idx)\r\n    354         except StopIteration as err:\r\n    355             raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\n\r\nJSONDecodeError: Unterminated string starting at: line 1 column 46983 (char 46982)\r\n\r\n163502 examples [02:10, 2710.68 examples/s]   \r\n```\r\n\r\nI am not sure how to trace back to the specific JSON file that has the \"Unterminated string\". Also, I do not get this error on colab so I suspect it may be MacOS specific. Copy pasting the relevant lines from `transformers-cli env` below:\r\n\r\n- Platform: Darwin-19.5.0-x86_64-i386-64bit\r\n- Python version: 3.7.5\r\n- PyTorch version (GPU?): 1.5.0 (False)\r\n- Tensorflow version (GPU?): 2.2.0 (False)\r\n\r\nAny ideas?",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 642942182,
    "title": "Don't test community datasets",
    "dateCreated": "2020-06-22T10:15:33Z",
    "dateModified": "2020-06-22T10:15:33Z",
    "description": "This PR disables testing for community datasets on aws.\r\n\r\nIt should fix the CI that is currently failing.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 642897797,
    "title": "Update metadata for x_stance dataset",
    "dateCreated": "2020-06-22T09:13:26Z",
    "dateModified": "2020-06-22T09:13:26Z",
    "description": "Thank you for featuring the x_stance dataset in your library. This PR updates some metadata:\r\n- Citation: Replace preprint with proceedings\r\n- URL: Use a URL with long-term availability\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 642688450,
    "title": "break statement not required",
    "dateCreated": "2020-06-22T01:40:55Z",
    "dateModified": "2020-06-22T01:40:55Z",
    "description": "",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 641978286,
    "title": "ConnectionError - Eli5 dataset download",
    "dateCreated": "2020-06-19T13:40:33Z",
    "dateModified": "2020-06-19T13:40:33Z",
    "description": "Hi, I have a problem with downloading Eli5 dataset. When typing `nlp.load_dataset('eli5')`, I get ConnectionError: Couldn't reach https://storage.googleapis.com/huggingface-nlp/cache/datasets/eli5/LFQA_reddit/1.0.0/explain_like_im_five-train_eli5.arrow\r\n\r\nI would appreciate if you could help me with this issue.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 641934194,
    "title": "update xsum",
    "dateCreated": "2020-06-19T12:28:32Z",
    "dateModified": "2020-06-19T12:28:32Z",
    "description": "This PR makes the following update to the xsum dataset:\r\n\r\n- Manual download is not required anymore\r\n\r\n- dataset can be loaded as follow: `nlp.load_dataset('xsum')`\r\n\r\n\r\n**Important** \r\nInstead of using on outdated url to download the data:  \"https://raw.githubusercontent.com/EdinburghNLP/XSum/master/XSum-Dataset/XSum-TRAINING-DEV-TEST-SPLIT-90-5-5.json\" \r\n\r\na more up-to-date url stored here: https://s3.amazonaws.com/datasets.huggingface.co/summarization/xsum.tar.gz is used\r\n, so that the user does not need to manually download the data anymore. \r\nThere might be slight breaking changes here for xsum. ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 641888610,
    "title": "Error at the first example in README: AttributeError: module 'dill' has no attribute '_dill'",
    "dateCreated": "2020-06-19T11:01:22Z",
    "dateModified": "2020-06-19T11:01:22Z",
    "description": "/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/Users/parasol_tree/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"/Users/parasol_tree/Resource/019 - Github/AcademicEnglishToolkit /test.py\", line 7, in <module>\r\n    import nlp\r\n  File \"/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/nlp/__init__.py\", line 27, in <module>\r\n    from .arrow_dataset import Dataset\r\n  File \"/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/nlp/arrow_dataset.py\", line 31, in <module>\r\n    from nlp.utils.py_utils import dumps\r\n  File \"/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/nlp/utils/__init__.py\", line 20, in <module>\r\n    from .download_manager import DownloadManager, GenerateMode\r\n  File \"/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/nlp/utils/download_manager.py\", line 25, in <module>\r\n    from .py_utils import flatten_nested, map_nested, size_str\r\n  File \"/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/nlp/utils/py_utils.py\", line 244, in <module>\r\n    class Pickler(dill.Pickler):\r\n  File \"/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/nlp/utils/py_utils.py\", line 247, in Pickler\r\n    dispatch = dill._dill.MetaCatchingDict(dill.Pickler.dispatch.copy())\r\nAttributeError: module 'dill' has no attribute '_dill'",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 641800227,
    "title": "fix squad_v2 metric",
    "dateCreated": "2020-06-19T08:24:46Z",
    "dateModified": "2020-06-19T08:24:46Z",
    "description": "Fix #280 \r\nThe imports were wrong",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 641585758,
    "title": "Add ANLI dataset.",
    "dateCreated": "2020-06-18T22:27:30Z",
    "dateModified": "2020-06-18T22:27:30Z",
    "description": "I completed all the steps in https://github.com/huggingface/nlp/blob/master/CONTRIBUTING.md#how-to-add-a-dataset and push the code for ANLI. Please let me know if there are any errors.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 641360702,
    "title": "Consistent formatting of citations",
    "dateCreated": "2020-06-18T16:25:23Z",
    "dateModified": "2020-06-18T16:25:23Z",
    "description": "#283 ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 641337217,
    "title": "Fix manual download instructions",
    "dateCreated": "2020-06-18T15:59:57Z",
    "dateModified": "2020-06-18T15:59:57Z",
    "description": "This PR replaces the static `DatasetBulider` variable `MANUAL_DOWNLOAD_INSTRUCTIONS` by a property function `manual_download_instructions()`. \r\n\r\nSome datasets like XTREME and all WMT need the manual data dir only for a small fraction of the possible configs.\r\n\r\nAfter some brainstorming with @mariamabarham and @lhoestq, we came to the conclusion that having a property function `manual_download_instructions()` gives us more flexibility to decide on a per config basis in the dataset builder if manual download instructions are needed.\r\n\r\nAlso this PR should unblock solves a bug with `wmt16 - ro-en` \r\n@sshleifer from this branch you should be able to succesfully run\r\n\r\n```python \r\nimport nlp \r\nds = nlp.load_dataset('./datasets/wmt16', 'ro-en')\r\n```\r\n\r\nand once this PR is merged S3 should be synched so that \r\n\r\n```python\r\nimport nlp\r\nds = nlp.load_dataset(\"wmt16\", \"ro-en\")\r\n```\r\n\r\nworks as well.\r\n\r\n**Important**: Since `MANUAL_DOWNLOAD_INSTRUCTIONS` was not really exposed to the user, this PR should not be a problem regarding backward compatibility.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 641270439,
    "title": "Consistent formatting of citations",
    "dateCreated": "2020-06-18T14:48:45Z",
    "dateModified": "2020-06-18T14:48:45Z",
    "description": "The citations are all of a different format, some have \"```\" and have text inside, others are proper bibtex. \r\n\r\nCan we make it so that they all are proper citations, i.e. parse by the bibtex spec:\r\n\r\nhttps://bibtexparser.readthedocs.io/en/master/",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 641217759,
    "title": "Update dataset_info from gcs",
    "dateCreated": "2020-06-18T13:41:15Z",
    "dateModified": "2020-06-18T13:41:15Z",
    "description": "Some datasets are hosted on gcs (wikipedia for example). In this PR I make sure that, when a user loads such datasets, the file_instructions are built using the dataset_info.json from gcs and not from the info extracted from the local `dataset_infos.json` (the one that contain the info for each config). Indeed local files may end up outdated.\r\n\r\nFurthermore, to avoid outdated dataset_infos.json, I now make sure that each time you run `load_dataset` it also tries to update the file locally.\r\n",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 641067856,
    "title": "Private/sensitive data",
    "dateCreated": "2020-06-18T09:47:27Z",
    "dateModified": "2020-06-18T09:47:27Z",
    "description": "Hi all,\r\nThanks for this fantastic library, it makes it very easy to do prototyping for NLP projects interchangeably between TF/Pytorch. \r\n\r\nUnfortunately, there is data that cannot easily be shared publicly as it may contain sensitive information. \r\nIs there support/a plan to support such data with NLP, e.g. by reading it from local sources?\r\n\r\nUse case flow could look like this: use NLP to prototype an approach on similar, public data and apply the resulting prototype on sensitive/private data without the need to rethink data processing pipelines. \r\n\r\nMany thanks for your responses ahead of time and kind regards,\r\nMFreidank",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 640677615,
    "title": "Error with SquadV2 Metrics",
    "dateCreated": "2020-06-17T19:10:54Z",
    "dateModified": "2020-06-17T19:10:54Z",
    "description": "I can't seem to import squad v2 metrics. \r\n\r\n**squad_metric = nlp.load_metric('squad_v2')**\r\n\r\n**This throws me an error.:**\r\n\r\n\r\n```\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-8-170b6a170555> in <module>\r\n----> 1 squad_metric = nlp.load_metric('squad_v2')\r\n\r\n~/env/lib64/python3.6/site-packages/nlp/load.py in load_metric(path, name, process_id, num_process, data_dir, experiment_id, in_memory, download_config, **metric_init_kwargs)\r\n    426     \"\"\"\r\n    427     module_path = prepare_module(path, download_config=download_config, dataset=False)\r\n--> 428     metric_cls = import_main_class(module_path, dataset=False)\r\n    429     metric = metric_cls(\r\n    430         name=name,\r\n\r\n~/env/lib64/python3.6/site-packages/nlp/load.py in import_main_class(module_path, dataset)\r\n     55     \"\"\"\r\n     56     importlib.invalidate_caches()\r\n---> 57     module = importlib.import_module(module_path)\r\n     58 \r\n     59     if dataset:\r\n\r\n/usr/lib64/python3.6/importlib/__init__.py in import_module(name, package)\r\n    124                 break\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n    128 \r\n\r\n/usr/lib64/python3.6/importlib/_bootstrap.py in _gcd_import(name, package, level)\r\n\r\n/usr/lib64/python3.6/importlib/_bootstrap.py in _find_and_load(name, import_)\r\n\r\n/usr/lib64/python3.6/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\n/usr/lib64/python3.6/importlib/_bootstrap.py in _load_unlocked(spec)\r\n\r\n/usr/lib64/python3.6/importlib/_bootstrap_external.py in exec_module(self, module)\r\n\r\n/usr/lib64/python3.6/importlib/_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\n~/env/lib64/python3.6/site-packages/nlp/metrics/squad_v2/a15e787c76889174874386d3def75321f0284c11730d2a57e28fe1352c9b5c7a/squad_v2.py in <module>\r\n     16 \r\n     17 import nlp\r\n---> 18 from .evaluate import evaluate\r\n     19 \r\n     20 _CITATION = \"\"\"\\\r\n\r\nImportError: cannot import name 'evaluate'\r\n```",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 640611692,
    "title": "Dataset Preprocessing Cache with .map() function not working as expected",
    "dateCreated": "2020-06-17T17:17:21Z",
    "dateModified": "2020-06-17T17:17:21Z",
    "description": "I've been having issues with reproducibility when loading and processing datasets with the `.map` function. I was only able to resolve them by clearing all of the cache files on my system. \r\n\r\nIs there a way to disable using the cache when processing a dataset? As I make minor processing changes on the same dataset, I want to be able to be certain the data is being re-processed rather than loaded from a cached file. \r\n\r\nCould you also help me understand a bit more about how the caching functionality is used for pre-processing? E.g. how is it determined when to load from a cache vs. reprocess. \r\nI was particularly having an issue where the correct dataset splits were loaded, but as soon as I applied the `.map()` function to each split independently, they somehow all exited this process having been converted to the test set.\r\nThanks!",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 640518917,
    "title": "MemoryError when loading German Wikipedia",
    "dateCreated": "2020-06-17T15:06:21Z",
    "dateModified": "2020-06-17T15:06:21Z",
    "description": "Hi, first off let me say thank you for all the awesome work you're doing at Hugging Face across all your projects (NLP, Transformers, Tokenizers) - they're all amazing contributions to us working with NLP models :)\r\n\r\nI'm trying to download the German Wikipedia dataset as follows:\r\n\r\n```\r\nwiki = nlp.load_dataset(\"wikipedia\", \"20200501.de\", split=\"train\")\r\n```\r\n\r\nHowever, when I do so, I get the following error:\r\n\r\n```\r\nDownloading and preparing dataset wikipedia/20200501.de (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/ubuntu/.cache/huggingface/datasets/wikipedia/20200501.de/1.0.0...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/anaconda3/envs/albert/lib/python3.7/site-packages/nlp/load.py\", line 520, in load_dataset\r\n    save_infos=save_infos,\r\n  File \"/home/ubuntu/anaconda3/envs/albert/lib/python3.7/site-packages/nlp/builder.py\", line 433, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/ubuntu/anaconda3/envs/albert/lib/python3.7/site-packages/nlp/builder.py\", line 824, in _download_and_prepare\r\n    \"\\n\\t`{}`\".format(usage_example)\r\nnlp.builder.MissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https://beam.apache.org/documentation/runners/capability-matrix/\r\nIf you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). \r\nExample of usage: \r\n\t`load_dataset('wikipedia', '20200501.de', beam_runner='DirectRunner')`\r\n```\r\n\r\nSo, following on from the example usage at the bottom, I tried specifying `beam_runner='DirectRunner`, however when I do this after about 20 min after the data has all downloaded, I get a `MemoryError` as warned.\r\n\r\nThis isn't an issue for the English or French Wikipedia datasets (I've tried both), as neither seem to require that `beam_runner` be specified. Can you please clarify why this is an issue for the German dataset?\r\n\r\nMy nlp version is 0.2.1.\r\n\r\nThank you!",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 640163053,
    "title": "Empty samples in glue/qqp",
    "dateCreated": "2020-06-17T05:54:52Z",
    "dateModified": "2020-06-17T05:54:52Z",
    "description": "```\r\nqqp = nlp.load_dataset('glue', 'qqp')\r\nprint(qqp['train'][310121])\r\nprint(qqp['train'][362225])\r\n```\r\n```\r\n{'question1': 'How can I create an Android app?', 'question2': '', 'label': 0, 'idx': 310137}\r\n{'question1': 'How can I develop android app?', 'question2': '', 'label': 0, 'idx': 362246}\r\n```\r\nNotice that question 2 is empty string. \r\nBTW, I have checked and these two are the only naughty ones in all splits of qqp.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 639490858,
    "title": "Fix metric compute (original_instructions missing)",
    "dateCreated": "2020-06-16T08:52:01Z",
    "dateModified": "2020-06-16T08:52:01Z",
    "description": "When loading arrow data we added in cc8d250 a way to specify the instructions that were used to store them with the loaded dataset.\r\nHowever metrics load data the same way but don't need instructions (we use one single file).\r\n\r\nIn this PR I just make `original_instructions` optional when reading files to load a `Dataset` object.\r\n\r\nThis should fix #269 ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 639439052,
    "title": "NonMatchingChecksumError when loading pubmed dataset",
    "dateCreated": "2020-06-16T07:31:51Z",
    "dateModified": "2020-06-16T07:31:51Z",
    "description": "I get this error when i run `nlp.load_dataset('scientific_papers', 'pubmed', split = 'train[:50%]')`.\r\nThe error is:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-2-7742dea167d0> in <module>()\r\n----> 1 df = nlp.load_dataset('scientific_papers', 'pubmed', split = 'train[:50%]')\r\n      2 df = pd.DataFrame(df)\r\n      3 gc.collect()\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    518         download_mode=download_mode,\r\n    519         ignore_verifications=ignore_verifications,\r\n--> 520         save_infos=save_infos,\r\n    521     )\r\n    522 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    431                 verify_infos = not save_infos and not ignore_verifications\r\n    432                 self._download_and_prepare(\r\n--> 433                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    434                 )\r\n    435                 # Sync info\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    468         # Checksums verification\r\n    469         if verify_infos:\r\n--> 470             verify_checksums(self.info.download_checksums, dl_manager.get_recorded_sizes_checksums())\r\n    471         for split_generator in split_generators:\r\n    472             if str(split_generator.split_info.name).lower() == \"all\":\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums)\r\n     34     bad_urls = [url for url in expected_checksums if expected_checksums[url] != recorded_checksums[url]]\r\n     35     if len(bad_urls) > 0:\r\n---> 36         raise NonMatchingChecksumError(str(bad_urls))\r\n     37     logger.info(\"All the checksums matched successfully.\")\r\n     38 \r\n\r\nNonMatchingChecksumError: ['https://drive.google.com/uc?id=1b3rmCSIoh6VhD4HKWjI4HOW-cSwcwbeC&export=download', 'https://drive.google.com/uc?id=1lvsqvsFi3W-pE1SqNZI0s8NR9rC1tsja&export=download']\r\n```\r\nI'm currently working on google colab.\r\n\r\nThat is quite strange because yesterday it was fine.\r\n",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 639156625,
    "title": "PG-19",
    "dateCreated": "2020-06-15T21:02:26Z",
    "dateModified": "2020-06-15T21:02:26Z",
    "description": "Hi, and thanks for all your open-sourced work, as always!\r\n\r\nI was wondering if you would be open to adding PG-19 to your collection of datasets. https://github.com/deepmind/pg19 It is often used for benchmarking long-range language modeling.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 638968054,
    "title": "update cos_e to add cos_e v1.0",
    "dateCreated": "2020-06-15T16:03:22Z",
    "dateModified": "2020-06-15T16:03:22Z",
    "description": "This PR updates the cos_e dataset to add  v1.0 as requested here #163 \r\n@nazneenrajani",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 638307313,
    "title": "asd",
    "dateCreated": "2020-06-14T08:20:38Z",
    "dateModified": "2020-06-14T08:20:38Z",
    "description": "",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 638135754,
    "title": "Fix allocin\u00e9 dataset configuration",
    "dateCreated": "2020-06-13T10:12:10Z",
    "dateModified": "2020-06-13T10:12:10Z",
    "description": "This is a patch for #244. According to the [live nlp viewer](url), the Allocin\u00e9 dataset must be loaded with :\r\n```python\r\ndataset = load_dataset('allocine', 'allocine')\r\n```\r\nThis is redundant, as there is only one \"dataset configuration\", and should only be:\r\n```python\r\ndataset = load_dataset('allocine')\r\n```\r\n\r\nThis is my mistake, because the code for [`allocine.py`](https://github.com/huggingface/nlp/blob/master/datasets/allocine/allocine.py) was inspired by [`imdb.py`](https://github.com/huggingface/nlp/blob/master/datasets/imdb/imdb.py), which also force the user to specify the \"dataset configuration\" (even if there is only one).\r\n\r\nI believe this PR should solve this issue, making the Allocin\u00e9 dataset more convenient to use.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 638121617,
    "title": "c4 dataset is not viewable in nlpviewer demo",
    "dateCreated": "2020-06-13T08:26:16Z",
    "dateModified": "2020-06-13T08:26:16Z",
    "description": "I get the following error when I try to view the c4 dataset in [nlpviewer](https://huggingface.co/nlp/viewer/)\r\n\r\n```python\r\nModuleNotFoundError: No module named 'langdetect'\r\nTraceback:\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/streamlit/ScriptRunner.py\", line 322, in _run_script\r\n    exec(code, module.__dict__)\r\nFile \"/home/sasha/nlp_viewer/run.py\", line 54, in <module>\r\n    configs = get_confs(option.id)\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/streamlit/caching.py\", line 591, in wrapped_func\r\n    return get_or_create_cached_value()\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/streamlit/caching.py\", line 575, in get_or_create_cached_value\r\n    return_value = func(*args, **kwargs)\r\nFile \"/home/sasha/nlp_viewer/run.py\", line 48, in get_confs\r\n    builder_cls = nlp.load.import_main_class(module_path, dataset=True)\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/nlp/load.py\", line 57, in import_main_class\r\n    module = importlib.import_module(module_path)\r\nFile \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nFile \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\nFile \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\nFile \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\nFile \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\nFile \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\nFile \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/nlp/datasets/c4/88bb1b1435edad3fb772325710c4a43327cbf4a23b9030094556e6f01e14ec19/c4.py\", line 29, in <module>\r\n    from .c4_utils import (\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/nlp/datasets/c4/88bb1b1435edad3fb772325710c4a43327cbf4a23b9030094556e6f01e14ec19/c4_utils.py\", line 29, in <module>\r\n    import langdetect\r\n```",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 638106774,
    "title": "Error in metric.compute: missing `original_instructions` argument",
    "dateCreated": "2020-06-13T06:26:54Z",
    "dateModified": "2020-06-13T06:26:54Z",
    "description": "I'm running into an error using metrics for computation in the latest master as well as version 0.2.1. Here is a minimal example:\r\n\r\n```python\r\nimport nlp\r\nrte_metric = nlp.load_metric('glue', name=\"rte\")\r\nrte_metric.compute(\r\n    [0, 0, 1, 1],\r\n    [0, 1, 0, 1],\r\n)\r\n```\r\n\r\n```\r\n    181             # Read the predictions and references\r\n    182             reader = ArrowReader(path=self.data_dir, info=None)\r\n--> 183             self.data = reader.read_files(node_files)\r\n    184 \r\n    185             # Release all of our locks\r\n\r\nTypeError: read_files() missing 1 required positional argument: 'original_instructions'\r\n```\r\n\r\nI believe this might have been introduced with cc8d2508b75f7ba0e5438d0686ee02dcec43c7f4, which added the `original_instructions` argument. Elsewhere, an empty-string default is provided--perhaps that could be done here too?",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 637848056,
    "title": "add Rotten Tomatoes Movie Review sentences sentiment dataset",
    "dateCreated": "2020-06-12T15:53:59Z",
    "dateModified": "2020-06-12T15:53:59Z",
    "description": "Sentence-level movie reviews v1.0 from here: http://www.cs.cornell.edu/people/pabo/movie-review-data/",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 637415545,
    "title": "How can I load/find WMT en-romanian?",
    "dateCreated": "2020-06-12T01:09:37Z",
    "dateModified": "2020-06-12T01:09:37Z",
    "description": "I believe it is from `wmt16`\r\n\r\nWhen I run\r\n\r\n```python\r\nwmt = nlp.load_dataset('wmt16')\r\n```\r\nI get:\r\n```python\r\nAssertionError: The dataset wmt16 with config cs-en requires manual data. \r\n Please follow the manual download instructions:   Some of the wmt configs here, require a manual download.\r\n  Please look into wmt.py to see the exact path (and file name) that has to\r\n  be downloaded.\r\n  . \r\n Manual data can be loaded with `nlp.load(wmt16, data_dir='<path/to/manual/data>')\r\n```\r\nThere is no wmt.py,as the error message suggests, and wmt16.py doesn't have manual download instructions.\r\n\r\nAny idea how to do this?\r\n\r\nThanks in advance!\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 637156392,
    "title": "Add sort, shuffle, test_train_split and select methods",
    "dateCreated": "2020-06-11T16:22:20Z",
    "dateModified": "2020-06-11T16:22:20Z",
    "description": "Add a bunch of methods to reorder/split/select rows in a dataset:\r\n- `dataset.select(indices)`: Create a new dataset with rows selected following the list/array of indices (which can have a different size than the dataset and contain duplicated indices, the only constrain is that all the integers in the list must be smaller than the dataset size, otherwise we're indexing outside the dataset...)\r\n- `dataset.sort(column_name)`: sort a dataset according to a column (has to be a column with a numpy compatible type)\r\n- `dataset.shuffle(seed)`: shuffle a dataset rows\r\n- `dataset.train_test_split(test_size, train_size)`: Return a dictionary with two random train and test subsets (`train` and `test` ``Dataset`` splits)\r\n\r\nAll these methods are **not** in-place which means they return new ``Dataset``.\r\nThis is the default behavior in the library.\r\n\r\nFix #147 #166 #259 ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 637139220,
    "title": "Add pyarrow warning colab",
    "dateCreated": "2020-06-11T15:57:51Z",
    "dateModified": "2020-06-11T15:57:51Z",
    "description": "When a user installs `nlp` on google colab, then google colab doesn't update pyarrow, and the runtime needs to be restarted to use the updated version of pyarrow.\r\n\r\nThis is an issue because `nlp` requires the updated version to work correctly.\r\n\r\nIn this PR I added en error that is shown to the user in google colab if the user tries to `import nlp` without having restarted the runtime. The error tells the user to restart the runtime.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 637106170,
    "title": "Fix small issues creating dataset",
    "dateCreated": "2020-06-11T15:20:16Z",
    "dateModified": "2020-06-11T15:20:16Z",
    "description": "Fix many small issues mentioned in #249:\r\n- don't force to install apache beam for commands\r\n- fix None cache dir when using `dl_manager.download_custom`\r\n- added new extras in `setup.py` named `dev` that contains tests and quality dependencies\r\n- mock dataset sizes when running tests with dummy data\r\n- add a note about the naming convention of datasets (camel case - snake case) in CONTRIBUTING.md\r\n\r\nThis should help users create their datasets.\r\nNext step is the `add_dataset.md` docs :)",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 637028015,
    "title": "[Feature request] Support for external modality for language datasets",
    "dateCreated": "2020-06-11T13:42:18Z",
    "dateModified": "2020-06-11T13:42:18Z",
    "description": "# Background\r\n\r\nIn recent years many researchers have advocated that learning meanings from text-based only datasets is just like asking a human to \"learn to speak by listening to the radio\" [[E. Bender and A. Koller,2020](https://openreview.net/forum?id=GKTvAcb12b), [Y. Bisk et. al, 2020](https://arxiv.org/abs/2004.10151)]. Therefore, the importance of multi-modal datasets for the NLP community is of paramount importance for next-generation models. For this reason, I raised a [concern](https://github.com/huggingface/nlp/pull/236#issuecomment-639832029) related to the best way to integrate external features in NLP datasets (e.g., visual features associated with an image, audio features associated with a recording, etc.). This would be of great importance for a more systematic way of representing data for ML models that are learning from multi-modal data. \r\n\r\n# Language + Vision\r\n\r\n## Use case\r\nTypically, people working on Language+Vision tasks, have a reference dataset (either in JSON or JSONL format) and for each example, they have an identifier that specifies the reference image. For a practical example, you can refer to the [GQA](https://cs.stanford.edu/people/dorarad/gqa/download.html#seconddown) dataset.\r\n\r\nCurrently, images are represented by either pooling-based features (average pooling of ResNet or VGGNet features, see [DeVries et.al, 2017](https://arxiv.org/abs/1611.08481), [Shekhar et.al, 2019](https://www.aclweb.org/anthology/N19-1265.pdf)) where you have a single vector for every image. Another option is to use a set of feature maps for every image extracted from a specific layer of a CNN (see [Xu et.al, 2015](https://arxiv.org/abs/1502.03044)). A more recent option, especially with large-scale multi-modal transformers [Li et. al, 2019](https://arxiv.org/abs/1908.03557), is to use FastRCNN features. \r\n\r\nFor all these types of features, people use one of the following formats:\r\n1. [HD5F](https://pypi.org/project/h5py/)\r\n2. [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.savez.html)\r\n3. [LMDB](https://lmdb.readthedocs.io/en/release/)\r\n\r\n## Implementation considerations\r\n\r\nI was thinking about possible ways of implementing this feature. As mentioned above, depending on the model, different visual features can be used. This step usually relies on another model (say ResNet-101) that is used to generate the visual features for each image used in the dataset. Typically, this step is done in a separate script that completes the feature generation procedure. The usual processing steps for these datasets are the following:\r\n\r\n1. Download dataset\r\n2. Download images associated with the dataset\r\n3. Write a script that generates the visual features for every image and store them in a specific file\r\n4. Create a DataLoader that maps the visual features to the corresponding language example\r\n\r\nIn my personal projects, I've decided to ignore HD5F because it doesn't have out-of-the-box support for multi-processing (see this PyTorch [issue](https://github.com/pytorch/pytorch/issues/11929)). I've been successfully using a NumPy compressed file for each image so that I can store any sort of information in it.\r\n\r\nFor ease of use of all these Language+Vision datasets, it would be really handy to have a way to associate the visual features with the text and store them in an efficient way. That's why I immediately thought about the HuggingFace NLP backend based on Apache Arrow. The assumption here is that the external modality will be mapped to a N-dimensional tensor so easily represented by a NumPy array. \r\n\r\nLooking forward to hearing your thoughts about it!",
    "status": "open",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 636702849,
    "title": "Add new dataset ANLI Round 1",
    "dateCreated": "2020-06-11T04:14:57Z",
    "dateModified": "2020-06-11T04:14:57Z",
    "description": "Adding new dataset [ANLI](https://github.com/facebookresearch/anli/).\r\n\r\nI'm not familiar with how to add new dataset. Let me know if there is any issue. I only include round 1 data here. There will be round 2, round 3 and more in the future with potentially different format. I think it will be better to separate them.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 636372380,
    "title": "Downloading dataset error with pyarrow.lib.RecordBatch",
    "dateCreated": "2020-06-10T16:04:19Z",
    "dateModified": "2020-06-10T16:04:19Z",
    "description": "I am trying to download `sentiment140` and I have the following error\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    518         download_mode=download_mode,\r\n    519         ignore_verifications=ignore_verifications,\r\n--> 520         save_infos=save_infos,\r\n    521     )\r\n    522 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    418                 verify_infos = not save_infos and not ignore_verifications\r\n    419                 self._download_and_prepare(\r\n--> 420                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    421                 )\r\n    422                 # Sync info\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    472             try:\r\n    473                 # Prepare split will record examples associated to the split\r\n--> 474                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    475             except OSError:\r\n    476                 raise OSError(\"Cannot find data file. \" + (self.MANUAL_DOWNLOAD_INSTRUCTIONS or \"\"))\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _prepare_split(self, split_generator)\r\n    652         for key, record in utils.tqdm(generator, unit=\" examples\", total=split_info.num_examples, leave=False):\r\n    653             example = self.info.features.encode_example(record)\r\n--> 654             writer.write(example)\r\n    655         num_examples, num_bytes = writer.finalize()\r\n    656 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in write(self, example, writer_batch_size)\r\n    143             self._build_writer(pa_table=pa.Table.from_pydict(example))\r\n    144         if writer_batch_size is not None and len(self.current_rows) >= writer_batch_size:\r\n--> 145             self.write_on_file()\r\n    146 \r\n    147     def write_batch(\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in write_on_file(self)\r\n    127             else:\r\n    128                 # All good\r\n--> 129                 self._write_array_on_file(pa_array)\r\n    130             self.current_rows = []\r\n    131 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in _write_array_on_file(self, pa_array)\r\n     96     def _write_array_on_file(self, pa_array):\r\n     97         \"\"\"Write a PyArrow Array\"\"\"\r\n---> 98         pa_batch = pa.RecordBatch.from_struct_array(pa_array)\r\n     99         self._num_bytes += pa_array.nbytes\r\n    100         self.pa_writer.write_batch(pa_batch)\r\n\r\nAttributeError: type object 'pyarrow.lib.RecordBatch' has no attribute 'from_struct_array'\r\n```\r\n\r\nI installed the last version and ran the following command:\r\n\r\n```python\r\nimport nlp\r\nsentiment140 = nlp.load_dataset('sentiment140', cache_dir='/content')\r\n```",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 636261118,
    "title": "Consistency fixes",
    "dateCreated": "2020-06-10T13:44:42Z",
    "dateModified": "2020-06-10T13:44:42Z",
    "description": "A few bugs I've found while hacking",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 636239529,
    "title": "documentation missing how to split a dataset",
    "dateCreated": "2020-06-10T13:18:13Z",
    "dateModified": "2020-06-10T13:18:13Z",
    "description": "I am trying to understand how to split a dataset ( as arrow_dataset). \r\nI know I can do something like this to access a split which is already in the original dataset : \r\n\r\n`ds_test = nlp.load_dataset('imdb, split='test') `\r\n\r\nBut how can I split ds_test into a test and a validation set (without reading the data into memory and keeping the arrow_dataset as container)?\r\nI guess it has something to do with the module split :-) but there is no real documentation in the code but only a reference to a longer description: \r\n\r\n> See the  [guide on splits](https://github.com/huggingface/nlp/tree/master/docs/splits.md)  for more information.\r\n\r\nBut the guide seems to be missing.\r\n\r\nTo clarify: I know that this has been modelled after the dataset of tensorflow and that some of the documentation there can be used [like this one](https://www.tensorflow.org/datasets/splits). But to come back to the example above: I cannot simply split the testset doing this: \r\n`ds_test = nlp.load_dataset('imdb, split='test'[:5000]) `\r\n`ds_val = nlp.load_dataset('imdb, split='test'[5000:])`\r\n\r\nbecause the imdb test data is sorted by class (probably not a good idea anyway)\r\n",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 635859525,
    "title": "Why is dataset after tokenization far more larger than the orginal one ?",
    "dateCreated": "2020-06-10T01:27:07Z",
    "dateModified": "2020-06-10T01:27:07Z",
    "description": "I tokenize wiki dataset by `map` and cache the results.\r\n```\r\ndef tokenize_tfm(example):\r\n    example['input_ids'] = hf_fast_tokenizer.convert_tokens_to_ids(hf_fast_tokenizer.tokenize(example['text']))\r\n    return example\r\nwiki = nlp.load_dataset('wikipedia', '20200501.en', cache_dir=cache_dir)['train']\r\nwiki.map(tokenize_tfm, cache_file_name=cache_dir/\"wikipedia/20200501.en/1.0.0/tokenized_wiki.arrow\")\r\n```\r\nand when I see their size\r\n```\r\nls -l --block-size=M\r\n17460M  wikipedia-train.arrow\r\n47511M  tokenized_wiki.arrow\r\n```\r\nThe tokenized one is over 2x size of original one.\r\nIs there something I did wrong ?",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 635620979,
    "title": "Tokenizer pickling issue fix not landed in `nlp` yet?",
    "dateCreated": "2020-06-09T17:12:34Z",
    "dateModified": "2020-06-09T17:12:34Z",
    "description": "Unless I recreate an arrow_dataset from my loaded nlp dataset myself (which I think does not use the cache by default), I get the following error when applying the map function:\r\n\r\n```\r\ndataset = nlp.load_dataset('cos_e')\r\ntokenizer = GPT2TokenizerFast.from_pretrained('gpt2', cache_dir=cache_dir)\r\n\r\nfor split in dataset.keys():\r\n    dataset[split].map(lambda x: some_function(x, tokenizer))\r\n```\r\n```\r\n06/09/2020 10:09:19 - INFO - nlp.builder -   Constructing Dataset for split train[:10], from /home/sarahw/.cache/huggingface/datasets/cos_e/default/0.0.1\r\nTraceback (most recent call last):\r\n  File \"generation/input_to_label_and_rationale.py\", line 390, in <module>\r\n    main()\r\n  File \"generation/input_to_label_and_rationale.py\", line 263, in main\r\n    dataset[split] = dataset[split].map(lambda x: input_to_explanation_plus_label(x, tokenizer, max_length, datasource=data_args.task_name, wt5=(model_class=='t5'), expl_only=model_args.rationale_only), batched=False)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/nlp/arrow_dataset.py\", line 522, in map\r\n    cache_file_name = self._get_cache_file_path(function, cache_kwargs)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/nlp/arrow_dataset.py\", line 381, in _get_cache_file_path\r\n    function_bytes = dumps(function)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/nlp/utils/py_utils.py\", line 257, in dumps\r\n    dump(obj, file)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/nlp/utils/py_utils.py\", line 250, in dump\r\n    Pickler(file).dump(obj)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/dill/_dill.py\", line 445, in dump\r\n    StockPickler.dump(self, obj)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 485, in dump\r\n    self.save(obj)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 558, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/dill/_dill.py\", line 1410, in save_function\r\n    pickler.save_reduce(_create_function, (obj.__code__,\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 690, in save_reduce\r\n    save(args)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 558, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 899, in save_tuple\r\n    save(element)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 558, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 899, in save_tuple\r\n    save(element)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 558, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/dill/_dill.py\", line 1147, in save_cell\r\n    pickler.save_reduce(_create_cell, (f,), obj=obj)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 690, in save_reduce\r\n    save(args)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 558, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 884, in save_tuple\r\n    save(element)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 601, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 715, in save_reduce\r\n    save(state)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 558, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/dill/_dill.py\", line 912, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 969, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 995, in _batch_setitems\r\n    save(v)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 601, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 715, in save_reduce\r\n    save(state)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 558, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/dill/_dill.py\", line 912, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 969, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 995, in _batch_setitems\r\n    save(v)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py\", line 576, in save\r\n    rv = reduce(self.proto)\r\nTypeError: cannot pickle 'Tokenizer' object\r\n```\r\nFix seems to be in the tokenizers [`0.8.0.dev1 pre-release`](https://github.com/huggingface/tokenizers/issues/87), which I can't install with any package managers. ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 635596295,
    "title": "[Feature request] Add a feature to dataset",
    "dateCreated": "2020-06-09T16:38:12Z",
    "dateModified": "2020-06-09T16:38:12Z",
    "description": "Is there a straightforward way to add a field to the arrow_dataset, prior to performing map?",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 635300822,
    "title": "Add dataset/piaf",
    "dateCreated": "2020-06-09T10:16:01Z",
    "dateModified": "2020-06-09T10:16:01Z",
    "description": "Small SQuAD-like French QA dataset [PIAF](https://www.aclweb.org/anthology/2020.lrec-1.673.pdf)",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 635057568,
    "title": "[Feature request] Be able to remove a specific sample of the dataset",
    "dateCreated": "2020-06-09T02:22:13Z",
    "dateModified": "2020-06-09T02:22:13Z",
    "description": "As mentioned in #117, it's currently not possible to remove a sample of the dataset.\r\n\r\nBut it is a important use case : After applying some preprocessing, some samples might be empty for example. We should be able to remove these samples from the dataset, or at least mark them as `removed` so when iterating the dataset, we don't iterate these samples.\r\n\r\nI think it should be a feature. What do you think ?\r\n\r\n---\r\n\r\nAny work-around in the meantime ?",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 634791939,
    "title": "add flue dataset",
    "dateCreated": "2020-06-08T17:11:09Z",
    "dateModified": "2020-06-08T17:11:09Z",
    "description": "This PR add the Flue dataset as requested in this issue #223  . @lbourdois made  a detailed description in that issue.\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 634563239,
    "title": "NonMatchingSplitsSizesError error when reading the IMDB dataset",
    "dateCreated": "2020-06-08T12:26:24Z",
    "dateModified": "2020-06-08T12:26:24Z",
    "description": "Hi!\r\n\r\nI am trying to load the `imdb` dataset with this line:\r\n\r\n`dataset = nlp.load_dataset('imdb', data_dir='/A/PATH', cache_dir='/A/PATH')`\r\n\r\nbut I am getting the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/mounts/Users/cisintern/antmarakis/anaconda3/lib/python3.7/site-packages/nlp/load.py\", line 517, in load_dataset\r\n    save_infos=save_infos,\r\n  File \"/mounts/Users/cisintern/antmarakis/anaconda3/lib/python3.7/site-packages/nlp/builder.py\", line 363, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/mounts/Users/cisintern/antmarakis/anaconda3/lib/python3.7/site-packages/nlp/builder.py\", line 421, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File \"/mounts/Users/cisintern/antmarakis/anaconda3/lib/python3.7/site-packages/nlp/utils/info_utils.py\", line 70, in verify_splits\r\n    raise NonMatchingSplitsSizesError(str(bad_splits))\r\nnlp.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=33442202, num_examples=25000, dataset_name='imdb'), 'recorded': SplitInfo(name='train', num_bytes=5929447, num_examples=4537, dataset_name='imdb')}, {'expected': SplitInfo(name='unsupervised', num_bytes=67125548, num_examples=50000, dataset_name='imdb'), 'recorded': SplitInfo(name='unsupervised', num_bytes=0, num_examples=0, dataset_name='imdb')}]\r\n```\r\n\r\nAm I overlooking something? Thanks!",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 634544977,
    "title": "Better access to all dataset information",
    "dateCreated": "2020-06-08T11:56:50Z",
    "dateModified": "2020-06-08T11:56:50Z",
    "description": "Moves all the dataset info down one level from `dataset.info.XXX` to `dataset.XXX`\r\nThis way it's easier to access `dataset.feature['label']` for instance\r\n\r\nAlso, add the original split instructions used to create the dataset in `dataset.split`\r\nEx:\r\n```\r\nfrom nlp import load_dataset\r\nstsb = load_dataset('glue', name='stsb', split='train')\r\nstsb.split\r\n>>> NamedSplit('train')\r\n```",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 634416751,
    "title": "Remove checksum download in c4",
    "dateCreated": "2020-06-08T09:13:00Z",
    "dateModified": "2020-06-08T09:13:00Z",
    "description": "There was a line from the original tfds script that was still there and causing issues when loading the c4 script. This one should fix #233 and allow anyone to load the c4 script to generate the dataset",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 633393443,
    "title": "[Dataset created] some critical small issues when I was creating a dataset",
    "dateCreated": "2020-06-07T12:58:54Z",
    "dateModified": "2020-06-07T12:58:54Z",
    "description": "Hi, I successfully created a dataset and has made a pr #248.\r\nBut I have encountered several problems when I was creating it, and those should be easy to fix.\r\n\r\n1. Not found dataset_info.json\r\nshould be fixed by #241 , eager to wait it be merged.\r\n\r\n2. Forced to install `apach_beam`\r\nIf we should install it, then it might be better to include it in the pakcage dependency or specified in `CONTRIBUTING.md`\r\n```\r\nTraceback (most recent call last):\r\n  File \"nlp-cli\", line 10, in <module>\r\n    from nlp.commands.run_beam import RunBeamCommand\r\n  File \"/home/yisiang/nlp/src/nlp/commands/run_beam.py\", line 6, in <module>\r\n    import apache_beam as beam\r\nModuleNotFoundError: No module named 'apache_beam'\r\n```\r\n\r\n3.  `cached_dir` is `None`\r\n```\r\nFile \"/home/yisiang/nlp/src/nlp/datasets/bookscorpus/aea0bd5142d26df645a8fce23d6110bb95ecb81772bb2a1f29012e329191962c/bookscorpus.py\", line 88, in _split_generators\r\n    downloaded_path_or_paths = dl_manager.download_custom(_GDRIVE_FILE_ID, download_file_from_google_drive)\r\n  File \"/home/yisiang/nlp/src/nlp/utils/download_manager.py\", line 128, in download_custom\r\n    downloaded_path_or_paths = map_nested(url_to_downloaded_path, url_or_urls)\r\n  File \"/home/yisiang/nlp/src/nlp/utils/py_utils.py\", line 172, in map_nested\r\n    return function(data_struct)\r\n  File \"/home/yisiang/nlp/src/nlp/utils/download_manager.py\", line 126, in url_to_downloaded_path\r\n    return os.path.join(self._download_config.cache_dir, hash_url_to_filename(url))\r\n  File \"/home/yisiang/miniconda3/envs/nlppr/lib/python3.7/posixpath.py\", line 80, in join\r\n    a = os.fspath(a)\r\n```\r\nThis is because this line\r\nhttps://github.com/huggingface/nlp/blob/2e0a8639a79b1abc848cff5c669094d40bba0f63/src/nlp/commands/test.py#L30-L32\r\nAnd I add `--cache_dir=\"....\"` to `python nlp-cli test datasets/<your-dataset-folder> --save_infos --all_configs`  in the doc, finally I could pass this error.\r\nBut it seems to ignore my arg and use `/home/yisiang/.cache/huggingface/datasets/bookscorpus/plain_text/1.0.0` as cahe_dir\r\n\r\n4. There is no `pytest`\r\nSo maybe in the doc we should specify a step to install pytest\r\n\r\n5. Not enough capacity in my `/tmp`\r\nWhen run test for dummy data, I don't know why it ask me for 5.6g to download something, \r\n```\r\ndef download_and_prepare\r\n...\r\nif not utils.has_sufficient_disk_space(self.info.size_in_bytes or 0, directory=self._cache_dir_root):\r\n                raise IOError(\r\n                    \"Not enough disk space. Needed: {} (download: {}, generated: {})\".format(\r\n                        utils.size_str(self.info.size_in_bytes or 0),\r\n                        utils.size_str(self.info.download_size or 0),\r\n>                       utils.size_str(self.info.dataset_size or 0),\r\n                    )\r\n                )\r\nE               OSError: Not enough disk space. Needed: 5.62 GiB (download: 1.10 GiB, generated: 4.52 GiB)\r\n```\r\nI add a `processed_temp_dir=\"some/dir\"; raw_temp_dir=\"another/dir\"` to 71, and the test passed\r\nhttps://github.com/huggingface/nlp/blob/a67a6c422dece904b65d18af65f0e024e839dbe8/tests/test_dataset_common.py#L70-L72\r\n\r\nI suggest we can create tmp dir under the `/home/user/tmp` but not `/tmp`, because take our lab server for example, everyone use `/tmp` thus it has not much capacity. Or at least we can improve error message, so the user know is what directory has no space and how many has it lefted. Or we could do both.\r\n\r\n6. name of datasets\r\nI was surprised by the dataset name `books_corpus`, and didn't know it is from `class BooksCorpus(nlp.GeneratorBasedBuilder)` . I change it to `Bookscorpus` afterwards. I think this point shold be also on the doc.\r\n\r\n7. More thorough doc to how to create `dataset.py`\r\nI believe there will be.\r\n\r\n**Feel free to close this issue** if you think these are solved.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 633390427,
    "title": "add Toronto BooksCorpus",
    "dateCreated": "2020-06-07T12:54:56Z",
    "dateModified": "2020-06-07T12:54:56Z",
    "description": "1. I knew there is a branch `toronto_books_corpus`\r\n - After I downloaded it, I found it is all non-english, and only have one row. \r\n- It seems that it cites the wrong paper\r\n- according to papar using it, it is called `BooksCorpus` but not `TornotoBooksCorpus`\r\n\r\n2. It use a text mirror in google drive\r\n- `bookscorpus.py` include a function `download_file_from_google_drive` , maybe you will want to put it elsewhere.\r\n- text mirror is found in this [comment on the issue](https://github.com/soskek/bookcorpus/issues/24#issuecomment-556024973), and it said to have the same statistics as the one in the paper.\r\n- You may want to download it and put it on your gs in case of it disappears someday.\r\n\r\n3. Copyright ?\r\nThe paper has said\r\n\r\n> **The BookCorpus Dataset.** In order to train our sentence similarity model we collected a corpus of 11,038 books ***from the web***. These are __**free books written by yet unpublished authors**__. We only included books that had more than 20K words in order to filter out perhaps noisier shorter stories. The dataset has books in 16 different genres, e.g., Romance (2,865 books), Fantasy (1,479), Science fiction (786), Teen (430), etc. Table 2 highlights the summary statistics of our book corpus.\r\n\r\nand we have changed the form (not books), so I don't think it should have that problems. Or we can state that use it at your own risk or only for academic use. I know @thomwolf should know these things more.\r\n\r\nThis should solved #131 ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 632380078,
    "title": "Make all dataset downloads deterministic by applying `sorted` to glob and os.listdir",
    "dateCreated": "2020-06-06T11:02:10Z",
    "dateModified": "2020-06-06T11:02:10Z",
    "description": "This PR makes all datasets loading deterministic by applying `sorted()` to all `glob.glob` and `os.listdir` statements.\r\n\r\nAre there other \"non-deterministic\" functions apart from `glob.glob()` and `os.listdir()` that you can think of @thomwolf @lhoestq @mariamabarham @jplu ?\r\n\r\n**Important** \r\nIt does break backward compatibility for these datasets because\r\n1. When loading the complete dataset the order in which the examples are saved is different now\r\n2. When loading only part of a split, the examples themselves might be different.\r\n\r\n@patrickvonplaten - the nlp / longformer notebook has to be updated since the examples might now be different",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 632380054,
    "title": "What is the best way to cache a dataset? ",
    "dateCreated": "2020-06-06T11:02:07Z",
    "dateModified": "2020-06-06T11:02:07Z",
    "description": "For example if I want to use streamlit with a nlp dataset:\r\n\r\n```\r\n@st.cache\r\ndef load_data():\r\n    return nlp.load_dataset('squad')\r\n```\r\nThis code raises the error \"uncachable object\"\r\n\r\nRight now I just fixed with a constant for my specific case:\r\n```\r\n    @st.cache(hash_funcs={pyarrow.lib.Buffer: lambda b: 0})\r\n```\r\nBut I was curious to know what is the best way in general\r\n\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 631985108,
    "title": "SST-2 test labels are all -1",
    "dateCreated": "2020-06-05T21:41:42Z",
    "dateModified": "2020-06-05T21:41:42Z",
    "description": "I'm trying to test a model on the SST-2 task, but all the labels I see in the test set are -1.\r\n```\r\n>>> import nlp\r\n>>> glue = nlp.load_dataset('glue', 'sst2')\r\n>>> glue\r\n{'train': Dataset(schema: {'sentence': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 67349), 'validation': Dataset(schema: {'sentence': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 872), 'test': Dataset(schema: {'sentence': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 1821)}\r\n>>> list(l['label'] for l in glue['test'])\r\n[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\r\n```",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 631869155,
    "title": "Add Allocin\u00e9 Dataset",
    "dateCreated": "2020-06-05T19:19:26Z",
    "dateModified": "2020-06-05T19:19:26Z",
    "description": "This is a french binary sentiment classification dataset, which was used to train this model: https://huggingface.co/tblard/tf-allocine.\r\n\r\nBasically, it's a french \"IMDB\" dataset, with more reviews.\r\n\r\nMore info on [this repo](https://github.com/TheophileBlard/french-sentiment-analysis-with-bert). ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 631735848,
    "title": "Specify utf-8 encoding for GLUE",
    "dateCreated": "2020-06-05T16:33:00Z",
    "dateModified": "2020-06-05T16:33:00Z",
    "description": "#242 \r\nThis makes the GLUE-MNLI dataset readable on my machine, not sure if it's a Windows-only bug.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 631733683,
    "title": "UnicodeDecodeError when downloading GLUE-MNLI",
    "dateCreated": "2020-06-05T16:30:01Z",
    "dateModified": "2020-06-05T16:30:01Z",
    "description": "When I run\r\n```python\r\ndataset = nlp.load_dataset('glue', 'mnli')\r\n```\r\nI get an encoding error (could it be because I'm using Windows?) :\r\n```python\r\n# Lots of error log lines later...\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\std.py in __iter__(self)\r\n   1128         try:\r\n-> 1129             for obj in iterable:\r\n   1130                 yield obj\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\datasets\\glue\\5256cc2368cf84497abef1f1a5f66648522d5854b225162148cb8fc78a5a91cc\\glue.py in _generate_examples(self, data_file, split, mrpc_files)\r\n    529 \r\n--> 530                 for n, row in enumerate(reader):\r\n    531                     if is_cola_non_test:\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\csv.py in __next__(self)\r\n    110             self.fieldnames\r\n--> 111         row = next(self.reader)\r\n    112         self.line_num = self.reader.line_num\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\encodings\\cp1252.py in decode(self, input, final)\r\n     22     def decode(self, input, final=False):\r\n---> 23         return codecs.charmap_decode(input,self.errors,decoding_table)[0]\r\n     24 \r\n\r\nUnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 6744: character maps to <undefined>\r\n```\r\nAnyway this can be solved by specifying to decode in UTF when reading the csv file. I am proposing a PR if that's okay.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 631703079,
    "title": "Fix empty cache dir",
    "dateCreated": "2020-06-05T15:45:22Z",
    "dateModified": "2020-06-05T15:45:22Z",
    "description": "If the cache dir of a dataset is empty, the dataset fails to load and throws a FileNotFounfError. We could end up with empty cache dir because there was a line in the code that created the cache dir without using a temp dir. Using a temp dir is useful as it gets renamed to the real cache dir only if the full process is successful.\r\n\r\nSo I removed this bad line, and I also reordered things a bit to make sure that we always use a temp dir. I also added warning if we still end up with empty cache dirs in the future.\r\n\r\nThis should fix #239\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 631434677,
    "title": "Deterministic dataset loading",
    "dateCreated": "2020-06-05T09:03:26Z",
    "dateModified": "2020-06-05T09:03:26Z",
    "description": "When calling:\r\n```python \r\nimport nlp\r\ndataset = nlp.load_dataset(\"trivia_qa\", split=\"validation[:1%]\")\r\n```\r\n\r\nthe resulting dataset is not deterministic over different google colabs. \r\nAfter talking to @thomwolf, I suspect the reason to be the use of `glob.glob` in line:\r\n\r\nhttps://github.com/huggingface/nlp/blob/2e0a8639a79b1abc848cff5c669094d40bba0f63/datasets/trivia_qa/trivia_qa.py#L180\r\n\r\nwhich seems to return an ordering of files that depends on the filesystem:\r\nhttps://stackoverflow.com/questions/6773584/how-is-pythons-glob-glob-ordered\r\n\r\nI think we should go through all the dataset scripts and make sure to have deterministic behavior.\r\n\r\nA simple solution for `glob.glob()` would be to just replace it with `sorted(glob.glob())` to have everything sorted by name. \r\n\r\nWhat do you think @lhoestq?",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 631340440,
    "title": "[Creating new dataset] Not found dataset_info.json",
    "dateCreated": "2020-06-05T06:15:04Z",
    "dateModified": "2020-06-05T06:15:04Z",
    "description": "Hi, I am trying to create Toronto Book Corpus. #131 \r\n\r\nI ran\r\n`~/nlp % python nlp-cli test datasets/bookcorpus --save_infos --all_configs`\r\nbut this doesn't create `dataset_info.json` and try to use it\r\n```\r\nINFO:nlp.load:Checking datasets/bookcorpus/bookcorpus.py for additional imports.\r\nINFO:filelock:Lock 139795325778640 acquired on datasets/bookcorpus/bookcorpus.py.lock\r\nINFO:nlp.load:Found main folder for dataset datasets/bookcorpus/bookcorpus.py at /home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/datasets/bookcorpus\r\nINFO:nlp.load:Found specific version folder for dataset datasets/bookcorpus/bookcorpus.py at /home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/datasets/bookcorpus/8e84759446cf68d0b0deb3417e60cc331f30a3bbe58843de18a0f48e87d1efd9\r\nINFO:nlp.load:Found script file from datasets/bookcorpus/bookcorpus.py to /home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/datasets/bookcorpus/8e84759446cf68d0b0deb3417e60cc331f30a3bbe58843de18a0f48e87d1efd9/bookcorpus.py\r\nINFO:nlp.load:Couldn't find dataset infos file at datasets/bookcorpus/dataset_infos.json\r\nINFO:nlp.load:Found metadata file for dataset datasets/bookcorpus/bookcorpus.py at /home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/datasets/bookcorpus/8e84759446cf68d0b0deb3417e60cc331f30a3bbe58843de18a0f48e87d1efd9/bookcorpus.json\r\nINFO:filelock:Lock 139795325778640 released on datasets/bookcorpus/bookcorpus.py.lock\r\nINFO:nlp.builder:Overwrite dataset info from restored data version.\r\nINFO:nlp.info:Loading Dataset info from /home/yisiang/.cache/huggingface/datasets/book_corpus/plain_text/1.0.0\r\nTraceback (most recent call last):\r\n  File \"nlp-cli\", line 37, in <module>\r\n    service.run()\r\n  File \"/home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/commands/test.py\", line 78, in run\r\n    builders.append(builder_cls(name=config.name, data_dir=self._data_dir))\r\n  File \"/home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/builder.py\", line 610, in __init__\r\n    super(GeneratorBasedBuilder, self).__init__(*args, **kwargs)\r\n  File \"/home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/builder.py\", line 152, in __init__\r\n    self.info = DatasetInfo.from_directory(self._cache_dir)\r\n  File \"/home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/info.py\", line 157, in from_directory\r\n    with open(os.path.join(dataset_info_dir, DATASET_INFO_FILENAME), \"r\") as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/yisiang/.cache/huggingface/datasets/book_corpus/plain_text/1.0.0/dataset_info.json'\r\n```\r\nbtw, `ls /home/yisiang/.cache/huggingface/datasets/book_corpus/plain_text/1.0.0/` show me nothing is in the directory.\r\n\r\nI have also pushed the script to my fork [bookcorpus.py](https://github.com/richardyy1188/nlp/blob/bookcorpusdev/datasets/bookcorpus/bookcorpus.py).\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 631260143,
    "title": "[Metric] Bertscore : Warning : Empty candidate sentence; Setting recall to be 0.",
    "dateCreated": "2020-06-05T02:14:47Z",
    "dateModified": "2020-06-05T02:14:47Z",
    "description": "When running BERT-Score, I'm meeting this warning :\r\n\r\n> Warning: Empty candidate sentence; Setting recall to be 0.\r\n\r\nCode :\r\n\r\n```\r\nimport nlp\r\nmetric = nlp.load_metric(\"bertscore\")\r\nscores = metric.compute([\"swag\", \"swags\"], [\"swags\", \"totally something different\"], lang=\"en\", device=0)\r\n```\r\n\r\n---\r\n\r\n**What am I doing wrong / How can I hide this warning ?**",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 631199940,
    "title": "Can't download MultiNLI",
    "dateCreated": "2020-06-04T23:05:21Z",
    "dateModified": "2020-06-04T23:05:21Z",
    "description": "When I try to download MultiNLI with \r\n```python\r\ndataset = load_dataset('multi_nli')\r\n```\r\n\r\nI get this long error:\r\n```python\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-13-3b11f6be4cb9> in <module>\r\n      1 # Load a dataset and print the first examples in the training set\r\n      2 # nli_dataset = nlp.load_dataset('multi_nli')\r\n----> 3 dataset = load_dataset('multi_nli')\r\n      4 # nli_dataset = nlp.load_dataset('multi_nli', split='validation_matched[:10%]')\r\n      5 # print(nli_dataset['train'][0])\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    514 \r\n    515     # Download and prepare data\r\n--> 516     builder_instance.download_and_prepare(\r\n    517         download_config=download_config,\r\n    518         download_mode=download_mode,\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    417             with utils.temporary_assignment(self, \"_cache_dir\", tmp_data_dir):\r\n    418                 verify_infos = not save_infos and not ignore_verifications\r\n--> 419                 self._download_and_prepare(\r\n    420                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    421                 )\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    455         split_dict = SplitDict(dataset_name=self.name)\r\n    456         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 457         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    458         # Checksums verification\r\n    459         if verify_infos:\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\datasets\\multi_nli\\60774175381b9f3f1e6ae1028229e3cdb270d50379f45b9f2c01008f50f09e6b\\multi_nli.py in _split_generators(self, dl_manager)\r\n     99     def _split_generators(self, dl_manager):\r\n    100 \r\n--> 101         downloaded_dir = dl_manager.download_and_extract(\r\n    102             \"http://storage.googleapis.com/tfds-data/downloads/multi_nli/multinli_1.0.zip\"\r\n    103         )\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\utils\\download_manager.py in download_and_extract(self, url_or_urls)\r\n    214             extracted_path(s): `str`, extracted paths of given URL(s).\r\n    215         \"\"\"\r\n--> 216         return self.extract(self.download(url_or_urls))\r\n    217 \r\n    218     def get_recorded_sizes_checksums(self):\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\utils\\download_manager.py in extract(self, path_or_paths)\r\n    194                 path_or_paths.\r\n    195         \"\"\"\r\n--> 196         return map_nested(\r\n    197             lambda path: cached_path(path, extract_compressed_file=True, force_extract=False), path_or_paths,\r\n    198         )\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\utils\\py_utils.py in map_nested(function, data_struct, dict_only, map_tuple)\r\n    168                 return tuple(mapped)\r\n    169     # Singleton\r\n--> 170     return function(data_struct)\r\n    171 \r\n    172 \r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\utils\\download_manager.py in <lambda>(path)\r\n    195         \"\"\"\r\n    196         return map_nested(\r\n--> 197             lambda path: cached_path(path, extract_compressed_file=True, force_extract=False), path_or_paths,\r\n    198         )\r\n    199 \r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\utils\\file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    231             if is_zipfile(output_path):\r\n    232                 with ZipFile(output_path, \"r\") as zip_file:\r\n--> 233                     zip_file.extractall(output_path_extracted)\r\n    234                     zip_file.close()\r\n    235             elif tarfile.is_tarfile(output_path):\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\zipfile.py in extractall(self, path, members, pwd)\r\n   1644 \r\n   1645         for zipinfo in members:\r\n-> 1646             self._extract_member(zipinfo, path, pwd)\r\n   1647 \r\n   1648     @classmethod\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\zipfile.py in _extract_member(self, member, targetpath, pwd)\r\n   1698 \r\n   1699         with self.open(member, pwd=pwd) as source, \\\r\n-> 1700              open(targetpath, \"wb\") as target:\r\n   1701             shutil.copyfileobj(source, target)\r\n   1702 \r\n\r\nOSError: [Errno 22] Invalid argument: 'C:\\\\Users\\\\Python\\\\.cache\\\\huggingface\\\\datasets\\\\3e12413b8ec69f22dfcfd54a79d1ba9e7aac2e18e334bbb6b81cca64fd16bffc\\\\multinli_1.0\\\\Icon\\r'\r\n```\r\n",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 631099875,
    "title": "CompGuessWhat?! dataset ",
    "dateCreated": "2020-06-04T19:45:50Z",
    "dateModified": "2020-06-04T19:45:50Z",
    "description": "Hello,\r\n\r\nThanks for the amazing library that you put together. I'm Alessandro Suglia, the first author of CompGuessWhat?!, a recently released dataset for grounded language learning accepted to ACL 2020 ([https://compguesswhat.github.io](https://compguesswhat.github.io)).\r\n\r\nThis pull-request adds the CompGuessWhat?! splits that have been extracted from the original dataset. This is only part of our evaluation framework because there is also an additional split of the dataset that has a completely different set of games. I didn't integrate it yet because I didn't know what would be the best practice in this case. Let me clarify the scenario.\r\n\r\nIn our paper, we have a main dataset (let's call it `compguesswhat-gameplay`) and a zero-shot dataset (let's call it `compguesswhat-zs-gameplay`). In the current code of the pull-request, I have only integrated `compguesswhat-gameplay`. I was thinking that it would be nice to have the `compguesswhat-zs-gameplay` in the same dataset class by simply specifying some particular option to the `nlp.load_dataset()` factory. For instance:\r\n\r\n```python\r\n\r\ncgw = nlp.load_dataset(\"compguesswhat\")\r\ncgw_zs = nlp.load_dataset(\"compguesswhat\", zero_shot=True)\r\n```\r\n\r\nThe other option would be to have a separate dataset class. Any preferences?  ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 630952297,
    "title": "Add experimental datasets",
    "dateCreated": "2020-06-04T15:54:56Z",
    "dateModified": "2020-06-04T15:54:56Z",
    "description": "## Adding an *experimental datasets* folder\r\n\r\nAfter using the \ud83e\udd17nlp library for some time, I find that while it makes it super easy to create new memory-mapped datasets with lots of cool utilities, a lot of what I want to do doesn't work well with the current `MockDownloader` based testing paradigm, making it hard to share my work with the community.\r\n\r\nMy suggestion would be to add a **datasets\\_experimental** folder so we can start making these new datasets public without having to completely re-think testing for every single one. We would allow contributors to submit dataset PRs in this folder, but require an explanation for why the current testing suite doesn't work for them. We can then aggregate the feedback and periodically see what's missing from the current tests.\r\n\r\nI have added a **datasets\\_experimental** folder to the repository and S3 bucket with two initial datasets: ELI5 (explainlikeimfive) and a Wikipedia Snippets dataset to support indexing (wiki\\_snippets)\r\n\r\n### ELI5\r\n#### Dataset description\r\nThis allows people to download the [ELI5: Long Form Question Answering](https://arxiv.org/abs/1907.09190) dataset, along with two variants based on the r/askscience and r/AskHistorians. Full Reddit dumps for each month are downloaded from [pushshift](https://files.pushshift.io/reddit/), filtered for submissions and comments from the desired subreddits, then deleted one at a time to save space. The resulting dataset is split into a training, validation, and test dataset for r/explainlikeimfive, r/askscience, and r/AskHistorians respectively, where each item is a question along with all of its high scoring answers.\r\n\r\n#### Issues with the current testing\r\n1. the list of files to be downloaded is not pre-defined, but rather determined by parsing an index web page at run time. This is necessary as the name and compression type of the dump files changes from month to month as the pushshift website is maintained.  Currently, the dummy folder requires the user to know which files will be downloaded.\r\n2. to save time, the script works on the compressed files using the corresponding python packages rather than first running `download\\_and\\_extract` then filtering the extracted files.  \r\n\r\n### Wikipedia Snippets\r\n#### Dataset description\r\nThis script creates a *snippets* version of a source Wikipedia dataset: each article is split into passages of fixed length which can then be indexed using ElasticSearch or a dense indexer. The script currently handles all **wikipedia** and **wiki40b** source datasets, and allows the user to choose the passage length and how much overlap they want across passages. In addition to the passage text, each snippet also has the article title, list of titles of sections covered by the text, and information to map the passage back to the initial dataset at the paragraph and character level.\r\n\r\n#### Issues with the current testing\r\n1. The DatasetBuilder needs to call `nlp.load_dataset()`. Currently, testing is not recursive (the test doesn't know where to find the dummy data for the source dataset)\r\n",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 630534427,
    "title": "Huggingface NLP, Uploading custom dataset",
    "dateCreated": "2020-06-04T05:59:06Z",
    "dateModified": "2020-06-04T05:59:06Z",
    "description": "Hello,\r\n\r\nDoes anyone know how we can call our custom dataset using the nlp.load command? Let's say that I have a dataset based on the same format as that of squad-v1.1, how am I supposed to load it using huggingface nlp.\r\n\r\nThank you!",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 630432132,
    "title": "Fail to download c4 english corpus",
    "dateCreated": "2020-06-04T01:06:38Z",
    "dateModified": "2020-06-04T01:06:38Z",
    "description": "i run following code to download c4 English corpus.\r\n\r\n```\r\ndataset = nlp.load_dataset('c4', 'en', beam_runner='DirectRunner'\r\n, data_dir='/mypath')\r\n```\r\n\r\nand i met failure as follows\r\n\r\n```\r\nDownloading and preparing dataset c4/en (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/adam/.cache/huggingface/datasets/c4/en/2.3.0...\r\nTraceback (most recent call last):\r\n  File \"download_corpus.py\", line 38, in <module>\r\n    , data_dir='/home/adam/data/corpus/en/c4')\r\n  File \"/home/adam/anaconda3/envs/adam/lib/python3.7/site-packages/nlp/load.py\", line 520, in load_dataset\r\n    save_infos=save_infos,\r\n  File \"/home/adam/anaconda3/envs/adam/lib/python3.7/site-packages/nlp/builder.py\", line 420, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/adam/anaconda3/envs/adam/lib/python3.7/site-packages/nlp/builder.py\", line 816, in _download_and_prepare\r\n    dl_manager, verify_infos=False, pipeline=pipeline,\r\n  File \"/home/adam/anaconda3/envs/adam/lib/python3.7/site-packages/nlp/builder.py\", line 457, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/adam/anaconda3/envs/adam/lib/python3.7/site-packages/nlp/datasets/c4/f545de9f63300d8d02a6795e2eb34e140c47e62a803f572ac5599e170ee66ecc/c4.py\", line 175, in _split_generators\r\n    dl_manager.download_checksums(_CHECKSUMS_URL)\r\nAttributeError: 'DownloadManager' object has no attribute 'download_checksums\r\n\r\n```\r\ncan i get any advice?",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 630029568,
    "title": "Nlp cli fix endpoints",
    "dateCreated": "2020-06-03T14:10:39Z",
    "dateModified": "2020-06-03T14:10:39Z",
    "description": "With this PR users will be able to upload their own datasets and metrics.\r\n\r\nAs mentioned in #181, I had to use the new endpoints and revert the use of dataclasses (just in case we have changes in the API in the future).\r\n\r\nWe now distinguish commands for datasets and commands for metrics:\r\n```bash\r\nnlp-cli upload_dataset <path/to/dataset>\r\nnlp-cli upload_metric <path/to/metric>\r\nnlp-cli s3_datasets {rm, ls}\r\nnlp-cli s3_metrics {rm, ls}\r\n```\r\n\r\nDoes it sound good to you @julien-c @thomwolf ?",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 629988694,
    "title": "Add .download to MockDownloadManager",
    "dateCreated": "2020-06-03T13:20:00Z",
    "dateModified": "2020-06-03T13:20:00Z",
    "description": "One method from the DownloadManager was missing and some users couldn't run the tests because of that.\r\n@yjernite ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 629983684,
    "title": "Don't force to install apache beam for wikipedia dataset",
    "dateCreated": "2020-06-03T13:13:07Z",
    "dateModified": "2020-06-03T13:13:07Z",
    "description": "As pointed out in #227, we shouldn't force users to install apache beam if the processed dataset can be downloaded. I moved the imports of some datasets to avoid this problem",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 629956490,
    "title": "Rename dataset_infos.json to dataset_info.json",
    "dateCreated": "2020-06-03T12:31:44Z",
    "dateModified": "2020-06-03T12:31:44Z",
    "description": "As the file required for the viewing in the live nlp viewer is named as  dataset_info.json",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 629952402,
    "title": "Not able to access the XNLI dataset",
    "dateCreated": "2020-06-03T12:25:14Z",
    "dateModified": "2020-06-03T12:25:14Z",
    "description": "When I try to access the XNLI dataset, I get the following error. The option of plain_text get selected automatically and then I get the following error.\r\n\r\n```\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/sasha/.cache/huggingface/datasets/xnli/plain_text/1.0.0/dataset_info.json'\r\nTraceback:\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/streamlit/ScriptRunner.py\", line 322, in _run_script\r\n    exec(code, module.__dict__)\r\nFile \"/home/sasha/nlp_viewer/run.py\", line 86, in <module>\r\n    dts, fail = get(str(option.id), str(conf_option.name) if conf_option else None)\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/streamlit/caching.py\", line 591, in wrapped_func\r\n    return get_or_create_cached_value()\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/streamlit/caching.py\", line 575, in get_or_create_cached_value\r\n    return_value = func(*args, **kwargs)\r\nFile \"/home/sasha/nlp_viewer/run.py\", line 72, in get\r\n    builder_instance = builder_cls(name=conf)\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/nlp/builder.py\", line 610, in __init__\r\n    super(GeneratorBasedBuilder, self).__init__(*args, **kwargs)\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/nlp/builder.py\", line 152, in __init__\r\n    self.info = DatasetInfo.from_directory(self._cache_dir)\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/nlp/info.py\", line 157, in from_directory\r\n    with open(os.path.join(dataset_info_dir, DATASET_INFO_FILENAME), \"r\") as f:\r\n```\r\n\r\nIs it possible to see if the dataset_info.json is correctly placed?",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 629845704,
    "title": "Should we still have to force to install apache_beam to download wikipedia ?",
    "dateCreated": "2020-06-03T09:33:20Z",
    "dateModified": "2020-06-03T09:33:20Z",
    "description": "Hi, first thanks to @lhoestq 's revolutionary work, I successfully downloaded processed wikipedia according to the doc. \ud83d\ude0d\ud83d\ude0d\ud83d\ude0d\r\n\r\nBut at the first try, it tell me to install `apache_beam` and `mwparserfromhell`, which I thought wouldn't be used according to #204 , it was kind of confusing me at that time.\r\n\r\nMaybe we should not force users to install these ? Or we just add them to`nlp`'s dependency ?",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 628344520,
    "title": "add BlendedSkillTalk dataset",
    "dateCreated": "2020-06-01T10:54:45Z",
    "dateModified": "2020-06-01T10:54:45Z",
    "description": "This PR add the BlendedSkillTalk dataset, which is used to fine tune the blenderbot.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 628083366,
    "title": "[ROUGE] Different scores with `files2rouge`",
    "dateCreated": "2020-06-01T00:50:36Z",
    "dateModified": "2020-06-01T00:50:36Z",
    "description": "It seems that the ROUGE score of `nlp` is lower than the one of `files2rouge`.\r\n\r\nHere is a self-contained notebook to reproduce both scores : https://colab.research.google.com/drive/14EyAXValB6UzKY9x4rs_T3pyL7alpw_F?usp=sharing\r\n\r\n---\r\n\r\n`nlp` : (Only mid F-scores)\r\n\r\n>rouge1 0.33508031962733364\r\nrouge2 0.14574333776191592\r\nrougeL 0.2321187823256159\r\n\r\n`files2rouge` :\r\n\r\n>Running ROUGE...\r\n===========================\r\n1 ROUGE-1 Average_R: 0.48873 (95%-conf.int. 0.41192 - 0.56339)\r\n1 ROUGE-1 Average_P: 0.29010 (95%-conf.int. 0.23605 - 0.34445)\r\n1 ROUGE-1 Average_F: 0.34761 (95%-conf.int. 0.29479 - 0.39871)\r\n===========================\r\n1 ROUGE-2 Average_R: 0.20280 (95%-conf.int. 0.14969 - 0.26244)\r\n1 ROUGE-2 Average_P: 0.12772 (95%-conf.int. 0.08603 - 0.17752)\r\n1 ROUGE-2 Average_F: 0.14798 (95%-conf.int. 0.10517 - 0.19240)\r\n===========================\r\n1 ROUGE-L Average_R: 0.32960 (95%-conf.int. 0.26501 - 0.39676)\r\n1 ROUGE-L Average_P: 0.19880 (95%-conf.int. 0.15257 - 0.25136)\r\n1 ROUGE-L Average_F: 0.23619 (95%-conf.int. 0.19073 - 0.28663)\r\n\r\n---\r\n\r\nWhen using longer predictions/gold, the difference is bigger.  \r\n**How can I reproduce same score as `files2rouge` ?**\r\n\r\n@lhoestq \r\n",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 627791693,
    "title": "[Feature Request/Help] BLEURT model -> PyTorch",
    "dateCreated": "2020-05-30T18:30:40Z",
    "dateModified": "2020-05-30T18:30:40Z",
    "description": "Hi, I am interested in porting google research's new BLEURT learned metric to PyTorch (because I wish to do something experimental with language generation and backpropping through BLEURT). I noticed that you guys don't have it yet so I am partly just asking if you plan to add it (@thomwolf said you want to do so on Twitter).\r\n\r\nI had a go of just like manually using the checkpoint that they publish which includes the weights. It seems like the architecture is exactly aligned with the out-of-the-box BertModel in transformers just with a single linear layer on top of the CLS embedding. I loaded all the weights to the PyTorch model but I am not able to get the same numbers as the BLEURT package's python api. Here is my colab notebook where I tried  https://colab.research.google.com/drive/1Bfced531EvQP_CpFvxwxNl25Pj6ptylY?usp=sharing . If you have any pointers on what might be going wrong that would be much appreciated!\r\n\r\nThank you muchly!",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 627683386,
    "title": "[Feature request] Add FLUE dataset ",
    "dateCreated": "2020-05-30T08:52:15Z",
    "dateModified": "2020-05-30T08:52:15Z",
    "description": "Hi,\r\n\r\nI think it would be interesting to add the FLUE dataset for francophones or anyone wishing to work on French.\r\n\r\nIn other requests, I read that you are already working on some datasets, and I was wondering if FLUE was planned.\r\n\r\nIf it is not the case, I can provide each of the cleaned FLUE datasets (in the form of a directly exploitable dataset rather than in the original xml formats which require additional processing, with the French part for cases where the dataset is based on a multilingual dataframe, etc.).",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 627586690,
    "title": "Colab Notebook breaks when downloading the squad dataset",
    "dateCreated": "2020-05-29T22:55:59Z",
    "dateModified": "2020-05-29T22:55:59Z",
    "description": "When I run the notebook in Colab\r\nhttps://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb\r\nbreaks when running this cell:\r\n![image](https://user-images.githubusercontent.com/338917/83311709-ffd1b800-a1dd-11ea-8394-3a87df0d7f8b.png)\r\n",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 627300648,
    "title": "Fix tests/test_dataset_common.py",
    "dateCreated": "2020-05-29T14:12:15Z",
    "dateModified": "2020-05-29T14:12:15Z",
    "description": "When I run the command `RUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_real_dataset_arcd` while working on #220. I get the error ` unexpected keyword argument \"'download_and_prepare_kwargs'\"` at the level of  `load_dataset`. Indeed, this [function](https://github.com/huggingface/nlp/blob/master/src/nlp/load.py#L441) no longer has the argument `download_and_prepare_kwargs` but rather `download_config`. So here I change the tests accordingly. ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 627280683,
    "title": "dataset_arcd",
    "dateCreated": "2020-05-29T13:46:50Z",
    "dateModified": "2020-05-29T13:46:50Z",
    "description": "Added Arabic Reading Comprehension Dataset (ARCD): https://arxiv.org/abs/1906.05394",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 627235893,
    "title": "force mwparserfromhell as third party",
    "dateCreated": "2020-05-29T12:33:17Z",
    "dateModified": "2020-05-29T12:33:17Z",
    "description": "This should fix your env because you had `mwparserfromhell ` as a first party for `isort` @patrickvonplaten ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 627173407,
    "title": "Add Natual Questions and  C4 scripts",
    "dateCreated": "2020-05-29T10:40:30Z",
    "dateModified": "2020-05-29T10:40:30Z",
    "description": "Scripts are ready !\r\nHowever they are not processed nor directly available from gcp yet.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 627128403,
    "title": "Multi-task dataset mixing",
    "dateCreated": "2020-05-29T09:22:26Z",
    "dateModified": "2020-05-29T09:22:26Z",
    "description": "It seems like many of the best performing models on the GLUE benchmark make some use of multitask learning (simultaneous training on multiple tasks).\r\n\r\nThe [T5 paper](https://arxiv.org/pdf/1910.10683.pdf) highlights multiple ways of mixing the tasks together during finetuning:\r\n- **Examples-proportional mixing** - sample from tasks proportionally to their dataset size\r\n- **Equal mixing** - sample uniformly from each task\r\n- **Temperature-scaled mixing** - The generalized approach used by multilingual BERT which uses a temperature T, where the mixing rate of each task is raised to the power 1/T and renormalized. When T=1 this is equivalent to equal mixing, and becomes closer to equal mixing with increasing T.\r\n\r\nFollowing this discussion https://github.com/huggingface/transformers/issues/4340 in [transformers](https://github.com/huggingface/transformers), @enzoampil suggested that the `nlp` library might be a better place for this functionality.\r\n\r\nSome method for combining datasets could be implemented ,e.g.\r\n```\r\ndataset = nlp.load_multitask(['squad','imdb','cnn_dm'], temperature=2.0, ...)\r\n```\r\n\r\nWe would need a few additions:\r\n- Method of identifying the tasks - how can we support adding a string to each task as an identifier: e.g. 'summarisation: '?\r\n- Method of combining the metrics - a standard approach is to use the specific metric for each task and add them together for a combined score.\r\n\r\nIt would be great to support common use cases such as pretraining on the GLUE benchmark before fine-tuning on each GLUE task in turn. \r\n\r\nI'm willing to write bits/most of this I just need some guidance on the interface and other library details so I can integrate it properly.\r\n\r\n",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 626896890,
    "title": "\u2753 How to get ROUGE-2 with the ROUGE metric ?",
    "dateCreated": "2020-05-28T23:47:32Z",
    "dateModified": "2020-05-28T23:47:32Z",
    "description": "I'm trying to use ROUGE metric, but I don't know how to get the ROUGE-2 metric.\r\n\r\n---\r\n\r\nI compute scores with :\r\n\r\n```python\r\nimport nlp\r\n\r\nrouge = nlp.load_metric('rouge')\r\nwith open(\"pred.txt\") as p, open(\"ref.txt\") as g:\r\n    for lp, lg in zip(p, g):\r\n        rouge.add([lp], [lg])\r\nscore = rouge.compute()\r\n```\r\n\r\nthen : _(print only the F-score for readability)_\r\n\r\n```python\r\nfor k, s in score.items():\r\n    print(k, s.mid.fmeasure)\r\n```\r\n\r\nIt gives :\r\n\r\n>rouge1 0.7915168355671788\r\nrougeL 0.7915168355671788\r\n\r\n---\r\n\r\n**How can I get the ROUGE-2 score ?**\r\n\r\nAlso, it's seems weird that ROUGE-1 and ROUGE-L scores are the same. Did I made a mistake ?\r\n\r\n@lhoestq ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 626867879,
    "title": "NonMatchingSplitsSizesError when loading blog_authorship_corpus",
    "dateCreated": "2020-05-28T22:55:19Z",
    "dateModified": "2020-05-28T22:55:19Z",
    "description": "Getting this error when i run `nlp.load_dataset('blog_authorship_corpus')`. \r\n\r\n```\r\nraise NonMatchingSplitsSizesError(str(bad_splits))\r\nnlp.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', \r\nnum_bytes=610252351, num_examples=532812, dataset_name='blog_authorship_corpus'), \r\n'recorded': SplitInfo(name='train', num_bytes=616473500, num_examples=536323, \r\ndataset_name='blog_authorship_corpus')}, {'expected': SplitInfo(name='validation', \r\nnum_bytes=37500394, num_examples=31277, dataset_name='blog_authorship_corpus'), \r\n'recorded': SplitInfo(name='validation', num_bytes=30786661, num_examples=27766, \r\ndataset_name='blog_authorship_corpus')}]\r\n```\r\n\r\nUpon checking it seems like there is a disparity between the information in `datasets/blog_authorship_corpus/dataset_infos.json` and what was downloaded. Although I can get away with this by passing `ignore_verifications=True` in `load_dataset`, I'm thinking doing so might give problems later on.",
    "status": "open",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 626641549,
    "title": "[arrow_dataset.py] add new filter function",
    "dateCreated": "2020-05-28T16:21:40Z",
    "dateModified": "2020-05-28T16:21:40Z",
    "description": "The `.map()` function is super useful, but can IMO a bit tedious when filtering certain examples.\r\nI think, filtering out examples is also a very common operation people would like to perform on datasets.\r\n\r\nThis PR is a proposal to add a `.filter()` function in the same spirit than the `.map()` function.\r\n\r\nHere is a sample code you can play around with:\r\n\r\n```python\r\nds = nlp.load_dataset(\"squad\", split=\"validation[:10%]\")\r\n\r\n\r\ndef remove_under_idx_5(example, idx):\r\n    return idx < 5\r\n\r\n\r\ndef only_keep_examples_with_is_in_context(example):\r\n    return \"is\" in example[\"context\"]\r\n\r\n\r\nresult_keep_only_first_5 = ds.filter(remove_under_idx_5, with_indices=True, load_from_cache_file=False)\r\nresult_keep_examples_with_is_in_context = ds.filter(only_keep_examples_with_is_in_context, load_from_cache_file=False)\r\n\r\nprint(\"Original number of examples: {}\".format(len(ds)))\r\nprint(\"First five examples number of examples: {}\".format(len(result_keep_only_first_5)))\r\nprint(\"Is in context examples number of examples: {}\".format(len(result_keep_examples_with_is_in_context)))\r\n```",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 626587995,
    "title": "better message if missing beam options",
    "dateCreated": "2020-05-28T15:06:57Z",
    "dateModified": "2020-05-28T15:06:57Z",
    "description": "WDYT @yjernite ?\r\nFor example:\r\n```python\r\ndataset = nlp.load_dataset('wikipedia', '20200501.aa')\r\n```\r\nRaises:\r\n```\r\nMissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https://beam.apache.org/documentation/runners/capability-matrix/\r\nIf you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). \r\nExample of usage: \r\n\t`load_dataset('wikipedia', '20200501.aa', beam_runner='DirectRunner')`\r\n```",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 626580198,
    "title": "have 'add' and 'add_batch' for metrics",
    "dateCreated": "2020-05-28T14:56:47Z",
    "dateModified": "2020-05-28T14:56:47Z",
    "description": "This should fix #116 \r\n\r\nPreviously the `.add` method of metrics expected a batch of examples.\r\nNow `.add` expects one prediction/reference and `.add_batch` expects a batch.\r\nI think it is more coherent with the way the ArrowWriter works.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 626565994,
    "title": "[Arrow writer, Trivia_qa] Could not convert TagMe with type str: converting to null type",
    "dateCreated": "2020-05-28T14:38:14Z",
    "dateModified": "2020-05-28T14:38:14Z",
    "description": "Running the following code \r\n\r\n```\r\nimport nlp\r\nds = nlp.load_dataset(\"trivia_qa\", \"rc\", split=\"validation[:1%]\")  # this might take 2.3 min to download but it's cached afterwards...\r\nds.map(lambda x: x, load_from_cache_file=False)\r\n```\r\n\r\ntriggers a `ArrowInvalid: Could not convert TagMe with type str: converting to null type` error.\r\n\r\nOn the other hand if we remove a certain column of `trivia_qa` which seems responsible for the bug, it works:\r\n\r\n```\r\nimport nlp\r\nds = nlp.load_dataset(\"trivia_qa\", \"rc\", split=\"validation[:1%]\")  # this might take 2.3 min to download but it's cached afterwards...\r\nds.map(lambda x: x, remove_columns=[\"entity_pages\"], load_from_cache_file=False)\r\n```\r\n\r\n. Seems quite hard to debug what's going on here... @lhoestq @thomwolf - do you have a good first guess what the problem could be?\r\n\r\n**Note** BTW: I think this could be a good test to check that the datasets work correctly: Take a tiny portion of the dataset and check that it can be written correctly.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 626504243,
    "title": "fix xnli metric kwargs description",
    "dateCreated": "2020-05-28T13:21:44Z",
    "dateModified": "2020-05-28T13:21:44Z",
    "description": "The text was wrong as noticed in #202 ",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 626405849,
    "title": "Add a Google Drive exception for small files",
    "dateCreated": "2020-05-28T10:40:17Z",
    "dateModified": "2020-05-28T10:40:17Z",
    "description": "I tried to use the ``nlp`` library to load personnal datasets. I mainly copy-paste the code for ``multi-news`` dataset because my files are stored on Google Drive. \r\n\r\nOne of my dataset is small (< 25Mo) so it can be verified by Drive without asking the authorization to the user. This makes the download starts directly. \r\n\r\nCurrently the ``nlp`` raises a error: ``ConnectionError: Couldn't reach https://drive.google.com/uc?export=download&id=1DGnbUY9zwiThTdgUvVTSAvSVHoloCgun`` while the url is working. So I just add a new exception as you have already done for  ``firebasestorage.googleapis.com`` : \r\n\r\n```\r\nelif (response.status_code == 400 and \"firebasestorage.googleapis.com\" in url) or (response.status_code == 405 and \"drive.google.com\" in url)\r\n```\r\n\r\nI make an example of the error that you can run on [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ae_JJ9uvUt-9GBh0uGZhjbF5aXkl-BPv?usp=sharing)\r\n\r\nI avoid the error by adding an exception but there is maybe a proper way to do it.\r\n\r\nMany thanks :hugs:\r\nBest,",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 626398519,
    "title": "[Dummy data] insert config name instead of config ",
    "dateCreated": "2020-05-28T10:28:19Z",
    "dateModified": "2020-05-28T10:28:19Z",
    "description": "Thanks @yjernite for letting me know. in the dummy data command the config name shuold be passed to the dataset builder and not the config itself. \r\n\r\nAlso, @lhoestq fixed small import bug introduced by beam command I think.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 625932200,
    "title": "Remove test set from NLP viewer",
    "dateCreated": "2020-05-27T18:32:07Z",
    "dateModified": "2020-05-27T18:32:07Z",
    "description": "While the new [NLP viewer](https://huggingface.co/nlp/viewer/) is a great tool, I think it would be best to outright remove the option of looking at the test sets. At the very least, a warning should be displayed to users before showing the test set. Newcomers to the field might not be aware of best practices, and small things like this can help increase awareness.",
    "status": "open",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 625842989,
    "title": "[Question] Combine 2 datasets which have the same columns",
    "dateCreated": "2020-05-27T16:25:52Z",
    "dateModified": "2020-05-27T16:25:52Z",
    "description": "Hi,\r\n\r\nI am using ``nlp`` to load personal datasets. I created summarization datasets in multi-languages based on wikinews. I have one dataset for english and one for german (french is getting to be ready as well). I want to keep these datasets independent because they need different pre-processing (add different task-specific prefixes for T5 : *summarize:* for english and *zusammenfassen:* for german)\r\n\r\nMy issue is that I want to train T5 on the combined english and german datasets to see if it improves results. So I would like to combine 2 datasets (which have the same columns) to make one and train T5 on it. I was wondering if there is a proper way to do it? I assume that it can be done by combining all examples of each dataset but maybe you have a better solution.\r\n\r\nHoping this is clear enough,\r\n\r\nThanks a lot \ud83d\ude0a\r\nBest",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 625839335,
    "title": "Better arrow dataset iter",
    "dateCreated": "2020-05-27T16:20:21Z",
    "dateModified": "2020-05-27T16:20:21Z",
    "description": "I tried to play around with `tf.data.Dataset.from_generator` and I found out that the `__iter__` that we have for `nlp.arrow_dataset.Dataset` ignores the format that has been set (torch or tensorflow).\r\nWith these changes I should be able to come up with a `tf.data.Dataset` that uses lazy loading, as asked in #193.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 625655849,
    "title": "Add Dataflow support + Wikipedia + Wiki40b",
    "dateCreated": "2020-05-27T12:32:49Z",
    "dateModified": "2020-05-27T12:32:49Z",
    "description": "# Add Dataflow support + Wikipedia + Wiki40b\r\n\r\n## Support datasets processing with Apache Beam\r\n\r\nSome datasets are too big to be processed on a single machine, for example: wikipedia, wiki40b, etc. Apache Beam allows to process datasets on many execution engines like Dataflow, Spark, Flink, etc.\r\n\r\nTo process such datasets with Beam, I added a command to run beam pipelines `nlp-cli run_beam path/to/dataset/script`. Then I used it to process the english + french wikipedia, and the english of wiki40b.\r\nThe processed arrow files are on GCS and are the result of a Dataflow job.\r\n\r\nI added a markdown documentation file in `docs` that explains how to use it properly.\r\n\r\n## Load already processed datasets\r\n\r\nNow that we have those datasets already processed, I made it possible to load datasets that are already processed. You can do `load_dataset('wikipedia', '20200501.en')` and it will download the processed files from the Hugging Face GCS directly into the user's cache and be ready to use !\r\n\r\nThe Wikipedia dataset was already asked in #187 and this PR should soon allow to add Natural Questions as asked in #129 \r\n\r\n## Other changes in the code\r\n\r\nTo make things work, I had to do a few adjustments:\r\n- add a `ship_files_with_pipeline` method to the `DownloadManager`. This is because beam pipelines can be run in the cloud and therefore need to have access to your downloaded data. I used it in the wikipedia script:\r\n    ```python\r\n    if not pipeline.is_local():\r\n            downloaded_files = dl_manager.ship_files_with_pipeline(downloaded_files, pipeline)\r\n    ```\r\n- add parquet to arrow conversion. This is because the output of beam pipelines are parquet files so we need to convert them to arrow and have the arrow files on GCS\r\n- add a test script with a dummy beam dataset\r\n- minor adjustments to allow read/write operations on remote files using `apache_beam.io.filesystems.FileSystems` if we want (it can be connected to gcp, s3, hdfs, etc...)",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 625515488,
    "title": "Raise an error if no config name for datasets like glue",
    "dateCreated": "2020-05-27T09:03:58Z",
    "dateModified": "2020-05-27T09:03:58Z",
    "description": "Some datasets like glue (see #130) and scientific_papers (see #197) have many configs.\r\nFor example for glue there are cola, sst2, mrpc etc.\r\n\r\nCurrently if a user does `load_dataset('glue')`, then Cola is loaded by default and it can be confusing. Instead, we should raise an error to let the user know that he has to pick one of the available configs (as proposed in #152). For example for glue, the message looks like:\r\n```\r\nValueError: Config name is missing.\r\nPlease pick one among the available configs: ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'mnli_mismatched', 'mnli_matched', 'qnli', 'rte', 'wnli', 'ax']\r\nExample of usage:\r\n\t`load_dataset('glue', 'cola')`\r\n```\r\n\r\nThe error is raised if the config name is missing and if there are >=2 possible configs.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 625493983,
    "title": "Mistaken `_KWARGS_DESCRIPTION` for XNLI metric",
    "dateCreated": "2020-05-27T08:34:42Z",
    "dateModified": "2020-05-27T08:34:42Z",
    "description": "Hi!\r\n\r\nThe [`_KWARGS_DESCRIPTION`](https://github.com/huggingface/nlp/blob/7d0fa58641f3f462fb2861dcdd6ce7f0da3f6a56/metrics/xnli/xnli.py#L45) for the XNLI metric uses `Args` and `Returns` text from [BLEU](https://github.com/huggingface/nlp/blob/7d0fa58641f3f462fb2861dcdd6ce7f0da3f6a56/metrics/bleu/bleu.py#L58) metric:\r\n\r\n```\r\n_KWARGS_DESCRIPTION = \"\"\"\r\nComputes XNLI score which is just simple accuracy.\r\nArgs:\r\n    predictions: list of translations to score.\r\n        Each translation should be tokenized into a list of tokens.\r\n    references: list of lists of references for each translation.\r\n        Each reference should be tokenized into a list of tokens.\r\n    max_order: Maximum n-gram order to use when computing BLEU score.\r\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\r\nReturns:\r\n    'bleu': bleu score,\r\n    'precisions': geometric mean of n-gram precisions,\r\n    'brevity_penalty': brevity penalty,\r\n    'length_ratio': ratio of lengths,\r\n    'translation_length': translation_length,\r\n    'reference_length': reference_length\r\n\"\"\"\r\n```\r\n\r\nBut it should be something like:\r\n\r\n```\r\n_KWARGS_DESCRIPTION = \"\"\"\r\nComputes XNLI score which is just simple accuracy.\r\nArgs:\r\n    predictions: Predicted labels.\r\n    references: Ground truth labels.\r\nReturns:\r\n    'accuracy': accuracy\r\n```",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 625235430,
    "title": "Fix typo in README",
    "dateCreated": "2020-05-26T22:18:21Z",
    "dateModified": "2020-05-26T22:18:21Z",
    "description": "",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 625226638,
    "title": "[ArrowWriter] Set schema at first write example",
    "dateCreated": "2020-05-26T21:59:48Z",
    "dateModified": "2020-05-26T21:59:48Z",
    "description": "Right now if the schema was not specified when instantiating `ArrowWriter`, then it could be set with the first `write_table` for example (it calls `self._build_writer()` to do so).\r\n\r\nI noticed that it was not done if the first example is added via `.write`, so I added it for coherence.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 625217440,
    "title": "Fix GermEval 2014 dataset infos",
    "dateCreated": "2020-05-26T21:41:44Z",
    "dateModified": "2020-05-26T21:41:44Z",
    "description": "Hi,\r\n\r\nthis PR just removes the `dataset_info.json` file and adds a newly generated `dataset_infos.json` file.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 625200627,
    "title": "Index outside of table length",
    "dateCreated": "2020-05-26T21:09:40Z",
    "dateModified": "2020-05-26T21:09:40Z",
    "description": "The offset input box warns of numbers larger than a limit (like 2000) but then the errors start at a smaller value than that limit (like 1955).\r\n\r\n> ValueError: Index (2000) outside of table length (2000).\r\n> Traceback:\r\n> File \"/home/sasha/.local/lib/python3.7/site-packages/streamlit/ScriptRunner.py\", line 322, in _run_script\r\n>     exec(code, module.__dict__)\r\n> File \"/home/sasha/nlp_viewer/run.py\", line 116, in <module>\r\n>     v = d[item][k]\r\n> File \"/home/sasha/.local/lib/python3.7/site-packages/nlp/arrow_dataset.py\", line 338, in __getitem__\r\n>     output_all_columns=self._output_all_columns,\r\n> File \"/home/sasha/.local/lib/python3.7/site-packages/nlp/arrow_dataset.py\", line 290, in _getitem\r\n>     raise ValueError(f\"Index ({key}) outside of table length ({self._data.num_rows}).\")",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 624966904,
    "title": "Scientific Papers only downloading Pubmed",
    "dateCreated": "2020-05-26T15:18:47Z",
    "dateModified": "2020-05-26T15:18:47Z",
    "description": "Hi!\r\n\r\nI have been playing around with this module, and I am a bit confused about the `scientific_papers` dataset. I thought that it would download two separate datasets, arxiv and pubmed. But when I run the following:\r\n\r\n```\r\ndataset = nlp.load_dataset('scientific_papers', data_dir='.', cache_dir='.')\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.05k/5.05k [00:00<00:00, 2.66MB/s]\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.90k/4.90k [00:00<00:00, 2.42MB/s]\r\nDownloading and preparing dataset scientific_papers/pubmed (download: 4.20 GiB, generated: 2.33 GiB, total: 6.53 GiB) to ./scientific_papers/pubmed/1.1.1...\r\nDownloading: 3.62GB [00:40, 90.5MB/s]\r\nDownloading: 880MB [00:08, 101MB/s]\r\nDataset scientific_papers downloaded and prepared to ./scientific_papers/pubmed/1.1.1. Subsequent calls will reuse this data.\r\n```\r\n\r\nonly a pubmed folder is created. There doesn't seem to be something for arxiv. Are these two datasets merged? Or have I misunderstood something?\r\n\r\nThanks!",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 624901266,
    "title": "Check invalid config name",
    "dateCreated": "2020-05-26T13:52:51Z",
    "dateModified": "2020-05-26T13:52:51Z",
    "description": "As said in #194, we should raise an error if the config name has bad characters.\r\nBad characters are those that are not allowed for directory names on windows.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 624858686,
    "title": "[Dummy data command] add new case to command",
    "dateCreated": "2020-05-26T12:50:47Z",
    "dateModified": "2020-05-26T12:50:47Z",
    "description": "Qanta: #194 introduces a case that was not noticed before. This change in code helps community users to have an easier time creating the dummy data. ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 624854897,
    "title": "Add Dataset: Qanta",
    "dateCreated": "2020-05-26T12:44:35Z",
    "dateModified": "2020-05-26T12:44:35Z",
    "description": "Fixes dummy data for #169 @EntilZha",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 624655558,
    "title": "[Tensorflow] Use something else than `from_tensor_slices()`",
    "dateCreated": "2020-05-26T07:19:14Z",
    "dateModified": "2020-05-26T07:19:14Z",
    "description": "In the example notebook, the TF Dataset is built using `from_tensor_slices()` :\r\n\r\n```python\r\ncolumns = ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\r\ntrain_tf_dataset.set_format(type='tensorflow', columns=columns)\r\nfeatures = {x: train_tf_dataset[x] for x in columns[:3]} \r\nlabels = {\"output_1\": train_tf_dataset[\"start_positions\"]}\r\nlabels[\"output_2\"] = train_tf_dataset[\"end_positions\"]\r\ntfdataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)\r\n```\r\n\r\nBut according to [official tensorflow documentation](https://www.tensorflow.org/guide/data#consuming_numpy_arrays), this will load the entire dataset to memory.\r\n\r\n**This defeats one purpose of this library, which is lazy loading.**\r\n\r\nIs there any other way to load the `nlp` dataset into TF dataset lazily ?\r\n\r\n---\r\n\r\nFor example, is it possible to use [Arrow dataset](https://www.tensorflow.org/io/api_docs/python/tfio/arrow/ArrowDataset) ? If yes, is there any code example ?",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 624397592,
    "title": "[Question] Create Apache Arrow dataset from raw text file",
    "dateCreated": "2020-05-25T16:42:47Z",
    "dateModified": "2020-05-25T16:42:47Z",
    "description": "Hi guys, I have gathered and preprocessed about 2GB of COVID papers from CORD dataset @ Kggle. I have seen you have a text dataset as \"Crime and punishment\" in Apache arrow format. Do you have any script to do it from a raw txt file (preprocessed as for BERT like) or any guide?\r\nIs the worth of send it to you and add it to the NLP library?\r\nThanks, Manu\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 624394936,
    "title": "[Squad es] add dataset_infos",
    "dateCreated": "2020-05-25T16:35:52Z",
    "dateModified": "2020-05-25T16:35:52Z",
    "description": "@mariamabarham - was still about to upload this. Should have waited with my comment a bit more :D ",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 624124600,
    "title": "add squad Spanish v1 and v2",
    "dateCreated": "2020-05-25T08:08:40Z",
    "dateModified": "2020-05-25T08:08:40Z",
    "description": "This PR add the Spanish Squad versions 1 and 2 datasets. \r\nFixes #164 ",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 624048881,
    "title": "[Question] BERT-style multiple choice formatting",
    "dateCreated": "2020-05-25T05:11:05Z",
    "dateModified": "2020-05-25T05:11:05Z",
    "description": "Hello, I am wondering what the equivalent formatting of a dataset should be to allow for multiple-choice answering prediction, BERT-style. Previously, this was done by passing a list of `InputFeatures` to the dataloader instead of a list of `InputFeature`, where `InputFeatures` contained lists of length equal to the number of answer choices in the MCQ instead of single items. I'm a bit confused on what the output of my feature conversion function should be when using `dataset.map()` to ensure similar behavior.\r\n\r\nThanks!",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 623890430,
    "title": "When will the remaining math_dataset modules be added as dataset objects",
    "dateCreated": "2020-05-24T15:46:52Z",
    "dateModified": "2020-05-24T15:46:52Z",
    "description": "Currently only the algebra_linear_1d is supported. Is there a timeline for making the other modules supported. If no timeline is established, how can I help?",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 623627800,
    "title": "[Question] How to load wikipedia ? Beam runner ?",
    "dateCreated": "2020-05-23T10:18:52Z",
    "dateModified": "2020-05-23T10:18:52Z",
    "description": "When `nlp.load_dataset('wikipedia')`, I got\r\n* `WARNING:nlp.builder:Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided. Please pass a nlp.DownloadConfig(beam_runner=...) object to the builder.download_and_prepare(download_config=...) method. Default values will be used.`\r\n* `AttributeError: 'NoneType' object has no attribute 'size'`\r\n\r\nCould somebody tell me what should I do ? \r\n\r\n# Env\r\nOn Colab,\r\n```\r\ngit clone https://github.com/huggingface/nlp\r\ncd nlp\r\npip install -q .\r\n```\r\n```\r\n%pip install -q apache_beam mwparserfromhell\r\n-> ERROR: pydrive 1.3.1 has requirement oauth2client>=4.0.0, but you'll have oauth2client 3.0.0 which is incompatible.\r\nERROR: google-api-python-client 1.7.12 has requirement httplib2<1dev,>=0.17.0, but you'll have httplib2 0.12.0 which is incompatible.\r\nERROR: chainer 6.5.0 has requirement typing-extensions<=3.6.6, but you'll have typing-extensions 3.7.4.2 which is incompatible.\r\n```\r\n```\r\npip install -q apache-beam[interactive]\r\nERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 5.10.0 which is incompatible.\r\n```\r\n\r\n# The whole message\r\n```\r\nWARNING:nlp.builder:Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided. Please pass a nlp.DownloadConfig(beam_runner=...) object to the builder.download_and_prepare(download_config=...) method. Default values will be used.\r\n\r\nDownloading and preparing dataset wikipedia/20200501.aa (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/wikipedia/20200501.aa/1.0.0...\r\n\r\n---------------------------------------------------------------------------\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.process()\r\n\r\n44 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.PerWindowInvoker.invoke_process()\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window()\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/io/iobase.py in process(self, element, init_result)\r\n   1081       writer.write(e)\r\n-> 1082     return [window.TimestampedValue(writer.close(), timestamp.MAX_TIMESTAMP)]\r\n   1083 \r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/io/filebasedsink.py in close(self)\r\n    422   def close(self):\r\n--> 423     self.sink.close(self.temp_handle)\r\n    424     return self.temp_shard_path\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/io/parquetio.py in close(self, writer)\r\n    537     if len(self._buffer[0]) > 0:\r\n--> 538       self._flush_buffer()\r\n    539     if self._record_batches_byte_size > 0:\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/io/parquetio.py in _flush_buffer(self)\r\n    569       for b in x.buffers():\r\n--> 570         size = size + b.size\r\n    571     self._record_batches_byte_size = self._record_batches_byte_size + size\r\n\r\nAttributeError: 'NoneType' object has no attribute 'size'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n<ipython-input-9-340aabccefff> in <module>()\r\n----> 1 dset = nlp.load_dataset('wikipedia')\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    518         download_mode=download_mode,\r\n    519         ignore_verifications=ignore_verifications,\r\n--> 520         save_infos=save_infos,\r\n    521     )\r\n    522 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, dl_manager, **download_and_prepare_kwargs)\r\n    370                 verify_infos = not save_infos and not ignore_verifications\r\n    371                 self._download_and_prepare(\r\n--> 372                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    373                 )\r\n    374                 # Sync info\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos)\r\n    770         with beam.Pipeline(runner=beam_runner, options=beam_options,) as pipeline:\r\n    771             super(BeamBasedBuilder, self)._download_and_prepare(\r\n--> 772                 dl_manager, pipeline=pipeline, verify_infos=False\r\n    773             )  # TODO{beam} verify infos\r\n    774 \r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/pipeline.py in __exit__(self, exc_type, exc_val, exc_tb)\r\n    501   def __exit__(self, exc_type, exc_val, exc_tb):\r\n    502     if not exc_type:\r\n--> 503       self.run().wait_until_finish()\r\n    504 \r\n    505   def visit(self, visitor):\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/pipeline.py in run(self, test_runner_api)\r\n    481       return Pipeline.from_runner_api(\r\n    482           self.to_runner_api(use_fake_coders=True), self.runner,\r\n--> 483           self._options).run(False)\r\n    484 \r\n    485     if self._options.view_as(TypeOptions).runtime_type_check:\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/pipeline.py in run(self, test_runner_api)\r\n    494       finally:\r\n    495         shutil.rmtree(tmpdir)\r\n--> 496     return self.runner.run_pipeline(self, self._options)\r\n    497 \r\n    498   def __enter__(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/direct/direct_runner.py in run_pipeline(self, pipeline, options)\r\n    128       runner = BundleBasedDirectRunner()\r\n    129 \r\n--> 130     return runner.run_pipeline(pipeline, options)\r\n    131 \r\n    132 \r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in run_pipeline(self, pipeline, options)\r\n    553 \r\n    554     self._latest_run_result = self.run_via_runner_api(\r\n--> 555         pipeline.to_runner_api(default_environment=self._default_environment))\r\n    556     return self._latest_run_result\r\n    557 \r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in run_via_runner_api(self, pipeline_proto)\r\n    563     # TODO(pabloem, BEAM-7514): Create a watermark manager (that has access to\r\n    564     #   the teststream (if any), and all the stages).\r\n--> 565     return self.run_stages(stage_context, stages)\r\n    566 \r\n    567   @contextlib.contextmanager\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in run_stages(self, stage_context, stages)\r\n    704               stage,\r\n    705               pcoll_buffers,\r\n--> 706               stage_context.safe_coders)\r\n    707           metrics_by_stage[stage.name] = stage_results.process_bundle.metrics\r\n    708           monitoring_infos_by_stage[stage.name] = (\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in _run_stage(self, worker_handler_factory, pipeline_components, stage, pcoll_buffers, safe_coders)\r\n   1071         cache_token_generator=cache_token_generator)\r\n   1072 \r\n-> 1073     result, splits = bundle_manager.process_bundle(data_input, data_output)\r\n   1074 \r\n   1075     def input_for(transform_id, input_id):\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in process_bundle(self, inputs, expected_outputs)\r\n   2332 \r\n   2333     with UnboundedThreadPoolExecutor() as executor:\r\n-> 2334       for result, split_result in executor.map(execute, part_inputs):\r\n   2335 \r\n   2336         split_result_list += split_result\r\n\r\n/usr/lib/python3.6/concurrent/futures/_base.py in result_iterator()\r\n    584                     # Careful not to keep a reference to the popped future\r\n    585                     if timeout is None:\r\n--> 586                         yield fs.pop().result()\r\n    587                     else:\r\n    588                         yield fs.pop().result(end_time - time.monotonic())\r\n\r\n/usr/lib/python3.6/concurrent/futures/_base.py in result(self, timeout)\r\n    430                 raise CancelledError()\r\n    431             elif self._state == FINISHED:\r\n--> 432                 return self.__get_result()\r\n    433             else:\r\n    434                 raise TimeoutError()\r\n\r\n/usr/lib/python3.6/concurrent/futures/_base.py in __get_result(self)\r\n    382     def __get_result(self):\r\n    383         if self._exception:\r\n--> 384             raise self._exception\r\n    385         else:\r\n    386             return self._result\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/utils/thread_pool_executor.py in run(self)\r\n     42       # If the future wasn't cancelled, then attempt to execute it.\r\n     43       try:\r\n---> 44         self._future.set_result(self._fn(*self._fn_args, **self._fn_kwargs))\r\n     45       except BaseException as exc:\r\n     46         # Even though Python 2 futures library has #set_exection(),\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in execute(part_map)\r\n   2329           self._registered,\r\n   2330           cache_token_generator=self._cache_token_generator)\r\n-> 2331       return bundle_manager.process_bundle(part_map, expected_outputs)\r\n   2332 \r\n   2333     with UnboundedThreadPoolExecutor() as executor:\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in process_bundle(self, inputs, expected_outputs)\r\n   2243             process_bundle_descriptor_id=self._bundle_descriptor.id,\r\n   2244             cache_tokens=[next(self._cache_token_generator)]))\r\n-> 2245     result_future = self._worker_handler.control_conn.push(process_bundle_req)\r\n   2246 \r\n   2247     split_results = []  # type: List[beam_fn_api_pb2.ProcessBundleSplitResponse]\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in push(self, request)\r\n   1557       self._uid_counter += 1\r\n   1558       request.instruction_id = 'control_%s' % self._uid_counter\r\n-> 1559     response = self.worker.do_instruction(request)\r\n   1560     return ControlFuture(request.instruction_id, response)\r\n   1561 \r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/sdk_worker.py in do_instruction(self, request)\r\n    413       # E.g. if register is set, this will call self.register(request.register))\r\n    414       return getattr(self, request_type)(\r\n--> 415           getattr(request, request_type), request.instruction_id)\r\n    416     else:\r\n    417       raise NotImplementedError\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/sdk_worker.py in process_bundle(self, request, instruction_id)\r\n    448         with self.maybe_profile(instruction_id):\r\n    449           delayed_applications, requests_finalization = (\r\n--> 450               bundle_processor.process_bundle(instruction_id))\r\n    451           monitoring_infos = bundle_processor.monitoring_infos()\r\n    452           monitoring_infos.extend(self.state_cache_metrics_fn())\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/bundle_processor.py in process_bundle(self, instruction_id)\r\n    837         for data in data_channel.input_elements(instruction_id,\r\n    838                                                 expected_transforms):\r\n--> 839           input_op_by_transform_id[data.transform_id].process_encoded(data.data)\r\n    840 \r\n    841       # Finish all operations.\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/bundle_processor.py in process_encoded(self, encoded_windowed_values)\r\n    214       decoded_value = self.windowed_coder_impl.decode_from_stream(\r\n    215           input_stream, True)\r\n--> 216       self.output(decoded_value)\r\n    217 \r\n    218   def try_split(self, fraction_of_remainder, total_buffer_size):\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.Operation.output()\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.Operation.output()\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.SingletonConsumerSet.receive()\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.DoOperation.process()\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.DoOperation.process()\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.process()\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner._reraise_augmented()\r\n\r\n/usr/local/lib/python3.6/dist-packages/future/utils/__init__.py in raise_with_traceback(exc, traceback)\r\n    417         if traceback == Ellipsis:\r\n    418             _, _, traceback = sys.exc_info()\r\n--> 419         raise exc.with_traceback(traceback)\r\n    420 \r\n    421 else:\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.process()\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.PerWindowInvoker.invoke_process()\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window()\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/io/iobase.py in process(self, element, init_result)\r\n   1080     for e in bundle[1]:  # values\r\n   1081       writer.write(e)\r\n-> 1082     return [window.TimestampedValue(writer.close(), timestamp.MAX_TIMESTAMP)]\r\n   1083 \r\n   1084 \r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/io/filebasedsink.py in close(self)\r\n    421 \r\n    422   def close(self):\r\n--> 423     self.sink.close(self.temp_handle)\r\n    424     return self.temp_shard_path\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/io/parquetio.py in close(self, writer)\r\n    536   def close(self, writer):\r\n    537     if len(self._buffer[0]) > 0:\r\n--> 538       self._flush_buffer()\r\n    539     if self._record_batches_byte_size > 0:\r\n    540       self._write_batches(writer)\r\n\r\n/usr/local/lib/python3.6/dist-packages/apache_beam/io/parquetio.py in _flush_buffer(self)\r\n    568     for x in arrays:\r\n    569       for b in x.buffers():\r\n--> 570         size = size + b.size\r\n    571     self._record_batches_byte_size = self._record_batches_byte_size + size\r\n\r\nAttributeError: 'NoneType' object has no attribute 'size' [while running 'train/Save to parquet/Write/WriteImpl/WriteBundles']\r\n```",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 623595180,
    "title": "Weird-ish: Not creating unique caches for different phases",
    "dateCreated": "2020-05-23T06:40:58Z",
    "dateModified": "2020-05-23T06:40:58Z",
    "description": "Sample code:\r\n\r\n```python\r\nimport nlp\r\ndataset = nlp.load_dataset('boolq')\r\n\r\ndef func1(x):\r\n    return x\r\n\r\ndef func2(x):\r\n    return None\r\n\r\ntrain_output = dataset[\"train\"].map(func1)\r\nvalid_output = dataset[\"validation\"].map(func1)\r\nprint()\r\nprint(len(train_output), len(valid_output))\r\n# Output: 9427 9427\r\n```\r\n\r\nThe map method in both cases seem to be pointing to the same cache, so the latter call based on the validation data will return the processed train data cache.\r\n\r\nWhat's weird is that the following doesn't seem to be an issue:\r\n\r\n```python\r\ntrain_output = dataset[\"train\"].map(func2)\r\nvalid_output = dataset[\"validation\"].map(func2)\r\nprint()\r\nprint(len(train_output), len(valid_output))\r\n# 9427 3270\r\n```",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 623172484,
    "title": "[Commands] In-detail instructions to create dummy data folder",
    "dateCreated": "2020-05-22T12:26:25Z",
    "dateModified": "2020-05-22T12:26:25Z",
    "description": "### Dummy data command \r\n\r\nThis PR adds a new command `python nlp-cli dummy_data <path_to_dataset_folder>` that gives in-detail instructions on how to add the dummy data files. \r\n\r\nIt would be great if you can try it out by moving the current dummy_data folder of any dataset in `./datasets` with `mv datasets/<dataset_script>/dummy_data datasets/<dataset_name>/dummy_data_copy` and running the command `python nlp-cli dummy_data ./datasets/<dataset_name>` to see if you like the instructions. \r\n\r\n### CONTRIBUTING.md\r\nAlso the CONTRIBUTING.md is made cleaner including a new section on \"How to add a dataset\". \r\n\r\n### Current PRs \r\nIt would be nice if we can try out if this command helps current PRs, *e.g.* #169  to add a dataset. I comment on those PRs.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 623120929,
    "title": "Use IndexError instead of ValueError when index out of range",
    "dateCreated": "2020-05-22T10:43:42Z",
    "dateModified": "2020-05-22T10:43:42Z",
    "description": "**`default __iter__ needs IndexError`**.\r\n\r\nWhen I want to create a wrapper of arrow dataset to adapt to fastai,\r\nI don't know how to initialize it, so I didn't use inheritance but use object composition.\r\nI wrote sth like this.\r\n```\r\nclas HF_dataset():\r\n  def __init__(self, arrow_dataset):\r\n    self.dset = arrow_dataset\r\n  def __getitem__(self, i):\r\n    return self.my_get_item(self.dset)\r\n```\r\nBut `for sample in my_dataset:` gave me `ValueError(f\"Index ({key}) outside of table length ({self._data.num_rows}).\")` . This is because default `__iter__` will stop when it catched `IndexError`.\r\n\r\nYou can also see my [work](https://github.com/richardyy1188/Pretrain-MLM-and-finetune-on-GLUE-with-fastai/blob/master/GLUE_with_fastai.ipynb) that uses fastai2 to show/load batches from huggingface/nlp GLUE datasets\r\n\r\nSo I hope we can use `IndexError` instead to let other people who want to wrap it for any purpose won't be caught by this caveat.\r\n\r\nBTW, I super appreciate your work, both transformers and nlp save my life. \ud83d\udc96\ud83d\udc96\ud83d\udc96\ud83d\udc96\ud83d\udc96\ud83d\udc96\ud83d\udc96\r\n\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 623054270,
    "title": "[Bug] labels of glue/ax are all -1 ",
    "dateCreated": "2020-05-22T08:43:36Z",
    "dateModified": "2020-05-22T08:43:36Z",
    "description": "```\r\nax = nlp.load_dataset('glue', 'ax')\r\nfor i in range(30): print(ax['test'][i]['label'], end=', ')\r\n```\r\n```\r\n-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \r\n```",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 622646770,
    "title": "Update newsroom.py",
    "dateCreated": "2020-05-21T17:07:43Z",
    "dateModified": "2020-05-21T17:07:43Z",
    "description": "Updated the URL for Newsroom download so it's more robust to future changes.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 622634420,
    "title": "Cannot upload my own dataset",
    "dateCreated": "2020-05-21T16:45:52Z",
    "dateModified": "2020-05-21T16:45:52Z",
    "description": "I look into `nlp-cli` and `user.py` to learn how to upload my own data.\r\n\r\nIt is supposed to work like this\r\n- Register to get username, password at huggingface.co\r\n- `nlp-cli login` and type username, passworld\r\n- I have a single file to upload at `./ttc/ttc_freq_extra.csv`\r\n- `nlp-cli upload ttc/ttc_freq_extra.csv`\r\n\r\nBut I got this error.\r\n\r\n```\r\n2020-05-21 16:33:52.722464: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\nAbout to upload file /content/ttc/ttc_freq_extra.csv to S3 under filename ttc/ttc_freq_extra.csv and namespace korakot\r\nProceed? [Y/n] y\r\nUploading... This might take a while if files are large\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/nlp-cli\", line 33, in <module>\r\n    service.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/nlp/commands/user.py\", line 234, in run\r\n    token=token, filename=filename, filepath=filepath, organization=self.args.organization\r\n  File \"/usr/local/lib/python3.6/dist-packages/nlp/hf_api.py\", line 141, in presign_and_upload\r\n    urls = self.presign(token, filename=filename, organization=organization)\r\n  File \"/usr/local/lib/python3.6/dist-packages/nlp/hf_api.py\", line 132, in presign\r\n    return PresignedUrl(**d)\r\nTypeError: __init__() got an unexpected keyword argument 'cdn'\r\n```",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 622556861,
    "title": "Add hall of fame",
    "dateCreated": "2020-05-21T14:53:48Z",
    "dateModified": "2020-05-21T14:53:48Z",
    "description": "powered by https://github.com/sourcerer-io/hall-of-fame",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 622525410,
    "title": "[Feature request] separate split name and split instructions",
    "dateCreated": "2020-05-21T14:10:51Z",
    "dateModified": "2020-05-21T14:10:51Z",
    "description": "Currently, the name of an nlp.NamedSplit is parsed in arrow_reader.py and used as the instruction.\r\n\r\nThis makes it impossible to have several training sets, which can occur when:\r\n- A dataset corresponds to a collection of sub-datasets\r\n- A dataset was built in stages, adding new examples at each stage\r\n\r\nWould it be possible to have two separate fields in the Split class, a name /instruction and a unique ID that is used as the key in the builder's split_dict ?",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 621979849,
    "title": "[Manual data] improve error message for manual data in general",
    "dateCreated": "2020-05-20T18:10:45Z",
    "dateModified": "2020-05-20T18:10:45Z",
    "description": "`nlp.load(\"xsum\")` now leads to the following error message:\r\n\r\n![Screenshot from 2020-05-20 20-05-28](https://user-images.githubusercontent.com/23423619/82481825-3587ea00-9ad6-11ea-9ca2-5794252c6ac7.png)\r\n\r\nI guess the manual download instructions for `xsum` can also be improved.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 621975368,
    "title": "Xsum manual download instruction",
    "dateCreated": "2020-05-20T18:02:41Z",
    "dateModified": "2020-05-20T18:02:41Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 621934638,
    "title": "[Tests] Refactor MockDownloadManager",
    "dateCreated": "2020-05-20T17:07:36Z",
    "dateModified": "2020-05-20T17:07:36Z",
    "description": "Clean mock download manager class. \r\nThe print function was not of much help I think. \r\nWe should think about adding a command that creates the dummy folder structure for the user.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 621929428,
    "title": "[Manual data dir] Error message: nlp.load_dataset('xsum') -> TypeError",
    "dateCreated": "2020-05-20T17:00:32Z",
    "dateModified": "2020-05-20T17:00:32Z",
    "description": "v 0.1.0 from pip\r\n\r\n```python\r\nimport nlp\r\nxsum = nlp.load_dataset('xsum')\r\n```\r\n\r\nIssue is `dl_manager.manual_dir`is `None`\r\n\r\n```python\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-42-8a32f066f3bd> in <module>\r\n----> 1 xsum = nlp.load_dataset('xsum')\r\n\r\n~/miniconda3/envs/nb/lib/python3.7/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    515         download_mode=download_mode,\r\n    516         ignore_verifications=ignore_verifications,\r\n--> 517         save_infos=save_infos,\r\n    518     )\r\n    519 \r\n\r\n~/miniconda3/envs/nb/lib/python3.7/site-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, dl_manager, **download_and_prepare_kwargs)\r\n    361                 verify_infos = not save_infos and not ignore_verifications\r\n    362                 self._download_and_prepare(\r\n--> 363                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    364                 )\r\n    365                 # Sync info\r\n\r\n~/miniconda3/envs/nb/lib/python3.7/site-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    397         split_dict = SplitDict(dataset_name=self.name)\r\n    398         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 399         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    400         # Checksums verification\r\n    401         if verify_infos:\r\n\r\n~/miniconda3/envs/nb/lib/python3.7/site-packages/nlp/datasets/xsum/5c5fca23aaaa469b7a1c6f095cf12f90d7ab99bcc0d86f689a74fd62634a1472/xsum.py in _split_generators(self, dl_manager)\r\n    102         with open(dl_path, \"r\") as json_file:\r\n    103             split_ids = json.load(json_file)\r\n--> 104         downloaded_path = os.path.join(dl_manager.manual_dir, \"xsum-extracts-from-downloads\")\r\n    105         return [\r\n    106             nlp.SplitGenerator(\r\n\r\n~/miniconda3/envs/nb/lib/python3.7/posixpath.py in join(a, *p)\r\n     78     will be discarded.  An empty last part will result in a path that\r\n     79     ends with a separator.\"\"\"\r\n---> 80     a = os.fspath(a)\r\n     81     sep = _get_sep(a)\r\n     82     path = a\r\n\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\n\r\n\r\n```\r\n",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 621928403,
    "title": "nlp.load_dataset('xsum') -> TypeError",
    "dateCreated": "2020-05-20T16:59:09Z",
    "dateModified": "2020-05-20T16:59:09Z",
    "description": "",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 621764932,
    "title": "Rm extracted test dirs",
    "dateCreated": "2020-05-20T13:30:48Z",
    "dateModified": "2020-05-20T13:30:48Z",
    "description": "All the dummy data used for tests were duplicated. For each dataset, we had one zip file but also its extracted directory. I removed all these directories\r\n\r\nFurthermore instead of extracting next to the dummy_data.zip file, we extract in the temp `cached_dir` used for tests, so that all the extracted directories get removed after testing.\r\n\r\nFinally there was a bug in the `mock_download_manager` that would let it create directories with invalid names, as in #172. I fixed that by encoding url arguments. I had to rename the dummy data for `scientific_papers` and `cnn_dailymail` (the aws tests don't pass for those 2 in this PR, but they will once aws will be synced, as the local ones do)\r\n\r\nLet me know if it sounds good to you @patrickvonplaten . I'm still not entirely familiar with the mock downloader",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-7.png",
      "dateCreated": "2019-10-19T09:00:33.580Z",
      "dateModified": "2019-10-19T09:00:33.580Z",
      "fullName": "Wilfredo Steuber",
      "id": 7
    }
  },
  {
    "id": 621377386,
    "title": "Clone not working on Windows environment",
    "dateCreated": "2020-05-20T00:45:14Z",
    "dateModified": "2020-05-20T00:45:14Z",
    "description": "Cloning in a windows environment is not working because of use of special character '?' in folder name ..\r\nPlease consider changing the folder name ....\r\nReference to folder -\r\nnlp/datasets/cnn_dailymail/dummy/3.0.0/3.0.0/dummy_data-zip-extracted/dummy_data/uc?export=download&id=0BwmD_VLjROrfM1BxdkxVaTY2bWs/dailymail/stories/\r\n\r\nerror log:\r\nfatal: cannot create directory at 'datasets/cnn_dailymail/dummy/3.0.0/3.0.0/dummy_data-zip-extracted/dummy_data/uc?export=download&id=0BwmD_VLjROrfM1BxdkxVaTY2bWs': Invalid argument\r\n\r\n",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 621199128,
    "title": "fix squad metric format",
    "dateCreated": "2020-05-19T18:37:36Z",
    "dateModified": "2020-05-19T18:37:36Z",
    "description": "The format of the squad metric was wrong.\r\nThis should fix #143 \r\n\r\nI tested with\r\n```python3\r\npredictions = [\r\n    {'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}\r\n]\r\nreferences = [\r\n    {'answers': [{'text': 'Denver Broncos'}], 'id': '56be4db0acb8001400a502ec'}\r\n]\r\n```",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 621119747,
    "title": "Rename anli dataset",
    "dateCreated": "2020-05-19T16:26:57Z",
    "dateModified": "2020-05-19T16:26:57Z",
    "description": "What we have now as the `anli` dataset is actually the \u03b1NLI dataset from the ART challenge dataset. This name is confusing because `anli` is also the name of adversarial NLI (see [https://github.com/facebookresearch/anli](https://github.com/facebookresearch/anli)).\r\n\r\nI renamed the current `anli` dataset by `art`.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 621099682,
    "title": "Adding Qanta (Quizbowl) Dataset",
    "dateCreated": "2020-05-19T16:03:01Z",
    "dateModified": "2020-05-19T16:03:01Z",
    "description": "This PR adds the qanta question answering datasets from [Quizbowl: The Case for Incremental Question Answering](https://arxiv.org/abs/1904.04792) and [Trick Me If You Can: Human-in-the-loop Generation of Adversarial Question Answering Examples](https://www.aclweb.org/anthology/Q19-1029/) (adversarial fold)\r\n\r\nThis partially continues a discussion around fixing dummy data from https://github.com/huggingface/nlp/issues/161\r\n\r\nI ran the following code to double check that it works and did some sanity checks on the output. The majority of the code itself is from our `allennlp` version of the dataset reader.\r\n\r\n```python\r\nimport nlp\r\n# Default is full question\r\ndata = nlp.load_dataset('./datasets/qanta') \r\n# Four configs\r\n# Primarily useful for training\r\ndata = nlp.load_dataset('./datasets/qanta', 'mode=sentences,char_skip=25')  \r\n# Primarily used in evaluation\r\ndata = nlp.load_dataset('./datasets/qanta', 'mode=first,char_skip=25')  \r\ndata = nlp.load_dataset('./datasets/qanta', 'mode=full,char_skip=25')  \r\n# Primarily useful in evaluation and \"live\" play\r\ndata = nlp.load_dataset('./datasets/qanta', 'mode=runs,char_skip=25')  \r\n```",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 620959819,
    "title": "Loading 'wikitext' dataset fails",
    "dateCreated": "2020-05-19T13:04:29Z",
    "dateModified": "2020-05-19T13:04:29Z",
    "description": "Loading the 'wikitext' dataset fails with Attribute error:\r\n\r\nCode to reproduce (From example notebook):\r\n\r\nimport nlp\r\nwikitext_dataset = nlp.load_dataset('wikitext')\r\n\r\n\r\nError:\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-17-d5d9df94b13c> in <module>()\r\n     11 \r\n     12 # Load a dataset and print the first examples in the training set\r\n---> 13 wikitext_dataset = nlp.load_dataset('wikitext')\r\n     14 print(wikitext_dataset['train'][0])\r\n\r\n6 frames\r\n/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    518         download_mode=download_mode,\r\n    519         ignore_verifications=ignore_verifications,\r\n--> 520         save_infos=save_infos,\r\n    521     )\r\n    522 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, dl_manager, **download_and_prepare_kwargs)\r\n    363                 verify_infos = not save_infos and not ignore_verifications\r\n    364                 self._download_and_prepare(\r\n--> 365                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    366                 )\r\n    367                 # Sync info\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    416             try:\r\n    417                 # Prepare split will record examples associated to the split\r\n--> 418                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    419             except OSError:\r\n    420                 raise OSError(\"Cannot find data file. \" + (self.MANUAL_DOWNLOAD_INSTRUCTIONS or \"\"))\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _prepare_split(self, split_generator)\r\n    594             example = self.info.features.encode_example(record)\r\n    595             writer.write(example)\r\n--> 596         num_examples, num_bytes = writer.finalize()\r\n    597 \r\n    598         assert num_examples == num_examples, f\"Expected to write {split_info.num_examples} but wrote {num_examples}\"\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in finalize(self, close_stream)\r\n    173     def finalize(self, close_stream=True):\r\n    174         if self.pa_writer is not None:\r\n--> 175             self.write_on_file()\r\n    176             self.pa_writer.close()\r\n    177         if close_stream:\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in write_on_file(self)\r\n    124             else:\r\n    125                 # All good\r\n--> 126                 self._write_array_on_file(pa_array)\r\n    127             self.current_rows = []\r\n    128 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in _write_array_on_file(self, pa_array)\r\n     93     def _write_array_on_file(self, pa_array):\r\n     94         \"\"\"Write a PyArrow Array\"\"\"\r\n---> 95         pa_batch = pa.RecordBatch.from_struct_array(pa_array)\r\n     96         self._num_bytes += pa_array.nbytes\r\n     97         self.pa_writer.write_batch(pa_batch)\r\n\r\nAttributeError: type object 'pyarrow.lib.RecordBatch' has no attribute 'from_struct_array'",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 620908786,
    "title": "[Tests] refactor tests",
    "dateCreated": "2020-05-19T11:43:32Z",
    "dateModified": "2020-05-19T11:43:32Z",
    "description": "This PR separates AWS and Local tests to remove these ugly statements in the script:\r\n```python\r\n        if \"/\" not in dataset_name:\r\n            logging.info(\"Skip {} because it is a canonical dataset\")\r\n            return\r\n```\r\n\r\nTo run a `aws` test, one should now run the following command: \r\n\r\n```python \r\npytest -s tests/test_dataset_common.py::AWSDatasetTest::test_builder_class_wmt14\r\n```\r\n\r\nThe same `local` test, can be run with:\r\n```python \r\npytest -s tests/test_dataset_common.py::LocalDatasetTest::test_builder_class_wmt14\r\n```",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 620850218,
    "title": "Add a method to shuffle a dataset",
    "dateCreated": "2020-05-19T10:08:46Z",
    "dateModified": "2020-05-19T10:08:46Z",
    "description": "Could maybe be a `dataset.shuffle(generator=None, seed=None)` signature method.\r\n\r\nAlso, we could maybe have a clear indication of which method modify in-place and which methods return/cache a modified dataset. I kinda like torch conversion of having an underscore suffix for all the methods which modify a dataset in-place. What do you think?",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 620758221,
    "title": "ANLI",
    "dateCreated": "2020-05-19T07:50:57Z",
    "dateModified": "2020-05-19T07:50:57Z",
    "description": "Can I recommend the following:\r\n\r\nFor ANLI, use https://github.com/facebookresearch/anli. As that paper says, \"Our dataset is not\r\nto be confused with abductive NLI (Bhagavatula et al., 2019), which calls itself \u03b1NLI, or ART.\". \r\n\r\nIndeed, the paper cited under what is currently called anli says in the abstract \"We introduce a challenge dataset, ART\".\r\n\r\nThe current naming will confuse people :)",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 620540250,
    "title": "Add Spanish POR and NER Datasets",
    "dateCreated": "2020-05-18T22:18:21Z",
    "dateModified": "2020-05-18T22:18:21Z",
    "description": "Hi guys,\r\nIn order to cover multilingual support a little step could be adding standard Datasets used for Spanish NER and POS tasks.\r\nI can provide it in raw and preprocessed formats.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 620534307,
    "title": "[Feature request] Add cos-e v1.0",
    "dateCreated": "2020-05-18T22:05:26Z",
    "dateModified": "2020-05-18T22:05:26Z",
    "description": "I noticed the second release of cos-e (v1.11) is included in this repo. I wanted to request inclusion of v1.0, since this is the version on which results are reported on in [the paper](https://www.aclweb.org/anthology/P19-1487/), and v1.11 has noted [annotation](https://github.com/salesforce/cos-e/issues/2) [issues](https://arxiv.org/pdf/2004.14546.pdf).",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 620513554,
    "title": "fix prev files hash in map",
    "dateCreated": "2020-05-18T21:20:51Z",
    "dateModified": "2020-05-18T21:20:51Z",
    "description": "Fix the `.map` issue in #160.\r\nThis makes sure it takes the previous files when computing the hash.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 620487535,
    "title": "Discussion on version identifier & MockDataLoaderManager for test data",
    "dateCreated": "2020-05-18T20:31:30Z",
    "dateModified": "2020-05-18T20:31:30Z",
    "description": "Hi, I'm working on adding a dataset and ran into an error due to `download` not being defined on `MockDataLoaderManager`, but being defined in `nlp/utils/download_manager.py`. The readme step running this: `RUN_SLOW=1 pytest tests/test_dataset_common.py::DatasetTest::test_load_real_dataset_localmydatasetname` triggers the error. If I can get something to work, I can include it in my data PR once I'm done.",
    "status": "open",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 620448236,
    "title": "caching in map causes same result to be returned for train, validation and test",
    "dateCreated": "2020-05-18T19:22:03Z",
    "dateModified": "2020-05-18T19:22:03Z",
    "description": "hello,\r\n\r\nI am working on a program that uses the `nlp` library with the `SST2` dataset.\r\n\r\nThe rough outline of the program is:\r\n\r\n```\r\nimport nlp as nlp_datasets\r\n...\r\nparser.add_argument('--dataset', help='HuggingFace Datasets id', default=['glue', 'sst2'], nargs='+')\r\n...\r\ndataset = nlp_datasets.load_dataset(*args.dataset)\r\n...\r\n# Create feature vocabs\r\nvocabs = create_vocabs(dataset.values(), vectorizers)\r\n...\r\n# Create a function to vectorize based on vectorizers and vocabs:\r\n\r\nprint('TS', train_set.num_rows)\r\nprint('VS', valid_set.num_rows)\r\nprint('ES', test_set.num_rows)\r\n\r\n# factory method to create a `convert_to_features` function based on vocabs\r\nconvert_to_features = create_featurizer(vectorizers, vocabs)\r\ntrain_set = train_set.map(convert_to_features, batched=True)\r\ntrain_set.set_format(type='torch', columns=list(vectorizers.keys()) + ['y', 'lengths'])\r\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batchsz)\r\n\r\nvalid_set = valid_set.map(convert_to_features, batched=True)\r\nvalid_set.set_format(type='torch', columns=list(vectorizers.keys()) + ['y', 'lengths'])\r\nvalid_loader = torch.utils.data.DataLoader(valid_set, batch_size=args.batchsz)\r\n\r\ntest_set = test_set.map(convert_to_features, batched=True)\r\ntest_set.set_format(type='torch', columns=list(vectorizers.keys()) + ['y', 'lengths'])\r\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batchsz)\r\n\r\nprint('TS', train_set.num_rows)\r\nprint('VS', valid_set.num_rows)\r\nprint('ES', test_set.num_rows)\r\n\r\n```\r\nIm not sure if Im using it incorrectly, but the results are not what I expect.  Namely, the `.map()`  seems to grab the datset from the cache and then loses track of what the specific dataset is, instead using my training data for all datasets:\r\n\r\n```\r\nTS 67349\r\nVS 872\r\nES 1821\r\nTS 67349\r\nVS 67349\r\nES 67349\r\n```\r\n\r\nThe behavior changes if I turn off the caching but then the results fail:\r\n\r\n```\r\ntrain_set = train_set.map(convert_to_features, batched=True, load_from_cache_file=False)\r\n...\r\nvalid_set = valid_set.map(convert_to_features, batched=True, load_from_cache_file=False)\r\n...\r\ntest_set = test_set.map(convert_to_features, batched=True, load_from_cache_file=False)\r\n```\r\n\r\nNow I get the right set of features back...\r\n```\r\nTS 67349\r\nVS 872\r\nES 1821\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 68/68 [00:00<00:00, 92.78it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 75.47it/s]\r\n  0%|          | 0/2 [00:00<?, ?it/s]TS 67349\r\nVS 872\r\nES 1821\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 77.19it/s]\r\n```\r\nbut I think its losing track of the original training set:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/dpressel/dev/work/baseline/api-examples/layers-classify-hf-datasets.py\", line 148, in <module>\r\n    for x in train_loader:\r\n  File \"/home/dpressel/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 345, in __next__\r\n    data = self._next_data()\r\n  File \"/home/dpressel/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 385, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"/home/dpressel/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/home/dpressel/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/home/dpressel/anaconda3/lib/python3.7/site-packages/nlp/arrow_dataset.py\", line 338, in __getitem__\r\n    output_all_columns=self._output_all_columns,\r\n  File \"/home/dpressel/anaconda3/lib/python3.7/site-packages/nlp/arrow_dataset.py\", line 294, in _getitem\r\n    outputs = self._unnest(self._data.slice(key, 1).to_pydict())\r\n  File \"pyarrow/table.pxi\", line 1211, in pyarrow.lib.Table.slice\r\n  File \"pyarrow/public-api.pxi\", line 390, in pyarrow.lib.pyarrow_wrap_table\r\n  File \"pyarrow/error.pxi\", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Column 3: In chunk 0: Invalid: Length spanned by list offsets (15859698) larger than values array (length 100000)\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nThe full-example program (minus the print stmts) is here:\r\nhttps://github.com/dpressel/mead-baseline/pull/620/files\r\n\r\n",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 620420700,
    "title": "How can we add more datasets to nlp library?",
    "dateCreated": "2020-05-18T18:35:31Z",
    "dateModified": "2020-05-18T18:35:31Z",
    "description": "",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 620396658,
    "title": "add Toronto Books Corpus",
    "dateCreated": "2020-05-18T17:54:45Z",
    "dateModified": "2020-05-18T17:54:45Z",
    "description": "This PR adds the Toronto Books Corpus.\r\n.\r\nIt on consider TMX and plain text files (Moses) defined in the table **Statistics and TMX/Moses Downloads** [here](http://opus.nlpl.eu/Books.php )",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 620356542,
    "title": "nlp.load_dataset() gives \"TypeError: list_() takes exactly one argument (2 given)\"",
    "dateCreated": "2020-05-18T16:46:38Z",
    "dateModified": "2020-05-18T16:46:38Z",
    "description": "I'm trying to load datasets from nlp but there seems to have error saying \r\n\"TypeError: list_() takes exactly one argument (2 given)\"\r\n\r\ngist can be found here\r\nhttps://gist.github.com/saahiluppal/c4b878f330b10b9ab9762bc0776c0a6a",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 620263687,
    "title": "SyntaxError with WMT datasets",
    "dateCreated": "2020-05-18T14:38:18Z",
    "dateModified": "2020-05-18T14:38:18Z",
    "description": "The following snippet produces a syntax error:\r\n\r\n```\r\nimport nlp\r\n\r\ndataset = nlp.load_dataset('wmt14')\r\nprint(dataset['train'][0])\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/tom/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n\r\n  File \"<ipython-input-8-3206959998b9>\", line 3, in <module>\r\n    dataset = nlp.load_dataset('wmt14')\r\n\r\n  File \"/home/tom/.local/lib/python3.6/site-packages/nlp/load.py\", line 505, in load_dataset\r\n    builder_cls = import_main_class(module_path, dataset=True)\r\n\r\n  File \"/home/tom/.local/lib/python3.6/site-packages/nlp/load.py\", line 56, in import_main_class\r\n    module = importlib.import_module(module_path)\r\n\r\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n\r\n  File \"/home/tom/.local/lib/python3.6/site-packages/nlp/datasets/wmt14/c258d646f4f5870b0245f783b7aa0af85c7117e06aacf1e0340bd81935094de2/wmt14.py\", line 21, in <module>\r\n    from .wmt_utils import Wmt, WmtConfig\r\n\r\n  File \"/home/tom/.local/lib/python3.6/site-packages/nlp/datasets/wmt14/c258d646f4f5870b0245f783b7aa0af85c7117e06aacf1e0340bd81935094de2/wmt_utils.py\", line 659\r\n    <<<<<<< HEAD\r\n     ^\r\nSyntaxError: invalid syntax\r\n```\r\n\r\nPython version:\r\n`3.6.9 (default, Apr 18 2020, 01:56:04)  [GCC 8.4.0]`\r\nRunning on Ubuntu 18.04, via a Jupyter notebook",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 620067946,
    "title": "Include more links in README, fix typos",
    "dateCreated": "2020-05-18T09:47:08Z",
    "dateModified": "2020-05-18T09:47:08Z",
    "description": "Include more links and fix typos in README",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 620059066,
    "title": "add Ubuntu Dialogs Corpus datasets",
    "dateCreated": "2020-05-18T09:34:48Z",
    "dateModified": "2020-05-18T09:34:48Z",
    "description": "This PR adds the Ubuntu Dialog Corpus datasets version 2.0. ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 619972246,
    "title": "Meta-datasets (GLUE/XTREME/...) \u2013 Special care to attributions and citations",
    "dateCreated": "2020-05-18T07:24:22Z",
    "dateModified": "2020-05-18T07:24:22Z",
    "description": "Meta-datasets are interesting in terms of standardized benchmarks but they also have specific behaviors, in particular in terms of attribution and authorship. It's very important that each specific dataset inside a meta dataset is properly referenced and the citation/specific homepage/etc are very visible and accessible and not only the generic citation of the meta-dataset itself.\r\n\r\nLet's take GLUE as an example:\r\n\r\nThe configuration has the citation for each dataset included (e.g. [here](https://github.com/huggingface/nlp/blob/master/datasets/glue/glue.py#L154-L161)) but it should be copied inside the dataset info so that, when people access `dataset.info.citation` they get both the citation for GLUE and the citation for the specific datasets inside GLUE that they have loaded.",
    "status": "open",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 619971900,
    "title": "Add GLUE config name check",
    "dateCreated": "2020-05-18T07:23:43Z",
    "dateModified": "2020-05-18T07:23:43Z",
    "description": "Fixes #130 by adding a name check to the Glue class",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 619968480,
    "title": "Fix JSON tests.",
    "dateCreated": "2020-05-18T07:17:38Z",
    "dateModified": "2020-05-18T07:17:38Z",
    "description": "",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 619809645,
    "title": "Add WNUT 17 NER dataset",
    "dateCreated": "2020-05-17T22:19:04Z",
    "dateModified": "2020-05-17T22:19:04Z",
    "description": "Hi,\r\n\r\nthis PR adds the WNUT 17 dataset to `nlp`.\r\n\r\n> Emerging and Rare entity recognition\r\n> This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarisation), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for example the tweet \u201cso.. kktny in 30 mins?\u201d - even human experts find entity kktny hard to detect and resolve. This task will evaluate the ability to detect and classify novel, emerging, singleton named entities in noisy text.\r\n> \r\n> The goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities.\r\n\r\nMore information about the dataset can be found on the [shared task page](https://noisy-text.github.io/2017/emerging-rare-entities.html).\r\n\r\nDataset is taken is taken from their [GitHub repository](https://github.com/leondz/emerging_entities_17), because the data provided in this repository contains minor fixes in the dataset format.\r\n\r\n## Usage\r\n\r\nThen the WNUT 17 dataset can be used in `nlp` like this:\r\n\r\n```python\r\nimport nlp\r\n\r\nwnut_17 = nlp.load_dataset(\"./datasets/wnut_17/wnut_17.py\")\r\n\r\nprint(wnut_17)\r\n```\r\n\r\nThis outputs:\r\n\r\n```txt\r\n'train': Dataset(schema: {'id': 'string', 'tokens': 'list<item: string>', 'labels': 'list<item: string>'}, num_rows: 3394)\r\n'validation': Dataset(schema: {'id': 'string', 'tokens': 'list<item: string>', 'labels': 'list<item: string>'}, num_rows: 1009)\r\n'test': Dataset(schema: {'id': 'string', 'tokens': 'list<item: string>', 'labels': 'list<item: string>'}, num_rows: 1287)\r\n```\r\n\r\nNumber are identical with the ones in [this paper](https://www.ijcai.org/Proceedings/2019/0702.pdf) and are the same as using the `dataset` reader in Flair.\r\n\r\n## Features\r\n\r\nThe following feature format is used to represent a sentence in the WNUT 17 dataset:\r\n\r\n| Feature | Example | Description\r\n| ---- | ---- | -----------------\r\n| `id` | `0` | Number (id) of current sentence\r\n| `tokens` | `[\"AHFA\", \"extends\", \"deadline\"]` | List of tokens (strings) for a sentence\r\n| `labels` | `[\"B-group\", \"O\", \"O\"]` | List of labels (outer span)\r\n\r\nThe following labels are used in WNUT 17:\r\n\r\n```txt\r\nO\r\nB-corporation\r\nI-corporation\r\nB-location\r\nI-location\r\nB-product\r\nI-product\r\nB-person\r\nI-person\r\nB-group\r\nI-group\r\nB-creative-work\r\nI-creative-work\r\n```",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 619735739,
    "title": "[Feature request] Add Ubuntu Dialogue Corpus dataset",
    "dateCreated": "2020-05-17T15:42:39Z",
    "dateModified": "2020-05-17T15:42:39Z",
    "description": "https://github.com/rkadlec/ubuntu-ranking-dataset-creator or http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 619590555,
    "title": "_download_and_prepare() got an unexpected keyword argument 'verify_infos'",
    "dateCreated": "2020-05-17T01:48:53Z",
    "dateModified": "2020-05-17T01:48:53Z",
    "description": "# Reproduce\r\nIn Colab,\r\n```\r\n%pip install -q  nlp\r\n%pip install -q apache_beam mwparserfromhell\r\n\r\ndataset = nlp.load_dataset('wikipedia')\r\n```\r\nget\r\n```\r\nDownloading and preparing dataset wikipedia/20200501.aa (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/wikipedia/20200501.aa/1.0.0...\r\n\r\n---------------------------------------------------------------------------\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n\r\n<ipython-input-6-52471d2a0088> in <module>()\r\n----> 1 dataset = nlp.load_dataset('wikipedia')\r\n\r\n1 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    515         download_mode=download_mode,\r\n    516         ignore_verifications=ignore_verifications,\r\n--> 517         save_infos=save_infos,\r\n    518     )\r\n    519 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, dl_manager, **download_and_prepare_kwargs)\r\n    361                 verify_infos = not save_infos and not ignore_verifications\r\n    362                 self._download_and_prepare(\r\n--> 363                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    364                 )\r\n    365                 # Sync info\r\n\r\nTypeError: _download_and_prepare() got an unexpected keyword argument 'verify_infos'\r\n```",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 619581907,
    "title": "Error with sklearn train_test_split",
    "dateCreated": "2020-05-17T00:28:24Z",
    "dateModified": "2020-05-17T00:28:24Z",
    "description": "It would be nice if we could use sklearn `train_test_split` to quickly generate subsets from the dataset objects returned by `nlp.load_dataset`. At the moment the code:\r\n\r\n```python\r\ndata = nlp.load_dataset('imdb', cache_dir=data_cache)\r\nf_half, s_half = train_test_split(data['train'], test_size=0.5, random_state=seed)\r\n```\r\nthrows:\r\n```\r\nValueError: Can only get row(s) (int or slice) or columns (string).\r\n```\r\nIt's not a big deal, since there are other ways to split the data, but it would be a cool thing to have.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 619564653,
    "title": "Add BERTScore to metrics",
    "dateCreated": "2020-05-16T22:09:39Z",
    "dateModified": "2020-05-16T22:09:39Z",
    "description": "This PR adds [BERTScore](https://arxiv.org/abs/1904.09675) to metrics.\r\nHere is an example of how to use it.\r\n\r\n```sh\r\nimport nlp\r\nbertscore = nlp.load_metric('metrics/bertscore') # or simply nlp.load_metric('bertscore') after this is added to huggingface's s3 bucket\r\npredictions = ['example', 'fruit']\r\nreferences = [['this is an example.', 'this is one example.'], ['apple']]\r\nresults = bertscore.compute(predictions, references, lang='en')\r\nprint(results)\r\n```",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 619480549,
    "title": "[AWS Tests] Follow-up PR from #144",
    "dateCreated": "2020-05-16T13:53:46Z",
    "dateModified": "2020-05-16T13:53:46Z",
    "description": "I forgot to add this line in PR #145 . ",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 619477367,
    "title": "[AWS tests] AWS test should not run for canonical datasets",
    "dateCreated": "2020-05-16T13:39:30Z",
    "dateModified": "2020-05-16T13:39:30Z",
    "description": "AWS tests should in general not run for canonical datasets. Only local tests will run in this case. This way a PR is able to pass when adding a new dataset.\r\n\r\nThis PR changes to logic to the following: \r\n\r\n1) All datasets that are present in `nlp/datasets` are tested only locally. This way when one adds a canonical dataset, the PR includes his dataset in the tests.\r\n\r\n2) All datasets that are only present on AWS, such as `webis/tl_dr` atm are tested only on AWS. \r\n\r\nI think the testing structure might need a bigger refactoring and better documentation very soon. \r\n\r\nMerging for now to unblock new PRs @thomwolf @mariamabarham .",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 619457641,
    "title": "ArrowTypeError in squad metrics",
    "dateCreated": "2020-05-16T12:06:37Z",
    "dateModified": "2020-05-16T12:06:37Z",
    "description": "`squad_metric.compute` is giving following error\r\n```\r\nArrowTypeError: Could not convert [{'text': 'Denver Broncos'}, {'text': 'Denver Broncos'}, {'text': 'Denver Broncos'}] with type list: was not a dict, tuple, or recognized null value for conversion to struct type\r\n```\r\n\r\nThis is how my predictions and references look like\r\n```\r\npredictions[0]\r\n# {'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}\r\n```\r\n\r\n```\r\nreferences[0]\r\n# {'answers': [{'text': 'Denver Broncos'},\r\n  {'text': 'Denver Broncos'},\r\n  {'text': 'Denver Broncos'}],\r\n 'id': '56be4db0acb8001400a502ec'}\r\n```\r\n\r\nThese are structured as per the `squad_metric.compute` help string.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 619450068,
    "title": "[WMT] Add all wmt",
    "dateCreated": "2020-05-16T11:28:46Z",
    "dateModified": "2020-05-16T11:28:46Z",
    "description": "This PR adds all wmt datasets scripts. At the moment the script is **not** functional for the language pairs \"cs-en\", \"ru-en\", \"hi-en\" because apparently it takes up to a week to get the manual data for these datasets: see http://ufal.mff.cuni.cz/czeng. \r\n\r\nThe datasets are fully functional though for the \"big\" language pairs \"de-en\" and \"fr-en\". \r\n\r\nOverall I think the scripts are very messy and might need a big refactoring at some point.\r\n\r\nFor now I think there are good to merge (most dataset configs can be used). I will add \"cs\", \"ru\" and \"hi\" when the manual data is available. ",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 619447090,
    "title": "[Clean up] remove bogus folder",
    "dateCreated": "2020-05-16T11:13:42Z",
    "dateModified": "2020-05-16T11:13:42Z",
    "description": "@mariamabarham  - I think you accidentally placed it there.",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 619443613,
    "title": "[Tests] run local tests as default",
    "dateCreated": "2020-05-16T10:56:06Z",
    "dateModified": "2020-05-16T10:56:06Z",
    "description": "This PR also enables local tests by default\r\n\r\nI think it's safer for now to enable both local and aws tests for every commit. The problem currently is that when we do a PR to add a dataset, the dataset is not yet on AWS on therefore not tested on the PR itself. Thus the PR will always be green even if the datasets are not correct. This PR aims at fixing this.\r\n\r\n## Suggestion on how to commit to the repo from now on:\r\nNow since the repo is \"online\", I think we should adopt a couple of best practices:\r\n1) - No direct committing to the repo anymore. Every change should be opened in a PR and be well documented so that we can find it later\r\n2) - Every PR has to be reviewed by at least x people (I guess @thomwolf you should decide here) because we now have to be much more careful when doing changes to the API for backward compatibility, etc...\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 619327409,
    "title": "Add GermEval 2014 NER dataset",
    "dateCreated": "2020-05-15T23:42:09Z",
    "dateModified": "2020-05-15T23:42:09Z",
    "description": "Hi, \r\n\r\nthis PR adds the GermEval 2014 NER dataset \ud83d\ude03\r\n\r\n> The GermEval 2014 NER Shared Task builds on a new dataset with German Named Entity annotation [1] with the following properties:\r\n\r\n> - The data was sampled from German Wikipedia and News Corpora as a collection of citations.\r\n> - The dataset covers over 31,000 sentences corresponding to over 590,000 tokens.\r\n> - The NER annotation uses the NoSta-D guidelines, which extend the T\u00fcbingen Treebank guidelines, using four main NER categories with sub-structure, and annotating embeddings among NEs such as [ORG FC Kickers [LOC Darmstadt]].\r\n\r\nDataset will be downloaded from the [official GermEval 2014 website](https://sites.google.com/site/germeval2014ner/data).\r\n\r\n## Dataset format\r\n\r\nHere's an example of the dataset format from the original dataset:\r\n\r\n```tsv\r\n# http://de.wikipedia.org/wiki/Manfred_Korfmann [2009-10-17]\r\n1 Aufgrund O O\r\n2 seiner O O\r\n3 Initiative O O\r\n4 fand O O\r\n5 2001/2002 O O\r\n6 in O O\r\n7 Stuttgart B-LOC O\r\n8 , O O\r\n9 Braunschweig B-LOC O\r\n10 und O O\r\n11 Bonn B-LOC O\r\n12 eine O O\r\n13 gro\u00dfe O O\r\n14 und O O\r\n15 publizistisch O O\r\n16 vielbeachtete O O\r\n17 Troia-Ausstellung B-LOCpart O\r\n18 statt O O\r\n19 , O O\r\n20 \u201e O O\r\n21 Troia B-OTH B-LOC\r\n22 - I-OTH O\r\n23 Traum I-OTH O\r\n24 und I-OTH O\r\n25 Wirklichkeit I-OTH O\r\n26 \u201c O O\r\n27 . O O\r\n```\r\n\r\nThe sentence is encoded as one token per line (tab separated columns.\r\n\r\nThe first column contains either a `#`, which signals the source the sentence is cited from and the date it was retrieved, or the token number within the sentence.\r\n\r\nThe second column contains the token.\r\n\r\nColumn three and four contain the named entity (in IOB2 scheme).\r\nOuter spans are encoded in the third column, embedded/nested spans in the fourth column.\r\n\r\n## Features\r\n\r\nI decided to keep most information from the dataset. That means the so called \"source\" information (where the sentences come from + date information) is also returned for each sentence in the feature vector.\r\n\r\nFor each sentence in the dataset, one feature vector (`nlp.Features` definition) will be returned:\r\n\r\n| Feature | Example | Description\r\n| ---- | ---- | -----------------\r\n| `id` | `0` | Number (id) of current sentence\r\n| `source` | `http://de.wikipedia.org/wiki/Manfred_Korfmann [2009-10-17]` | URL and retrieval date as string\r\n| `tokens` | `[\"Schwartau\", \"sagte\", \":\"]` | List of tokens (strings) for a sentence\r\n| `labels` | `[\"B-PER\", \"O\", \"O\"]` | List of labels (outer span)\r\n| `nested-labels` | `[\"O\", \"O\", \"O\"]` | List of labels for nested span\r\n\r\n## Example\r\n\r\nThe following command downloads the dataset from the official GermEval 2014 page and pre-processed it:\r\n\r\n```bash\r\npython nlp-cli test datasets/germeval_14 --all_configs\r\n```\r\n\r\nIt then outputs the number for training, development and testset. The training set consists of 24,000 sentences, the development set of 2,200 and the test of 5,100 sentences.\r\n\r\nNow it can be imported and used with `nlp`:\r\n\r\n```python\r\nimport nlp\r\n\r\ngermeval = nlp.load_dataset(\"./datasets/germeval_14/germeval_14.py\")\r\nassert len(germeval[\"train\"]) == 24000\r\n\r\n# Show first sentence of training set:\r\ngermeval[\"train\"][0]\r\n```",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 619225191,
    "title": "Consider renaming to nld",
    "dateCreated": "2020-05-15T20:23:27Z",
    "dateModified": "2020-05-15T20:23:27Z",
    "description": "Hey :)\r\n\r\nJust making a thread here recording what I said on Twitter, as it's impossible to follow discussion there. It's also just really not a good way to talk about this sort of thing.\r\n\r\nThe issue is that modules go into the global namespace, so you shouldn't use variable names that conflict with module names. This means the package makes `nlp` a bad variable name everywhere in the codebase. I've always used `nlp` as the canonical variable name of spaCy's `Language` objects, and this is a convention that a lot of other code has followed (Stanza, flair, etc). And actually, your `transformers` library uses `nlp` as the name for its `Pipeline` instance in your readme.\r\n\r\nIf you stick with the `nlp` name for this package, if anyone uses it then they should rewrite all of that code. If `nlp` is a bad choice of variable anywhere, it's a bad choice of variable everywhere --- because you shouldn't have to notice whether some other function uses a module when you're naming variables within a function. You want to have one convention that you can stick to everywhere.\r\n\r\nIf people use your `nlp` package and continue to use the `nlp` variable name, they'll find themselves with confusing bugs. There will be many many bits of code cut-and-paste from tutorials that give confusing results when combined with the data loading from the `nlp` library. The problem will be especially bad for shadowed modules (people might reasonably have a module named `nlp.py` within their codebase) and notebooks, as people might run notebook cells for data loading out-of-order.\r\n\r\nI don't think it's an exaggeration to say that if your library becomes popular, we'll all be answering issues around this about once a week for the next few years. That seems pretty unideal, so I do hope you'll reconsider.\r\n\r\nI suggest `nld` as a better name. It more accurately represents what the package actually does. It's pretty unideal to have a package named `nlp` that doesn't do any processing, and contains data about natural language generation or other non-NLP tasks. The name is equally short, and is sort of a visual pun on `nlp`, since a d is a rotated p.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 619214645,
    "title": "Tokenized BLEU considered harmful - Discussion on community-based process",
    "dateCreated": "2020-05-15T20:08:34Z",
    "dateModified": "2020-05-15T20:08:34Z",
    "description": "https://github.com/huggingface/nlp/blob/7d1526dfeeb29248d832f1073192dbf03ad642da/metrics/bleu/bleu.py#L76 assumes the inputs are tokenized by the user.  This is bad practice because the user's tokenizer is usually not the same as the one used by `mteval-v13a.pl`, the closest thing we have to a standard.  Moreover, tokenizers are like window managers: they can be endlessly customized and nobody has quite the same options.  \r\n\r\nAs @mjpost reported in https://www.aclweb.org/anthology/W18-6319.pdf BLEU configurations can vary by 1.8.  Yet people are incorrectly putting non-comparable BLEU scores in the same table, such as Table 1 in https://arxiv.org/abs/2004.04902 .  \r\n\r\nThere are a few use cases for tokenized BLEU like Thai.  For Chinese, people seem to use character BLEU for better or worse.\r\n\r\nThe default easy option should be the one that's correct more often.  And that is sacrebleu.  Please don't make it easy for people to run what is usually the wrong option; it definitely shouldn't be `bleu`.  \r\n\r\nAlso, I know this is inherited from TensorFlow and, paging @lmthang, they should discourage it too.  ",
    "status": "open",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 619211018,
    "title": "Update README.md",
    "dateCreated": "2020-05-15T20:01:07Z",
    "dateModified": "2020-05-15T20:01:07Z",
    "description": "small typo",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 619206708,
    "title": "Fix print statement in READ.md",
    "dateCreated": "2020-05-15T19:52:23Z",
    "dateModified": "2020-05-15T19:52:23Z",
    "description": "print statement was throwing generator object instead of printing names of available datasets/metrics",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 619112641,
    "title": "Update README.md",
    "dateCreated": "2020-05-15T16:56:14Z",
    "dateModified": "2020-05-15T16:56:14Z",
    "description": "",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 619094954,
    "title": "[Question] Using/adding a local dataset",
    "dateCreated": "2020-05-15T16:26:06Z",
    "dateModified": "2020-05-15T16:26:06Z",
    "description": "Users may want to either create/modify a local copy of a dataset, or use a custom-built dataset with the same `Dataset` API as externally downloaded datasets.\r\n\r\nIt appears to be possible to point to a local dataset path rather than downloading the external ones, but I'm not exactly sure how to go about doing this.\r\n\r\nA notebook/example script demonstrating this would be very helpful.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 619077851,
    "title": "[Feature Request] Add the OpenWebText dataset",
    "dateCreated": "2020-05-15T15:57:29Z",
    "dateModified": "2020-05-15T15:57:29Z",
    "description": "The OpenWebText dataset is an open clone of OpenAI's WebText dataset. It can be used to train ELECTRA as is specified in the [README](https://www.github.com/google-research/electra).\r\n\r\nMore information and the download link are available [here](https://skylion007.github.io/OpenWebTextCorpus/).",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 619073731,
    "title": "[Feature request] Add Toronto BookCorpus dataset",
    "dateCreated": "2020-05-15T15:50:44Z",
    "dateModified": "2020-05-15T15:50:44Z",
    "description": "I know the copyright/distribution of this one is complex, but it would be great to have! That, combined with the existing `wikitext`, would provide a complete dataset for pretraining models like BERT.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 619035440,
    "title": "Loading GLUE dataset loads CoLA by default",
    "dateCreated": "2020-05-15T14:55:50Z",
    "dateModified": "2020-05-15T14:55:50Z",
    "description": "If I run:\r\n\r\n```python\r\ndataset = nlp.load_dataset('glue')\r\n```\r\nThe resultant dataset seems to be CoLA be default, without throwing any error. This is in contrast to calling:\r\n\r\n```python\r\nmetric = nlp.load_metric(\"glue\")\r\n```\r\nwhich throws an error telling the user that they need to specify a task in GLUE. Should the same apply for loading datasets?",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 618997725,
    "title": "[Feature request] Add Google Natural Question dataset",
    "dateCreated": "2020-05-15T14:14:20Z",
    "dateModified": "2020-05-15T14:14:20Z",
    "description": "Would be great to have https://github.com/google-research-datasets/natural-questions as an alternative to SQuAD.",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 618951117,
    "title": "Some error inside nlp.load_dataset()",
    "dateCreated": "2020-05-15T13:01:29Z",
    "dateModified": "2020-05-15T13:01:29Z",
    "description": "First of all, nice work!\r\n\r\nI am going through [this overview notebook](https://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb)\r\n\r\nIn simple step `dataset = nlp.load_dataset('squad', split='validation[:10%]')`\r\n\r\nI get an error, which is connected with some inner code, I think:\r\n```\r\n---------------------------------------------------------------------------\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n\r\n<ipython-input-8-d848d3a99b8c> in <module>()\r\n      1 # Downloading and loading a dataset\r\n      2 \r\n----> 3 dataset = nlp.load_dataset('squad', split='validation[:10%]')\r\n\r\n8 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    515         download_mode=download_mode,\r\n    516         ignore_verifications=ignore_verifications,\r\n--> 517         save_infos=save_infos,\r\n    518     )\r\n    519 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, dl_manager, **download_and_prepare_kwargs)\r\n    361                 verify_infos = not save_infos and not ignore_verifications\r\n    362                 self._download_and_prepare(\r\n--> 363                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    364                 )\r\n    365                 # Sync info\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    414             try:\r\n    415                 # Prepare split will record examples associated to the split\r\n--> 416                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    417             except OSError:\r\n    418                 raise OSError(\"Cannot find data file. \" + (self.MANUAL_DOWNLOAD_INSTRUCTIONS or \"\"))\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _prepare_split(self, split_generator)\r\n    585         fname = \"{}-{}.arrow\".format(self.name, split_generator.name)\r\n    586         fpath = os.path.join(self._cache_dir, fname)\r\n--> 587         examples_type = self.info.features.type\r\n    588         writer = ArrowWriter(data_type=examples_type, path=fpath, writer_batch_size=self._writer_batch_size)\r\n    589 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/features.py in type(self)\r\n    460     @property\r\n    461     def type(self):\r\n--> 462         return get_nested_type(self)\r\n    463 \r\n    464     @classmethod\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/features.py in get_nested_type(schema)\r\n    370     # Nested structures: we allow dict, list/tuples, sequences\r\n    371     if isinstance(schema, dict):\r\n--> 372         return pa.struct({key: get_nested_type(value) for key, value in schema.items()})\r\n    373     elif isinstance(schema, (list, tuple)):\r\n    374         assert len(schema) == 1, \"We defining list feature, you should just provide one example of the inner type\"\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/features.py in <dictcomp>(.0)\r\n    370     # Nested structures: we allow dict, list/tuples, sequences\r\n    371     if isinstance(schema, dict):\r\n--> 372         return pa.struct({key: get_nested_type(value) for key, value in schema.items()})\r\n    373     elif isinstance(schema, (list, tuple)):\r\n    374         assert len(schema) == 1, \"We defining list feature, you should just provide one example of the inner type\"\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/features.py in get_nested_type(schema)\r\n    379         # We allow to reverse list of dict => dict of list for compatiblity with tfds\r\n    380         if isinstance(inner_type, pa.StructType):\r\n--> 381             return pa.struct(dict((f.name, pa.list_(f.type, schema.length)) for f in inner_type))\r\n    382         return pa.list_(inner_type, schema.length)\r\n    383 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/features.py in <genexpr>(.0)\r\n    379         # We allow to reverse list of dict => dict of list for compatiblity with tfds\r\n    380         if isinstance(inner_type, pa.StructType):\r\n--> 381             return pa.struct(dict((f.name, pa.list_(f.type, schema.length)) for f in inner_type))\r\n    382         return pa.list_(inner_type, schema.length)\r\n    383 \r\n\r\nTypeError: list_() takes exactly one argument (2 given)\r\n```",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 618909042,
    "title": "Update Overview.ipynb",
    "dateCreated": "2020-05-15T11:46:48Z",
    "dateModified": "2020-05-15T11:46:48Z",
    "description": "update notebook",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 618897499,
    "title": "remove webis",
    "dateCreated": "2020-05-15T11:25:20Z",
    "dateModified": "2020-05-15T11:25:20Z",
    "description": "Remove webis from dataset folder.\r\n\r\nOur first dataset script that only lives on AWS :-) https://s3.console.aws.amazon.com/s3/buckets/datasets.huggingface.co/nlp/datasets/webis/tl_dr/?region=us-east-1 @julien-c @jplu ",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 618869048,
    "title": "[Newsroom] add newsroom",
    "dateCreated": "2020-05-15T10:34:34Z",
    "dateModified": "2020-05-15T10:34:34Z",
    "description": "I checked it with the data link of the mail you forwarded @thomwolf => works well!",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 618864284,
    "title": "Xsum, require manual download of some files",
    "dateCreated": "2020-05-15T10:26:13Z",
    "dateModified": "2020-05-15T10:26:13Z",
    "description": "",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 618820140,
    "title": "[Tests] Local => aws",
    "dateCreated": "2020-05-15T09:12:25Z",
    "dateModified": "2020-05-15T09:12:25Z",
    "description": "## Change default Test from local => aws\r\n\r\nAs a default we set` aws=True`, `Local=False`, `slow=False`\r\n\r\n### 1. RUN_AWS=1 (default)\r\nThis runs 4 tests per dataset script.\r\n\r\na) Does the dataset script have a valid etag / Can it be reached on AWS? \r\nb) Can we load its `builder_class`?\r\nc) Can we load **all** dataset configs?\r\nd) _Most importantly_: Can we load the dataset? \r\n\r\nImportant - we currently only test the first config of each dataset to reduce test time. Total test time is around 1min20s.\r\n\r\n### 2. RUN_LOCAL=1 RUN_AWS=0\r\n\r\n***This should be done when debugging dataset scripts of the ./datasets folder***\r\n\r\nThis only runs 1 test per dataset test, which is equivalent to aws d) - Can we load the dataset from the local `datasets` directory?\r\n\r\n### 3. RUN_SLOW=1\r\n\r\nWe should set up to run these tests maybe 1 time per week ? @thomwolf \r\n\r\nThe `slow` tests include two more important tests. \r\n\r\ne) Can we load the dataset with all possible configs? This test will probably fail at the moment because a lot of dummy data is missing. We should add the dummy data step by step to be sure that all configs work.\r\n\r\nf) Test that the actual dataset can be loaded. This will take quite some time to run, but is important to make sure that the \"real\" data can be loaded. It will also test whether the dataset script has the correct checksums file which is currently not tested with `aws=True`. @lhoestq - is there an easy way to check cheaply whether the `dataset_info.json` is correct for each dataset script? ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 618813182,
    "title": "Final cleanup of readme and metrics",
    "dateCreated": "2020-05-15T09:00:52Z",
    "dateModified": "2020-05-15T09:00:52Z",
    "description": "",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 618790040,
    "title": "make style",
    "dateCreated": "2020-05-15T08:23:36Z",
    "dateModified": "2020-05-15T08:23:36Z",
    "description": "",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 618737783,
    "title": "\ud83d\udc1b `map` not working",
    "dateCreated": "2020-05-15T06:43:08Z",
    "dateModified": "2020-05-15T06:43:08Z",
    "description": "I'm trying to run a basic example (mapping function to add a prefix).  \r\n[Here is the colab notebook I'm using.](https://colab.research.google.com/drive/1YH4JCAy0R1MMSc-k_Vlik_s1LEzP_t1h?usp=sharing)\r\n\r\n```python\r\nimport nlp\r\n\r\ndataset = nlp.load_dataset('squad', split='validation[:10%]')\r\n\r\ndef test(sample):\r\n    sample['title'] = \"test prefix @@@ \" + sample[\"title\"]\r\n    return sample\r\n\r\nprint(dataset[0]['title'])\r\ndataset.map(test)\r\nprint(dataset[0]['title'])\r\n```\r\nOutput :\r\n> Super_Bowl_50\r\nSuper_Bowl_50\r\n\r\nExpected output :\r\n> Super_Bowl_50\r\ntest prefix @@@ Super_Bowl_50",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 618652145,
    "title": "\ud83d\udc1b Colab : type object 'pyarrow.lib.RecordBatch' has no attribute 'from_struct_array'",
    "dateCreated": "2020-05-15T02:27:26Z",
    "dateModified": "2020-05-15T02:27:26Z",
    "description": "I'm trying to load CNN/DM dataset on Colab.\r\n\r\n[Colab notebook](https://colab.research.google.com/drive/11Mf7iNhIyt6GpgA1dBEtg3cyMHmMhtZS?usp=sharing)\r\n\r\nBut I meet this error :\r\n\r\n> AttributeError: type object 'pyarrow.lib.RecordBatch' has no attribute 'from_struct_array'\r\n",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 618643088,
    "title": "\u2753 How to apply a map to all subsets ?",
    "dateCreated": "2020-05-15T01:58:52Z",
    "dateModified": "2020-05-15T01:58:52Z",
    "description": "I'm working with CNN/DM dataset, where I have 3 subsets : `train`, `test`, `validation`.\r\n\r\nShould I apply my map function on the subsets one by one ?\r\n\r\n```python\r\nimport nlp\r\n\r\ncnn_dm = nlp.load_dataset('cnn_dailymail')\r\nfor corpus in ['train', 'test', 'validation']:\r\n         cnn_dm[corpus] = cnn_dm[corpus].map(my_func)\r\n```\r\n\r\nOr is there a better way to do this ?",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 618632573,
    "title": "\u2753 How to remove specific rows of a dataset ?",
    "dateCreated": "2020-05-15T01:25:06Z",
    "dateModified": "2020-05-15T01:25:06Z",
    "description": "I saw on the [example notebook](https://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb#scrollTo=efFhDWhlvSVC) how to remove a specific column :\r\n\r\n```python\r\ndataset.drop('id')\r\n```\r\n\r\nBut I didn't find how to remove a specific row. \r\n\r\n**For example, how can I remove all sample with `id` < 10 ?**",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 618628264,
    "title": "\ud83d\udc1b Trying to use ROUGE metric : pyarrow.lib.ArrowInvalid: Column 1 named references expected length 534 but got length 323",
    "dateCreated": "2020-05-15T01:12:06Z",
    "dateModified": "2020-05-15T01:12:06Z",
    "description": "I'm trying to use rouge metric.\r\n\r\nI have to files : `test.pred.tokenized` and `test.gold.tokenized` with each line containing a sentence.  \r\nI tried :\r\n\r\n```python\r\nimport nlp\r\n\r\nrouge = nlp.load_metric('rouge')\r\nwith open(\"test.pred.tokenized\") as p, open(\"test.gold.tokenized\") as g:\r\n    for lp, lg in zip(p, g):\r\n            rouge.add(lp, lg)\r\n```\r\n\r\nBut I meet following error :\r\n\r\n> pyarrow.lib.ArrowInvalid: Column 1 named references expected length 534 but got length 323\r\n\r\n---\r\n\r\nFull stack-trace :\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 3, in <module>\r\n  File \"/home/me/.venv/transformers/lib/python3.6/site-packages/nlp/metric.py\", line 224, in add\r\n    self.writer.write_batch(batch)\r\n  File \"/home/me/.venv/transformers/lib/python3.6/site-packages/nlp/arrow_writer.py\", line 148, in write_batch\r\n    pa_table: pa.Table = pa.Table.from_pydict(batch_examples, schema=self._schema)\r\n  File \"pyarrow/table.pxi\", line 1550, in pyarrow.lib.Table.from_pydict\r\n  File \"pyarrow/table.pxi\", line 1503, in pyarrow.lib.Table.from_arrays\r\n  File \"pyarrow/public-api.pxi\", line 390, in pyarrow.lib.pyarrow_wrap_table\r\n  File \"pyarrow/error.pxi\", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Column 1 named references expected length 534 but got length 323\r\n```\r\n\r\n(`nlp` installed from source)",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 618615855,
    "title": "AttributeError: 'dict' object has no attribute 'info'",
    "dateCreated": "2020-05-15T00:29:47Z",
    "dateModified": "2020-05-15T00:29:47Z",
    "description": "I'm trying to access the information of CNN/DM dataset :\r\n\r\n```python\r\ncnn_dm = nlp.load_dataset('cnn_dailymail')\r\nprint(cnn_dm.info)\r\n```\r\n\r\nreturns :\r\n\r\n> AttributeError: 'dict' object has no attribute 'info'",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 618611310,
    "title": "Couldn't reach CNN/DM dataset",
    "dateCreated": "2020-05-15T00:16:17Z",
    "dateModified": "2020-05-15T00:16:17Z",
    "description": "I can't get CNN / DailyMail dataset.\r\n\r\n```python\r\nimport nlp\r\n\r\nassert \"cnn_dailymail\" in [dataset.id for dataset in nlp.list_datasets()]\r\ncnn_dm = nlp.load_dataset('cnn_dailymail')\r\n```\r\n\r\n[Colab notebook](https://colab.research.google.com/drive/1zQ3bYAVzm1h0mw0yWPqKAg_4EUlSx5Ex?usp=sharing)\r\n\r\ngives following error :\r\n\r\n```\r\nConnectionError: Couldn't reach https://s3.amazonaws.com/datasets.huggingface.co/nlp/cnn_dailymail/cnn_dailymail.py\r\n```",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 618590562,
    "title": "Adding docstrings and some doc",
    "dateCreated": "2020-05-14T23:14:41Z",
    "dateModified": "2020-05-14T23:14:41Z",
    "description": "Some doc",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 618569195,
    "title": "Qa4mre - add dataset",
    "dateCreated": "2020-05-14T22:17:51Z",
    "dateModified": "2020-05-14T22:17:51Z",
    "description": "Added dummy data test only for the first config. Will do the rest later.\r\nI had to do add some minor hacks to an important function to make it work. \r\nThere might be a cleaner way to handle it - can you take a look @thomwolf ?",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 618528060,
    "title": "[Clean-up] remove under construction datastes",
    "dateCreated": "2020-05-14T20:52:13Z",
    "dateModified": "2020-05-14T20:52:13Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 618520325,
    "title": "fix reddit tifu dummy data",
    "dateCreated": "2020-05-14T20:37:37Z",
    "dateModified": "2020-05-14T20:37:37Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 618508359,
    "title": "[Reclor] fix reclor",
    "dateCreated": "2020-05-14T20:16:26Z",
    "dateModified": "2020-05-14T20:16:26Z",
    "description": "- That's probably one me. Could have made the manual data test more flexible. @mariamabarham ",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 618386394,
    "title": "convert can use manual dir as second argument",
    "dateCreated": "2020-05-14T16:52:32Z",
    "dateModified": "2020-05-14T16:52:32Z",
    "description": "@mariamabarham ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 618373045,
    "title": "add writer_batch_size to GeneratorBasedBuilder",
    "dateCreated": "2020-05-14T16:35:39Z",
    "dateModified": "2020-05-14T16:35:39Z",
    "description": "You can now specify `writer_batch_size` in the builder arguments or directly in `load_dataset`",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 618361418,
    "title": "Add data dir test command",
    "dateCreated": "2020-05-14T16:18:39Z",
    "dateModified": "2020-05-14T16:18:39Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 618345191,
    "title": "[New structure on AWS] Adapt paths",
    "dateCreated": "2020-05-14T15:55:57Z",
    "dateModified": "2020-05-14T15:55:57Z",
    "description": "Some small changes so that we have the correct paths. @julien-c ",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 618277081,
    "title": "Add trivia_q",
    "dateCreated": "2020-05-14T14:27:19Z",
    "dateModified": "2020-05-14T14:27:19Z",
    "description": "Currently tested only for one config to pass tests. Needs to add more dummy data later.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 618233637,
    "title": "[Manual downloads] add logic proposal for manual downloads and add wikihow",
    "dateCreated": "2020-05-14T13:30:36Z",
    "dateModified": "2020-05-14T13:30:36Z",
    "description": "Wikihow is an example that needs to manually download two files as stated in: https://github.com/mahnazkoupaee/WikiHow-Dataset. \r\n\r\nThe user can then store these files under a hard-coded name: `wikihowAll.csv` and `wikihowSep.csv` in this case in a directory of his choice, e.g. `~/wikihow/manual_dir`.\r\n\r\nThe dataset can then be loaded via:\r\n\r\n```python\r\nimport nlp\r\nnlp.load_dataset(\"wikihow\", data_dir=\"~/wikihow/manual_dir\")\r\n```\r\n\r\nI added/changed so that there are explicit error messages when using manually downloaded files.\r\n\r\n",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 618231216,
    "title": "Run save infos",
    "dateCreated": "2020-05-14T13:27:26Z",
    "dateModified": "2020-05-14T13:27:26Z",
    "description": "I replaced the old checksum file with the new `dataset_infos.json` by running the script on almost all the datasets we have. The only one that is still running on my side is the cornell dialog",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 618111651,
    "title": "[Reddit] add reddit",
    "dateCreated": "2020-05-14T10:25:02Z",
    "dateModified": "2020-05-14T10:25:02Z",
    "description": "- Everything worked fine @mariamabarham. Made my computer nearly crash, but all seems to be working :-) ",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 618081602,
    "title": "Add per type scores in seqeval metric",
    "dateCreated": "2020-05-14T09:37:52Z",
    "dateModified": "2020-05-14T09:37:52Z",
    "description": "This PR add a bit more detail in the seqeval metric. Now the usage and output are:\r\n\r\n```python\r\nimport nlp\r\nmet = nlp.load_metric('metrics/seqeval')\r\nreferences = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\r\npredictions =  [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\r\nmet.compute(predictions, references)\r\n\r\n#Output: {'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0, 'number': 1}, 'overall_precision': 0.5, 'overall_recall': 0.5, 'overall_f1': 0.5, 'overall_accuracy': 0.8}\r\n```\r\n\r\nIt is also possible to compute scores for non IOB notations, POS tagging for example hasn't this kind of notation. Add `suffix` parameter:\r\n\r\n```python\r\nimport nlp\r\nmet = nlp.load_metric('metrics/seqeval')\r\nreferences = [['O', 'O', 'O', 'MISC', 'MISC', 'MISC', 'O'], ['PER', 'PER', 'O']]\r\npredictions =  [['O', 'O', 'MISC', 'MISC', 'MISC', 'MISC', 'O'], ['PER', 'PER', 'O']]\r\nmet.compute(predictions, references, metrics_kwargs={\"suffix\": True})\r\n\r\n#Output: {'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0, 'number': 1}, 'overall_precision': 0.5, 'overall_recall': 0.5, 'overall_f1': 0.5, 'overall_accuracy': 0.9}\r\n```",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 618026700,
    "title": "[Cmrc 2018] fix cmrc2018",
    "dateCreated": "2020-05-14T08:22:03Z",
    "dateModified": "2020-05-14T08:22:03Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 617957739,
    "title": "Webis tl-dr",
    "dateCreated": "2020-05-14T06:22:18Z",
    "dateModified": "2020-05-14T06:22:18Z",
    "description": "Add the Webid TL:DR dataset.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 617809431,
    "title": "[Csv] add tests for csv dataset script",
    "dateCreated": "2020-05-13T23:06:11Z",
    "dateModified": "2020-05-13T23:06:11Z",
    "description": "Adds dummy data tests for csv.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 617739521,
    "title": "lm1b",
    "dateCreated": "2020-05-13T20:38:44Z",
    "dateModified": "2020-05-13T20:38:44Z",
    "description": "Add lm1b dataset.",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 617703037,
    "title": "Replace checksums files by Dataset infos json",
    "dateCreated": "2020-05-13T19:36:16Z",
    "dateModified": "2020-05-13T19:36:16Z",
    "description": "### Better verifications when loading a dataset\r\n\r\nI replaced the `urls_checksums` directory that used to contain `checksums.txt` and `cached_sizes.txt`, by a single file `dataset_infos.json`. It's just a dict `config_name` -> `DatasetInfo`.\r\n\r\nIt simplifies and improves how verifications of checksums and splits sizes are done, as they're all stored in `DatasetInfo` (one per config). Also, having already access to `DatasetInfo` enables to check disk space before running `download_and_prepare` for a given config.\r\n\r\nThe dataset infos json file is user readable, you can take a look at the squad one that I generated in this PR.\r\n\r\n### Renaming\r\n\r\nAccording to these changes, I did some renaming:\r\n`save_checksums` -> `save_infos`\r\n`ignore_checksums` -> `ignore_verifications`\r\n\r\nfor example, when you are creating a dataset you have to run\r\n```nlp-cli test path/to/my/dataset --save_infos --all_configs```\r\ninstead of\r\n```nlp-cli test path/to/my/dataset --save_checksums --all_configs```\r\n\r\n### And now, the fun part\r\n\r\nWe'll have to rerun the `nlp-cli test ... --save_infos --all_configs` for all the datasets\r\n\r\n-----------------\r\n\r\nfeedback appreciated !",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 617571340,
    "title": "Librispeech",
    "dateCreated": "2020-05-13T16:04:14Z",
    "dateModified": "2020-05-13T16:04:14Z",
    "description": "Add librispeech dataset and remove some useless content.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 617522029,
    "title": "Cleanup notebooks and various fixes",
    "dateCreated": "2020-05-13T14:58:58Z",
    "dateModified": "2020-05-13T14:58:58Z",
    "description": "Fixes on dataset (more flexible) metrics (fix) and general clean ups",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 617341505,
    "title": "[WIP] add wmt14",
    "dateCreated": "2020-05-13T10:42:03Z",
    "dateModified": "2020-05-13T10:42:03Z",
    "description": "WMT14 takes forever to download :-/ \r\n\r\n- WMT is the first dataset that uses an abstract class IMO, so I had to modify the `load_dataset_module` a bit.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 617339484,
    "title": "[Paracrawl] add paracrawl",
    "dateCreated": "2020-05-13T10:39:00Z",
    "dateModified": "2020-05-13T10:39:00Z",
    "description": "- Huge dataset - took ~1h to download\r\n- Also this PR reformats all dataset scripts and adds `datasets` to `make style`",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 617311877,
    "title": "Add download gg drive",
    "dateCreated": "2020-05-13T09:56:02Z",
    "dateModified": "2020-05-13T09:56:02Z",
    "description": "We can now add datasets that download from google drive",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 617295069,
    "title": "Add list and inspect methods - cleanup hf_api",
    "dateCreated": "2020-05-13T09:30:15Z",
    "dateModified": "2020-05-13T09:30:15Z",
    "description": "Add a bunch of methods to easily list and inspect the processing scripts up-loaded on S3:\r\n```python\r\nnlp.list_datasets()\r\nnlp.list_metrics()\r\n# Copy and prepare the scripts at `local_path` for easy inspection/modification.\r\nnlp.inspect_dataset(path, local_path) \r\n# Copy and prepare the scripts at `local_path` for easy inspection/modification.\r\nnlp.inspect_metric(path, local_path) \r\n```\r\n\r\nAlso clean up the `HfAPI` to use `dataclasses` for better user-experience",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 617284664,
    "title": "Add wiki40b",
    "dateCreated": "2020-05-13T09:16:01Z",
    "dateModified": "2020-05-13T09:16:01Z",
    "description": "This one is a beam dataset that downloads files using tensorflow.\r\nI tested it on a small config and it works fine",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 617267118,
    "title": "Add Flores",
    "dateCreated": "2020-05-13T08:51:29Z",
    "dateModified": "2020-05-13T08:51:29Z",
    "description": "Beautiful language for sure!",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 617260972,
    "title": "[Load => load_dataset] change naming",
    "dateCreated": "2020-05-13T08:43:00Z",
    "dateModified": "2020-05-13T08:43:00Z",
    "description": "Rename leftovers @thomwolf ",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 617253428,
    "title": "Add boolq",
    "dateCreated": "2020-05-13T08:32:27Z",
    "dateModified": "2020-05-13T08:32:27Z",
    "description": "I just added the dummy data for this dataset.\r\nThis one was uses `tf.io.gfile.copy` to download the data but I added the support for custom download in the mock_download_manager. I also had to add a `tensorflow` dependency for tests.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 617249815,
    "title": "[TedHrLr] add left dummy data",
    "dateCreated": "2020-05-13T08:27:20Z",
    "dateModified": "2020-05-13T08:27:20Z",
    "description": "",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 616863601,
    "title": "New datasets",
    "dateCreated": "2020-05-12T18:22:27Z",
    "dateModified": "2020-05-12T18:22:27Z",
    "description": "",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 616805194,
    "title": "[Datasets] add ted_hrlr",
    "dateCreated": "2020-05-12T16:46:50Z",
    "dateModified": "2020-05-12T16:46:50Z",
    "description": "@thomwolf - After looking at `xnli` I think it's better to leave the translation features and add a `translation` key to make them work in our framework. \r\n\r\nThe result looks like this:\r\n![Screenshot from 2020-05-12 18-34-43](https://user-images.githubusercontent.com/23423619/81721933-ee1faf00-9480-11ea-9e95-d6557cbd0ce0.png)\r\n\r\nyou can see that each split has a `translation` key which value is the nlp.features.Translation object. \r\n\r\nThat's a simple change. If it's ok for you, I will add dummy data for the other configs and treat the other translation scripts in the same way.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 616793010,
    "title": "add tests",
    "dateCreated": "2020-05-12T16:28:19Z",
    "dateModified": "2020-05-12T16:28:19Z",
    "description": "Tests for py_utils functions and for the BaseReader used to read from arrow and parquet.\r\nI also removed unused utils functions.",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 616786803,
    "title": "Add nbytes + nexamples check",
    "dateCreated": "2020-05-12T16:18:43Z",
    "dateModified": "2020-05-12T16:18:43Z",
    "description": "### Save size and number of examples\r\nNow when you do `save_checksums`, it also create `cached_sizes.txt` right next to the checksum file.\r\nThis new file stores the bytes sizes and the number of examples of each split that has been prepared and stored in the cache. Example:\r\n\r\n```\r\n# Cached sizes: <full_config_name> <num_bytes> <num_examples>\r\nhansards/house/1.0.0/test 22906629 122290\r\nhansards/house/1.0.0/train 191459584 947969\r\nhansards/senate/1.0.0/test 5711686 25553\r\nhansards/senate/1.0.0/train 40324278 182135\r\n```\r\n\r\n### Check processing output \r\n\r\nIf there is a `caches_sizes.txt`, then each time we run `download_and_prepare` it will make sure that the sizes match. You can set `ignore_checksums=True` if you don't want that to happen.\r\n\r\n### Fill Dataset Info\r\n\r\nAll the split infos and the checksums are now stored correctly in DatasetInfo after `download_and_prepare`\r\n\r\n### Check space on disk before running `download_and_prepare`\r\n\r\nCheck if the space is lower than the sum of the sizes of the files in `checksums.txt` and `cached_files.txt`. This is not ideal though as it considers the files for all configs.\r\n\r\nTODO:\r\nA better way to do it would be to have save the `DatasetInfo` instead of the `checksums.txt` and `cached_sizes.txt`, in order to have one file per dataset config (and therefore consider only the sizes of the files for one config and not all of them). It can also be the occasion to factorize all the `download_and_prepare` verifications. Maybe next PR ?",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 616785613,
    "title": "[Convert] add new pattern",
    "dateCreated": "2020-05-12T16:16:51Z",
    "dateModified": "2020-05-12T16:16:51Z",
    "description": "",
    "status": "closed",
    "estimate": 1,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 616774275,
    "title": "[Tests] skip beam dataset tests for now",
    "dateCreated": "2020-05-12T16:00:58Z",
    "dateModified": "2020-05-12T16:00:58Z",
    "description": "For now we will skip tests for Beam Datasets",
    "status": "closed",
    "estimate": 3,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 616674601,
    "title": "New datasets",
    "dateCreated": "2020-05-12T13:51:59Z",
    "dateModified": "2020-05-12T13:51:59Z",
    "description": "",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 616579228,
    "title": "pin flake 8",
    "dateCreated": "2020-05-12T11:25:29Z",
    "dateModified": "2020-05-12T11:25:29Z",
    "description": "Flake 8's new version does not like our format. Pinning the version for now.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 19,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 616520163,
    "title": "WIP adding metrics",
    "dateCreated": "2020-05-12T09:52:00Z",
    "dateModified": "2020-05-12T09:52:00Z",
    "description": "Adding the following metrics as identified by @mariamabarham:\r\n\r\n1. BLEU:  BiLingual Evaluation Understudy: https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py,  https://github.com/chakki-works/sumeval/blob/master/sumeval/metrics/bleu.py (multilingual)\r\n2. GLEU:  Google-BLEU:  https://github.com/cnap/gec-ranking/blob/master/scripts/compute_gleu\r\n3. Sacrebleu: https://pypi.org/project/sacrebleu/1.4.8/ (pypi package), https://github.com/mjpost/sacrebleu (github implementation)\r\n4. ROUGE: Recall-Oriented Understudy for Gisting Evaluation: https://github.com/google-research/google-research/tree/master/rouge, https://github.com/chakki-works/sumeval/blob/master/sumeval/metrics/rouge.py (multilingual)\r\n5. Seqeval: https://github.com/chakki-works/seqeval (github implementation), https://pypi.org/project/seqeval/0.0.12/ (pypi package)\r\n6. Coval:  coreference evaluation package for the CoNLL and ARRAU datasets https://github.com/ns-moosavi/coval\r\n7. SQuAD v1 evaluation script\r\n8. SQuAD V2 evaluation script: https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\r\n9. GLUE\r\n10. XNLI\r\n\r\n\r\nNot now:\r\n1. Perplexity: https://github.com/allenai/allennlp/blob/master/allennlp/training/metrics/perplexity.py\r\n2. Spearman: https://github.com/allenai/allennlp/blob/master/allennlp/training/metrics/spearman_correlation.py\r\n3. F1_measure: https://github.com/allenai/allennlp/blob/master/allennlp/training/metrics/f1_measure.py\r\n4. Pearson_corelation: https://github.com/allenai/allennlp/blob/master/allennlp/training/metrics/pearson_correlation.py\r\n5. AUC: https://github.com/allenai/allennlp/blob/master/allennlp/training/metrics/auc.py \r\n6. Entropy: https://github.com/allenai/allennlp/blob/master/allennlp/training/metrics/entropy.py",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 616511101,
    "title": "fix overflow check",
    "dateCreated": "2020-05-12T09:38:01Z",
    "dateModified": "2020-05-12T09:38:01Z",
    "description": "I did some tests and unfortunately the test\r\n```\r\npa_array.nbytes > MAX_BATCH_BYTES\r\n```\r\ndoesn't work. Indeed for a StructArray, `nbytes` can be less 2GB even if there is an overflow (it loops...).\r\n\r\nI don't think we can do a proper overflow test for the limit of 2GB...\r\n\r\nFor now I replaced it with a sanity check on the first element.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 616417845,
    "title": "JSON script",
    "dateCreated": "2020-05-12T07:11:22Z",
    "dateModified": "2020-05-12T07:11:22Z",
    "description": "Add a JSONS script to read JSON datasets from files.",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 616225010,
    "title": "[README dummy data tests] README to better understand how the dummy data structure works",
    "dateCreated": "2020-05-11T22:19:03Z",
    "dateModified": "2020-05-11T22:19:03Z",
    "description": "In this PR a README.md is added to tests to shine more light on how the dummy data structure works. I try to explain the different possible cases. IMO the best way to understand the logic is to checkout the dummy data structure of the different datasets I mention in the README.md since those are the \"edge cases\". \r\n\r\n@mariamabarham @thomwolf @lhoestq @jplu - I'd be happy to checkout the dummy data structure and get some feedback on possible improvements.",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 615942180,
    "title": "Fix arrow writer for big datasets using writer_batch_size",
    "dateCreated": "2020-05-11T14:45:36Z",
    "dateModified": "2020-05-11T14:45:36Z",
    "description": "This PR fixes Yacine's bug.\r\nAccording to [this](https://github.com/apache/arrow/blob/master/docs/source/cpp/arrays.rst#size-limitations-and-recommendations), it is not recommended to have pyarrow arrays bigger than 2Go.\r\n\r\nTherefore I set a default batch size of 100 000 examples per batch. In general it shouldn't exceed 2Go. If it does, I reduce the batch_size on the fly, and I notify the user with a warning.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 615679102,
    "title": "adding RACE, QASC, Super_glue and Tiny_shakespear datasets",
    "dateCreated": "2020-05-11T08:07:49Z",
    "dateModified": "2020-05-11T08:07:49Z",
    "description": "",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 615450534,
    "title": "fix cache dir in builder tests",
    "dateCreated": "2020-05-10T18:39:21Z",
    "dateModified": "2020-05-10T18:39:21Z",
    "description": "minor fix",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 614882655,
    "title": "[CSV] re-add csv",
    "dateCreated": "2020-05-08T17:38:29Z",
    "dateModified": "2020-05-08T17:38:29Z",
    "description": "Re-adding csv under the datasets under construction to keep circle ci happy - will have to see how to include it in the tests.\r\n\r\n@lhoestq noticed that I accidently deleted it in https://github.com/huggingface/nlp/pull/63#discussion_r422263729.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 13,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 614798483,
    "title": "[Tests] Test files locally",
    "dateCreated": "2020-05-08T15:02:43Z",
    "dateModified": "2020-05-08T15:02:43Z",
    "description": "This PR adds a `aws` and a `local` decorator to the tests so that tests now run on the local datasets. \r\n\r\nBy default, the `aws` is deactivated and `local` is activated and `slow` is deactivated, so that only 1 test per dataset runs on circle ci. \r\n\r\n**When local is activated all folders in `./datasets` are tested.**\r\n\r\n**Important** When adding a dataset, we should no longer upload it to AWS. The steps are:\r\n1. Open a PR\r\n2. Add a dataset as described in `datasets/README.md`\r\n3. If all tests pass, push to master\r\n\r\nCurrently we have 49 functional datasets in our code base. \r\n\r\nWe have 6 datasets \"under-construction\" that don't pass the tests - so I put them in a folder \"datasets_under_construction\" - it would be nice to open a PR to fix them and put them in the `datasets` folder.\r\n\r\n**Important** when running tests locally, the datasets are cached so to rerun them delete your local cache via:\r\n`rm -r ~/.cache/huggingface/datasets/*` \r\n\r\n@thomwolf @mariamabarham @lhoestq ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 614748552,
    "title": "[Datasets] ReadME",
    "dateCreated": "2020-05-08T13:37:43Z",
    "dateModified": "2020-05-08T13:37:43Z",
    "description": "",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 614746516,
    "title": "fix math dataset and xcopa",
    "dateCreated": "2020-05-08T13:33:55Z",
    "dateModified": "2020-05-08T13:33:55Z",
    "description": "- fixes math dataset and xcopa, uploaded both of the to S3",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 614737057,
    "title": "[Datasets] Make master ready for datasets adding",
    "dateCreated": "2020-05-08T13:17:00Z",
    "dateModified": "2020-05-08T13:17:00Z",
    "description": "Add all relevant files so that datasets can now be added on master",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 614666365,
    "title": "[Dataset scripts] add all datasets scripts",
    "dateCreated": "2020-05-08T10:50:15Z",
    "dateModified": "2020-05-08T10:50:15Z",
    "description": "As mentioned, we can have the canonical datasets in the master. For now I also want to include all the data as present on S3 to make the synchronization easier when uploading new datastes. \r\n\r\n@mariamabarham @lhoestq @thomwolf - what do you think? \r\n\r\nIf this is ok for you, I can sync up the master with the `add_dataset` branch: https://github.com/huggingface/nlp/pull/37 so that master is up to date. ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 614630830,
    "title": "[Cached Path] Better error message",
    "dateCreated": "2020-05-08T09:39:47Z",
    "dateModified": "2020-05-08T09:39:47Z",
    "description": "IMO returning `None` in this function only leads to confusion and is never helpful.",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 614607474,
    "title": "[Load] rename setup_module to prepare_module",
    "dateCreated": "2020-05-08T08:54:22Z",
    "dateModified": "2020-05-08T08:54:22Z",
    "description": "rename setup_module to prepare_module due to issues with pytests `setup_module` function.\r\nSee: PR #59. ",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 614372553,
    "title": "Update to simplify some datasets conversion",
    "dateCreated": "2020-05-07T22:02:24Z",
    "dateModified": "2020-05-07T22:02:24Z",
    "description": "This PR updates the encoding of `Values` like `integers`, `boolean` and `float` to use python casting and avoid having to cast in the dataset scripts, as mentioned here: https://github.com/huggingface/nlp/pull/37#discussion_r420176626\r\n\r\nWe could also change (not included in this PR yet):\r\n- `supervized_keys` to make them a NamedTuple instead of a dataclass, and\r\n- handle specifically the `Translation` features.\r\nas mentioned here: https://github.com/huggingface/nlp/pull/37#discussion_r421740236\r\n\r\n@patrickvonplaten @mariamabarham tell me if you want these two last changes as well.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 614366045,
    "title": "Fix tests",
    "dateCreated": "2020-05-07T21:48:09Z",
    "dateModified": "2020-05-07T21:48:09Z",
    "description": "@patrickvonplaten I've broken a bit the tests with #25 while simplifying and re-organizing the `load.py` and `download_manager.py` scripts.\r\n\r\nI'm trying to fix them here but I have a weird error, do you think you can have a look?\r\n```bash\r\n(datasets) MacBook-Pro-de-Thomas:datasets thomwolf$ python -m pytest -sv ./tests/test_dataset_common.py::DatasetTest::test_builder_class_snli\r\n============================================================================= test session starts =============================================================================\r\nplatform darwin -- Python 3.7.7, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 -- /Users/thomwolf/miniconda2/envs/datasets/bin/python\r\ncachedir: .pytest_cache\r\nrootdir: /Users/thomwolf/Documents/GitHub/datasets\r\nplugins: xdist-1.31.0, forked-1.1.3\r\ncollected 1 item                                                                                                                                                              \r\n\r\ntests/test_dataset_common.py::DatasetTest::test_builder_class_snli ERROR\r\n\r\n=================================================================================== ERRORS ====================================================================================\r\n____________________________________________________________ ERROR at setup of DatasetTest.test_builder_class_snli ____________________________________________________________\r\n\r\nfile_path = <module 'tests.test_dataset_common' from '/Users/thomwolf/Documents/GitHub/datasets/tests/test_dataset_common.py'>\r\ndownload_config = DownloadConfig(cache_dir=None, force_download=False, resume_download=False, local_files_only=False, proxies=None, user_agent=None, extract_compressed_file=True, force_extract=True)\r\ndownload_kwargs = {}\r\n\r\n    def setup_module(file_path: str, download_config: Optional[DownloadConfig] = None, **download_kwargs,) -> DatasetBuilder:\r\n        r\"\"\"\r\n            Download/extract/cache a dataset to add to the lib from a path or url which can be:\r\n                - a path to a local directory containing the dataset processing python script\r\n                - an url to a S3 directory with a dataset processing python script\r\n    \r\n            Dataset codes are cached inside the lib to allow easy import (avoid ugly sys.path tweaks)\r\n            and using cloudpickle (among other things).\r\n    \r\n            Return: tuple of\r\n                the unique id associated to the dataset\r\n                the local path to the dataset\r\n        \"\"\"\r\n        if download_config is None:\r\n            download_config = DownloadConfig(**download_kwargs)\r\n        download_config.extract_compressed_file = True\r\n        download_config.force_extract = True\r\n    \r\n>       name = list(filter(lambda x: x, file_path.split(\"/\")))[-1] + \".py\"\r\nE       AttributeError: module 'tests.test_dataset_common' has no attribute 'split'\r\n\r\nsrc/nlp/load.py:169: AttributeError\r\n============================================================================== warnings summary ===============================================================================\r\n/Users/thomwolf/miniconda2/envs/datasets/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py:15\r\n  /Users/thomwolf/miniconda2/envs/datasets/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n    import imp\r\n\r\n-- Docs: https://docs.pytest.org/en/latest/warnings.html\r\n=========================================================================== short test summary info ===========================================================================\r\nERROR tests/test_dataset_common.py::DatasetTest::test_builder_class_snli - AttributeError: module 'tests.test_dataset_common' has no attribute 'split'\r\n========================================================================= 1 warning, 1 error in 3.63s =========================================================================\r\n```\r\n",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 614362308,
    "title": "Aborted PR - Fix tests",
    "dateCreated": "2020-05-07T21:40:19Z",
    "dateModified": "2020-05-07T21:40:19Z",
    "description": "@patrickvonplaten I've broken a bit the tests with #25 while simplifying and re-organizing the `load.py` and `download_manager.py` scripts.\r\n\r\nI'm trying to fix them here but I have a weird error, do you think you can have a look?\r\n```bash\r\n(datasets) MacBook-Pro-de-Thomas:datasets thomwolf$ python -m pytest -sv ./tests/test_dataset_common.py::DatasetTest::test_builder_class_snli\r\n============================================================================= test session starts =============================================================================\r\nplatform darwin -- Python 3.7.7, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 -- /Users/thomwolf/miniconda2/envs/datasets/bin/python\r\ncachedir: .pytest_cache\r\nrootdir: /Users/thomwolf/Documents/GitHub/datasets\r\nplugins: xdist-1.31.0, forked-1.1.3\r\ncollected 1 item                                                                                                                                                              \r\n\r\ntests/test_dataset_common.py::DatasetTest::test_builder_class_snli ERROR\r\n\r\n=================================================================================== ERRORS ====================================================================================\r\n____________________________________________________________ ERROR at setup of DatasetTest.test_builder_class_snli ____________________________________________________________\r\n\r\nfile_path = <module 'tests.test_dataset_common' from '/Users/thomwolf/Documents/GitHub/datasets/tests/test_dataset_common.py'>\r\ndownload_config = DownloadConfig(cache_dir=None, force_download=False, resume_download=False, local_files_only=False, proxies=None, user_agent=None, extract_compressed_file=True, force_extract=True)\r\ndownload_kwargs = {}\r\n\r\n    def setup_module(file_path: str, download_config: Optional[DownloadConfig] = None, **download_kwargs,) -> DatasetBuilder:\r\n        r\"\"\"\r\n            Download/extract/cache a dataset to add to the lib from a path or url which can be:\r\n                - a path to a local directory containing the dataset processing python script\r\n                - an url to a S3 directory with a dataset processing python script\r\n    \r\n            Dataset codes are cached inside the lib to allow easy import (avoid ugly sys.path tweaks)\r\n            and using cloudpickle (among other things).\r\n    \r\n            Return: tuple of\r\n                the unique id associated to the dataset\r\n                the local path to the dataset\r\n        \"\"\"\r\n        if download_config is None:\r\n            download_config = DownloadConfig(**download_kwargs)\r\n        download_config.extract_compressed_file = True\r\n        download_config.force_extract = True\r\n    \r\n>       name = list(filter(lambda x: x, file_path.split(\"/\")))[-1] + \".py\"\r\nE       AttributeError: module 'tests.test_dataset_common' has no attribute 'split'\r\n\r\nsrc/nlp/load.py:169: AttributeError\r\n============================================================================== warnings summary ===============================================================================\r\n/Users/thomwolf/miniconda2/envs/datasets/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py:15\r\n  /Users/thomwolf/miniconda2/envs/datasets/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n    import imp\r\n\r\n-- Docs: https://docs.pytest.org/en/latest/warnings.html\r\n=========================================================================== short test summary info ===========================================================================\r\nERROR tests/test_dataset_common.py::DatasetTest::test_builder_class_snli - AttributeError: module 'tests.test_dataset_common' has no attribute 'split'\r\n========================================================================= 1 warning, 1 error in 3.63s =========================================================================\r\n```\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 614261638,
    "title": "Better cached path",
    "dateCreated": "2020-05-07T18:36:00Z",
    "dateModified": "2020-05-07T18:36:00Z",
    "description": "### Changes:\r\n- The `cached_path` no longer returns None if the file is missing/the url doesn't work. Instead, it can raise `FileNotFoundError` (missing file), `ConnectionError` (no cache and unreachable url) or `ValueError` (parsing error)\r\n- Fix requests to firebase API that doesn't handle HEAD requests...\r\n- Allow custom download in datasets script: it allows to use `tf.io.gfile.copy` for example, to download from google storage. I added an example: the `boolq` script",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 614236869,
    "title": "[Dataset] Tester add mock function",
    "dateCreated": "2020-05-07T17:51:37Z",
    "dateModified": "2020-05-07T17:51:37Z",
    "description": "need to add an empty `extract()` function to make `hansard` dataset test work.",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 613968072,
    "title": "Beam datasets",
    "dateCreated": "2020-05-07T11:04:32Z",
    "dateModified": "2020-05-07T11:04:32Z",
    "description": "# Beam datasets\r\n\r\n## Intro\r\n\r\nBeam Datasets are using beam pipelines for preprocessing (basically lots of `.map` over objects called PCollections).\r\nThe advantage of apache beam is that you can choose which type of runner you want to use to preprocess your data. The main runners are:\r\n- the `DirectRunner` to run the pipeline locally (default). However I encountered memory issues for big datasets (like the french or english wikipedia). Small dataset work fine\r\n- Google Dataflow. I didn't play with it.\r\n- Spark or Flink, two well known data processing frameworks. I tried to use the Spark/Flink local runners provided by apache beam for python and wasn't able to make them work properly though...\r\n\r\n## From tfds beam datasets to our own beam datasets\r\n\r\nTensorflow datasets used beam and a complicated pipeline to shard the TFRecords files.\r\nTo allow users to download beam datasets and not having to preprocess them, they also allow to download the already preprocessed datasets from their google storage (the beam pipeline doesn't run in that case).\r\n\r\nOn our side, we replace TFRecords by something else. Arrow or Parquet do the job but I chose Parquet as: 1) there is a builtin apache beam parquet writer that is quite convenient, and 2) reading parquet from the pyarrow library is also simple and effective (there is a mmap option !)\r\n\r\nMoreover we don't shard datasets in many many files like tfds (they were doing probably doing that mainly because of the limit of 2Gb per TFRecord file). Therefore we have a simpler pipeline that saves each split into one parquet file. We also removed the utilities to use their google storage (for now maybe ? we'll have to discuss it).\r\n\r\n## Main changes\r\n\r\n- Added a BeamWriter to save the output of beam pipelines into parquet files and fill dataset infos\r\n- Create a ParquetReader and refactor a bit the arrow_reader.py\r\n\r\n\\> **With this, we can now try to add beam datasets from tfds**\r\n\r\nI already added the wikipedia one, and I will also try to add the Wiki40b dataset\r\n\r\n## Test the wikipedia script\r\n\r\nYou can download and run the beam pipeline for wikipedia (using the `DirectRunner` by default) like this:\r\n\r\n```\r\n>>> import nlp\r\n>>> nlp.load(\"datasets/nlp/wikipedia\", dataset_config=\"20200501.frr\")\r\n```\r\n\r\nThis wikipedia dataset (lang: frr, North Frisian) is a small one (~10Mb), but feel free to try bigger ones (and fill 20Gb of swap memory if you try the english one lol)\r\n\r\n## Next\r\n\r\nShould we allow to download preprocessed datasets from the tfds google storage ?\r\nShould we try to optimize the beam pipelines to run locally without memory issues ?\r\nShould we try other data processing frameworks for big datasets, like spark ?\r\n\r\n\r\n## About this PR\r\n\r\nIt should be merged after #25 \r\n\r\n-----------------\r\n\r\nI'd be happy to have your feedback and your ideas to improve the processing of big datasets like wikipedia :)",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 613513348,
    "title": "[Tests] Improved Error message for dummy folder structure",
    "dateCreated": "2020-05-06T18:11:48Z",
    "dateModified": "2020-05-06T18:11:48Z",
    "description": "Improved Error message",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 613436158,
    "title": "[Features] Typo in generate_from_dict",
    "dateCreated": "2020-05-06T16:05:23Z",
    "dateModified": "2020-05-06T16:05:23Z",
    "description": "Change `isinstance` test in features when generating features from dict.",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 613339071,
    "title": "allow dummy folder structure to handle dict of lists",
    "dateCreated": "2020-05-06T13:54:35Z",
    "dateModified": "2020-05-06T13:54:35Z",
    "description": "`esnli.py` needs that extension of the dummy data testing.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 613266668,
    "title": "[Testing] Improved testing structure",
    "dateCreated": "2020-05-06T12:03:07Z",
    "dateModified": "2020-05-06T12:03:07Z",
    "description": "This PR refactors the test design a bit and puts the mock download manager in the `utils` files as it is just a test helper class.\r\n\r\nas @mariamabarham pointed out, creating a dummy folder structure can be quite hard to grasp.\r\nThis PR tries to change that to some extent.\r\n\r\nIt follows the following logic for the `dummy` folder structure now:\r\n1.) The data bulider has no config -> the  `dummy` folder structure is:\r\n`dummy/<version>/dummy_data.zip`\r\n2) The data builder has >= 1 configs -> the `dummy` folder structure is: \r\n`dummy/<config_name_1>/<version>/dummy_data.zip`\r\n`dummy/<config_name_2>/<version>/dummy_data.zip`\r\n\r\nNow, the difficult part is how to create the `dummy_data.zip` file. There are two cases:\r\nA) The `data_urs` parameter inserted into the `download_and_extract` fn is a **string**:\r\n-> the `dummy_data.zip` file zips the folder: \r\n`dummy_data/<relative_path_of_folder_structure_of_url>`\r\nB) The `data_urs` parameter inserted into the `download_and_extract` fn is a **dict**:\r\n-> the `dummy_data.zip` file zips the folder: \r\n`dummy_data/<relative_path_of_folder_structure_of_url_behind _key_1>`\r\n`dummy_data/<relative_path_of_folder_structure_of_url_behind _key_2>`\r\n\r\nBy relative folder structure I mean `url_path.split('./')[-1]`. As an example the dataset **xquad** by deepmind has the following url path behind the key `de`: `https://github.com/deepmind/xquad/blob/master/xquad.de.json` \r\n-> This means that the relative url path should be `xquad.de.json`.\r\n\r\n\r\n@mariamabarham B) is a change from how is was before and I think is makes more sense. \r\nWhile before the `dummy_data.zip` file for xquad with config `de` looked like:\r\n`dummy_data/de` it would now look like `dummy_data/xquad.de.json`. I think this is better and easier to understand. \r\n\r\nTherefore there are currently 6 tests that would have to have changed their dummy folder structure, but which can easily be done (30min). \r\n\r\nI also added a function: `print_dummy_data_folder_structure` that prints out the expected structures when testing which should be quite helpful.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 612583126,
    "title": "[Tests] test only for fast test as a default",
    "dateCreated": "2020-05-05T12:59:22Z",
    "dateModified": "2020-05-05T12:59:22Z",
    "description": "Test only for one config on circle ci to speed up testing. Add all config test as a slow test. \r\n@mariamabarham @thomwolf ",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 612545483,
    "title": "fix flatten nested",
    "dateCreated": "2020-05-05T11:55:13Z",
    "dateModified": "2020-05-05T11:55:13Z",
    "description": "",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 612504687,
    "title": "[Command Convert] remove tensorflow import",
    "dateCreated": "2020-05-05T10:41:00Z",
    "dateModified": "2020-05-05T10:41:00Z",
    "description": "Remove all tensorflow import statements.",
    "status": "closed",
    "estimate": 17,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 612446493,
    "title": "[PyArrow Feature] fix py arrow bool",
    "dateCreated": "2020-05-05T08:56:28Z",
    "dateModified": "2020-05-05T08:56:28Z",
    "description": "To me it seems that `bool` can only be accessed with `bool_` when looking at the pyarrow types: https://arrow.apache.org/docs/python/api/datatypes.html. ",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 612398190,
    "title": "[Features] Strip str key before dict look-up",
    "dateCreated": "2020-05-05T07:31:45Z",
    "dateModified": "2020-05-05T07:31:45Z",
    "description": "The dataset `anli.py` currently fails because it tries to look up a key `1\\n` in a dict that only has the key `1`. Added an if statement to strip key if it cannot be found in dict.",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-17.png",
      "dateCreated": "2020-02-03T12:18:37.716Z",
      "dateModified": "2020-02-03T12:18:37.716Z",
      "fullName": "Charlie Kshlerin",
      "id": 17
    }
  },
  {
    "id": 612386583,
    "title": "[Load] Separate Module kwargs and builder kwargs.",
    "dateCreated": "2020-05-05T07:09:54Z",
    "dateModified": "2020-05-05T07:09:54Z",
    "description": "Kwargs for the `load_module` fn should be passed with `module_xxxx` to `builder_kwargs` of `load` fn.\r\n\r\nThis is a follow-up PR of: https://github.com/huggingface/nlp/pull/41",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 611873486,
    "title": "[Tests] Fix tests for datasets with no config",
    "dateCreated": "2020-05-04T13:25:38Z",
    "dateModified": "2020-05-04T13:25:38Z",
    "description": "Forgot to fix `None` problem for datasets that have no config this in PR: https://github.com/huggingface/nlp/pull/42",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 14,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 611773279,
    "title": "[Checksums] If no configs exist prevent to run over empty list",
    "dateCreated": "2020-05-04T10:39:42Z",
    "dateModified": "2020-05-04T10:39:42Z",
    "description": "`movie_rationales` e.g. has no configs.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 611754343,
    "title": "[Tests] allow tests for builders without config",
    "dateCreated": "2020-05-04T10:06:22Z",
    "dateModified": "2020-05-04T10:06:22Z",
    "description": "Some dataset scripts have no configs - the tests have to be adapted for this case. \r\nIn this case the dummy data will be saved as:\r\n- natural_questions\r\n  -> dummy\r\n  -> -> 1.0.0 (version num)\r\n  -> -> -> dummy_data.zip\r\n   ",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 611739219,
    "title": "[Load module] allow kwargs into load module",
    "dateCreated": "2020-05-04T09:42:11Z",
    "dateModified": "2020-05-04T09:42:11Z",
    "description": "Currenly it is not possible to force a re-download of the dataset script. \r\n\r\nThis simple change allows to pass ``force_reload=True`` as ``builder_kwargs`` in the ``load.py`` function.",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 611721308,
    "title": "Update remote checksums instead of overwrite",
    "dateCreated": "2020-05-04T09:13:14Z",
    "dateModified": "2020-05-04T09:13:14Z",
    "description": "When the user uploads a dataset on S3, checksums are also uploaded with the `--upload_checksums` parameter.\r\n\r\nIf the user uploads the dataset in several steps, then the remote checksums file was previously overwritten. Now it's going to be updated with the new checksums.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 611712135,
    "title": "[Test] improve slow testing",
    "dateCreated": "2020-05-04T08:58:33Z",
    "dateModified": "2020-05-04T08:58:33Z",
    "description": "",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 611677656,
    "title": "[Checksums] Error for some datasets",
    "dateCreated": "2020-05-04T08:00:16Z",
    "dateModified": "2020-05-04T08:00:16Z",
    "description": "The checksums command works very nicely for `squad`. But for `crime_and_punish` and `xnli`, \r\nthe same bug happens:\r\n\r\nWhen running: \r\n```\r\npython nlp-cli nlp-cli test xnli --save_checksums\r\n```\r\n\r\nleads to:\r\n\r\n```\r\n  File \"nlp-cli\", line 33, in <module>\r\n    service.run()\r\n  File \"/home/patrick/python_bin/nlp/commands/test.py\", line 61, in run\r\n    ignore_checksums=self._ignore_checksums,\r\n  File \"/home/patrick/python_bin/nlp/builder.py\", line 383, in download_and_prepare\r\n    self._download_and_prepare(dl_manager=dl_manager, download_config=download_config)\r\n  File \"/home/patrick/python_bin/nlp/builder.py\", line 627, in _download_and_prepare\r\n    dl_manager=dl_manager, max_examples_per_split=download_config.max_examples_per_split,\r\n  File \"/home/patrick/python_bin/nlp/builder.py\", line 431, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/patrick/python_bin/nlp/datasets/xnli/8bf4185a2da1ef2a523186dd660d9adcf0946189e7fa5942ea31c63c07b68a7f/xnli.py\", line 95, in _split_generators\r\n    dl_dir = dl_manager.download_and_extract(_DATA_URL)\r\n  File \"/home/patrick/python_bin/nlp/utils/download_manager.py\", line 246, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/home/patrick/python_bin/nlp/utils/download_manager.py\", line 186, in download\r\n    self._record_sizes_checksums(url_or_urls, downloaded_path_or_paths)\r\n  File \"/home/patrick/python_bin/nlp/utils/download_manager.py\", line 166, in _record_sizes_checksums\r\n    self._recorded_sizes_checksums[url] = get_size_checksum(path)\r\n  File \"/home/patrick/python_bin/nlp/utils/checksums_utils.py\", line 81, in get_size_checksum\r\n    with open(path, \"rb\") as f:\r\nTypeError: expected str, bytes or os.PathLike object, not tuple\r\n```\r\n",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 611670295,
    "title": "[Datasets ToDo-List] add datasets",
    "dateCreated": "2020-05-04T07:47:39Z",
    "dateModified": "2020-05-04T07:47:39Z",
    "description": "## Description\r\n\r\nThis PR acts as a dashboard to see which datasets are added to the library and work. \r\n\r\nCicle-ci should always be green so that we can be sure that newly added datasets are functional. \r\nThis PR should not be merged.\r\n\r\n\r\n## Progress\r\n\r\n**For the following datasets the test commands**:\r\n```\r\nRUN_SLOW=1 pytest tests/test_dataset_common.py::DatasetTest::test_load_real_dataset_<your-dataset-name>\r\n```\r\nand \r\n```\r\nRUN_SLOW=1 pytest tests/test_dataset_common.py::DatasetTest::test_load_dataset_all_configs_<your-dataset-name>\r\n```\r\n\r\n**passes**.\r\n\r\n- [x] Squad\r\n- [x] Sentiment140\r\n- [x] XNLI\r\n- [x] Crime_and_Punish\r\n- [x] movie_rationales\r\n- [x] ai2_arc\r\n- [x] anli\r\n- [x] event2Mind\r\n- [x] Fquad\r\n- [x] blimp\r\n- [x] empathetic_dialogues\r\n- [x] cosmos_qa\r\n- [x] xquad\r\n- [x] blog_authorship_corpus\r\n- [x] SNLI\r\n- [x] break_data\r\n- [x] SQuAD v2\r\n- [x] cfq\r\n- [x] eraser_multi_rc\r\n- [x] Glue\r\n- [x] Tydiqa\r\n- [x] wiki_qa\r\n- [x] wikitext\r\n- [x] winogrande\r\n- [x] wiqa\r\n- [x] esnli\r\n- [x] civil_comments\r\n- [x] commonsense_qa\r\n- [x] com_qa\r\n- [x] coqa\r\n- [x] wiki_split\r\n- [x] cos_e\r\n- [x] xcopa\r\n- [x] quarel\r\n- [x] quartz\r\n- [x] squad_it\r\n- [x] quoref \r\n- [x] squad_pt\r\n- [x] cornell_movie_dialog\r\n- [x] SciQ\r\n- [x] Scifact\r\n- [x] hellaswag\r\n- [x] ted_multi (in translate)\r\n- [x] Aeslc (summarization)\r\n- [x] drop\r\n- [x] gap\r\n- [x] hansard\r\n- [x] opinosis\r\n- [x] MLQA\r\n- [x] math_dataset\r\n\r\n## How-To-Add a dataset\r\n\r\n**Before adding a dataset make sure that your branch is up to date**:\r\n1. `git checkout add_datasets`\r\n2. `git pull`\r\n\r\n**Add a dataset via the `convert_dataset.sh` bash script:**  \r\n\r\nRunning `bash convert_dataset.sh <file/to/tfds/datascript.py>` (*e.g.* `bash convert_dataset.sh ../tensorflow-datasets/tensorflow_datasets/text/movie_rationales.py`) will automatically run all the steps mentioned in **Add a dataset manually** below. \r\n\r\nMake sure that you run `convert_dataset.sh` from the root folder of `nlp`.\r\n\r\nThe conversion script should work almost always for step 1): \"convert dataset script from tfds to nlp format\" and 2) \"create checksum file\" and step 3) \"make style\".\r\n\r\nIt can also sometimes automatically run step 4) \"create the correct dummy data from tfds\", but this will only work if a) there is either no config name or only one config name and b) the `tfds testing/test_data/fake_example` is in the correct form.\r\n\r\nNevertheless, the script should always be run in the beginning until an error occurs to be more efficient. \r\n\r\nIf the conversion script does not work or fails at some step, then you can run the steps manually as follows:\r\n\r\n**Add a dataset manually** \r\n\r\nMake sure you run all of the following commands from the root of your `nlp` git clone.\r\nAlso make sure that you changed to this branch:\r\n```\r\ngit checkout add_datasets\r\n```\r\n\r\n1) the tfds datascript file should be converted to `nlp` style:\r\n\r\n```\r\npython nlp-cli convert --tfds_path <path/to/tensorflow_datasets/text/your_dataset_name>.py --nlp_directory datasets/nlp\r\n```\r\n\r\nThis will convert the tdfs script and create a folder with the correct name.\r\n\r\n2) the checksum file should be added. Use the command:\r\n```\r\npython nlp-cli test datasets/nlp/<your-dataset-folder> --save_checksums --all_configs\r\n```\r\n\r\nA checksums.txt file should be created in your folder and the structure should look as follows:\r\n\r\nsquad/\r\n\u251c\u2500\u2500 squad.py/\r\n\u2514\u2500\u2500 urls_checksums/\r\n...........\u2514\u2500\u2500 checksums.txt\r\n\r\nDelete the created `*.lock` file afterward - it should not be uploaded to AWS.\r\n\r\n3) run black and isort on your newly added datascript files so that they look nice:\r\n\r\n```\r\nmake style\r\n```\r\n\r\n4) the dummy data should be added. For this it might be useful to take a look into the structure of other examples as shown in the PR here and at `<path/to/tensorflow_datasets/testing/test_data/test_data/fake_examples>` whether the same  data can be used.\r\n\r\n5)  the data can be uploaded to AWS using the command\r\n```\r\naws s3 cp datasets/nlp/<your-dataset-folder> s3://datasets.huggingface.co/nlp/<your-dataset-folder> --recursive\r\n```\r\n\r\n6) check whether all works as expected using: \r\n```\r\nRUN_SLOW=1 pytest tests/test_dataset_common.py::DatasetTest::test_load_real_dataset_<your-dataset-name>\r\n```\r\nand \r\n```\r\nRUN_SLOW=1 pytest tests/test_dataset_common.py::DatasetTest::test_load_dataset_all_configs_<your-dataset-name>\r\n```\r\n\r\n7) push to this PR and rerun the circle ci workflow to check whether circle ci stays green.\r\n\r\n8) Edit this commend and tick off your newly added dataset :-) \r\n\r\n## TODO-list\r\n\r\nMaybe we can add a TODO-list here for everybody that feels like adding new datasets so that we will not add the same datasets.\r\n\r\nHere a link to available datasets: https://docs.google.com/spreadsheets/d/1zOtEqOrnVQwdgkC4nJrTY6d-Av02u0XFzeKAtBM2fUI/edit#gid=0\r\n\r\nPatrick:\r\n\r\n- [ ] boolq - *weird download link*\r\n- [ ] c4 - *beam dataset*",
    "status": "closed",
    "estimate": 20,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/females/image-9.png",
      "dateCreated": "2020-01-02T11:42:24.917Z",
      "dateModified": "2020-01-02T11:42:24.917Z",
      "fullName": "Tavares Macejkovic",
      "id": 9
    }
  },
  {
    "id": 611528349,
    "title": "Metrics - refactoring, adding support for download and distributed metrics",
    "dateCreated": "2020-05-03T23:00:17Z",
    "dateModified": "2020-05-03T23:00:17Z",
    "description": "Refactoring metrics to have a similar loading API than the datasets and improving the import system.\r\n\r\n# Import system\r\nThe import system has ben upgraded. There are now three types of imports allowed:\r\n1. `library` imports (identified as \"absolute imports\")\r\n```python\r\nimport seqeval\r\n```\r\n=> we'll test all the imports before running the scripts and if one cannot be imported we'll display an error message like this one:\r\n`ImportError: To be able to use this metric/dataset, you need to install the following dependencies ['seqeval'] using 'pip install seqeval' for instance'`\r\n\r\n2. `internal` imports (identified as \"relative imports\")\r\n```python\r\nimport .c4_utils\r\n```\r\n=> we'll assume this point to a file in the same directory/S3-directory as the main script and download this file.\r\n\r\n2. `external` imports (identified as \"relative imports\" with a comment starting with `# From:`)\r\n```python\r\nfrom .nmt_bleu import compute_bleu  # From: https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py\r\n```\r\n=> we'll assume this point to the URL of a python script (if it's a link to a github file, we'll take the raw file automatically).\r\n=> the script is downloaded and renamed to the import name (here above renamed from `bleu.py` to `nmt_bleu.py`). Renaming the file can be necessary if the distant file has the same name as the dataset/metric processing script. If you forgot to rename the distant script and it has the same name as the dataset/metric, you'll have an explicit error message asking to rename the import anyway.\r\n\r\n# Hosting metrics\r\n\r\nMetrics are hosted on a S3 bucket like the dataset processing scripts.\r\n\r\n# Metrics scripts\r\n\r\nMetrics scripts have a lot in common with datasets processing scripts. They also have a `metric.info` including citations, descriptions and links to relevant pages.\r\n\r\nMetrics have more documentation to supply to ensure they are used well.\r\n\r\nFour examples are already included for reference in [./metrics](./metrics): BLEU, ROUGE, SacreBLEU and SeqEVAL.\r\n\r\n# Automatic support for distributed/multi-processing metric computation\r\n\r\nWe've also added support for automatic distributed/multi-processing metric computation (e.g. when using DistributedDataParallel). We leverage our own dataset format for smart caching in this case. \r\n\r\nHere is a quick gist of a standard use of metrics (the simplest usage):\r\n```python\r\nimport nlp\r\nbleu_metric = nlp.load_metric('bleu')\r\n\r\n# If you only have a single iteration, you can easily compute the score like this\r\npredictions = model(inputs)\r\nscore = bleu_metric.compute(predictions, references)\r\n\r\n# If you have a loop, you can \"add\" your predictions and references at each iteration instead of having to save them yourself (the metric object store them efficiently for you)\r\nfor batch in dataloader:\r\n    model_input, targets = batch\r\n    predictions = model(model_inputs)\r\n    bleu.add(predictions, targets)\r\nscore = bleu_metric.compute()  # Compute the score from all the stored predictions/references\r\n```\r\n\r\nHere is a quick gist of a use in a distributed torch setup (should work for any python multi-process setup actually). It's pretty much identical to the second example above:\r\n```python\r\nimport nlp\r\n# You need to give the total number of parallel python processes (num_process) and the id of each process (process_id)\r\nbleu = nlp.load_metric('bleu', process_id=torch.distributed.get_rank(),b num_process=torch.distributed.get_world_size())\r\n\r\nfor batch in dataloader:\r\n    model_input, targets = batch\r\n    predictions = model(model_inputs)\r\n    bleu.add(predictions, targets)\r\nscore = bleu_metric.compute()  # Compute the score on the first node by default (can be set to compute on each node as well)\r\n```",
    "status": "closed",
    "estimate": 18,
    "priority": "Medium",
    "type": "PBI",
    "userId": 3,
    "assignee": {
      "avatar": "images/avatars/females/image-19.png",
      "dateCreated": "2019-07-20T18:29:21.492Z",
      "dateModified": "2019-07-20T18:29:21.492Z",
      "fullName": "Katelin McLaughlin",
      "id": 19
    }
  },
  {
    "id": 611413731,
    "title": "[Tests] fix typo",
    "dateCreated": "2020-05-03T13:23:49Z",
    "dateModified": "2020-05-03T13:23:49Z",
    "description": "@lhoestq - currently the slow test fail with:\r\n\r\n```\r\n_____________________________________________________________________________________ DatasetTest.test_load_real_dataset_xnli _____________________________________________________________________________________\r\n                                                                     \r\nself = <tests.test_dataset_common.DatasetTest testMethod=test_load_real_dataset_xnli>, dataset_name = 'xnli'\r\n                                   \r\n    @slow                                                                                               \r\n    def test_load_real_dataset(self, dataset_name):\r\n        with tempfile.TemporaryDirectory() as temp_data_dir:                                                                                                                                                      \r\n>           dataset = load(dataset_name, data_dir=temp_data_dir)\r\n                                                                                                                                                                                                                   \r\ntests/test_dataset_common.py:153:                                                                                                                                                                                  \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n../../python_bin/nlp/load.py:497: in load                                                                     \r\n    dbuilder.download_and_prepare(**download_and_prepare_kwargs)\r\n../../python_bin/nlp/builder.py:383: in download_and_prepare\r\n    self._download_and_prepare(dl_manager=dl_manager, download_config=download_config)\r\n../../python_bin/nlp/builder.py:627: in _download_and_prepare\r\n    dl_manager=dl_manager, max_examples_per_split=download_config.max_examples_per_split,\r\n../../python_bin/nlp/builder.py:431: in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n../../python_bin/nlp/datasets/xnli/8bf4185a2da1ef2a523186dd660d9adcf0946189e7fa5942ea31c63c07b68a7f/xnli.py:95: in _split_generators                                                                               \r\n    dl_dir = dl_manager.download_and_extract(_DATA_URL)\r\n../../python_bin/nlp/utils/download_manager.py:246: in download_and_extract\r\n    return self.extract(self.download(url_or_urls))         \r\n../../python_bin/nlp/utils/download_manager.py:186: in download                       \r\n    self._record_sizes_checksums(url_or_urls, downloaded_path_or_paths)\r\n../../python_bin/nlp/utils/download_manager.py:166: in _record_sizes_checksums           \r\n    self._recorded_sizes_checksums[url] = get_size_checksum(path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n                                                                              \r\npath = ('', '/tmp/tmpkajlg9yc/downloads/c0f7773c480a3f2d85639d777e0e17e65527460310d80760fd3fc2b2f2960556.c952a63cb17d3d46e412ceb7dbcd656ce2b15cc9ef17f50c28f81c48a7c853b5')\r\n                                                                                                                                                                                                                   \r\n    def get_size_checksum(path: str) -> Tuple[int, str]:\r\n        \"\"\"Compute the file size and the sha256 checksum of a file\"\"\"                                                                                            \r\n        m = sha256()\r\n>       with open(path, \"rb\") as f:                       \r\nE       TypeError: expected str, bytes or os.PathLike object, not tuple\r\n                                              \r\n../../python_bin/nlp/utils/checksums_utils.py:81: TypeError     \r\n```\r\n\r\n- the checksums probably need to be updated no? And we should also think about how to write a test for the checksums.",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 611385516,
    "title": "[Tests] add slow tests",
    "dateCreated": "2020-05-03T11:01:22Z",
    "dateModified": "2020-05-03T11:01:22Z",
    "description": "This PR adds a slow test that downloads the \"real\" dataset. The test is decorated as \"slow\" so that it will not automatically run on circle ci.\r\n\r\nBefore uploading a dataset, one should test that this test passes, manually by running \r\n\r\n```\r\nRUN_SLOW=1 pytest tests/test_dataset_common.py::DatasetTest::test_load_real_dataset_<your-dataset-script-name>\r\n```\r\n\r\nThis PR should be merged after PR: #33 ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/me/me.png",
      "dateCreated": "2019-06-20T03:24:18.724Z",
      "dateModified": "2019-06-20T03:24:18.724Z",
      "fullName": "Alex Ziskind",
      "id": 21
    }
  },
  {
    "id": 611052081,
    "title": "Big cleanup/refactoring for clean serialization",
    "dateCreated": "2020-05-01T23:45:57Z",
    "dateModified": "2020-05-01T23:45:57Z",
    "description": "This PR cleans many base classes to re-build them as `dataclasses`. We can thus use a simple serialization workflow for `DatasetInfo`, including it's `Features` and `SplitDict` based on `dataclasses` `asdict()`.\r\n\r\nThe resulting code is a lot shorter, can be easily serialized/deserialized, dataset info are human-readable and we can get rid of the `dataclass_json` dependency.\r\n\r\nThe scripts have breaking changes and the conversion tool is updated.\r\n\r\nExample of dataset info in SQuAD script now:\r\n```python\r\n  def _info(self):\r\n    return nlp.DatasetInfo(\r\n        description=_DESCRIPTION,\r\n        features=nlp.Features({\r\n            \"id\":\r\n                nlp.Value('string'),\r\n            \"title\":\r\n                nlp.Value('string'),\r\n            \"context\":\r\n                nlp.Value('string'),\r\n            \"question\":\r\n                nlp.Value('string'),\r\n            \"answers\":\r\n                nlp.Sequence({\r\n                    \"text\": nlp.Value('string'),\r\n                    \"answer_start\": nlp.Value('int32'),\r\n                }),\r\n        }),\r\n        # No default supervised_keys (as we have to pass both question\r\n        # and context as input).\r\n        supervised_keys=None,\r\n        homepage=\"https://rajpurkar.github.io/SQuAD-explorer/\",\r\n        citation=_CITATION,\r\n    )\r\n```\r\n\r\nExample of serialized dataset info:\r\n```bash\r\n{\r\n    \"description\": \"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\\n\",\r\n    \"citation\": \"@article{2016arXiv160605250R,\\n             author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\\n                                 Konstantin and {Liang}, Percy},\\n                title = \\\"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\\\",\\n            journal = {arXiv e-prints},\\n                 year = 2016,\\n                    eid = {arXiv:1606.05250},\\n                pages = {arXiv:1606.05250},\\narchivePrefix = {arXiv},\\n             eprint = {1606.05250},\\n}\\n\",\r\n    \"homepage\": \"https://rajpurkar.github.io/SQuAD-explorer/\",\r\n    \"license\": \"\",\r\n    \"features\": {\r\n        \"id\": {\r\n            \"dtype\": \"string\",\r\n            \"_type\": \"Value\"\r\n        },\r\n        \"title\": {\r\n            \"dtype\": \"string\",\r\n            \"_type\": \"Value\"\r\n        },\r\n        \"context\": {\r\n            \"dtype\": \"string\",\r\n            \"_type\": \"Value\"\r\n        },\r\n        \"question\": {\r\n            \"dtype\": \"string\",\r\n            \"_type\": \"Value\"\r\n        },\r\n        \"answers\": {\r\n            \"feature\": {\r\n                \"text\": {\r\n                    \"dtype\": \"string\",\r\n                    \"_type\": \"Value\"\r\n                },\r\n                \"answer_start\": {\r\n                    \"dtype\": \"int32\",\r\n                    \"_type\": \"Value\"\r\n                }\r\n            },\r\n            \"length\": -1,\r\n            \"_type\": \"Sequence\"\r\n        }\r\n    },\r\n    \"supervised_keys\": null,\r\n    \"name\": \"squad\",\r\n    \"version\": {\r\n        \"version_str\": \"1.0.0\",\r\n        \"description\": \"New split API (https://tensorflow.org/datasets/splits)\",\r\n        \"nlp_version_to_prepare\": null,\r\n        \"major\": 1,\r\n        \"minor\": 0,\r\n        \"patch\": 0\r\n    },\r\n    \"splits\": {\r\n        \"train\": {\r\n            \"name\": \"train\",\r\n            \"num_bytes\": 79426386,\r\n            \"num_examples\": 87599,\r\n            \"dataset_name\": \"squad\"\r\n        },\r\n        \"validation\": {\r\n            \"name\": \"validation\",\r\n            \"num_bytes\": 10491883,\r\n            \"num_examples\": 10570,\r\n            \"dataset_name\": \"squad\"\r\n        }\r\n    },\r\n    \"size_in_bytes\": 0,\r\n    \"download_size\": 35142551,\r\n    \"download_checksums\": []\r\n}\r\n```",
    "status": "closed",
    "estimate": 10,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 610715580,
    "title": "Fix map caching notebooks",
    "dateCreated": "2020-05-01T11:55:26Z",
    "dateModified": "2020-05-01T11:55:26Z",
    "description": "Previously, caching results with `.map()` didn't work in notebooks.\r\nTo reuse a result, `.map()` serializes the functions with `dill.dumps` and then it hashes it.\r\n\r\nThe problem is that when using `dill.dumps` to serialize a function, it also saves its origin (filename + line no.) and the origin of all the `globals` this function needs. However for notebooks and shells, the filename looks like \\<ipython-input-13-9ed2afe61d25\\> and the line no. changes often.\r\n\r\nTo fix the problem, I added a new dispatch function for code objects that ignore the origin of the code if it comes from a notebook or a python shell.\r\n\r\nI tested these cases in a notebook:\r\n- lambda functions\r\n- named functions\r\n- methods\r\n- classmethods\r\n- staticmethods\r\n- classes that implement `__call__`\r\n\r\nThe caching now works as expected for all of them :)\r\nI also tested the caching in the demo notebook and it works fine !",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 610677641,
    "title": "[Circle ci] Install a virtual env before running tests",
    "dateCreated": "2020-05-01T10:11:17Z",
    "dateModified": "2020-05-01T10:11:17Z",
    "description": "Install a virtual env before running tests to not running into sudo issues when dynamically downloading files. \r\n\r\nSame number of tests now pass / fail as on my local computer: \r\n![Screenshot from 2020-05-01 12-14-44](https://user-images.githubusercontent.com/23423619/80798814-8a0a0a80-8ba5-11ea-8db8-599d33bbfccd.png)\r\n\r\n",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 610549072,
    "title": "add metrics which require download files from github",
    "dateCreated": "2020-05-01T04:13:22Z",
    "dateModified": "2020-05-01T04:13:22Z",
    "description": "To download files from github, I copied the `load_dataset_module` and its dependencies (without the builder) in `load.py` to `metrics/metric_utils.py`. I made the following changes:\r\n\r\n- copy the needed files in a folder`metric_name` \r\n- delete all other files that are not needed\r\n\r\nFor metrics that require an external import, I first create a `<metric_name>_imports.py` file which contains all external urls. Then I create a `<metric_name>.py` in which I will load the external files using `<metric_name>_imports.py` ",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 610243997,
    "title": "Hf_api small changes",
    "dateCreated": "2020-04-30T17:06:43Z",
    "dateModified": "2020-04-30T17:06:43Z",
    "description": "From Patrick: \r\n```python \r\nfrom nlp import hf_api\r\napi = hf_api.HfApi()\r\napi.dataset_list()\r\n```\r\n\r\nworks :-) ",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 20,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 610241907,
    "title": "[Circle ci] Adds circle ci config",
    "dateCreated": "2020-04-30T17:03:35Z",
    "dateModified": "2020-04-30T17:03:35Z",
    "description": "@thomwolf can you take a look and set up circle ci on: \r\nhttps://app.circleci.com/projects/project-dashboard/github/huggingface\r\n\r\nI think for `nlp` only admins can set it up, which I guess is you :-) ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 610230476,
    "title": "[Cleanup] Removes all files in testing except test_dataset_common",
    "dateCreated": "2020-04-30T16:45:21Z",
    "dateModified": "2020-04-30T16:45:21Z",
    "description": "As far as I know, all files in `tests` were old `tfds test files` so I removed them. We can still look them up on the other library. ",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 610226047,
    "title": "[Tests] Clean tests",
    "dateCreated": "2020-04-30T16:38:29Z",
    "dateModified": "2020-04-30T16:38:29Z",
    "description": "the abseil testing library (https://abseil.io/docs/python/quickstart.html) is better than the one I had before, so I decided to switch to that and changed the `setup.py` config file. \r\nAbseil has more support and a cleaner API for parametrized testing I think. \r\n\r\nI added a list of all dataset scripts that are currently on AWS, but will replace that once the \r\nAPI is integrated into this lib. \r\n\r\nOne can now easily test for just a single function for a single dataset with:\r\n`tests/test_dataset_common.py::DatasetTest::test_load_dataset_wikipedia` \r\n\r\nNOTE: This PR is rebased on PR #29 so should be merged after.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 5,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 609708863,
    "title": "Add script csv datasets",
    "dateCreated": "2020-04-30T08:28:08Z",
    "dateModified": "2020-04-30T08:28:08Z",
    "description": "This is a PR allowing to create datasets from local CSV files. A usage might be:\r\n\r\n```python\r\nimport nlp\r\nds = nlp.load(\r\n    path=\"csv\",\r\n    name=\"bbc\",\r\n    dataset_files={\r\n        nlp.Split.TRAIN: [\"datasets/dummy_data/csv/train.csv\"],\r\n        nlp.Split.TEST: [\"\"datasets/dummy_data/csv/test.csv\"\"]\r\n    },\r\n    csv_kwargs={\r\n        \"skip_rows\": 0,\r\n        \"delimiter\": \",\",\r\n        \"quote_char\": \"\\\"\",\r\n        \"header_as_column_names\": True\r\n    }\r\n)\r\n```\r\n\r\n```\r\nDownloading and preparing dataset bbc/1.0.0 (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/jplu/.cache/huggingface/datasets/bbc/1.0.0...\r\nDataset bbc downloaded and prepared to /home/jplu/.cache/huggingface/datasets/bbc/1.0.0. Subsequent calls will reuse this data.\r\n{'test': Dataset(schema: {'category': 'string', 'text': 'string'}, num_rows: 49), 'train': Dataset(schema: {'category': 'string', 'text': 'string'}, num_rows: 99), 'validation': Dataset(schema: {'category': 'string', 'text': 'string'}, num_rows: 0)}\r\n```\r\n\r\nHow it is read:\r\n\r\n- `path`: the `csv` word means \"I want to create a CSV dataset\"\r\n- `name`: the name of this dataset is `bbc`\r\n- `dataset_files`: this is a dictionary where each key is the list of files corresponding to the key split.\r\n- `csv_kwargs`: this is the keywords arguments to \"explain\" how to read the CSV files\r\n  * `skip_rows`: number of rows have to be skipped, starting from the beginning of the file\r\n  * `delimiter`: which delimiter is used to separate the columns\r\n  * `quote_char`: which quote char is used to represent a column where the delimiter appears in one of them\r\n  * `header_as_column_names`: will use the first row (header) of the file as name for the features. Otherwise the names will be automatically generated as `f1`, `f2`, etc... Will be applied after the `skip_rows` parameter.\r\n\r\n**TODO**:  for now the `csv.py` is copied each time we create a new dataset as `ds_name.py`, this behavior will be modified to have only the `csv.py` script copied only once and not for all the CSV datasets.",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-18.png",
      "dateCreated": "2019-10-12T15:43:48.421Z",
      "dateModified": "2019-10-12T15:43:48.421Z",
      "fullName": "Destin Cole",
      "id": 18
    }
  },
  {
    "id": 609064987,
    "title": "Add checksums",
    "dateCreated": "2020-04-29T13:37:29Z",
    "dateModified": "2020-04-29T13:37:29Z",
    "description": "### Checksums files\r\n\r\nThey are stored next to the dataset script in urls_checksums/checksums.txt.\r\nThey are used to check the integrity of the datasets downloaded files.\r\nI kept the same format as tensorflow-datasets.\r\nThere is one checksums file for all configs.\r\n\r\n### Load a dataset\r\n\r\nWhen you do `load(\"squad\")`, it will also download the checksums file and put it next to the script in nlp/datasets/hash/urls_checksums/checksums.txt.\r\nIt also verifies that the downloaded files checksums match the expected ones.\r\n\r\nYou can ignore checksum tests with `load(\"squad\", ignore_checksums=True)` (under the hood it just adds `ignore_checksums=True` in the `DownloadConfig`)\r\n\r\n### Test a dataset\r\n\r\nThere is a new command `nlp-cli test squad` that runs `download_and_prepare` to see if it runs ok, and that verifies that all the checksums match. Allowed arguments are `--name`, `--all_configs`, `--ignore_checksums` and `--register_checksums`.\r\n\r\n### Register checksums\r\n\r\n1. If the dataset has external dataset files\r\n\r\nThe command `nlp-cli test squad --register_checksums --all_configs` runs `download_and_prepare` on all configs to see if it runs ok, and it creates the checksums file.\r\nYou can also register one config at a time using `--name` instead ; the checksums file will be completed and not overwritten.\r\n\r\nIf the script is a local script, the checksum file is moved to urls_checksums/checksums.txt next to the local script, to enable the user to upload both the script and the checksums file afterwards with `nlp-cli upload squad`.\r\n\r\n2. If the dataset files are all inside the directory of the dataset script\r\n\r\nThe user can directly do `nlp-cli upload squad --register_checksums`, as there is no need to download anything.\r\nIn this case however, all the dataset must be uploaded at once.\r\n\r\n--\r\n\r\nPS : it doesn't allow to register checksums for canonical datasets, the file has to be added manually on S3 for now (I guess ?)\r\n\r\nAlso I feel like we must be sure that this processes would not constrain too much any user from uploading its dataset.\r\nLet me know what you think :)",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 608508706,
    "title": "Add metrics",
    "dateCreated": "2020-04-28T18:02:05Z",
    "dateModified": "2020-04-28T18:02:05Z",
    "description": "This PR is a draft for adding metrics (sacrebleu and seqeval are added)\r\n\r\nuse case examples:\r\n`import nlp`\r\n**sacrebleu:**\r\n```\r\nrefs = [['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.'],\r\n        ['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.']]\r\nsys = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\r\nsacrebleu = nlp.load_metrics('sacrebleu')\r\nprint(sacrebleu.score)\r\n```\r\n\r\n**seqeval:**\r\n```\r\ny_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\r\ny_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\r\nseqeval = nlp.load_metrics('seqeval')\r\nprint(seqeval.accuracy_score(y_true, y_pred)\r\nprint(seqeval.f1_score(y_true, y_pred)\r\n```\r\n_examples are taken from the corresponding web page_\r\n\r\nyour comments and suggestions are more than welcomed\r\n\r\n\r\n\r\n",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 16,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  },
  {
    "id": 608298586,
    "title": "adding bleu score code",
    "dateCreated": "2020-04-28T13:00:50Z",
    "dateModified": "2020-04-28T13:00:50Z",
    "description": "this PR add the BLEU score metric to the lib. It can be tested by running the following code.\r\n\r\n` from nlp.metrics import bleu\r\n\r\nhyp1 = \"It is a guide to action which ensures that the military always obeys the commands of the party\"\r\nref1a = \"It is a guide to action that ensures that the military  forces always being under the commands of the party \"\r\n ref1b = \"It is the guiding principle which guarantees the military force always being under the command of the Party\"\r\nref1c = \"It is the practical guide for the army always to heed the directions of the party\"\r\n    \r\nlist_of_references = [[ref1a, ref1b, ref1c]]\r\nhypotheses = [hyp1]\r\nbleu = bleu.bleu_score(list_of_references, hypotheses,4, smooth=True)\r\nprint(bleu) `",
    "status": "closed",
    "estimate": 2,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 607914185,
    "title": "Cleanup Features - Updating convert command - Fix Download manager",
    "dateCreated": "2020-04-27T23:16:55Z",
    "dateModified": "2020-04-27T23:16:55Z",
    "description": "This PR makes a number of changes:\r\n\r\n# Updating `Features`\r\n\r\nFeatures are a complex mechanism provided in `tfds` to be able to modify a dataset on-the-fly when serializing to disk and when loading from disk.\r\n\r\nWe don't really need this because (1) it hides too much from the user and (2) our datatype can be directly mapped to Arrow tables on drive so we usually don't need to change the format before/after serialization.\r\n\r\nThis PR extracts and refactors these features in a single `features.py` files. It still keep a number of features classes for easy compatibility with tfds, namely the `Sequence`, `Tensor`, `ClassLabel` and `Translation` features.\r\n\r\nSome more complex features involving a pre-processing on-the-fly during serialization are kept:\r\n- `ClassLabel` which are able to convert from label strings to integers,\r\n- `Translation`which does some check on the languages.\r\n\r\n# Updating the `convert` command\r\n\r\nWe do a few updates here\r\n- following the simplification of the `features` (cf above), conversion are updated\r\n- we also makes it simpler to convert a single file\r\n- some code need to be fixed manually after conversion (e.g. to remove some encoding processing in former tfds `Text` features. We highlight this code with a \"git merge conflict\" style syntax for easy manual fixing.\r\n\r\n# Fix download manager iterator\r\n\r\nYou kept me up quite late on Tuesday night with this `os.scandir` change @lhoestq ;-)\r\n",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 607313557,
    "title": "remove boto3 and promise dependencies",
    "dateCreated": "2020-04-27T07:39:45Z",
    "dateModified": "2020-04-27T07:39:45Z",
    "description": "With the new download manager, we don't need `promise` anymore.\r\nI also removed `boto3` as in [this pr](https://github.com/huggingface/transformers/pull/3968)",
    "status": "closed",
    "estimate": 15,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 606400645,
    "title": "Replace tf.constant for TF",
    "dateCreated": "2020-04-24T15:32:06Z",
    "dateModified": "2020-04-24T15:32:06Z",
    "description": "Replace simple tf.constant type of Tensor to tf.ragged.constant which allows to have examples of different size in a tf.data.Dataset.\r\n\r\nNow the training works with TF. Here the same example than for the PT in collab:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport nlp\r\nfrom transformers import BertTokenizerFast, TFBertForQuestionAnswering\r\n\r\n# Load our training dataset and tokenizer\r\ntrain_dataset = nlp.load('squad', split=\"train[:1%]\")\r\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\r\n\r\ndef get_correct_alignement(context, answer):\r\n    start_idx = answer['answer_start'][0]\r\n    text = answer['text'][0]\r\n    end_idx = start_idx + len(text)\r\n    if context[start_idx:end_idx] == text:\r\n        return start_idx, end_idx       # When the gold label position is good\r\n    elif context[start_idx-1:end_idx-1] == text:\r\n        return start_idx-1, end_idx-1   # When the gold label is off by one character\r\n    elif context[start_idx-2:end_idx-2] == text:\r\n        return start_idx-2, end_idx-2   # When the gold label is off by two character\r\n    else:\r\n        raise ValueError()\r\n\r\n# Tokenize our training dataset\r\ndef convert_to_features(example_batch):\r\n    # Tokenize contexts and questions (as pairs of inputs)\r\n    input_pairs = list(zip(example_batch['context'], example_batch['question']))\r\n    encodings = tokenizer.batch_encode_plus(input_pairs, pad_to_max_length=True)\r\n\r\n    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methods.\r\n    start_positions, end_positions = [], []\r\n    for i, (context, answer) in enumerate(zip(example_batch['context'], example_batch['answers'])):\r\n        start_idx, end_idx = get_correct_alignement(context, answer)\r\n        start_positions.append([encodings.char_to_token(i, start_idx)])\r\n        end_positions.append([encodings.char_to_token(i, end_idx-1)])\r\n    \r\n    if start_positions and end_positions:\r\n      encodings.update({'start_positions': start_positions,\r\n                        'end_positions': end_positions})\r\n    return encodings\r\n\r\ntrain_dataset = train_dataset.map(convert_to_features, batched=True)\r\n\r\ncolumns = ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\r\ntrain_dataset.set_format(type='tensorflow', columns=columns)\r\nfeatures = {x: train_dataset[x] for x in columns[:3]} \r\nlabels = {\"output_1\": train_dataset[\"start_positions\"]}\r\nlabels[\"output_2\"] = train_dataset[\"end_positions\"]\r\ntfdataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)\r\nmodel = TFBertForQuestionAnswering.from_pretrained(\"bert-base-cased\")\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=True)\r\nopt = tf.keras.optimizers.Adam(learning_rate=3e-5)\r\nmodel.compile(optimizer=opt,\r\n              loss={'output_1': loss_fn, 'output_2': loss_fn},\r\n              loss_weights={'output_1': 1., 'output_2': 1.},\r\n              metrics=['accuracy'])\r\nmodel.fit(tfdataset, epochs=1, steps_per_epoch=3)\r\n```",
    "status": "closed",
    "estimate": 19,
    "priority": "Medium",
    "type": "PBI",
    "userId": 17,
    "assignee": {
      "avatar": "images/avatars/males/image-1.png",
      "dateCreated": "2019-11-25T03:50:45.319Z",
      "dateModified": "2019-11-25T03:50:45.319Z",
      "fullName": "Lue Bosco",
      "id": 1
    }
  },
  {
    "id": 606109196,
    "title": "Updating caching mechanism - Allow dependency in dataset processing scripts - Fix style and quality in the repo",
    "dateCreated": "2020-04-24T07:39:48Z",
    "dateModified": "2020-04-24T07:39:48Z",
    "description": "This PR has a lot of content (might be hard to review, sorry, in particular because I fixed the style in the repo at the same time).\r\n\r\n# Style & quality:\r\nYou can now install the style and quality tools with `pip install -e .[quality]`. This will install black, the compatible version of sort and flake8.\r\nYou can then clean the style and check the quality before merging your PR with:\r\n```bash\r\nmake style\r\nmake quality\r\n```\r\n\r\n# Allow dependencies in dataset processing scripts\r\nWe can now allow (some level) of imports in dataset processing scripts (in addition to PyPi imports).\r\nNamely, you can do the two following things:\r\n\r\nImport from a relative path to a file in the same folder as the dataset processing script:\r\n```python\r\nimport .c4_utils \r\n``` \r\n\r\nOr import from a relative path to a file in a folder/archive/github repo to which you provide an URL after the import state with `# From: [URL]`:\r\n```python\r\nimport .clicr.dataset_code.build_json_dataset  # From: https://github.com/clips/clicr\r\n```\r\n\r\nIn both these cases, after downloading the main dataset processing script, we will identify the location of these dependencies, download them and copy them in the dataset processing script folder.\r\n\r\nNote that only direct import in the dataset processing script will be handled.\r\nWe don't recursively explore the additional import to download further files.\r\nAlso, when we download from an additional directory (in the second case above), we recursively add `__init__.py` to all the sub-folder so you can import from them.\r\n\r\nThis part is still tested for now. If you've seen datasets which required external utilities, tell me and I can test it.\r\n\r\n# Update the cache to have a better local structure\r\n\r\nThe local structure in the `src/datasets` folder is now: `src/datasets/DATASET_NAME/DATASET_HASH/*`\r\n\r\nThe hash is computed from the full code of the dataset processing script as well as all the local and downloaded dependencies as mentioned above. This way if you change some code in a utility related to your dataset, a new hash should be computed.",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 7,
    "assignee": {
      "avatar": "images/avatars/males/image-16.png",
      "dateCreated": "2020-03-28T06:06:12.920Z",
      "dateModified": "2020-03-28T06:06:12.920Z",
      "fullName": "Joelle Spinka",
      "id": 16
    }
  },
  {
    "id": 605753027,
    "title": "Add Pandas as format type",
    "dateCreated": "2020-04-23T18:20:14Z",
    "dateModified": "2020-04-23T18:20:14Z",
    "description": "As detailed in the title ^^",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/males/image-5.png",
      "dateCreated": "2019-11-04T21:45:58.414Z",
      "dateModified": "2019-11-04T21:45:58.414Z",
      "fullName": "Eleanore Jakubowski",
      "id": 5
    }
  },
  {
    "id": 605661462,
    "title": "create our own DownloadManager",
    "dateCreated": "2020-04-23T16:08:07Z",
    "dateModified": "2020-04-23T16:08:07Z",
    "description": "I tried to create our own - and way simpler - download manager, by replacing all the complicated stuff with our own `cached_path` solution.\r\nWith this implementation, I tried `dataset = nlp.load('squad')` and it seems to work fine.\r\n\r\nFor the implementation, what I did exactly:\r\n- I copied the old download manager\r\n- I removed all the dependences to the old `download` files\r\n- I replaced all the download + extract calls by calls to `cached_path`\r\n- I removed unused parameters (extract_dir, compute_stats) (maybe compute_stats could be re-added later if we want to compute stats...)\r\n- I left some functions unimplemented for now. We will probably have to implement them because they are used by some datasets scripts (download_kaggle_data, iter_archive) or because we may need them at some point (download_checksums, _record_sizes_checksums)\r\n\r\nLet me know if you think that this is going the right direction or if you have remarks.\r\nNote: I didn't write any test yet as I wanted to read your remarks first",
    "status": "closed",
    "estimate": 12,
    "priority": "Medium",
    "type": "PBI",
    "userId": 6,
    "assignee": {
      "avatar": "images/avatars/males/image-13.png",
      "dateCreated": "2019-08-29T13:24:59.451Z",
      "dateModified": "2019-08-29T13:24:59.451Z",
      "fullName": "Ari Durgan",
      "id": 13
    }
  },
  {
    "id": 604906708,
    "title": "[Tests] General Test Design for all dataset scripts",
    "dateCreated": "2020-04-22T16:46:01Z",
    "dateModified": "2020-04-22T16:46:01Z",
    "description": "The general idea is similar to how testing is done in `transformers`. There is one general `test_dataset_common.py` file which has a `DatasetTesterMixin` class. This class implements all of the logic that can be used in a generic way for all dataset classes. The idea is to keep each individual dataset test file as minimal as possible. \r\n\r\nIn order to test whether the specific data set class can download the data and generate the examples **without** downloading the actual data all the time, a MockDataLoaderManager class is used which receives a `mock_folder_structure_fn` function from each individual dataset test file that create \"fake\" data and which returns the same folder structure that would have been created when using the real data downloader. ",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 1,
    "assignee": {
      "avatar": "images/avatars/males/image-10.png",
      "dateCreated": "2019-11-27T07:11:48.851Z",
      "dateModified": "2019-11-27T07:11:48.851Z",
      "fullName": "Werner Quitzon",
      "id": 10
    }
  },
  {
    "id": 604761315,
    "title": "[Download] Only create dir if not already exist",
    "dateCreated": "2020-04-22T13:32:51Z",
    "dateModified": "2020-04-22T13:32:51Z",
    "description": "This was quite annoying to find out :D. \r\nSome datasets have save in the same directory. So we should only create a new directory if it doesn't already exist.",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 12,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 604547951,
    "title": "[Make style]",
    "dateCreated": "2020-04-22T08:10:06Z",
    "dateModified": "2020-04-22T08:10:06Z",
    "description": "Added Makefile and applied make style to all. \r\nmake style runs the following code:\r\n\r\n```\r\nstyle:\r\n          black --line-length 119 --target-version py35 src\r\n          isort --recursive src\r\n```\r\n\r\nIt's the same code that is run in `transformers`.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 2,
    "assignee": {
      "avatar": "images/avatars/females/image-12.png",
      "dateCreated": "2019-09-26T10:21:48.204Z",
      "dateModified": "2019-09-26T10:21:48.204Z",
      "fullName": "Samantha White",
      "id": 12
    }
  },
  {
    "id": 604518583,
    "title": "[Map Function] add assert statement if map function does not return dict or None",
    "dateCreated": "2020-04-22T07:21:24Z",
    "dateModified": "2020-04-22T07:21:24Z",
    "description": "IMO, if a function is provided that is not a print statement (-> returns variable of type `None`) or a function that updates the datasets (-> returns variable of type `dict`), then a `TypeError` should be raised. \r\n\r\nNot sure whether you had cases in mind where the user should do something else @thomwolf , but I think a lot of silent errors can be avoided with this assert statement.",
    "status": "closed",
    "estimate": 8,
    "priority": "Medium",
    "type": "PBI",
    "userId": 8,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 603921624,
    "title": "[Convert TFDS to HFDS] Extend script to also allow just converting a single file",
    "dateCreated": "2020-04-21T11:25:33Z",
    "dateModified": "2020-04-21T11:25:33Z",
    "description": "Adds another argument to be able to convert only a single file",
    "status": "closed",
    "estimate": 6,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/females/image-15.png",
      "dateCreated": "2020-02-12T03:13:26.592Z",
      "dateModified": "2020-02-12T03:13:26.592Z",
      "fullName": "Deshaun Beier",
      "id": 15
    }
  },
  {
    "id": 603909327,
    "title": "Name json file \"squad.json\" instead of \"squad.py.json\"",
    "dateCreated": "2020-04-21T11:04:28Z",
    "dateModified": "2020-04-21T11:04:28Z",
    "description": "",
    "status": "closed",
    "estimate": 5,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 603894874,
    "title": "[Clean up] Datasets",
    "dateCreated": "2020-04-21T10:39:56Z",
    "dateModified": "2020-04-21T10:39:56Z",
    "description": "Clean up `nlp/datasets` folder. \r\n\r\nAs I understood, eventually the `nlp/datasets` shall not exist anymore at all. \r\n\r\nThe folder `nlp/datasets/nlp` is kept for the moment, but won't be needed in the future, since it will live on S3 (actually it already does) at: `https://s3.console.aws.amazon.com/s3/buckets/datasets.huggingface.co/nlp/?region=us-east-1` and the different `dataset downloader scripts will be added to `nlp/src/nlp` when downloaded by the user. \r\n\r\nThe folder `nlp/datasets/checksums` is kept for now, but won't be needed anymore in the future. \r\n\r\nThe remaining folders/ files are leftovers from tensorflow-datasets and are not needed. The can be looked up in the private tensorflow-dataset repo.",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 21,
    "assignee": {
      "avatar": "images/avatars/females/image-20.png",
      "dateCreated": "2020-04-06T11:15:23.361Z",
      "dateModified": "2020-04-06T11:15:23.361Z",
      "fullName": "Myrtle DuBuque",
      "id": 20
    }
  },
  {
    "id": 601783243,
    "title": "Fix issue 6: error when the citation is missing in the DatasetInfo",
    "dateCreated": "2020-04-17T08:04:26Z",
    "dateModified": "2020-04-17T08:04:26Z",
    "description": "",
    "status": "closed",
    "estimate": 16,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-11.png",
      "dateCreated": "2019-07-04T21:13:22.120Z",
      "dateModified": "2019-07-04T21:13:22.120Z",
      "fullName": "Cathryn Bashirian",
      "id": 11
    }
  },
  {
    "id": 601780534,
    "title": "Fix issue 5: allow empty datasets",
    "dateCreated": "2020-04-17T07:59:56Z",
    "dateModified": "2020-04-17T07:59:56Z",
    "description": "",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 600330836,
    "title": "Error when citation is not given in the DatasetInfo",
    "dateCreated": "2020-04-15T14:14:54Z",
    "dateModified": "2020-04-15T14:14:54Z",
    "description": "The following error is raised when the `citation` parameter is missing when we instantiate a `DatasetInfo`:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/info.py\", line 338, in __repr__\r\n    citation_pprint = _indent('\"\"\"{}\"\"\"'.format(self.citation.strip()))\r\nAttributeError: 'NoneType' object has no attribute 'strip'\r\n```\r\n\r\nI propose to do the following change in the `info.py` file. The method:\r\n```python\r\ndef __repr__(self):\r\n        splits_pprint = _indent(\"\\n\".join([\"{\"] + [\r\n                \"    '{}': {},\".format(k, split.num_examples)\r\n                for k, split in sorted(self.splits.items())\r\n        ] + [\"}\"]))\r\n        features_pprint = _indent(repr(self.features))\r\n        citation_pprint = _indent('\"\"\"{}\"\"\"'.format(self.citation.strip()))\r\n        return INFO_STR.format(\r\n                name=self.name,\r\n                version=self.version,\r\n                description=self.description,\r\n                total_num_examples=self.splits.total_num_examples,\r\n                features=features_pprint,\r\n                splits=splits_pprint,\r\n                citation=citation_pprint,\r\n                homepage=self.homepage,\r\n                supervised_keys=self.supervised_keys,\r\n                # Proto add a \\n that we strip.\r\n                license=str(self.license).strip())\r\n```\r\nBecomes:\r\n```python\r\ndef __repr__(self):\r\n        splits_pprint = _indent(\"\\n\".join([\"{\"] + [\r\n                \"    '{}': {},\".format(k, split.num_examples)\r\n                for k, split in sorted(self.splits.items())\r\n        ] + [\"}\"]))\r\n        features_pprint = _indent(repr(self.features))\r\n        ## the strip is done only is the citation is given\r\n        citation_pprint = self.citation\r\n\r\n        if self.citation:\r\n            citation_pprint = _indent('\"\"\"{}\"\"\"'.format(self.citation.strip()))\r\n        return INFO_STR.format(\r\n                name=self.name,\r\n                version=self.version,\r\n                description=self.description,\r\n                total_num_examples=self.splits.total_num_examples,\r\n                features=features_pprint,\r\n                splits=splits_pprint,\r\n                citation=citation_pprint,\r\n                homepage=self.homepage,\r\n                supervised_keys=self.supervised_keys,\r\n                # Proto add a \\n that we strip.\r\n                license=str(self.license).strip())\r\n```\r\nAnd now it is ok. @thomwolf are you ok with this fix?",
    "status": "closed",
    "estimate": 7,
    "priority": "Medium",
    "type": "PBI",
    "userId": 4,
    "assignee": {
      "avatar": "images/avatars/males/image-3.png",
      "dateCreated": "2019-11-14T00:54:46.719Z",
      "dateModified": "2019-11-14T00:54:46.719Z",
      "fullName": "Bartholome King",
      "id": 3
    }
  },
  {
    "id": 600295889,
    "title": "ValueError when a split is empty",
    "dateCreated": "2020-04-15T13:25:13Z",
    "dateModified": "2020-04-15T13:25:13Z",
    "description": "When a split is empty either TEST, VALIDATION or TRAIN I get the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/load.py\", line 295, in load\r\n    ds = dbuilder.as_dataset(**as_dataset_kwargs)\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/builder.py\", line 587, in as_dataset\r\n    datasets = utils.map_nested(build_single_dataset, split, map_tuple=True)\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/utils/py_utils.py\", line 158, in map_nested\r\n    for k, v in data_struct.items()\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/utils/py_utils.py\", line 158, in <dictcomp>\r\n    for k, v in data_struct.items()\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/utils/py_utils.py\", line 172, in map_nested\r\n    return function(data_struct)\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/builder.py\", line 601, in _build_single_dataset\r\n    split=split,\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/builder.py\", line 625, in _as_dataset\r\n    split_infos=self.info.splits.values(),\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/arrow_reader.py\", line 200, in read\r\n    return py_utils.map_nested(_read_instruction_to_ds, instructions)\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/utils/py_utils.py\", line 172, in map_nested\r\n    return function(data_struct)\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/arrow_reader.py\", line 191, in _read_instruction_to_ds\r\n    file_instructions = make_file_instructions(name, split_infos, instruction)\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/arrow_reader.py\", line 104, in make_file_instructions\r\n    absolute_instructions=absolute_instructions,\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/arrow_reader.py\", line 122, in _make_file_instructions_from_absolutes\r\n    'Split empty. This might means that dataset hasn\\'t been generated '\r\nValueError: Split empty. This might means that dataset hasn't been generated yet and info not restored from GCS, or that legacy dataset is used.\r\n``` \r\n\r\nHow to reproduce:\r\n```python\r\nimport csv\r\n\r\nimport nlp\r\n\r\n\r\nclass Bbc(nlp.GeneratorBasedBuilder):\r\n    VERSION = nlp.Version(\"1.0.0\")\r\n\r\n    def __init__(self, **config):\r\n        self.train = config.pop(\"train\", None)\r\n        self.validation = config.pop(\"validation\", None)\r\n        super(Bbc, self).__init__(**config)\r\n\r\n    def _info(self):\r\n        return nlp.DatasetInfo(builder=self, description=\"bla\", features=nlp.features.FeaturesDict({\"id\": nlp.int32, \"text\": nlp.string, \"label\": nlp.string}))\r\n\r\n    def _split_generators(self, dl_manager):\r\n        return [nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={\"filepath\": self.train}),\r\n                nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={\"filepath\": self.validation}),\r\n                nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={\"filepath\": None})]\r\n\r\n    def _generate_examples(self, filepath):\r\n        if not filepath:\r\n            return None, {}\r\n\r\n        with open(filepath) as f:\r\n            reader = csv.reader(f, delimiter=',', quotechar=\"\\\"\")\r\n            lines = list(reader)[1:]\r\n\r\n            for idx, line in enumerate(lines):\r\n                yield idx, {\"id\": idx, \"text\": line[1], \"label\": line[0]}\r\n```\r\n\r\n```python\r\nimport nlp\r\ndataset = nlp.load(\"bbc\", builder_kwargs={\"train\": \"bbc/data/train.csv\", \"validation\": \"bbc/data/test.csv\"})\r\n```",
    "status": "closed",
    "estimate": 9,
    "priority": "Medium",
    "type": "PBI",
    "userId": 18,
    "assignee": {
      "avatar": "images/avatars/males/image-2.png",
      "dateCreated": "2019-11-01T20:16:47.079Z",
      "dateModified": "2019-11-01T20:16:47.079Z",
      "fullName": "Aileen Gibson",
      "id": 2
    }
  },
  {
    "id": 600185417,
    "title": "[Feature] Keep the list of labels of a dataset as metadata",
    "dateCreated": "2020-04-15T10:17:10Z",
    "dateModified": "2020-04-15T10:17:10Z",
    "description": "It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata.",
    "status": "closed",
    "estimate": 13,
    "priority": "Medium",
    "type": "PBI",
    "userId": 9,
    "assignee": {
      "avatar": "images/avatars/females/image-14.png",
      "dateCreated": "2019-08-02T12:46:41.628Z",
      "dateModified": "2019-08-02T12:46:41.628Z",
      "fullName": "Kenny Hauck",
      "id": 14
    }
  },
  {
    "id": 600180050,
    "title": "[Feature] More dataset outputs",
    "dateCreated": "2020-04-15T10:08:14Z",
    "dateModified": "2020-04-15T10:08:14Z",
    "description": "Add the following dataset outputs:\r\n\r\n- Spark\r\n- Pandas",
    "status": "closed",
    "estimate": 4,
    "priority": "Medium",
    "type": "PBI",
    "userId": 15,
    "assignee": {
      "avatar": "images/avatars/males/image-8.png",
      "dateCreated": "2019-09-19T03:25:44.267Z",
      "dateModified": "2019-09-19T03:25:44.267Z",
      "fullName": "Reanna Collier",
      "id": 8
    }
  },
  {
    "id": 599767671,
    "title": "Issue to read a local dataset",
    "dateCreated": "2020-04-14T18:18:51Z",
    "dateModified": "2020-04-14T18:18:51Z",
    "description": "Hello,\r\n\r\nAs proposed by @thomwolf, I open an issue to explain what I'm trying to do without success. What I want to do is to create and load a local dataset, the script I have done is the following:\r\n```python\r\nimport os\r\nimport csv\r\n\r\nimport nlp\r\n\r\n\r\nclass BbcConfig(nlp.BuilderConfig):\r\n    def __init__(self, **kwargs):\r\n        super(BbcConfig, self).__init__(**kwargs)\r\n\r\n\r\nclass Bbc(nlp.GeneratorBasedBuilder):\r\n    _DIR = \"./data\"\r\n    _DEV_FILE = \"test.csv\"\r\n    _TRAINING_FILE = \"train.csv\"\r\n\r\n    BUILDER_CONFIGS = [BbcConfig(name=\"bbc\", version=nlp.Version(\"1.0.0\"))]\r\n\r\n    def _info(self):\r\n        return nlp.DatasetInfo(builder=self, features=nlp.features.FeaturesDict({\"id\": nlp.string, \"text\": nlp.string, \"label\": nlp.string}))\r\n\r\n    def _split_generators(self, dl_manager):\r\n        files = {\"train\": os.path.join(self._DIR, self._TRAINING_FILE), \"dev\": os.path.join(self._DIR, self._DEV_FILE)}\r\n\r\n        return [nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={\"filepath\": files[\"train\"]}),\r\n                nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={\"filepath\": files[\"dev\"]})]\r\n\r\n    def _generate_examples(self, filepath):\r\n        with open(filepath) as f:\r\n            reader = csv.reader(f, delimiter=',', quotechar=\"\\\"\")\r\n            lines = list(reader)[1:]\r\n\r\n            for idx, line in enumerate(lines):\r\n                yield idx, {\"idx\": idx, \"text\": line[1], \"label\": line[0]}\r\n\r\n```\r\n\r\nThe dataset is attached to this issue as well:\r\n[data.zip](https://github.com/huggingface/datasets/files/4476928/data.zip)\r\n\r\nNow the steps to reproduce what I would like to do:\r\n1. unzip data locally (I know the nlp lib can detect and extract archives but I want to reduce and facilitate the reproduction as much as possible)\r\n2. create the `bbc.py` script as above at the same location than the unziped `data` folder.\r\n\r\nNow I try to load the dataset in three different ways and none works, the first one with the name of the dataset like I would do with TFDS:\r\n```python\r\nimport nlp\r\nfrom bbc import Bbc\r\ndataset = nlp.load(\"bbc\")\r\n```\r\n\r\nI get:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/load.py\", line 280, in load\r\n    dbuilder: DatasetBuilder = builder(path, name, data_dir=data_dir, **builder_kwargs)\r\n  File \"/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/load.py\", line 166, in builder\r\n    builder_cls = load_dataset(path, name=name, **builder_kwargs)\r\n  File \"/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/load.py\", line 88, in load_dataset\r\n    local_files_only=local_files_only,\r\n  File \"/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/utils/file_utils.py\", line 214, in cached_path\r\n    if not is_zipfile(output_path) and not tarfile.is_tarfile(output_path):\r\n  File \"/opt/anaconda3/envs/transformers/lib/python3.7/zipfile.py\", line 203, in is_zipfile\r\n    with open(filename, \"rb\") as fp:\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\n```\r\n\r\nBut @thomwolf told me that no need to import the script, just put the path of it, then I tried three different way to do:\r\n```python\r\nimport nlp\r\ndataset = nlp.load(\"bbc.py\")\r\n```\r\nAnd\r\n```python\r\nimport nlp\r\ndataset = nlp.load(\"./bbc.py\")\r\n```\r\nAnd\r\n```python\r\nimport nlp\r\ndataset = nlp.load(\"/absolute/path/to/bbc.py\")\r\n```\r\n\r\nThese three ways gives me:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/load.py\", line 280, in load\r\n    dbuilder: DatasetBuilder = builder(path, name, data_dir=data_dir, **builder_kwargs)\r\n  File \"/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/load.py\", line 166, in builder\r\n    builder_cls = load_dataset(path, name=name, **builder_kwargs)\r\n  File \"/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/load.py\", line 124, in load_dataset\r\n    dataset_module = importlib.import_module(module_path)\r\n  File \"/opt/anaconda3/envs/transformers/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'nlp.datasets.2fd72627d92c328b3e9c4a3bf7ec932c48083caca09230cebe4c618da6e93688.bbc'\r\n```\r\nAny idea of what I'm missing? or I might have spot a bug :)",
    "status": "closed",
    "estimate": 11,
    "priority": "Medium",
    "type": "PBI",
    "userId": 11,
    "assignee": {
      "avatar": "images/avatars/females/image-4.png",
      "dateCreated": "2019-10-14T17:37:16.995Z",
      "dateModified": "2019-10-14T17:37:16.995Z",
      "fullName": "Monty Bednar",
      "id": 4
    }
  },
  {
    "id": 599457467,
    "title": "changing nlp.bool to nlp.bool_",
    "dateCreated": "2020-04-14T10:18:02Z",
    "dateModified": "2020-04-14T10:18:02Z",
    "description": "",
    "status": "closed",
    "estimate": 14,
    "priority": "Medium",
    "type": "PBI",
    "userId": 10,
    "assignee": {
      "avatar": "images/avatars/males/image-6.png",
      "dateCreated": "2020-03-26T13:44:46.252Z",
      "dateModified": "2020-03-26T13:44:46.252Z",
      "fullName": "Lorna Lesch",
      "id": 6
    }
  }
]